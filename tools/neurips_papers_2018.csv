Title,Authors,Abstract,PDF URL,Publication date,TF-IDF given Paper-wise Abstract,TF-IDF given Paper-wise Full Text,TF-IDF given the whole Title,TF-IDF given the whole Abstract
Synthesized Policies for Transfer and Adaptation across Tasks and Environments,"Hexiang Hu, Liyu Chen, Boqing Gong, Fei Sha","The ability to transfer in reinforcement learning is key towards building an agent of general artificial intelligence. In this paper, we consider the problem of learning to simultaneously transfer across both environments and tasks, probably more importantly, by learning from only sparse (environment, task) pairs out of all the possible combinations. We propose a novel compositional neural network architecture which depicts a meta rule for composing policies from  environment and task embeddings. Notably, one of the main challenges is to learn the embeddings jointly with the meta rule. We further propose new training methods to disentangle the embeddings, making them both distinctive signatures of the environments and tasks and effective building blocks for composing the policies. Experiments on GridWorld and THOR, of which the agent takes as input an egocentric view, show that our approach gives rise to high success rates on all the (environment, task) pairs after learning from only 40% of them.",https://proceedings.neurips.cc/paper_files/paper/2018/file/00ac8ed3b4327bdd4ebbebcb2ba10a00-Paper.pdf,2018,"learning, embeddings, environment, task, agent, building, composing, environments, meta, pairs","00, 10, 20, 30, 90, 40, latexit, 50, learning, 80","{'synthesized': 0.4352884193305339, 'tasks': 0.4352884193305339, 'across': 0.3935254598994455, 'environments': 0.3690956947131198, 'policies': 0.34466592952679415, 'adaptation': 0.3273327352820314, 'transfer': 0.3225100649357076}","embeddings, composing, environment, meta, rule, building, pairs, task, environments, transfer"
Self-Supervised Generation of Spatial Audio for 360° Video,"Pedro Morgado, Nuno Nvasconcelos, Timothy Langlois, Oliver Wang","We introduce an approach to convert mono audio recorded by a 360° video camera into spatial audio, a representation of the distribution of sound over the full viewing sphere. Spatial audio is an important component of immersive 360° video viewing, but spatial audio microphones are still rare in current 360° video production. Our system consists of end-to-end trainable neural networks that separate individual sound sources and localize them on the viewing sphere, conditioned on multi-modal analysis from the audio and 360° video frames. We introduce several datasets, including one filmed ourselves, and one collected in-the-wild from YouTube, consisting of 360° videos uploaded with spatial audio. During training, ground truth spatial audio serves as self-supervision and a mixed down mono track forms the input to our network. Using our approach we show that it is possible to infer the spatial localization of sounds based only on a synchronized 360° video and the mono audio track.",https://proceedings.neurips.cc/paper_files/paper/2018/file/01161aaa0b6d1345dd8fe4e481144d84-Paper.pdf,2018,"audio, 360, spatial, video, mono, viewing, approach, end, introduce, one","audio, spatial, video, 360, videos, mono, sound, sources, using, ambisonics","{'360': 0.4440407615399971, 'spatial': 0.4191197871794499, 'audio': 0.4014380744791615, 'self': 0.3670425853877161, 'video': 0.358835387418326, 'supervised': 0.3201994111752015, 'generation': 0.3162327003574904}","audio, 360, spatial, video, mono, viewing, sphere, track, sound, end"
On GANs and GMMs,"Eitan Richardson, Yair Weiss","A longstanding problem in machine learning is to find unsupervised methods that can learn the statistical structure of high dimensional signals. In recent years, GANs have gained much attention as a possible solution to the problem, and in particular have shown the ability to generate remarkably realistic high resolution sampled images. At the same time, many authors have pointed out that GANs may fail to model the full distribution (""mode collapse"") and that using the learned models for anything other than generating samples may be very difficult.In this paper, we examine the utility of GANs in learning statistical models of images by comparing them to perhaps the simplest statistical model, the Gaussian Mixture Model. First, we present a simple method to evaluate generative models based on relative proportions of samples that fall into predetermined bins. Unlike previous automatic methods for evaluating models, our method does not rely on an additional neural network nor does it require approximating intractable computations. Second, we compare the performance of GANs to GMMs trained on the same datasets. While GMMs have previously been shown to be successful in modeling small patches of images, we show how to train them on full sized images despite the high dimensionality. Our results show that GMMs can generate realistic samples (although less sharp than those of GANs) but also capture the full distribution, which GANs fail to do. Furthermore, GMMs allow efficient inference and explicit representation of the underlying statistical structure. Finally, we discuss how GMMs can be used to generate sharp images.",https://proceedings.neurips.cc/paper_files/paper/2018/file/0172d289da48c48de8c5ebf3de9f7ee1-Paper.pdf,2018,"gans, gmms, images, models, statistical, full, generate, high, model, samples","samples, model, images, gans, image, training, mfa, models, log, data","{'gmms': 0.7777809774829885, 'gans': 0.6285354016009019}","gmms, gans, images, statistical, full, generate, sharp, samples, fail, realistic"
Batch-Instance Normalization for Adaptively Style-Invariant Neural Networks,"Hyeonseob Nam, Hyo-Eun Kim","Real-world image recognition is often challenged by the variability of visual styles including object textures, lighting conditions, filter effects, etc. Although these variations have been deemed to be implicitly handled by more training data and deeper networks, recent advances in image style transfer suggest that it is also possible to explicitly manipulate the style information. Extending this idea to general visual recognition problems, we present Batch-Instance Normalization (BIN) to explicitly normalize unnecessary styles from images. Considering certain style features play an essential role in discriminative tasks, BIN learns to selectively normalize only disturbing styles while preserving useful styles. The proposed normalization module is easily incorporated into existing network architectures such as Residual Networks, and surprisingly improves the recognition performance in various scenarios. Furthermore, experiments verify that BIN effectively adapts to completely different tasks like object classification and style transfer, by controlling the trade-off between preserving and removing style variations. BIN can be implemented with only a few lines of code using popular deep learning frameworks.",https://proceedings.neurips.cc/paper_files/paper/2018/file/018b59ce1fd616d874afad0f44ba338d-Paper.pdf,2018,"style, bin, styles, recognition, explicitly, image, networks, normalization, normalize, object","style, bn, bin, normalization, image, domain, network, styles, 10, classiﬁcation","{'adaptively': 0.4229250620076958, 'instance': 0.40508281339782537, 'style': 0.40508281339782537, 'normalization': 0.37993557638080055, 'batch': 0.37037504058022686, 'invariant': 0.3620933277709302, 'neural': 0.2006726454660652, 'networks': 0.20010102190144058}","bin, style, styles, variations, normalize, recognition, normalization, preserving, explicitly, transfer"
Hierarchical Reinforcement Learning for Zero-shot Generalization with Subtask Dependencies,"Sungryull Sohn, Junhyuk Oh, Honglak Lee","We introduce a new RL problem where the agent is required to generalize to a previously-unseen environment characterized by a subtask graph which describes a set of subtasks and their dependencies. Unlike existing hierarchical multitask RL approaches that explicitly describe what the agent should do at a high level, our problem only describes properties of subtasks and relationships among them, which requires the agent to perform complex reasoning to find the optimal subtask to execute. To solve this problem, we propose a neural subtask graph solver (NSGS) which encodes the subtask graph using a recursive neural network embedding. To overcome the difficulty of training, we propose a novel non-parametric gradient-based policy, graph reward propagation, to pre-train our NSGS agent and further finetune it through actor-critic method. The experimental results on two 2D visual domains show that our agent can perform complex reasoning to find a near-optimal way of executing the subtask graph and generalize well to the unseen subtask graphs. In addition, we compare our agent with a Monte-Carlo tree search (MCTS) method showing that our method is much more efficient than MCTS, and the performance of NSGS can be further improved by combining it with MCTS.",https://proceedings.neurips.cc/paper_files/paper/2018/file/018dd1e07a2de4a08e6612341bf2323e-Paper.pdf,2018,"agent, subtask, graph, mcts, method, nsgs, problem, complex, describes, find","subtask, reward, agent, graph, nsgs, policy, subtasks, task, grprop, mcts","{'subtask': 0.458076653505868, 'dependencies': 0.4323679404192729, 'hierarchical': 0.3701779828905693, 'zero': 0.3701779828905693, 'shot': 0.34446926980397413, 'generalization': 0.33939412285519105, 'reinforcement': 0.2697360744609465, 'learning': 0.15018411115384636}","subtask, agent, nsgs, mcts, graph, subtasks, describes, unseen, reasoning, rl"
KDGAN: Knowledge Distillation with Generative Adversarial Networks,"Xiaojie Wang, Rui Zhang, Yu Sun, Jianzhong Qi","Knowledge distillation (KD) aims to train a lightweight classifier suitable to provide accurate inference with constrained resources in multi-label learning. Instead of directly consuming feature-label pairs, the classifier is trained by a teacher, i.e., a high-capacity model whose training may be resource-hungry. The accuracy of the classifier trained this way is usually suboptimal because it is difficult to learn the true data distribution from the teacher. An alternative method is to adversarially train the classifier against a discriminator in a two-player game akin to generative adversarial networks (GAN), which can ensure the classifier to learn the true data distribution at the equilibrium of this game. However, it may take excessively long time for such a two-player game to reach equilibrium due to high-variance gradient updates. To address these limitations, we propose a three-player game named KDGAN consisting of a classifier, a teacher, and a discriminator. The classifier and the teacher learn from each other via distillation losses and are adversarially trained against the discriminator via adversarial losses. By simultaneously optimizing the distillation and adversarial losses, the classifier will learn the true data distribution at the equilibrium. We approximate the discrete distribution learned by the classifier (or the teacher) with a concrete distribution. From the concrete distribution, we generate continuous samples to obtain low-variance gradient updates, which speed up the training. Extensive experiments using real datasets confirm the superiority of KDGAN in both accuracy and training speed.",https://proceedings.neurips.cc/paper_files/paper/2018/file/019d385eb67632a7e958e23f24bd07d7-Paper.pdf,2018,"classifier, distribution, teacher, game, learn, adversarial, data, discriminator, distillation, equilibrium","training, kdgan, classiﬁer, pϱ, teacher, distribution, nagan, labels, discriminator, data","{'kdgan': 0.5530876597508713, 'distillation': 0.5000226654385105, 'knowledge': 0.4041153867791066, 'generative': 0.3408276825014283, 'adversarial': 0.3218541874099965, 'networks': 0.2469989913639353}","classifier, teacher, game, distribution, distillation, player, discriminator, equilibrium, losses, kdgan"
Contour location via entropy reduction leveraging multiple information sources,"Alexandre Marques, Remi Lam, Karen Willcox","We introduce an algorithm to locate contours of functions that are expensive to evaluate. The problem of locating contours arises in many applications, including classification, constrained optimization, and  performance analysis of mechanical and dynamical systems (reliability, probability of failure, stability, etc.). Our algorithm locates contours using information from multiple sources, which are available in the form of relatively inexpensive, biased, and possibly noisy
 approximations to the original function. Considering multiple information sources can lead to significant cost savings. We also introduce the concept of contour entropy, a formal measure of uncertainty about the location of the zero contour of a function approximated by a statistical surrogate model. Our algorithm locates contours efficiently by maximizing the reduction of contour entropy per unit cost.",https://proceedings.neurips.cc/paper_files/paper/2018/file/01a0683665f38d8e5e567b3b15ca98bf-Paper.pdf,2018,"contours, algorithm, contour, cost, entropy, function, information, introduce, locates, multiple","contour, clover, information, entropy, surrogate, evaluations, function, zero, model, sources","{'contour': 0.39922426950214557, 'location': 0.39922426950214557, 'leveraging': 0.3768185386636829, 'sources': 0.3768185386636829, 'entropy': 0.30502102947418375, 'reduction': 0.30502102947418375, 'multiple': 0.30021283894501605, 'information': 0.27780710810655335, 'via': 0.2094644320517334}","contours, contour, locates, sources, entropy, inexpensive, locating, cost, locate, mechanical"
Contextual bandits with surrogate losses: Margin bounds and efficient algorithms,"Dylan J. Foster, Akshay Krishnamurthy","We use surrogate losses to obtain several new regret bounds and new algorithms for contextual bandit learning. Using the ramp loss, we derive a new margin-based regret bound in terms of standard sequential complexity measures of a benchmark class of real-valued regression functions. Using the hinge loss, we derive an efficient algorithm with a $\sqrt{dT}$-type mistake bound against benchmark policies induced by $d$-dimensional regressors. Under realizability assumptions, our results also yield classical regret bounds.",https://proceedings.neurips.cc/paper_files/paper/2018/file/01e9565cecc4e989123f9620c1d09c09-Paper.pdf,2018,"new, regret, benchmark, bound, bounds, derive, loss, using, algorithm, algorithms","loss, regret, hinge, bound, algorithm, contextual, class, bandit, margin, information","{'surrogate': 0.45281277553150634, 'losses': 0.39538253207019797, 'margin': 0.3839551858899033, 'contextual': 0.35854189831240135, 'bandits': 0.3459644853854263, 'bounds': 0.2997340855709903, 'algorithms': 0.2945079865447885, 'efficient': 0.2576673527869918}","regret, benchmark, derive, dt, hinge, mistake, ramp, realizability, new, bounds"
Adaptive Sampling Towards Fast Graph Representation Learning,"Wenbing Huang, Tong Zhang, Yu Rong, Junzhou Huang","Graph Convolutional Networks (GCNs) have become a crucial tool on learning representations of graph vertices. The main challenge of adapting GCNs on large-scale graphs is the scalability issue that it incurs heavy cost both in computation and memory due to the uncontrollable neighborhood expansion across layers. In this paper, we accelerate the training of GCNs through developing an adaptive layer-wise sampling method. By constructing the network layer by layer in a top-down passway, we sample the lower layer conditioned on the top one, where the sampled neighborhoods are shared by different parent nodes and the over expansion is avoided owing to the fixed-size sampling. More importantly, the proposed sampler is adaptive and applicable for explicit variance reduction, which in turn enhances the training of our method. Furthermore, we propose a novel and economical approach to promote the message passing over distant nodes by applying skip connections.
Intensive experiments on several benchmarks verify the effectiveness of our method regarding the classification accuracy while enjoying faster convergence speed.",https://proceedings.neurips.cc/paper_files/paper/2018/file/01eee509ee2f68dc6014898c309e86bf-Paper.pdf,2018,"layer, gcns, method, adaptive, expansion, graph, nodes, sampling, top, training","layer, sampling, vi, uj, wise, nodes, node, eq, method, skip","{'towards': 0.4346197084931717, 'representation': 0.4081968746958501, 'adaptive': 0.40380685975035313, 'sampling': 0.3882940346948355, 'fast': 0.3848383856160873, 'graph': 0.38152382344998176, 'learning': 0.19232205337226999}","gcns, layer, expansion, top, nodes, adaptive, graph, distant, economical, passway"
Analytic solution and stationary phase approximation for the Bayesian lasso and elastic net,Tom Michoel,"The lasso and elastic net linear regression models impose a double-exponential prior distribution on the model parameters to achieve   regression shrinkage and variable selection,  allowing the inference of robust models from large data sets.  However, there has been limited success in deriving estimates for the full posterior distribution of regression coefficients in these models, due to a need to evaluate analytically intractable partition function integrals. Here, the Fourier transform is used to express these integrals as complex-valued oscillatory integrals over ""regression frequencies"". This results in an analytic expansion and stationary phase approximation for the partition functions of the Bayesian lasso and elastic net, where the non-differentiability of the double-exponential prior has so far eluded such an approach. Use of this approximation leads to highly accurate numerical estimates for the expectation values and marginal posterior distributions of the regression coefficients, and allows for Bayesian inference of much higher dimensional models than previously possible.",https://proceedings.neurips.cc/paper_files/paper/2018/file/0245952ecff55018e2a459517fdb40e3-Paper.pdf,2018,"regression, models, integrals, approximation, bayesian, coefficients, distribution, double, elastic, estimates","regression, approximation, function, likelihood, maximum, elastic, net, posterior, bayesian, coefﬁcients","{'analytic': 0.385739390738824, 'elastic': 0.385739390738824, 'lasso': 0.36409047401974615, 'solution': 0.348730323162033, 'phase': 0.336816064543479, 'stationary': 0.336816064543479, 'net': 0.31172125558524205, 'approximation': 0.274712188008451, 'bayesian': 0.22069998461059778}","integrals, regression, elastic, lasso, double, partition, net, coefficients, models, exponential"
Identification and Estimation of Causal Effects from Dependent Data,"Eli Sherman, Ilya Shpitser","The assumption that data samples are independent and identically distributed (iid) is standard in many areas of statistics and machine learning. Nevertheless, in some settings, such as social networks, infectious disease modeling, and reasoning with spatial and temporal data, this assumption is false. An extensive literature exists on making causal inferences under the iid assumption [12, 8, 21, 16], but, as pointed out in [14], causal inference in non-iid contexts is challenging due to the combination of unobserved confounding bias and data dependence. In this paper we develop a general theory describing when causal inferences are possible in such scenarios. We use segregated graphs [15], a generalization of latent projection mixed graphs [23], to represent causal models of this type and provide a complete algorithm for non-parametric identification in these models. We then demonstrate how statistical inferences may be performed on causal parameters identified by this algorithm, even in cases where parts of the model exhibit full interference, meaning only a single sample is available for parts of the model [19]. We apply these techniques to a synthetic data set which considers the adoption of fake news articles given the social network structure, articles read by each person, and baseline demographics and socioeconomic covariates.",https://proceedings.neurips.cc/paper_files/paper/2018/file/024677efb8e4aee2eaeef17b54695bbe-Paper.pdf,2018,"causal, data, assumption, iid, inferences, algorithm, articles, graphs, model, models","causal, set, pag, models, variables, factorization, data, graph, network, cg","{'identification': 0.4902336088462939, 'effects': 0.46955176539056037, 'dependent': 0.44040234422160435, 'causal': 0.36553085215400843, 'estimation': 0.35384715205472694, 'data': 0.2939682171367947}","causal, iid, inferences, articles, assumption, parts, social, graphs, data, demographics"
Streamlining Variational Inference for Constraint Satisfaction Problems,"Aditya Grover, Tudor Achim, Stefano Ermon","Several algorithms for solving constraint satisfaction problems are based on survey propagation, a variational inference scheme used to obtain approximate marginal probability estimates for variable assignments. These marginals correspond to how frequently each variable is set to true among satisfying assignments, and are used to inform branching decisions during search; however, marginal estimates obtained via survey propagation are approximate and can be self-contradictory. We introduce a more general branching strategy based on streamlining constraints, which sidestep hard assignments to variables. We show that streamlined solvers consistently outperform decimation-based solvers on random k-SAT instances for several problem sizes, shrinking the gap between empirical performance and theoretical limits of satisfiability by 16.3% on average for k = 3, 4, 5, 6.",https://proceedings.neurips.cc/paper_files/paper/2018/file/02ed812220b0705fabb868ddbf17ea20-Paper.pdf,2018,"assignments, based, approximate, branching, estimates, marginal, propagation, several, solvers, survey","survey, variable, variables, streamlining, propagation, constraints, decimation, µi, algorithm, constraint","{'satisfaction': 0.4885519733636105, 'streamlining': 0.4885519733636105, 'constraint': 0.42658892750908256, 'problems': 0.40383540190712414, 'inference': 0.2989781385681982, 'variational': 0.2969593682585239}","assignments, branching, survey, solvers, marginal, propagation, estimates, variable, contradictory, decimation"
A Spectral View of Adversarially Robust Features,"Shivam Garg, Vatsal Sharan, Brian Zhang, Gregory Valiant","Given the apparent difficulty of learning models that are robust to adversarial perturbations, we propose tackling the simpler problem of developing adversarially robust features.  Specifically, given a dataset and metric of interest, the goal is to return a function (or multiple functions) that 1) is robust to adversarial perturbations, and 2) has significant variation across the datapoints.  We establish strong connections between adversarially robust features and a natural spectral property of the geometry of the dataset and metric of interest.  This connection can be leveraged to provide both robust features, and a lower bound on the robustness of any function that has significant variance across the dataset.  Finally, we provide empirical evidence that the adversarially robust features given by this spectral approach can be fruitfully leveraged to learn a robust (and accurate) model.",https://proceedings.neurips.cc/paper_files/paper/2018/file/033cc385728c51d97360020ed57776f0-Paper.pdf,2018,"robust, features, adversarially, dataset, given, across, adversarial, function, interest, leveraged","robust, adversarial, features, dataset, points, robustness, graph, spectral, fx, models","{'adversarially': 0.5017368510902057, 'view': 0.4872356523174668, 'spectral': 0.44660642536977, 'features': 0.4198446377317542, 'robust': 0.36760676320653507}","robust, adversarially, features, leveraged, spectral, dataset, perturbations, given, metric, interest"
Dimensionality Reduction has Quantifiable Imperfections: Two Geometric Bounds,"Kry Lui, Gavin Weiguang Ding, Ruitong Huang, Robert McCann","In this paper, we investigate Dimensionality reduction (DR) maps in an information retrieval setting from a quantitative topology point of view. In particular, we show that no DR maps can achieve perfect precision and perfect recall simultaneously. Thus a continuous DR map must have imperfect precision. We further prove an upper bound on the precision of Lipschitz continuous DR maps. While precision is a natural measure in an information retrieval setting, it does not measure `how' wrong the retrieved data is. We therefore propose a new measure based on Wasserstein distance that comes with similar theoretical guarantee. A key technical step in our proofs is a particular optimization problem of the $L_2$-Wasserstein distance over a constrained set of distributions. We provide a complete solution to this optimization problem, which can be of independent interest on the technical side.",https://proceedings.neurips.cc/paper_files/paper/2018/file/037a595e6f4f0576a9efe43154d71c18-Paper.pdf,2018,"dr, precision, maps, measure, continuous, distance, information, optimization, particular, perfect","precision, recall, dr, ru, bound, continuous, measure, rv, map, perfect","{'imperfections': 0.4317698724370161, 'quantifiable': 0.4317698724370161, 'geometric': 0.39034449361833257, 'dimensionality': 0.37700850033524647, 'two': 0.37700850033524647, 'reduction': 0.32988698595631805, 'bounds': 0.28580498361625284}","dr, precision, maps, measure, technical, perfect, retrieval, wasserstein, distance, continuous"
Learning SMaLL Predictors,"Vikas Garg, Ofer Dekel, Lin Xiao","We introduce a new framework for learning in severely resource-constrained settings. Our technique delicately amalgamates the representational richness of multiple linear predictors with the sparsity of Boolean relaxations, and thereby yields classifiers that are compact, interpretable, and accurate. We provide a rigorous formalism of the learning problem, and establish fast convergence of the ensuing algorithm via relaxation to a minimax saddle point objective. We supplement the theoretical foundations of our work with an extensive empirical evaluation.",https://proceedings.neurips.cc/paper_files/paper/2018/file/03b2ceb73723f8b53cd533e4fba898ee-Paper.pdf,2018,"learning, accurate, algorithm, amalgamates, boolean, classifiers, compact, constrained, convergence, delicately","xi, 04, small, 03, 05, si, algorithm, set, problem, i2œm","{'predictors': 0.7004735082324355, 'small': 0.670922120525164, 'learning': 0.24331126660013458}","amalgamates, delicately, ensuing, richness, supplement, formalism, foundations, boolean, predictors, relaxations"
ResNet with one-neuron hidden layers is a Universal Approximator,"Hongzhou Lin, Stefanie Jegelka","We demonstrate that a very deep ResNet with stacked modules that have one neuron per hidden layer and ReLU activation functions can uniformly approximate any Lebesgue integrable function in d dimensions, i.e. \ell_1(R^d). Due to the identity mapping inherent to ResNets, our network has alternating layers of dimension one and d. This stands in sharp contrast to fully connected networks, which are not universal approximators if their width is the input dimension d [21,11]. Hence, our result implies an increase in representational power for narrow deep networks by the ResNet architecture.",https://proceedings.neurips.cc/paper_files/paper/2018/file/03bfc1d4783966c69cc6aef8247e0103-Paper.pdf,2018,"deep, dimension, networks, one, resnet, 11, 21, activation, alternating, approximate","function, resnet, one, approximation, networks, hidden, universal, connected, fully, layer","{'approximator': 0.4130948628572401, 'resnet': 0.4130948628572401, 'universal': 0.3899106703744702, 'neuron': 0.3734612240270793, 'hidden': 0.3607020421850319, 'layers': 0.3607020421850319, 'one': 0.3270928390615394}","resnet, dimension, integrable, lebesgue, stands, 21, 11, approximators, ell_1, stacked"
How Many Samples are Needed to Estimate a Convolutional Neural Network?,"Simon S. Du, Yining Wang, Xiyu Zhai, Sivaraman Balakrishnan, Russ R. Salakhutdinov, Aarti Singh","A widespread folklore for explaining the success of Convolutional Neural Networks (CNNs) is that CNNs use a more compact representation than the Fully-connected Neural Network (FNN) and thus require fewer training samples to accurately estimate their parameters. We initiate the study of rigorously characterizing the sample complexity of estimating CNNs. We show that for an $m$-dimensional convolutional filter with linear activation acting on a $d$-dimensional input, the sample complexity of achieving population prediction error of $\epsilon$ is $\widetilde{O(m/\epsilon^2)$, whereas the sample-complexity for its FNN counterpart is lower bounded by $\Omega(d/\epsilon^2)$ samples. Since, in typical settings $m \ll d$, this result demonstrates the advantage of using a CNN. We further consider the sample complexity of estimating a one-hidden-layer CNN with linear activation where both the $m$-dimensional convolutional filter and the $r$-dimensional output weights are unknown. For this model, we show that the sample complexity is $\widetilde{O}\left((m+r)/\epsilon^2\right)$ when the ratio between the stride size and the filter size is a constant. For both models, we also present lower bounds showing our sample complexities are tight up to logarithmic factors. Our main tools for deriving these results are a localized empirical process analysis and a new lemma characterizing the convolutional structure. We believe that these tools may inspire further developments in understanding CNNs.",https://proceedings.neurips.cc/paper_files/paper/2018/file/03c6b06952c750899bb03d998e631860-Paper.pdf,2018,"sample, complexity, cnns, convolutional, dimensional, epsilon, filter, activation, characterizing, cnn","error, size, ﬁlter, convolutional, wn, 10, cnn, linear, sample, al","{'estimate': 0.47263529680127964, 'many': 0.4461095065278267, 'needed': 0.4461095065278267, 'samples': 0.3906787311187111, 'convolutional': 0.31576343327465045, 'network': 0.28538825933788825, 'neural': 0.21167337404296668}","sample, cnns, epsilon, filter, complexity, convolutional, fnn, dimensional, characterizing, widetilde"
Objective and efficient inference for couplings in neuronal networks,"Yu Terada, Tomoyuki Obuchi, Takuya Isomura, Yoshiyuki Kabashima","Inferring directional couplings from the spike data of networks is desired in various scientific fields such as neuroscience. Here, we apply a recently proposed objective procedure to the spike data obtained from the Hodgkin-Huxley type models and in vitro neuronal networks cultured in a circular structure. As a result, we succeed in reconstructing synaptic connections accurately from the evoked activity as well as the spontaneous one. To obtain the results, we invent an analytic formula approximately implementing a method of screening relevant couplings. This significantly reduces the computational cost of the screening method employed in the proposed objective procedure, making it possible to treat large-size systems as in this study.",https://proceedings.neurips.cc/paper_files/paper/2018/file/03cf87174debaccd689c90c34577b82f-Paper.pdf,2018,"couplings, data, method, networks, objective, procedure, proposed, screening, spike, accurately","10, ms, time, couplings, 100, data, δτ, fig, spike, neurons","{'couplings': 0.5142852064917833, 'neuronal': 0.4925886807123818, 'objective': 0.47575954787023217, 'inference': 0.33343974692184464, 'efficient': 0.3100483539851833, 'networks': 0.2433267843699616}","couplings, screening, spike, procedure, cultured, hodgkin, huxley, invent, spontaneous, vitro"
Unsupervised Adversarial Invariance,"Ayush Jaiswal, Rex Yue Wu, Wael Abd-Almageed, Prem Natarajan","Data representations that contain all the information about target variables but are invariant to nuisance factors benefit supervised learning algorithms by preventing them from learning associations between these factors and the targets, thus reducing overfitting. We present a novel unsupervised invariance induction framework for neural networks that learns a split representation of data through competitive training between the prediction task and a reconstruction task coupled with disentanglement, without needing any labeled information about nuisance factors or domain knowledge. We describe an adversarial instantiation of this framework and provide analysis of its working. Our unsupervised model outperforms state-of-the-art methods, which are supervised, at inducing invariance to inherent nuisance factors, effectively using synthetic data augmentation to learn invariance, and domain adaptation. Our method can be applied to any prediction task, eg., binary/multi-class classification or regression, without loss of generality.",https://proceedings.neurips.cc/paper_files/paper/2018/file/03e7ef47cee6fa4ae7567394b99912b7-Paper.pdf,2018,"factors, data, invariance, nuisance, task, domain, framework, information, learning, prediction","e1, model, e2, information, data, invariance, factors, nuisance, learning, framework","{'invariance': 0.7210065884097391, 'unsupervised': 0.5315575925807978, 'adversarial': 0.4445177445720877}","nuisance, factors, invariance, task, unsupervised, supervised, domain, instantiation, prediction, inducing"
Critical initialisation for deep signal propagation in noisy rectifier neural networks,"Arnu Pretorius, Elan van Biljon, Steve Kroon, Herman Kamper","Stochastic regularisation is an important weapon in the arsenal of a deep learning practitioner. However, despite recent theoretical advances, our understanding of how noise influences signal propagation in deep neural networks remains limited. By extending recent work based on mean field theory, we develop a new framework for signal propagation in stochastic regularised neural networks. Our \textit{noisy signal propagation} theory can incorporate several common noise distributions, including additive and multiplicative Gaussian noise as well as dropout. We use this framework to investigate initialisation strategies for noisy ReLU networks. We show that no critical initialisation strategy exists using additive noise, with signal propagation exploding regardless of the selected noise distribution. For multiplicative noise (e.g.\ dropout), we identify alternative critical initialisation strategies that depend on the second moment of the noise distribution.  Simulations and experiments on real-world data confirm that our proposed initialisation is able to stably propagate signals in deep networks, while using an initialisation disregarding noise fails to do so. Furthermore, we analyse correlation dynamics between inputs. Stronger noise regularisation is shown to reduce the depth to which discriminatory information about the inputs to a noisy ReLU network is able to propagate, even when initialised at criticality. We support our theoretical predictions for these trainable depths with simulations, as well as with experiments on MNIST and CIFAR-10.",https://proceedings.neurips.cc/paper_files/paper/2018/file/045cf83ab0722e782cf72d14e44adf98-Paper.pdf,2018,"noise, initialisation, networks, propagation, signal, deep, noisy, able, additive, critical","noise, σ2, networks, network, initialisation, 10, relu, µ2, signal, critical","{'critical': 0.40942669518039804, 'initialisation': 0.40942669518039804, 'rectifier': 0.40942669518039804, 'signal': 0.40942669518039804, 'propagation': 0.3471666687667043, 'noisy': 0.3241883454755548, 'deep': 0.19183910321382736, 'neural': 0.1833649128167726, 'networks': 0.18284259097839764}","initialisation, noise, propagation, signal, regularisation, noisy, propagate, dropout, multiplicative, additive"
Learning sparse neural networks via sensitivity-driven regularization,"Enzo Tartaglione, Skjalg Lepsøy, Attilio Fiandrotti, Gianluca Francini","The ever-increasing number of parameters in deep neural networks poses challenges for memory-limited applications. Regularize-and-prune methods aim at meeting these challenges by sparsifying the network weights. In this context we quantify the output sensitivity to the parameters (i.e. their relevance to the network output) and introduce a regularization term that gradually lowers the absolute value of parameters with low sensitivity.  Thus, a very large fraction of the parameters approach zero and are eventually set to zero by simple thresholding. Our method surpasses most of the recent techniques both in terms of sparsity and error rates. In some cases, the method reaches twice the sparsity obtained by other techniques at equal error rates.",https://proceedings.neurips.cc/paper_files/paper/2018/file/04df4d434d481c5bb723be1b6df1ee65-Paper.pdf,2018,"parameters, challenges, error, method, network, output, rates, sensitivity, sparsity, techniques","network, sensitivity, wn, regularization, parameters, output, method, parameter, term, error","{'sensitivity': 0.5372959645042737, 'driven': 0.4555913240013682, 'regularization': 0.3826462984098456, 'sparse': 0.3624223789312582, 'via': 0.2819076961150809, 'neural': 0.24063215429741422, 'networks': 0.23994670457165154, 'learning': 0.1761567987323529}","parameters, sensitivity, challenges, sparsity, zero, rates, eventually, lowers, output, techniques"
Cooperative neural networks (CoNN): Exploiting prior independence structure for improved classification,"Harsh Shrivastava, Eugene Bart, Bob Price, Hanjun Dai, Bo Dai, Srinivas Aluru","We propose a new approach, called cooperative neural networks (CoNN), which use a set of cooperatively trained neural networks to capture latent representations that exploit prior given independence structure. The model is more flexible than traditional graphical models based on exponential family distributions, but incorporates more domain specific prior structure than traditional deep networks or variational autoencoders. The framework is very general and can be used to exploit the independence structure of any graphical model. We illustrate the technique by showing that we can transfer the independence structure of the popular Latent Dirichlet Allocation (LDA) model to a cooperative neural network, CoNN-sLDA. Empirical evaluation of CoNN-sLDA on supervised text classification tasks demonstrate that the theoretical advantages of prior independence structure can be realized in practice - we demonstrate a 23 percent reduction in error on the challenging MultiSent data set compared to state-of-the-art.",https://proceedings.neurips.cc/paper_files/paper/2018/file/051928341be67dcba03f0e04104d9047-Paper.pdf,2018,"structure, independence, conn, model, networks, neural, prior, cooperative, demonstrate, exploit","slda, model, networks, conn, neural, distributions, structure, embeddings, lda, space","{'conn': 0.4056942231669549, 'independence': 0.4056942231669549, 'cooperative': 0.3667706253151984, 'exploiting': 0.3353454670053919, 'prior': 0.3212329350423421, 'improved': 0.29642186915363544, 'classification': 0.2889234296116853, 'structure': 0.2889234296116853, 'neural': 0.18169329635064435, 'networks': 0.18117573617453198}","independence, conn, structure, slda, cooperative, prior, graphical, traditional, exploit, cooperatively"
Data center cooling using model-predictive control,"Nevena Lazic, Craig Boutilier, Tyler Lu, Eehern Wong, Binz Roy, MK Ryu, Greg Imwalle","Despite impressive recent advances in reinforcement learning (RL), its deployment in real-world physical systems is often complicated by unexpected events, limited data, and the potential for expensive failures. In this paper, we describe an application of RL “in the wild” to the task of regulating temperatures and airflow inside a large-scale data center (DC). Adopting a data-driven, model-based approach, we demonstrate that an RL agent with little prior knowledge is able to effectively and safely regulate conditions on a server floor after just a few hours of exploration, while improving operational efficiency relative to existing PID controllers.",https://proceedings.neurips.cc/paper_files/paper/2018/file/059fdcd96baeb75112f09fa1dcc740cc-Paper.pdf,2018,"data, rl, able, adopting, advances, agent, airflow, application, approach, based","model, control, data, state, time, controllers, cooling, ahus, controls, cost","{'center': 0.4819324939203084, 'cooling': 0.4819324939203084, 'predictive': 0.3983637626811123, 'control': 0.3682129486164499, 'model': 0.3012851367351185, 'using': 0.27573666715198103, 'data': 0.27277139958516256}","rl, dc, floor, operational, pid, regulate, regulating, airflow, unexpected, controllers"
Generalization Bounds for Uniformly Stable Algorithms,"Vitaly Feldman, Jan Vondrak","Uniform stability of a learning algorithm is a classical notion of algorithmic stability introduced to derive high-probability bounds on the generalization error (Bousquet and Elisseeff, 2002).  Specifically, for a loss function with range bounded in $[0,1]$, the generalization error of $\gamma$-uniformly stable learning algorithm on $n$ samples is known to be at most $O((\gamma +1/n) \sqrt{n \log(1/\delta)})$ with probability at least $1-\delta$. Unfortunately, this bound does not lead to meaningful generalization bounds in many common settings where $\gamma \geq 1/\sqrt{n}$. At the same time the bound is known to be tight only when $\gamma = O(1/n)$.
  Here we prove substantially stronger generalization bounds for uniformly stable algorithms without any additional assumptions. First, we show that the generalization error in this setting is at most $O(\sqrt{(\gamma + 1/n) \log(1/\delta)})$ with probability at least $1-\delta$. In addition, we prove a tight bound of $O(\gamma^2 + 1/n)$ on the second moment of the generalization error. The best previous bound on the second moment of the generalization error is $O(\gamma + 1/n)$. Our proofs are based on new analysis techniques and our results imply substantially stronger generalization guarantees for several well-studied algorithms.",https://proceedings.neurips.cc/paper_files/paper/2018/file/05a624166c8eb8273b8464e8d9cb5bd9-Paper.pdf,2018,"generalization, gamma, error, bound, delta, bounds, probability, sqrt, algorithm, algorithms","si, stability, error, sj, generalization, bounds, sℓ, bound, algorithm, algorithms","{'uniformly': 0.5431380536570266, 'stable': 0.4879291581608224, 'generalization': 0.42634489303588835, 'bounds': 0.38090122342218646, 'algorithms': 0.3742599116440694}","gamma, generalization, delta, error, bound, moment, sqrt, probability, uniformly, stronger"
COLA: Decentralized Linear Learning,"Lie He, An Bian, Martin Jaggi","Decentralized machine learning is a promising emerging paradigm in view of global challenges of data ownership and privacy. We consider learning of linear classification and regression models, in the setting where the training data is decentralized over many user devices, and the learning algorithm must run on-device, on an arbitrary communication network, without a central coordinator.
We propose COLA, a new decentralized training algorithm with strong theoretical guarantees and superior practical performance. Our framework overcomes many limitations of existing methods, and achieves communication efficiency, scalability, elasticity as well as resilience to changes in data and allows for unreliable and heterogeneous participating devices.",https://proceedings.neurips.cc/paper_files/paper/2018/file/05a70454516ecd9194c293b0e415777f-Paper.pdf,2018,"data, decentralized, learning, algorithm, communication, devices, many, training, achieves, allows","al, et, decentralized, cola, local, vk, data, network, convex, learning","{'cola': 0.6545486511488011, 'decentralized': 0.5917491656820313, 'linear': 0.4187436480273311, 'learning': 0.21459903408605496}","decentralized, devices, communication, cola, coordinator, elasticity, ownership, participating, resilience, unreliable"
Leveraging the Exact Likelihood of Deep Latent Variable Models,"Pierre-Alexandre Mattei, Jes Frellsen","Deep latent variable models (DLVMs) combine the approximation abilities of deep neural networks and the statistical foundations of generative models. Variational methods are commonly used for inference; however, the exact likelihood of these models has been largely overlooked. The purpose of this work is to study the general properties of this quantity and to show how they can be leveraged in practice. We focus on important inferential problems that rely on the likelihood: estimation and missing data imputation. First, we investigate maximum likelihood estimation for DLVMs: in particular, we show that most unconstrained models used for continuous data have an unbounded likelihood function. This problematic behaviour is demonstrated to be a source of mode collapse. We also show how to ensure the existence of maximum likelihood estimates, and draw useful connections with nonparametric mixture models. Finally, we describe an algorithm for missing data imputation using the exact conditional likelihood of a DLVM. On several data sets, our algorithm consistently and significantly outperforms the usual imputation scheme used for DLVMs.",https://proceedings.neurips.cc/paper_files/paper/2018/file/0609154fa35b3194026346c9cac2a248-Paper.pdf,2018,"likelihood, models, data, dlvms, imputation, show, used, algorithm, deep, estimation","likelihood, al, et, data, models, distribution, gibbs, xmiss, dlvms, model","{'leveraging': 0.4564077982118981, 'exact': 0.4371529889449266, 'likelihood': 0.42221779856934305, 'variable': 0.42221779856934305, 'latent': 0.3582645933187959, 'models': 0.25370620186434906, 'deep': 0.2265680724199318}","likelihood, dlvms, imputation, models, missing, maximum, exact, data, used, estimation"
A General Method for Amortizing Variational Filtering,"Joseph Marino, Milan Cvitkovic, Yisong Yue","We introduce the variational filtering EM algorithm, a simple, general-purpose method for performing variational inference in dynamical latent variable models using information from only past and present variables, i.e. filtering. The algorithm is derived from the variational objective in the filtering setting and consists of an optimization procedure at each time step. By performing each inference optimization procedure with an iterative amortized inference model, we obtain a computationally efficient implementation of the algorithm, which we call amortized variational filtering. We present experiments demonstrating that this general-purpose method improves inference performance across several recent deep dynamical latent variable models.",https://proceedings.neurips.cc/paper_files/paper/2018/file/060afc8a563aaccd288f98b7c8723b61-Paper.pdf,2018,"filtering, inference, variational, algorithm, amortized, dynamical, general, latent, method, models","inference, ﬁltering, models, model, step, variational, approximate, posterior, data, latent","{'amortizing': 0.5512741230501983, 'filtering': 0.49838312523370915, 'general': 0.42119233067904865, 'method': 0.39752577890509533, 'variational': 0.3350841348386461}","filtering, variational, inference, amortized, purpose, dynamical, performing, procedure, variable, latent"
One-Shot Unsupervised Cross Domain Translation,"Sagie Benaim, Lior Wolf","Given a single image $x$ from domain $A$ and a set of images from domain $B$, our task is to generate the analogous of $x$ in $B$. We argue that this task could be a key AI capability that underlines the ability of cognitive agents to act in the world and present empirical evidence that the existing unsupervised domain translation methods fail on this task. Our method follows a two step process. First, a variational autoencoder for domain $B$ is trained. Then, given the new sample $x$, we create a variational autoencoder for domain $A$ by adapting the layers that are close to the image in order to directly fit $x$, and only indirectly adapt the other layers. Our experiments indicate that the new method does as well, when trained on one sample $x$, as the existing domain transfer methods, when these enjoy a multitude of training samples from domain $A$. Our code is made publicly available at https://github.com/sagiebenaim/OneShotTranslation",https://proceedings.neurips.cc/paper_files/paper/2018/file/062ddb6c727310e76b6200b7c71f63b5-Paper.pdf,2018,"domain, task, autoencoder, existing, given, image, layers, method, methods, new","domain, one, samples, images, translation, sample, loss, image, layers, ost","{'cross': 0.4380652242604047, 'one': 0.42922755413679436, 'translation': 0.42922755413679436, 'shot': 0.40764176937044977, 'unsupervised': 0.3772183144804949, 'domain': 0.3621612954447164}","domain, autoencoder, task, layers, oneshottranslation, sagiebenaim, underlines, variational, analogous, multitude"
Query K-means Clustering and the Double Dixie Cup Problem,"I Chien, Chao Pan, Olgica Milenkovic","We consider the problem of approximate $K$-means clustering with outliers and side information provided by same-cluster queries and possibly noisy answers. Our solution shows that, under some mild assumptions on the smallest cluster size, one can obtain an $(1+\epsilon)$-approximation for the optimal potential with probability at least $1-\delta$, where $\epsilon>0$ and $\delta\in(0,1)$, using an expected number of $O(\frac{K^3}{\epsilon \delta})$ noiseless same-cluster queries and comparison-based clustering of complexity $O(ndK + \frac{K^3}{\epsilon \delta})$; here, $n$ denotes the number of points and $d$ the dimension of space. Compared to a handful of other known approaches that perform importance sampling to account for small cluster sizes, the proposed query technique reduces the number of queries by a factor of roughly $O(\frac{K^6}{\epsilon^3})$, at the cost of possibly missing very small clusters. We extend this settings to the case where some queries to the oracle produce erroneous information, and where certain points, termed outliers, do not belong to any clusters. Our proof techniques differ from previous methods used for $K$-means clustering analysis, as they rely on estimating the sizes of the clusters and the number of points needed for accurate centroid estimation and subsequent nontrivial generalizations of the double Dixie cup problem. We illustrate the performance of the proposed algorithm both on synthetic and real datasets, including MNIST and CIFAR $10$.",https://proceedings.neurips.cc/paper_files/paper/2018/file/0655f117444fc1911ab9c6f6b0139051-Paper.pdf,2018,"epsilon, cluster, delta, number, queries, clustering, clusters, frac, points, information","cluster, points, query, clusters, clustering, number, outliers, algorithm, queries, one","{'cup': 0.42070654407817887, 'dixie': 0.42070654407817887, 'double': 0.3803426162719839, 'query': 0.3803426162719839, 'means': 0.3673483338911645, 'problem': 0.3567312321235439, 'clustering': 0.3073906726778797}","epsilon, delta, cluster, queries, clusters, frac, clustering, points, number, outliers"
Probabilistic Neural Programmed Networks for Scene Generation,"Zhiwei Deng, Jiacheng Chen, YIFANG FU, Greg Mori","In this paper we address the text to scene image generation problem. Generative models that capture the variability in complicated scenes containing rich semantics is a grand goal of image generation. Complicated scene images contain rich visual elements, compositional visual concepts, and complicated relations between objects. Generative models, as an analysis-by-synthesis process, should encompass the following three core components: 1) the generation process that composes the scene; 2) what are the primitive visual elements and how are they composed; 3) the rendering of abstract concepts into their pixel-level realizations.  We propose PNP-Net, a variational auto-encoder framework that addresses these three challenges: it flexibly composes images with a dynamic network structure, learns a set of distribution transformers that can compose distributions based on semantics, and decodes samples from these distributions into realistic images.",https://proceedings.neurips.cc/paper_files/paper/2018/file/06964dce9addb1c5cb5d6e3d9838f733-Paper.pdf,2018,"complicated, generation, images, scene, visual, composes, concepts, distributions, elements, generative","latent, model, image, distribution, concepts, distributions, generation, object, images, semantic","{'programmed': 0.5663281444681517, 'scene': 0.44842455636771844, 'probabilistic': 0.43269411917591727, 'generation': 0.403322158516474, 'neural': 0.2536344406911037, 'networks': 0.25291195346439327}","complicated, composes, scene, generation, visual, semantics, elements, images, concepts, rich"
Escaping Saddle Points in Constrained Optimization,"Aryan Mokhtari, Asuman Ozdaglar, Ali Jadbabaie","In this paper, we study the problem of escaping from saddle points in smooth
nonconvex optimization problems subject to a convex set $\mathcal{C}$. We propose a generic framework that yields convergence to a second-order stationary point of the problem, if the convex set $\mathcal{C}$ is simple for a quadratic objective function. Specifically, our results hold if one can find a $\rho$-approximate solution of a quadratic program subject to $\mathcal{C}$ in polynomial time, where $\rho<1$ is a positive constant that depends on the structure of the set $\mathcal{C}$. Under this condition, we show that the sequence of iterates generated by the proposed framework reaches an $(\epsilon,\gamma)$-second order stationary point (SOSP) in at most $\mathcal{O}(\max\{\epsilon^{-2},\rho^{-3}\gamma^{-3}\})$ iterations.  We further characterize the overall complexity of reaching an SOSP when the convex set $\mathcal{C}$ can be written as a set of quadratic constraints and the objective function Hessian
has a specific structure over the convex $\mathcal{C}$. Finally, we extend our results to the stochastic setting and characterize the number of stochastic gradient and Hessian evaluations to reach an $(\epsilon,\gamma)$-SOSP.",https://proceedings.neurips.cc/paper_files/paper/2018/file/069654d5ce089c13f642d19f09a3d1c0-Paper.pdf,2018,"mathcal, set, convex, epsilon, gamma, quadratic, rho, sosp, characterize, framework","xt, problem, point, order, ﬁrst, sosp, function, set, r2f, rf","{'escaping': 0.519069635897812, 'saddle': 0.4971712658992159, 'points': 0.4663072468687582, 'constrained': 0.4274231814883331, 'optimization': 0.28853842187333467}","mathcal, sosp, rho, gamma, quadratic, set, convex, epsilon, subject, hessian"
Adversarial Text Generation via Feature-Mover's Distance,"Liqun Chen, Shuyang Dai, Chenyang Tao, Haichao Zhang, Zhe Gan, Dinghan Shen, Yizhe Zhang, Guoyin Wang, Ruiyi Zhang, Lawrence Carin","Generative adversarial networks (GANs) have achieved significant success in generating real-valued data. However, the discrete nature of text hinders the application of GAN to text-generation tasks. Instead of using the standard GAN objective, we propose to improve text-generation GAN via a novel approach inspired by optimal transport. Specifically, we consider matching the latent feature distributions of real and synthetic sentences using a novel metric, termed the feature-mover's distance (FMD). This formulation leads to a highly discriminative critic and easy-to-optimize objective, overcoming the mode-collapsing and brittle-training problems in existing methods. Extensive experiments are conducted on a variety of tasks to evaluate the proposed model empirically, including unconditional text generation, style transfer from non-parallel text, and unsupervised cipher cracking. The proposed model yields superior performance, demonstrating wide applicability and effectiveness.",https://proceedings.neurips.cc/paper_files/paper/2018/file/074177d3eb6371e32c16c55a3b8f706b-Paper.pdf,2018,"text, gan, generation, feature, model, novel, objective, proposed, real, tasks","gan, text, model, generation, feature, sentence, sentences, data, distribution, bleu","{'mover': 0.49960252445092995, 'distance': 0.4129697499047762, 'text': 0.40373560480072024, 'feature': 0.3701611057112964, 'generation': 0.35580214497561535, 'adversarial': 0.2907299805017638, 'via': 0.26213075464131785}","text, gan, generation, feature, cipher, cracking, fmd, objective, collapsing, overcoming"
On gradient regularizers for MMD GANs,"Michael Arbel, Danica J. Sutherland, Mikołaj Bińkowski, Arthur Gretton","We propose a principled method for gradient-based regularization of the critic of GAN-like models trained by adversarially optimizing the kernel of a Maximum Mean Discrepancy (MMD). We show that controlling the gradient of the critic is vital to having a sensible loss function, and devise a method to enforce exact, analytical gradient constraints at no additional cost compared to existing approximate techniques based on additive regularizers. The new loss function is provably continuous, and experiments show that it stabilizes and accelerates training, giving image generation models that outperform state-of-the art methods on $160 \times 160$ CelebA and $64 \times 64$ unconditional ImageNet.",https://proceedings.neurips.cc/paper_files/paper/2018/file/07f75d9144912970de5a09f5a305e10c-Paper.pdf,2018,"gradient, 160, 64, based, critic, function, loss, method, models, show","mmd, critic, φψ, sn, gan, gradient, 10, qθ, generator, kψ","{'mmd': 0.5974680501045732, 'regularizers': 0.5401450595773623, 'gans': 0.48282206905015146, 'gradient': 0.3437413174998574}","160, 64, critic, times, gradient, accelerates, mmd, stabilizes, discrepancy, unconditional"
Differentially Private Bayesian Inference for Exponential Families,"Garrett Bernstein, Daniel R. Sheldon","The study of private inference has been sparked by growing concern regarding the analysis of data when it stems from sensitive sources. We present the first method for private Bayesian inference in exponential families that properly accounts for noise introduced by the privacy mechanism. It is efficient because it works only with sufficient statistics and not individual data. Unlike other methods, it gives properly calibrated posterior beliefs in the non-asymptotic data regime.",https://proceedings.neurips.cc/paper_files/paper/2018/file/08040837089cdf46631a10aca5258e16-Paper.pdf,2018,"data, inference, private, properly, accounts, analysis, asymptotic, bayesian, beliefs, calibrated","exponential, sufﬁcient, posterior, statistics, inference, private, data, privacy, mechanism, algorithm","{'exponential': 0.5190601081444025, 'families': 0.4692598256740564, 'differentially': 0.4109972301331092, 'private': 0.39032824543323674, 'inference': 0.31764813857894486, 'bayesian': 0.29697915387907237}","properly, private, sparked, concern, stems, accounts, beliefs, calibrated, families, inference"
Stochastic Primal-Dual Method for Empirical Risk Minimization with O(1) Per-Iteration Complexity,"Conghui Tan, Tong Zhang, Shiqian Ma, Ji Liu","Regularized empirical risk minimization problem with linear predictor appears frequently in machine learning. In this paper, we propose a new stochastic primal-dual method to solve this class of problems. Different from existing methods, our proposed methods only require O(1) operations in each iteration. We also develop a variance-reduction variant of the algorithm that converges linearly. Numerical experiments suggest that our methods are faster than existing ones such as proximal SGD, SVRG and SAGA on high-dimensional problems.",https://proceedings.neurips.cc/paper_files/paper/2018/file/08048a9c5630ccb67789a198f35d30ec-Paper.pdf,2018,"methods, existing, problems, algorithm, also, appears, class, converges, develop, different","spd1, 10, iteration, vr, xt, primal, psgd, method, yt, dual","{'per': 0.38626499998862307, 'iteration': 0.349205503783821, 'primal': 0.349205503783821, 'empirical': 0.31928533711389884, 'dual': 0.3121460075790189, 'risk': 0.3121460075790189, 'complexity': 0.3058486726329665, 'minimization': 0.3002155145683215, 'method': 0.27853709899289425, 'stochastic': 0.2210007108421596}","methods, saga, svrg, appears, frequently, primal, problems, existing, proximal, predictor"
Answerer in Questioner's Mind: Information Theoretic Approach to Goal-Oriented Visual Dialog,"Sang-Woo Lee, Yu-Jung Heo, Byoung-Tak Zhang","Goal-oriented dialog has been given attention due to its numerous applications in artificial intelligence.
Goal-oriented dialogue tasks occur when a questioner asks an action-oriented question and an answerer responds with the intent of letting the questioner know a correct action to take. 
To ask the adequate question, deep learning and reinforcement learning have been recently applied. 
However, these approaches struggle to find a competent recurrent neural questioner, owing to the complexity of learning a series of sentences.
Motivated by theory of mind, we propose ""Answerer in Questioner's Mind"" (AQM), a novel information theoretic algorithm for goal-oriented dialog. 
With AQM, a questioner asks and infers based on an approximated probabilistic model of the answerer.
The questioner figures out the answerer’s intention via selecting a plausible question by explicitly calculating the information gain of the candidate intentions and possible answers to each question.
We test our framework on two goal-oriented visual dialog tasks: ""MNIST Counting Dialog"" and ""GuessWhat?!"".
In our experiments, AQM outperforms comparative algorithms by a large margin.",https://proceedings.neurips.cc/paper_files/paper/2018/file/0829424ffa0d3a2547b6c9622c77de03-Paper.pdf,2018,"questioner, oriented, answerer, dialog, goal, question, aqm, learning, action, asks","aqm, dialog, questioner, question, answerer, a1, goal, q1, questions, deep","{'answerer': 0.3588954693190805, 'mind': 0.3588954693190805, 'questioner': 0.3588954693190805, 'goal': 0.3387531185177753, 'dialog': 0.3244618931898871, 'oriented': 0.3244618931898871, 'theoretic': 0.3133767576264569, 'visual': 0.2555947409315003, 'approach': 0.24974361545808316, 'information': 0.24974361545808316}","questioner, oriented, answerer, dialog, aqm, question, goal, asks, mind, action"
Learning Plannable Representations with Causal InfoGAN,"Thanard Kurutach, Aviv Tamar, Ge Yang, Stuart J. Russell, Pieter Abbeel","In recent years, deep generative models have been shown to 'imagine' convincing high-dimensional observations such as images, audio, and even video, learning directly from raw data. In this work, we ask how to imagine goal-directed visual plans -- a plausible sequence of observations that transition a dynamical system from its current configuration to a desired goal state, which can later be used as a reference trajectory for control. We focus on systems with high-dimensional observations, such as images, and propose an approach that naturally combines representation learning and planning. Our framework learns a generative model of sequential observations, where the generative process is induced by a transition in a low-dimensional planning model, and an additional noise. By maximizing the mutual information between the generated observations and the transition in the planning model, we obtain a low-dimensional representation that best explains the causal nature of the data. We structure the planning model to be compatible with efficient planning algorithms, and we propose several such models based on either discrete or continuous states. Finally, to generate a visual plan, we project the current and goal observations onto their respective states in the planning model, plan a trajectory, and then use the generative model to transform the trajectory to a sequence of observations. We demonstrate our method on imagining plausible visual plans of rope manipulation.",https://proceedings.neurips.cc/paper_files/paper/2018/file/08aac6ac98e59e523995c161e57875f5-Paper.pdf,2018,"observations, model, planning, dimensional, generative, goal, trajectory, transition, visual, current","planning, observations, state, model, infogan, data, observation, states, causal, cigan","{'infogan': 0.5701113875175606, 'plannable': 0.5701113875175606, 'causal': 0.4012324031762826, 'representations': 0.39245525758586736, 'learning': 0.18691559881454228}","observations, planning, trajectory, transition, imagine, plans, dimensional, generative, plan, goal"
Interactive Structure Learning with Structural Query-by-Committee,"Christopher Tosh, Sanjoy Dasgupta","In this work, we introduce interactive structure learning, a framework that unifies many different interactive learning tasks. We present a generalization of the query-by-committee active learning algorithm for this setting, and we study its consistency and rate of convergence, both theoretically and empirically, with and without noise.",https://proceedings.neurips.cc/paper_files/paper/2018/file/08c5433a60135c32e34f46a71175850c-Paper.pdf,2018,"learning, interactive, active, algorithm, committee, consistency, convergence, different, empirically, framework","learning, loss, query, qbc, structural, feedback, structure, g0, interactive, setting","{'committee': 0.4839237874190396, 'query': 0.4635081410102593, 'structural': 0.44767253539463997, 'interactive': 0.43473390350673324, 'structure': 0.3651283731856467, 'learning': 0.16809216661451556}","interactive, committee, unifies, query, learning, consistency, active, theoretically, noise, generalization"
The streaming rollout of deep networks - towards fully model-parallel execution,"Volker Fischer, Jan Koehler, Thomas Pfeil","Deep neural networks, and in particular recurrent networks, are promising candidates to control autonomous agents that interact in real-time with the physical world. However, this requires a seamless integration of temporal features into the network’s architecture. For the training of and inference with recurrent neural networks, they are usually rolled out over time, and different rollouts exist. Conventionally during inference, the layers of a network are computed in a sequential manner resulting in sparse temporal integration of information and long response times. In this study, we present a theoretical framework to describe rollouts, the level of model-parallelization they induce, and demonstrate differences in solving specific tasks. We prove that certain rollouts, also for networks with only skip and no recurrent connections, enable earlier and more frequent responses, and show empirically that these early responses have better performance. The streaming rollout maximizes these properties and enables a fully parallel execution of the network reducing runtime on massively parallel devices. Finally, we provide an open-source toolbox to design, train, evaluate, and interact with streaming rollouts.",https://proceedings.neurips.cc/paper_files/paper/2018/file/08f90c1a417155361a5c4b8d297e0d78-Paper.pdf,2018,"networks, rollouts, network, recurrent, inference, integration, interact, neural, parallel, responses","rollout, rollouts, network, streaming, networks, fig, time, sequential, see, different","{'rollout': 0.4297613367587239, 'execution': 0.40564176895739285, 'fully': 0.3752547072332203, 'parallel': 0.3644090954585003, 'streaming': 0.3552392613609064, 'towards': 0.31841498755717096, 'model': 0.26866979243410416, 'deep': 0.2013670100417928, 'networks': 0.19192367581376713}","rollouts, recurrent, interact, integration, streaming, responses, parallel, networks, temporal, network"
Contextual Stochastic Block Models,"Yash Deshpande, Subhabrata Sen, Andrea Montanari, Elchanan Mossel","We provide the first information theoretical tight analysis for inference of latent community structure given a sparse graph along with high dimensional node covariates, correlated with the same latent communities. Our work bridges recent theoretical breakthroughs in detection of latent community structure without nodes covariates and a large body of empirical work using diverse heuristics for combining node covariates with graphs for inference. The tightness of our analysis implies in particular, the information theoretic necessity of combining the different sources of information. 
Our analysis holds for networks of large degrees as well as for a Gaussian version of the model.",https://proceedings.neurips.cc/paper_files/paper/2018/file/08fc80de8121419136e443a70489c123-Paper.pdf,2018,"analysis, covariates, information, latent, combining, community, inference, large, node, structure","ηt, model, ag, graph, λ2, information, random, two, µ2, algorithm","{'block': 0.5976559817281608, 'contextual': 0.572504488790548, 'stochastic': 0.4136813735166124, 'models': 0.37935953793812877}","covariates, latent, community, node, combining, analysis, information, breakthroughs, bridges, tightness"
Unsupervised Learning of Artistic Styles with Archetypal Style Analysis,"Daan Wynen, Cordelia Schmid, Julien Mairal","In this paper, we introduce an unsupervised learning approach to automatically dis-
cover, summarize, and manipulate artistic styles from large collections of paintings.
Our method is based on archetypal analysis, which is an unsupervised learning
technique akin to sparse coding with a geometric interpretation. When applied
to deep image representations from a data collection, it learns a dictionary of
archetypal styles, which can be easily visualized. After training the model, the style
of a new image, which is characterized by local statistics of deep visual features,
is approximated by a sparse convex combination of archetypes. This allows us
to interpret which archetypal styles are present in the input image, and in which
proportion. Finally, our approach allows us to manipulate the coefficients of the
latent archetypal decomposition, and achieve various special effects such as style
enhancement, transfer, and interpolation between multiple archetypes.",https://proceedings.neurips.cc/paper_files/paper/2018/file/09060616068d2b9544dc33f2fbe4ce2d-Paper.pdf,2018,"archetypal, image, styles, allows, approach, archetypes, deep, learning, manipulate, sparse","style, image, archetypal, archetypes, styles, collection, archetype, transfer, analysis, content","{'archetypal': 0.4520644067075744, 'artistic': 0.4520644067075744, 'styles': 0.4520644067075744, 'style': 0.4086919055355839, 'analysis': 0.31457683086739563, 'unsupervised': 0.31457683086739563, 'learning': 0.1482129477371373}","archetypal, styles, archetypes, manipulate, style, image, unsupervised, us, sparse, artistic"
Generalisation in humans and deep neural networks,"Robert Geirhos, Carlos R. M. Temme, Jonas Rauber, Heiko H. Schütt, Matthias Bethge, Felix A. Wichmann","We compare the robustness of humans and current convolutional deep neural networks (DNNs) on object recognition under twelve different types of image degradations. First, using three well known DNNs (ResNet-152, VGG-19, GoogLeNet) we find the human visual system to be more robust to nearly all of the tested image manipulations, and we observe progressively diverging classification error-patterns between humans and DNNs when the signal gets weaker. Secondly, we show that DNNs trained directly on distorted images consistently surpass human performance on the exact distortion types they were trained on, yet they display extremely poor generalisation abilities when tested on other distortion types. For example, training on salt-and-pepper noise does not imply robustness on uniform white noise and vice versa. Thus, changes in the noise distribution between training and testing constitutes a crucial challenge to deep learning vision systems that can be systematically addressed in a lifelong machine learning approach. Our new dataset consisting of 83K carefully measured human psychophysical trials provide a useful reference for lifelong robustness against image degradations set by the human visual system.",https://proceedings.neurips.cc/paper_files/paper/2018/file/0937fb5864ed06ffb59ae5f9b5ed67a9-Paper.pdf,2018,"dnns, human, image, noise, robustness, types, deep, degradations, distortion, humans","human, noise, training, dnns, trained, distortions, performance, image, images, one","{'generalisation': 0.609096919366036, 'humans': 0.609096919366036, 'deep': 0.3023653747336078, 'neural': 0.2890088602793393, 'networks': 0.2881856076903415}","dnns, human, degradations, types, lifelong, robustness, noise, distortion, tested, humans"
Distributed Weight Consolidation: A Brain Segmentation Case Study,"Patrick McClure, Charles Y. Zheng, Jakub Kaczmarzyk, John Rogers-Lee, Satra Ghosh, Dylan Nielson, Peter A. Bandettini, Francisco Pereira","Collecting the large datasets needed to train deep neural networks can be very difficult, particularly for the many applications for which sharing and pooling data is complicated by practical, ethical, or legal concerns. However, it may be the case that derivative datasets or predictive models developed within individual sites can be shared and combined with fewer restrictions. Training on distributed data and combining the resulting networks is often viewed as continual learning, but these methods require networks to be trained sequentially. In this paper, we introduce distributed weight consolidation (DWC), a continual learning method to consolidate the weights of separate neural networks, each trained on an independent dataset. We evaluated DWC with a brain segmentation case study, where we consolidated dilated convolutional neural networks trained on independent structural magnetic resonance imaging (sMRI) datasets from different sites. We found that DWC led to increased performance on test sets from the different sites, while maintaining generalization performance for a very large and completely independent multi-site dataset, compared to an ensemble baseline.",https://proceedings.neurips.cc/paper_files/paper/2018/file/093b60fd0557804c8ba0cbf1453da22f-Paper.pdf,2018,"networks, datasets, dwc, independent, neural, sites, trained, case, continual, data","dwc, networks, training, d1, data, datasets, trained, used, dice, errors","{'consolidation': 0.42930171164540126, 'weight': 0.4052079394616276, 'brain': 0.3881131360459905, 'case': 0.3748533763721481, 'study': 0.3748533763721481, 'segmentation': 0.36401936386221684, 'distributed': 0.2955242037165239}","dwc, sites, continual, independent, networks, trained, datasets, distributed, consolidate, consolidated"
IntroVAE: Introspective Variational Autoencoders for Photographic Image Synthesis,"Huaibo Huang, zhihang li, Ran He, Zhenan Sun, Tieniu Tan","We present a novel introspective variational autoencoder (IntroVAE) model for synthesizing high-resolution photographic images. IntroVAE is capable of self-evaluating the quality of its generated samples and improving itself accordingly. Its inference and generator models are jointly trained in an introspective way. On one hand, the generator is required to reconstruct the input images from the noisy outputs of the inference model as normal VAEs. On the other hand, the inference model is encouraged to classify between the generated and real samples while the generator tries to fool it as GANs. These two famous generative frameworks are integrated in a simple yet efficient single-stream architecture that can be trained in a single stage. IntroVAE preserves the advantages of VAEs, such as stable training and nice latent manifold. Unlike most other hybrid models of VAEs and GANs, IntroVAE requires no extra discriminators, because the inference model itself serves as a discriminator to distinguish between the generated and real samples.  Experiments demonstrate that our method produces high-resolution photo-realistic images (e.g., CELEBA images at (1024^{2})), which are comparable to or better than the state-of-the-art GANs.",https://proceedings.neurips.cc/paper_files/paper/2018/file/093f65e080a295f8076b1c5722a46aa2-Paper.pdf,2018,"images, inference, introvae, model, gans, generated, generator, samples, vaes, hand","images, model, training, vaes, gans, samples, inference, high, generator, resolution","{'introspective': 0.4510570041055325, 'introvae': 0.4510570041055325, 'photographic': 0.4510570041055325, 'autoencoders': 0.3391905604772555, 'synthesis': 0.32956633991009093, 'image': 0.30134716806566575, 'variational': 0.2741685844917687}","introvae, vaes, gans, generator, introspective, images, inference, generated, resolution, samples"
MixLasso: Generalized Mixed Regression via Convex Atomic-Norm Regularization,"Ian En-Hsu Yen, Wei-Cheng Lee, Kai Zhong, Sung-En Chang, Pradeep K. Ravikumar, Shou-De Lin","We consider a generalization of mixed regression where the response is an additive combination of several mixture components. Standard mixed regression is a special case where each response is generated from exactly one component. Typical approaches to the mixture regression problem employ local search methods such as Expectation Maximization (EM) that are prone to spurious local optima. On the other hand, a number of recent theoretically-motivated \emph{Tensor-based methods} either have high sample complexity, or require the knowledge of the input distribution, which is not available in most of practical situations. In this work, we study a novel convex estimator \emph{MixLasso} for the estimation of generalized mixed regression, based on an atomic norm specifically constructed to regularize the number of mixture components. Our algorithm gives a risk bound that trades off between prediction accuracy and model sparsity without imposing stringent assumptions on the input/output distribution, and can be easily adapted to the case of non-linear functions. In our numerical experiments on mixtures of linear as well as nonlinear regressions, the proposed method yields high-quality solutions in a wider range of settings than existing approaches.",https://proceedings.neurips.cc/paper_files/paper/2018/file/09779bb7930c8a0a44360e12b538ae3c-Paper.pdf,2018,"regression, mixed, mixture, approaches, based, case, components, distribution, emph, high","em, mixlasso, random, tensor, regression, algorithm, problem, components, 10, 100","{'mixed': 0.41191864925193716, 'mixlasso': 0.41191864925193716, 'atomic': 0.3723978601069486, 'norm': 0.3723978601069486, 'generalized': 0.3147199708352546, 'regularization': 0.2933562818169715, 'convex': 0.2751991816902661, 'regression': 0.2751991816902661, 'via': 0.21612490148625715}","mixed, regression, mixture, response, emph, components, mixlasso, regressions, trades, case"
A Dual Framework for Low-rank Tensor Completion,"Madhav Nimishakavi, Pratik Kumar Jawanpuria, Bamdev Mishra","One of the popular approaches for low-rank tensor completion is to use the latent trace norm regularization. However, most existing works in this direction learn a sparse combination of tensors. In this work, we fill this gap by proposing a variant of the latent trace norm that helps in learning a non-sparse combination of tensors. We develop a dual framework for solving the low-rank tensor completion problem. We first show a novel characterization of the dual solution space with an interesting factorization of the optimal solution. Overall, the optimal solution is shown to lie on a Cartesian  product of Riemannian manifolds. Furthermore, we exploit the versatile Riemannian optimization framework for proposing computationally efficient trust region algorithm. The experiments illustrate the efficacy of the proposed algorithm on several real-world datasets across applications.",https://proceedings.neurips.cc/paper_files/paper/2018/file/09a5e2a11bea20817477e0b1dfe2cc21-Paper.pdf,2018,"solution, algorithm, combination, completion, dual, framework, latent, low, norm, optimal","tensor, rank, norm, trace, algorithm, tr, algorithms, completion, 10, latent","{'completion': 0.44483947830435994, 'dual': 0.42394926149708617, 'tensor': 0.4153963713913146, 'framework': 0.3945061545840408, 'rank': 0.38869380241732154, 'low': 0.3783024435694096}","riemannian, solution, trace, proposing, tensors, completion, tensor, dual, norm, combination"
Predict Responsibly: Improving Fairness and Accuracy by Learning to Defer,"David Madras, Toni Pitassi, Richard Zemel","In many machine learning applications, there are multiple decision-makers involved, both automated and human. The interaction between these agents often goes unaddressed in algorithmic development. In this work, we explore a simple version of this interaction with a two-stage framework containing an automated model and an external decision-maker. The model can choose to say PASS, and pass the decision downstream, as explored in rejection learning. We extend this concept by proposing ""learning to defer"", which generalizes rejection learning by considering the effect of other agents in the decision-making process. We propose a learning algorithm which accounts for potential biases held by external decision-makers in a system. Experiments demonstrate that learning to defer can make systems not only more accurate but also less biased. Even when working with inconsistent or biased users, we show that deferring models still greatly improve the accuracy and/or fairness of the entire system.",https://proceedings.neurips.cc/paper_files/paper/2018/file/09d37c08f7b129e96277388757530c72-Paper.pdf,2018,"learning, decision, agents, automated, biased, defer, external, interaction, makers, model","model, learning, dm, defer, ym, decision, accuracy, rejection, fairness, pass","{'defer': 0.43951728924766886, 'responsibly': 0.43951728924766886, 'accuracy': 0.4148501864835643, 'predict': 0.4148501864835643, 'fairness': 0.3633034985540036, 'improving': 0.34160462158634386, 'learning': 0.14409927447123994}","decision, defer, makers, rejection, automated, external, pass, biased, learning, interaction"
Enhancing the Accuracy and Fairness of Human Decision Making,"Isabel Valera, Adish Singla, Manuel Gomez Rodriguez","Societies often rely on human experts to take a wide variety of decisions affecting their members, from jail-or-release decisions taken by judges and stop-and-frisk decisions taken by police officers to accept-or-reject decisions taken  by academics. In this context, each decision is taken by an expert who is typically chosen uniformly at random from a pool of experts. However, these decisions may be imperfect due to limited experience, implicit biases, or faulty probabilistic reasoning. Can we improve the accuracy and fairness of the overall decision making process by optimizing the assignment between experts and decisions?In this paper, we address the above problem from the perspective of sequential decision making and show that, for different fairness notions from the literature, it reduces to a sequence of (constrained) weighted bipartite matchings, which can be solved efficiently using algorithms with approximation guarantees. Moreover, these algorithms also benefit from posterior sampling to actively trade off exploitation---selecting expert assignments which lead to accurate and fair decisions---and exploration---selecting expert assignments to learn about the experts' preferences and biases. We demonstrate the effectiveness of our algorithms on both synthetic and real-world data and show that they can significantly improve both the accuracy and fairness of the decisions taken by pools of experts.",https://proceedings.neurips.cc/paper_files/paper/2018/file/0a113ef6b61820daa5611c870ed8d5ee-Paper.pdf,2018,"decisions, experts, taken, algorithms, decision, expert, fairness, accuracy, assignments, biases","decision, decisions, fairness, zi, utility, experts, disparate, impact, thresholds, xi","{'enhancing': 0.4624495520924237, 'accuracy': 0.4364954180828059, 'making': 0.4180806669457332, 'human': 0.39212653293611544, 'fairness': 0.38225922913634414, 'decision': 0.3477576477894249}","decisions, experts, taken, fairness, expert, assignments, selecting, decision, biases, making"
Computing Higher Order Derivatives of Matrix and Tensor Expressions,"Soeren Laue, Matthias Mitterreiter, Joachim Giesen","Optimization is an integral part of most machine learning systems and most numerical optimization schemes rely on the computation of derivatives. Therefore, frameworks for computing derivatives are an active area of machine learning research. Surprisingly, as of yet, no existing framework is capable of computing higher order matrix and tensor derivatives directly.  Here, we close this fundamental gap and present an algorithmic framework for computing matrix and tensor derivatives that extends seamlessly to higher order derivatives. The framework can be used for symbolic as well as for forward and reverse mode automatic differentiation. Experiments show a speedup between one and four orders of magnitude over state-of-the-art frameworks when evaluating higher order derivatives.",https://proceedings.neurips.cc/paper_files/paper/2018/file/0a1bf96b7165e962e90cb14648c9462d-Paper.pdf,2018,"derivatives, computing, framework, higher, order, frameworks, learning, machine, matrix, optimization","matrix, derivatives, calculus, expression, order, expressions, computing, mode, ricci, tensor","{'expressions': 0.42977023150740234, 'computing': 0.40565016450459485, 'derivatives': 0.40565016450459485, 'higher': 0.40565016450459485, 'tensor': 0.34029657061233415, 'order': 0.32318311072568856, 'matrix': 0.3184215777760844}","derivatives, computing, higher, frameworks, tensor, order, framework, matrix, seamlessly, machine"
Efficient online algorithms for fast-rate regret bounds under sparsity,"Pierre Gaillard, Olivier Wintenberger","We consider the problem of online convex optimization in two different settings: arbitrary and  i.i.d. sequence of convex loss functions. In both settings, we provide efficient algorithms whose cumulative excess risks are controlled with fast-rate sparse bounds. 
First, the excess risks bounds depend on the sparsity of the objective rather than on the dimension of the parameters space. Second, their rates are faster than the slow-rate $1/\sqrt{T}$ under additional convexity assumptions on the loss functions. In the adversarial setting, we develop an algorithm BOA+ whose cumulative excess risks is controlled by several bounds with different trade-offs between sparsity and rate for strongly convex loss functions. In the i.i.d. setting under the Łojasiewicz's assumption, we establish new risk bounds that are sparse with a rate adaptive to the convexity of the risk (ranging from a rate $1/\sqrt{T}$ for general convex risk to $1/T$ for strongly convex risk). These results generalize previous works on sparse online learning under weak assumptions on the risk.",https://proceedings.neurips.cc/paper_files/paper/2018/file/0a348ede8ac3768875037baca5de6e26-Paper.pdf,2018,"convex, rate, risk, bounds, excess, functions, loss, risks, sparse, assumptions","ln, bound, b1, rate, sparse, rt, assumption, loss, functions, setting","{'rate': 0.441453483011806, 'sparsity': 0.4140483821234931, 'regret': 0.3795220382391562, 'bounds': 0.3232262976479743, 'online': 0.3232262976479743, 'fast': 0.32034972330509975, 'algorithms': 0.31759059346650637, 'efficient': 0.2778625070533337}","risk, risks, excess, rate, convex, bounds, cumulative, sparse, controlled, convexity"
Human-in-the-Loop Interpretability Prior,"Isaac Lage, Andrew Ross, Samuel J. Gershman, Been Kim, Finale Doshi-Velez","We often desire our models to be interpretable as well as accurate. Prior work on optimizing models for interpretability has relied on easy-to-quantify proxies for interpretability, such as sparsity or the number of operations required.  In this work, we optimize for interpretability by directly including humans in the optimization loop.  We develop an algorithm that minimizes the number of user studies to find models that are both predictive and interpretable and demonstrate our approach on several data sets.  Our human subjects results show trends towards different proxy notions of interpretability on different datasets, which suggests that different proxies are preferred on different tasks.",https://proceedings.neurips.cc/paper_files/paper/2018/file/0a7d83f084ec258aefd128569dda03d7-Paper.pdf,2018,"different, interpretability, models, interpretable, number, proxies, work, accurate, algorithm, approach","models, model, proxy, interpretability, human, proxies, interpretable, local, different, user","{'loop': 0.5490022371408472, 'human': 0.49319725913679996, 'interpretability': 0.49319725913679996, 'prior': 0.46055343965086337}","interpretability, proxies, different, interpretable, desire, relied, trends, models, preferred, subjects"
Deep Non-Blind Deconvolution via Generalized Low-Rank Approximation,"Wenqi Ren, Jiawei Zhang, Lin Ma, Jinshan Pan, Xiaochun Cao, Wangmeng Zuo, Wei Liu, Ming-Hsuan Yang","In this paper, we present a deep convolutional neural network to capture the inherent properties of image degradation,  which can handle different kernels and saturated pixels in a unified framework. The proposed neural network is motivated by the low-rank property of pseudo-inverse kernels. We first compute a generalized low-rank approximation for a large number of blur kernels, and then use separable filters to initialize the convolutional parameters in the network. Our analysis shows that the estimated decomposed matrices contain the most essential information of the input kernel,  which ensures the proposed network to handle various blurs in a unified framework and generate high-quality deblurring results. Experimental results on benchmark datasets with noise and saturated pixels demonstrate that the proposed algorithm performs favorably against state-of-the-art methods.",https://proceedings.neurips.cc/paper_files/paper/2018/file/0aa1883c6411f7873cb83dacb17b0afc-Paper.pdf,2018,"network, kernels, proposed, convolutional, framework, handle, low, neural, pixels, rank","kernels, deconvolution, kernel, blur, image, proposed, network, method, methods, inverse","{'deconvolution': 0.4226138969733428, 'blind': 0.3963783434460694, 'generalized': 0.3571584253782397, 'rank': 0.3463492181431723, 'low': 0.33708990145215056, 'approximation': 0.332913946993884, 'non': 0.2901182663377405, 'via': 0.2452682913480111, 'deep': 0.21903273782073765}","kernels, saturated, pixels, network, unified, handle, rank, proposed, convolutional, blurs"
Regret Bounds for Robust Adaptive Control of the Linear Quadratic Regulator,"Sarah Dean, Horia Mania, Nikolai Matni, Benjamin Recht, Stephen Tu","We consider adaptive control of the Linear Quadratic Regulator (LQR), where an
unknown linear system is controlled subject to quadratic costs. Leveraging recent
developments in the estimation of linear systems and in robust controller synthesis,
we present the first provably polynomial time algorithm that achieves sub-linear
regret on this problem. We further study the interplay between regret minimization
and parameter estimation by proving a lower bound on the expected regret in
terms of the exploration schedule used by any algorithm. Finally, we conduct a
numerical study comparing our robust adaptive algorithm to other methods from
the adaptive LQR literature, and demonstrate the flexibility of our proposed method
by extending it to a demand forecasting problem subject to state constraints.",https://proceedings.neurips.cc/paper_files/paper/2018/file/0ae3f79a30234b6c45a6f7d298ba1310-Paper.pdf,2018,"linear, adaptive, algorithm, regret, estimation, lqr, problem, quadratic, robust, study","system, regret, algorithm, control, adaptive, problem, φx, controller, lqr, estimation","{'regulator': 0.4661677444780031, 'quadratic': 0.3853326743341203, 'regret': 0.3623180699461239, 'control': 0.3561681395413626, 'adaptive': 0.32090217147557365, 'bounds': 0.30857425003037103, 'linear': 0.29822807146999236, 'robust': 0.29822807146999236}","lqr, adaptive, regret, linear, subject, quadratic, estimation, robust, interplay, regulator"
Binary Rating Estimation with Graph Side Information,"Kwangjun Ahn, Kangwook Lee, Hyunseung Cha, Changho Suh","Rich experimental evidences show that one can better estimate users' unknown ratings with the aid of graph side information such as social graphs. However, the gain is not theoretically quantified. In this work, we study the binary rating estimation problem to understand the fundamental value of graph side information. Considering a simple correlation model between a rating matrix and a graph, we characterize the sharp threshold on the number of observed entries required to recover the rating matrix (called the optimal sample complexity) as a function of the quality of graph side information (to be detailed). To the best of our knowledge, we are the first to reveal how much the graph side information reduces sample complexity. Further, we propose a computationally efficient algorithm that achieves the limit. Our experimental results demonstrate that the algorithm performs well even with real-world graphs.",https://proceedings.neurips.cc/paper_files/paper/2018/file/0b1ec366924b26fc98fa7b71a9c249cf-Paper.pdf,2018,"graph, information, side, rating, algorithm, complexity, experimental, graphs, matrix, sample","log, information, graph, rating, complexity, sample, algorithm, ratings, side, two","{'rating': 0.49184039170643895, 'side': 0.49184039170643895, 'binary': 0.42946027575973084, 'information': 0.34225563751510824, 'estimation': 0.3350828036597475, 'graph': 0.3198914237630312}","side, rating, graph, information, graphs, matrix, aid, evidences, experimental, quantified"
A Bayesian Nonparametric View on Count-Min Sketch,"Diana Cai, Michael Mitzenmacher, Ryan P. Adams","The count-min sketch is a time- and memory-efficient randomized data structure that provides a point estimate of the number of times an item has appeared in a data stream.  The count-min sketch and related hash-based data structures are ubiquitous in systems that must track frequencies of data such as URLs, IP addresses, and language n-grams.  We present a Bayesian view on the count-min sketch, using the same data structure, but providing a posterior distribution over the frequencies that characterizes the uncertainty arising from the hash-based approximation.  In particular, we take a nonparametric approach and consider tokens generated from a Dirichlet process (DP) random measure, which allows for an unbounded number of unique tokens.  Using properties of the DP, we show that it is possible to straightforwardly compute posterior marginals of the unknown true counts and that the modes of these marginals recover the count-min sketch estimator, inheriting the associated probabilistic guarantees.  Using simulated data with known ground truth, we investigate the properties of these estimators.  Lastly, we also study a modified problem in which the observation stream consists of collections of tokens (i.e., documents) arising from a random measure drawn from a stable beta process, which allows for power law scaling behavior in the number of unique tokens.",https://proceedings.neurips.cc/paper_files/paper/2018/file/0b9e57c46de934cee33b0e8d1839bfc2-Paper.pdf,2018,"data, count, min, sketch, tokens, number, using, allows, arising, based","latexit, sha1_base64, ab, count, sketch, posterior, hash, process, data, tokens","{'sketch': 0.47377323029879526, 'count': 0.44718357559222754, 'min': 0.44718357559222754, 'view': 0.4017282606382136, 'nonparametric': 0.37513860593164583, 'bayesian': 0.2710683615577488}","tokens, sketch, count, min, marginals, dp, hash, frequencies, data, stream"
Provable Variational Inference for Constrained Log-Submodular Models,"Josip Djolonga, Stefanie Jegelka, Andreas Krause","Submodular maximization problems appear in several areas of machine learning and data science, as many useful modelling concepts such as diversity and coverage satisfy this natural diminishing returns property. Because the data defining these functions, as well as the decisions made with the computed solutions, are subject to statistical noise and randomness, it is arguably necessary to go beyond computing a single approximate optimum and quantify its inherent uncertainty. To this end, we define a rich class of probabilistic models associated with constrained submodular maximization problems. These capture log-submodular dependencies of arbitrary order between the variables, but also satisfy hard combinatorial constraints. Namely, the variables are assumed to take on one of — possibly exponentially many — set of states, which form the bases of a matroid. To perform inference in these models we design novel variational inference algorithms, which carefully leverage the combinatorial and probabilistic properties of these objects. In addition to providing completely tractable and well-understood variational approximations, our approach results in the minimization of a convex upper bound on the log-partition function. The bound can be efficiently evaluated using greedy algorithms and optimized using any first-order method. Moreover, for the case of facility location and weighted coverage functions, we prove the first constant factor guarantee in this setting — an efficiently certifiable e/(e-1) approximation of the log-partition function. Finally, we empirically demonstrate the effectiveness of our approach on several instances.",https://proceedings.neurips.cc/paper_files/paper/2018/file/0c0a7566915f4f24853fc4192689aa7e-Paper.pdf,2018,"log, submodular, algorithms, approach, bound, combinatorial, coverage, data, efficiently, first","function, log, set, bound, models, partition, 10, also, inference, matroid","{'log': 0.4853377603013796, 'provable': 0.4360041488771268, 'constrained': 0.3996469746214876, 'submodular': 0.3809736292803249, 'inference': 0.3146715050787533, 'variational': 0.31254676948839977, 'models': 0.26978767731362335}","submodular, coverage, log, partition, combinatorial, satisfy, maximization, probabilistic, variables, efficiently"
Middle-Out Decoding,"Shikib Mehri, Leonid Sigal","Despite being virtually ubiquitous, sequence-to-sequence models are challenged by their lack of diversity and inability to be externally controlled. In this paper, we speculate that a fundamental shortcoming of sequence generation models is that the decoding is done strictly from left-to-right, meaning that outputs values generated earlier have a profound effect on those generated later. To address this issue, we propose a novel middle-out decoder architecture that begins from an initial middle-word and simultaneously expands the sequence in both directions. To facilitate information flow and maintain consistent decoding, we introduce a dual self-attention mechanism that allows us to model complex dependencies between the outputs. We illustrate the performance of our model on the task of video captioning, as well as a synthetic sequence de-noising task. Our middle-out decoder achieves significant improvements on de-noising and competitive performance in the task of video captioning, while quantifiably improving the caption diversity. Furthermore, we perform a qualitative analysis that demonstrates our ability to effectively control the generation process of our decoder.",https://proceedings.neurips.cc/paper_files/paper/2018/file/0c215f194276000be6a6df6528067151-Paper.pdf,2018,"sequence, decoder, middle, task, captioning, de, decoding, diversity, generated, generation","middle, et, attention, al, decoder, sequence, self, model, word, output","{'middle': 0.7532594220073888, 'decoding': 0.6577235309437349}","middle, sequence, decoder, noising, captioning, de, decoding, diversity, outputs, video"
Maximum-Entropy Fine Grained Classification,"Abhimanyu Dubey, Otkrist Gupta, Ramesh Raskar, Nikhil Naik","Fine-Grained Visual Classification (FGVC) is an important computer vision problem that involves small diversity within the different classes, and often requires expert annotators to collect data. Utilizing this notion of small visual diversity, we revisit Maximum-Entropy learning in the context of fine-grained classification, and provide a training routine that maximizes the entropy of the output probability distribution for training convolutional neural networks on FGVC tasks. We provide a theoretical as well as empirical justification of our approach, and achieve state-of-the-art performance across a variety of classification tasks in FGVC, that can potentially be extended to any fine-tuning task. Our method is robust to different hyperparameter values, amount of training data and amount of training label noise and can hence be a valuable tool in many similar problems.",https://proceedings.neurips.cc/paper_files/paper/2018/file/0c74b7f78409a4022a2c4c5a5ca3ee19-Paper.pdf,2018,"training, classification, fgvc, fine, amount, data, different, diversity, entropy, grained","entropy, training, maximum, ﬁne, classiﬁcation, grained, maxent, data, diversity, features","{'fine': 0.49803890802203826, 'grained': 0.49803890802203826, 'maximum': 0.44741425036659194, 'entropy': 0.40314455064182714, 'classification': 0.37577846139598164}","fgvc, fine, grained, diversity, training, classification, entropy, amount, visual, small"
Deep State Space Models for Unconditional Word Generation,"Florian Schmidt, Thomas Hofmann","Autoregressive feedback is considered a necessity for successful unconditional text generation using stochastic sequence models. However, such feedback is known to introduce systematic biases into the training process and it obscures a principle of generation: committing to global information and forgetting local nuances. We show that a non-autoregressive deep state space model with a clear separation of global and local uncertainty can be built from only two ingredients: An independent noise source and a deterministic transition function. Recent advances on flow-based variational inference can be used to train an evidence lower-bound without resorting to annealing, auxiliary losses or similar measures. The result is a highly interpretable generative model on par with comparable auto-regressive models on the task of word generation.",https://proceedings.neurips.cc/paper_files/paper/2018/file/0cd60efb5578cd967c3c23894f305800-Paper.pdf,2018,"generation, autoregressive, feedback, global, local, model, models, advances, annealing, auto","model, ht, 11, inference, models, state, wt, ξt, generation, training","{'unconditional': 0.49323964067550863, 'word': 0.430682057984709, 'state': 0.4182344848851487, 'space': 0.39055231355418935, 'generation': 0.3512706872973069, 'models': 0.25879228567025087, 'deep': 0.23111011433929146}","generation, autoregressive, feedback, global, annealing, committing, obscures, resorting, local, nuances"
Total stochastic gradient algorithms and applications in reinforcement learning,Paavo Parmas,"Backpropagation and the chain rule of derivatives have been prominent; however,
the total derivative rule has not enjoyed the same amount of attention. In this work
we show how the total derivative rule leads to an intuitive visual framework for
creating gradient estimators on graphical models. In particular, previous ”policy
gradient theorems” are easily derived. We derive new gradient estimators based
on density estimation, as well as a likelihood ratio gradient, which ”jumps” to an
intermediate node, not directly to the objective function. We evaluate our methods
on model-based policy gradient algorithms, achieve good performance, and present evidence towards demystifying the success of the popular PILCO algorithm.",https://proceedings.neurips.cc/paper_files/paper/2018/file/0d59701b3474225fca5563e015965886-Paper.pdf,2018,"gradient, rule, based, derivative, estimators, policy, total, achieve, algorithm, algorithms","gradient, policy, lr, distribution, cost, estimators, gradients, model, one, total","{'total': 0.5506463595259962, 'applications': 0.4808078827923362, 'algorithms': 0.3581386378771356, 'reinforcement': 0.32424526833662465, 'gradient': 0.3168033923602056, 'stochastic': 0.3150511614603364, 'learning': 0.18053383300062395}","rule, gradient, derivative, total, estimators, demystifying, jumps, pilco, policy, creating"
Neural Arithmetic Logic Units,"Andrew Trask, Felix Hill, Scott E. Reed, Jack Rae, Chris Dyer, Phil Blunsom","Neural networks can learn to represent and manipulate numerical information, but they seldom generalize well outside of the range of numerical values encountered during training. To encourage more systematic numerical extrapolation, we propose an architecture that represents numerical quantities as linear activations which are manipulated using primitive arithmetic operators, controlled by learned gates. We call this module a neural arithmetic logic unit (NALU), by analogy to the arithmetic logic unit in traditional processors. Experiments show that NALU-enhanced neural networks can learn to track time, perform arithmetic over images of numbers, translate numerical language into real-valued scalars, execute computer code, and count objects in images. In contrast to conventional architectures, we obtain substantially better generalization both inside and outside of the range of numerical values encountered during training, often extrapolating orders of magnitude beyond trained numerical ranges.",https://proceedings.neurips.cc/paper_files/paper/2018/file/0e64a7b00c83e3d22ce6b3acf2c582b6-Paper.pdf,2018,"numerical, arithmetic, neural, encountered, images, learn, logic, nalu, networks, outside","nac, training, nalu, numerical, task, model, neural, extrapolation, lstm, learn","{'arithmetic': 0.593891458066978, 'logic': 0.5369116171884468, 'units': 0.5369116171884468, 'neural': 0.2659788839198561}","numerical, arithmetic, nalu, encountered, logic, outside, unit, values, range, extrapolating"
Implicit Bias of Gradient Descent on Linear Convolutional Networks,"Suriya Gunasekar, Jason D. Lee, Daniel Soudry, Nati Srebro","We show that gradient descent on full-width linear convolutional networks of depth $L$ converges to a linear predictor related to the $\ell_{2/L}$ bridge penalty in the frequency domain. This is in contrast to linearly fully connected networks, where gradient descent converges to the hard margin linear SVM solution, regardless of depth.",https://proceedings.neurips.cc/paper_files/paper/2018/file/0e98aeeb54acf612b9eb4e48a269814c-Paper.pdf,2018,"linear, converges, depth, descent, gradient, networks, bridge, connected, contrast, convolutional","linear, networks, gradient, descent, bias, convolutional, network, al, et, loss","{'bias': 0.4700280514455957, 'implicit': 0.4700280514455957, 'convolutional': 0.3703374183107395, 'descent': 0.36366125971559066, 'linear': 0.3546240710965231, 'gradient': 0.3189179489000034, 'networks': 0.24755010918571985}","converges, depth, linear, descent, ell_, svm, regardless, penalty, gradient, bridge"
On Binary Classification in Extreme Regions,"Hamid JALALZAI, Stephan Clémençon, Anne Sabourin","In pattern recognition, a random label Y is to be predicted based upon observing a random vector X valued in $\mathbb{R}^d$ with d>1 by means of a classification rule with minimum probability of error. In a wide variety of applications, ranging from finance/insurance to environmental sciences through teletraffic data analysis for instance, extreme (i.e. very large) observations X are of crucial importance, while contributing in a negligible manner to the (empirical) error however, simply because of their rarity. As a consequence, empirical risk minimizers generally perform very poorly in extreme regions. It is the purpose of this paper to develop a general framework for classification in the extremes. Precisely, under non-parametric heavy-tail assumptions for the class distributions, we prove that a natural and asymptotic notion of risk, accounting for predictive performance in extreme regions of the input space, can be defined and show that minimizers of an empirical version of a non-asymptotic approximant of this dedicated risk, based on a fraction of the largest observations, lead to classification rules with good generalization capacity, by means of maximal deviation inequalities in low probability regions. Beyond theoretical results, numerical experiments are presented in order to illustrate the relevance of the approach developed.",https://proceedings.neurips.cc/paper_files/paper/2018/file/0ebcc77dc72360d0eb8e9504c78d38bd-Paper.pdf,2018,"classification, empirical, extreme, regions, risk, asymptotic, based, error, means, minimizers","risk, set, distribution, classiﬁer, extreme, extremes, classiﬁcation, rd, gs, probability","{'regions': 0.5691628147928427, 'extreme': 0.5145555188293174, 'binary': 0.49697589607282927, 'classification': 0.40534092690226675}","extreme, regions, risk, minimizers, empirical, classification, asymptotic, means, observations, probability"
Adaptive Skip Intervals: Temporal Abstraction for Recurrent Dynamical Models,"Alexander Neitz, Giambattista Parascandolo, Stefan Bauer, Bernhard Schölkopf","We introduce a method which enables a recurrent dynamics model to be temporally abstract. Our approach, which we call Adaptive Skip Intervals (ASI), is based on the observation that in many sequential prediction tasks, the exact time at which events occur is irrelevant to the underlying objective. Moreover, in many situations, there exist prediction intervals which result in particularly easy-to-predict transitions. We show that there are prediction tasks for which we gain both computational efficiency and prediction accuracy by allowing the model to make predictions at a sampling rate which it can choose itself.",https://proceedings.neurips.cc/paper_files/paper/2018/file/0f0ee3310223fe38a989b2c818709393-Paper.pdf,2018,"prediction, intervals, many, model, tasks, abstract, accuracy, adaptive, allowing, approach","model, skip, time, al, et, asi, training, frame, intervals, funnel","{'intervals': 0.4332460378368502, 'skip': 0.4332460378368502, 'abstraction': 0.4089308975706828, 'dynamical': 0.35011202416523507, 'temporal': 0.33673043684441356, 'adaptive': 0.29823941268333315, 'recurrent': 0.2842298770632601, 'models': 0.2273149259370663}","intervals, prediction, asi, temporally, abstract, irrelevant, transitions, many, tasks, occur"
Learning to Specialize with Knowledge Distillation for Visual Question Answering,"Jonghwan Mun, Kimin Lee, Jinwoo Shin, Bohyung Han","Visual Question Answering (VQA) is a notoriously challenging problem because it involves various heterogeneous tasks defined by questions within a unified framework. Learning specialized models for individual types of tasks is intuitively attracting but surprisingly difficult; it is not straightforward to outperform naive independent ensemble approach. We present a principled algorithm to learn specialized models with knowledge distillation under a multiple choice learning (MCL) framework, where training examples are assigned dynamically to a subset of models for updating network parameters. The assigned and non-assigned models are learned to predict ground-truth answers and imitate their own base models before specialization, respectively. Our approach alleviates the limitation of data deficiency in existing MCL frameworks, and allows each model to learn its own specialized expertise without forgetting general knowledge. The proposed framework is model-agnostic and applicable to any tasks other than VQA, e.g., image classification with a large number of labels but few per-class examples, which is known to be difficult under existing MCL schemes. Our experimental results indeed demonstrate that our method outperforms other baselines for VQA and image classification.",https://proceedings.neurips.cc/paper_files/paper/2018/file/0f2818101a7ac4b96ceeba38de4b934c-Paper.pdf,2018,"models, assigned, framework, mcl, specialized, tasks, vqa, approach, classification, difficult","models, mcl, model, specialized, learning, examples, vqa, knowledge, question, oracle","{'specialize': 0.4804376101435184, 'distillation': 0.4343428933292009, 'answering': 0.3971280968283103, 'question': 0.3971280968283103, 'knowledge': 0.35103338001399287, 'visual': 0.342153459700566, 'learning': 0.1575153304409961}","mcl, assigned, vqa, specialized, models, tasks, difficult, framework, examples, attracting"
A Model for Learned Bloom Filters and Optimizing by Sandwiching,Michael Mitzenmacher,"Recent work has suggested enhancing Bloom filters by using a pre-filter, based on applying machine learning to determine a function that models the data set the Bloom filter is meant to represent.  Here we model such learned Bloom filters, with the following outcomes: (1) we clarify what guarantees can and cannot be associated with such a structure; (2) we show how to estimate what size the learning function must obtain in order to obtain improved performance;  (3) we provide a simple method, sandwiching, for optimizing learned Bloom filters;  and (4) we propose a design and analysis approach for a learned Bloomier filter, based on our modeling approach.",https://proceedings.neurips.cc/paper_files/paper/2018/file/0f49c89d1e7298bb9930789c8ed59d48-Paper.pdf,2018,"bloom, filter, filters, learned, approach, based, function, learning, obtain, analysis","ﬁlter, bloom, false, learned, positive, set, function, rate, fp, probability","{'bloom': 0.4471813759861922, 'optimizing': 0.4471813759861922, 'sandwiching': 0.4471813759861922, 'filters': 0.422084140392738, 'learned': 0.3791801327640204, 'model': 0.2795601120676393}","bloom, filter, filters, learned, obtain, bloomier, enhancing, meant, sandwiching, clarify"
Random Feature Stein Discrepancies,"Jonathan Huggins, Lester Mackey","Computable Stein discrepancies have been deployed for a variety of applications, ranging from sampler selection in posterior inference to approximate Bayesian inference to goodness-of-fit testing. Existing convergence-determining Stein discrepancies admit strong theoretical guarantees but suffer from a computational cost that grows quadratically in the sample size. While linear-time Stein discrepancies have been proposed for goodness-of-fit testing, they exhibit avoidable degradations in testing power—even when power is explicitly optimized. To address these shortcomings, we introduce feature Stein discrepancies (ΦSDs), a new family of quality measures that can be cheaply approximated using importance sampling. We show how to construct ΦSDs that provably determine the convergence of a sample to its target and develop high-accuracy approximations—random ΦSDs (RΦSDs)—which are computable in near-linear time. In our experiments with sampler selection for approximate posterior inference and goodness-of-fit testing, RΦSDs perform as well or better than quadratic-time KSDs while being orders of magnitude faster to compute.",https://proceedings.neurips.cc/paper_files/paper/2018/file/0f840be9b8db4d3fbd5ba2ce59211f55-Paper.pdf,2018,"discrepancies, stein, testing, fit, goodness, inference, time, φsds, approximate, computable","qn, stein, sample, imq, kernel, rφsd, rφsds, distribution, ksd, testing","{'discrepancies': 0.5856855601173763, 'stein': 0.5294930192632564, 'feature': 0.433940990931645, 'random': 0.433940990931645}","discrepancies, stein, φsds, goodness, testing, fit, rφsds, computable, sampler, inference"
Nonparametric Bayesian Lomax delegate racing for survival analysis with competing risks,"Quan Zhang, Mingyuan Zhou","We propose Lomax delegate racing (LDR) to explicitly model the mechanism of survival under competing risks and to interpret how the covariates accelerate or decelerate the time to event. LDR explains non-monotonic covariate effects by racing a potentially infinite number of sub-risks, and consequently relaxes the ubiquitous proportional-hazards assumption which may be too restrictive. Moreover, LDR is naturally able to model not only censoring, but also missing event times or event types. For inference, we develop a Gibbs sampler under data augmentation for moderately sized data, along with a stochastic gradient descent maximum a posteriori inference algorithm for big data applications. Illustrative experiments are provided on both synthetic and real datasets, and comparison with various benchmark algorithms for survival analysis with competing risks demonstrates distinguished performance of LDR.",https://proceedings.neurips.cc/paper_files/paper/2018/file/0fe473396242072e84af286632d3f0ff-Paper.pdf,2018,"ldr, data, event, risks, competing, inference, model, racing, survival, able","ldr, time, risks, survival, risk, data, model, competing, event, racing","{'delegate': 0.3749961072376123, 'lomax': 0.3749961072376123, 'racing': 0.3749961072376123, 'competing': 0.35395013762580724, 'risks': 0.35395013762580724, 'survival': 0.35395013762580724, 'nonparametric': 0.29692584532518185, 'analysis': 0.2609475226363615, 'bayesian': 0.21455323745355126}","ldr, risks, event, racing, survival, competing, censoring, decelerate, delegate, distinguished"
Hessian-based Analysis of Large Batch Training and Robustness to Adversaries,"Zhewei Yao, Amir Gholami, Qi Lei, Kurt Keutzer, Michael W. Mahoney","Large batch size training of Neural Networks has been shown to incur accuracy
loss when trained with the current methods.  The exact underlying reasons for
this are still not completely understood.  Here, we study large batch size
training through the lens of the Hessian operator and robust optimization. In
particular, we perform a Hessian based study to analyze exactly how the landscape of the loss function changes when training with large batch size. We compute the true Hessian spectrum, without approximation, by back-propagating the second
derivative. Extensive experiments on multiple networks show that saddle-points are
not the cause for generalization gap of large batch size training, and the results
consistently show that large batch converges to points with noticeably higher Hessian spectrum. Furthermore, we show that robust training allows one to favor flat areas, as points with large Hessian spectrum show poor robustness to adversarial perturbation. We further study this relationship, and provide empirical and theoretical proof that the inner loop for robust training is a saddle-free optimization problem \textit{almost everywhere}. We present detailed experiments with five different network architectures, including a residual network, tested on MNIST, CIFAR-10/100 datasets.",https://proceedings.neurips.cc/paper_files/paper/2018/file/102f0bb6efb3a6128a3c750dd16729be-Paper.pdf,2018,"large, training, batch, hessian, show, size, points, robust, spectrum, study","training, batch, hessian, adversarial, large, results, spectrum, model, size, 100","{'hessian': 0.43350167794856115, 'adversaries': 0.40102760044117575, 'batch': 0.37963747241555806, 'robustness': 0.35090398831112496, 'analysis': 0.31959639761172665, 'large': 0.31615924987979493, 'based': 0.30683941085398314, 'training': 0.2962194907855996}","hessian, batch, spectrum, large, training, size, points, saddle, robust, study"
Adaptive Online Learning in Dynamic Environments,"Lijun Zhang, Shiyin Lu, Zhi-Hua Zhou","In this paper, we study online convex optimization in dynamic environments, and aim to bound the dynamic regret with respect to any sequence of comparators. Existing work have shown that online gradient descent enjoys an $O(\sqrt{T}(1+P_T))$ dynamic regret, where $T$ is the number of iterations and $P_T$ is the path-length of the comparator sequence.  However, this result is unsatisfactory, as there exists a large gap from the $\Omega(\sqrt{T(1+P_T)})$ lower bound established in our paper. To address this limitation, we develop a novel online method, namely adaptive learning for dynamic environment (Ader), which achieves an optimal $O(\sqrt{T(1+P_T)})$ dynamic regret. The basic idea is to maintain a set of experts, each attaining an optimal dynamic regret for a specific path-length, and combines them with an expert-tracking algorithm.  Furthermore, we propose an improved Ader based on the surrogate loss, and in this way the number of gradient evaluations per round is reduced from $O(\log T)$ to $1$. Finally, we extend Ader to the setting that a sequence of dynamical models is available to characterize the comparators.",https://proceedings.neurips.cc/paper_files/paper/2018/file/10a5ab2db37feedfdeaab192ead4ac0e-Paper.pdf,2018,"dynamic, p_t, regret, ader, online, sequence, sqrt, bound, comparators, gradient","regret, ft, algorithm, dynamic, xη, step, bound, xt, al, et","{'environments': 0.5483033038673879, 'dynamic': 0.5225542967477966, 'adaptive': 0.4451326312736507, 'online': 0.42803221688316845, 'learning': 0.21200437685104276}","p_t, dynamic, ader, regret, comparators, sqrt, online, sequence, length, path"
Learning in Games with Lossy Feedback,"Zhengyuan Zhou, Panayotis Mertikopoulos, Susan Athey, Nicholas Bambos, Peter W. Glynn, Yinyu Ye","We consider a game-theoretical multi-agent learning problem where the feedback information can be lost during the learning process and rewards are given by a broad class of games known as variationally stable games. We propose a simple variant of the classical online gradient descent algorithm, called reweighted online gradient descent (ROGD) and show that in variationally stable games, if each agent adopts ROGD, then almost sure convergence to the set of Nash equilibria is guaranteed, even when the feedback loss is asynchronous and arbitrarily corrrelated among agents. We then extend the framework to deal with unknown feedback loss probabilities by using an estimator (constructed from past data) in its replacement. Finally, we further extend the framework to accomodate both asynchronous loss and stochastic rewards and establish that multi-agent ROGD learning still converges to the set of Nash equilibria in such settings. Together, these results contribute to the broad lanscape of multi-agent online learning by significantly relaxing the feedback information that is required to achieve desirable outcomes.",https://proceedings.neurips.cc/paper_files/paper/2018/file/10c66082c124f8afe3df4886f5e516e0-Paper.pdf,2018,"agent, feedback, learning, games, loss, multi, online, rogd, asynchronous, broad","agent, games, learning, nash, feedback, convergence, multi, online, loss, ar","{'lossy': 0.6222205511389356, 'feedback': 0.5589730781399026, 'games': 0.5036651162160132, 'learning': 0.21612990159224119}","rogd, feedback, agent, variationally, games, online, asynchronous, equilibria, broad, nash"
Statistical Optimality of Stochastic Gradient Descent on Hard Learning Problems through Multiple Passes,"Loucas Pillaud-Vivien, Alessandro Rudi, Francis Bach","We consider stochastic gradient descent (SGD) for least-squares regression with potentially several passes over the data. While several passes have been widely reported to perform practically better in terms of predictive performance on unseen data, the existing theoretical analysis of SGD suggests that a single pass is statistically optimal. While this is true for low-dimensional easy problems, we show that for hard problems, multiple passes lead to statistically optimal predictions while single pass does not; we also show that in these hard models, the optimal number of passes over the data increases with sample size. In order to define the notion of hardness and show that our predictive performances are optimal, we consider potentially infinite-dimensional models and notions typically associated to kernel methods, namely, the decay of eigenvalues of the covariance matrix of the features and the complexity of the optimal predictor as measured through the covariance matrix.
We illustrate our results on synthetic experiments with non-linear kernel methods and on a classical benchmark with a linear model.",https://proceedings.neurips.cc/paper_files/paper/2018/file/10ff0b5e85e5b85cc3095d431d8c08b4-Paper.pdf,2018,"optimal, passes, data, show, consider, covariance, dimensional, hard, kernel, linear","2r, optimal, passes, number, problems, data, pass, rate, gradient, section","{'passes': 0.4179277391019337, 'optimality': 0.39447231029265173, 'hard': 0.36492196474478283, 'problems': 0.3454576497284007, 'statistical': 0.330919561090412, 'multiple': 0.3142776695067362, 'descent': 0.2741803491137784, 'gradient': 0.24044638308858712, 'stochastic': 0.23911648071894778, 'learning': 0.13702096700013652}","passes, optimal, statistically, pass, covariance, sgd, potentially, predictive, hard, kernel"
Multimodal Generative Models for Scalable Weakly-Supervised Learning,"Mike Wu, Noah Goodman","Multiple modalities often co-occur when describing natural phenomena. Learning a joint representation of these modalities should yield deeper and more useful representations.Previous generative approaches to multi-modal input either do not learn a joint distribution or require additional computation to handle missing data. Here, we introduce a multimodal variational autoencoder (MVAE) that uses a product-of-experts inference network and a sub-sampled training paradigm to solve the multi-modal inference problem. Notably, our model shares parameters to efficiently learn under any combination of missing modalities. We apply the MVAE on four datasets and match state-of-the-art performance using many fewer parameters. In addition, we show that the MVAE is directly applicable to weakly-supervised learning, and is robust to incomplete supervision. We then consider two case studies, one of learning image transformations---edge detection, colorization,  segmentation---as a set of modalities, followed by one of machine translation between two languages. We find appealing results across this range of tasks.",https://proceedings.neurips.cc/paper_files/paper/2018/file/1102a326d5f7c9e04fc3c89d0ede88c9-Paper.pdf,2018,"modalities, learning, mvae, inference, joint, learn, missing, modal, multi, one","mvae, x1, x2, inference, log, modalities, model, modal, xi, data","{'multimodal': 0.49280756989722335, 'weakly': 0.49280756989722335, 'scalable': 0.40579792154991895, 'supervised': 0.3764954519701805, 'generative': 0.32173842903278016, 'models': 0.2739399661847515, 'learning': 0.17117797120464395}","modalities, mvae, modal, missing, joint, colorization, shares, multi, parameters, followed"
Multi-Class Learning: From Theory to Algorithm,"Jian Li, Yong Liu, Rong Yin, Hua Zhang, Lizhong Ding, Weiping Wang","In this paper, we study the generalization performance of multi-class classification and obtain a shaper data-dependent generalization error bound with fast convergence rate, substantially improving the state-of-art bounds in the existing data-dependent generalization analysis. The theoretical analysis motivates us to devise two effective multi-class kernel learning algorithms with statistical guarantees. Experimental results show that our proposed methods can significantly outperform the existing multi-class classification methods.",https://proceedings.neurips.cc/paper_files/paper/2018/file/1141938ba2c2b13f5505d7c424ebae5f-Paper.pdf,2018,"class, generalization, multi, analysis, classification, data, dependent, existing, methods, algorithms","class, multi, rademacher, complexity, mkl, generalization, bound, log, bounds, classiﬁcation","{'class': 0.5697451750592029, 'theory': 0.509280867011292, 'algorithm': 0.4739115141712118, 'multi': 0.3856679912070651, 'learning': 0.2066192422966485}","generalization, multi, class, dependent, shaper, classification, motivates, analysis, existing, devise"
Learning and Inference in Hilbert Space with Quantum Graphical Models,"Siddarth Srinivasan, Carlton Downey, Byron Boots","Quantum Graphical Models (QGMs) generalize classical graphical models by adopting the formalism for reasoning about uncertainty from quantum mechanics. Unlike classical graphical models, QGMs represent uncertainty with density matrices in complex Hilbert spaces. Hilbert space embeddings (HSEs) also generalize Bayesian inference in Hilbert spaces. We investigate the link between QGMs and HSEs and show that the sum rule and Bayes rule for QGMs are equivalent to the kernel sum rule in HSEs and a special case of Nadaraya-Watson kernel regression, respectively. We show that these operations can be kernelized, and use these insights to propose a Hilbert Space Embedding of Hidden Quantum Markov Models (HSE-HQMM) to model dynamics. We present experimental results showing that HSE-HQMMs are competitive with state-of-the-art models like LSTMs and PSRNNs on several datasets, while also providing a nonparametric method for maintaining a probability distribution over continuous-valued features.",https://proceedings.neurips.cc/paper_files/paper/2018/file/11704817e347269b7254e744b5e22dac-Paper.pdf,2018,"models, hilbert, qgms, graphical, hses, quantum, rule, also, classical, generalize","quantum, matrix, density, rule, kernel, hilbert, space, al, et, model","{'hilbert': 0.48108691440926904, 'quantum': 0.48108691440926904, 'graphical': 0.4213100651947338, 'space': 0.4035798366799119, 'inference': 0.31191544489934825, 'models': 0.26742473353785257, 'learning': 0.16710677151094805}","qgms, hilbert, hses, quantum, rule, graphical, hse, models, spaces, classical"
Bayesian Structure Learning by Recursive Bootstrap,"Raanan Y. Rohekar, Yaniv Gurwicz, Shami Nisimov, Guy Koren, Gal Novik","We address the problem of Bayesian structure learning for domains with hundreds of variables by employing non-parametric bootstrap, recursively. We propose a method that covers both model averaging and model selection in the same framework. The proposed method deals with the main weakness of constraint-based learning---sensitivity to errors in the independence tests---by a novel way of combining bootstrap with constraint-based learning. Essentially, we provide an algorithm for learning a tree, in which each node represents a scored CPDAG for a subset of variables and the level of the node corresponds to the maximal order of conditional independencies that are encoded in the graph. As higher order independencies are tested in deeper recursive calls, they benefit from more bootstrap samples, and therefore are more resistant to the curse-of-dimensionality. Moreover, the re-use of stable low order independencies allows greater computational efficiency. We also provide an algorithm for sampling CPDAGs efficiently from their posterior given the learned tree. That is, not from the full posterior, but from a reduced space of CPDAGs encoded in the learned tree. We empirically demonstrate that the proposed algorithm scales well to hundreds of variables, and learns better MAP models and more reliable causal relationships between variables, than other state-of-the-art-methods.",https://proceedings.neurips.cc/paper_files/paper/2018/file/11e2ad6bf99300cd3808bb105b55d4b8-Paper.pdf,2018,"learning, variables, algorithm, bootstrap, independencies, order, tree, based, constraint, cpdags","rai, bootstrap, cpdags, cpdag, set, algorithm, tree, recursive, 10, tests","{'bootstrap': 0.5830105176499503, 'recursive': 0.5830105176499503, 'structure': 0.41520285140907087, 'bayesian': 0.33356824675517954, 'learning': 0.19114468226327105}","bootstrap, independencies, variables, cpdags, tree, hundreds, encoded, node, order, constraint"
Efficient Convex Completion of Coupled Tensors using Coupled Nuclear Norms,"Kishan Wimalawarne, Hiroshi Mamitsuka","Coupled norms have emerged as a convex method to solve coupled tensor completion. A limitation with coupled norms is that they only induce low-rankness using the multilinear rank of coupled tensors. In this paper, we introduce a new set of coupled norms known as coupled nuclear norms by constraining the CP rank of coupled tensors. We propose new coupled completion models using the coupled nuclear norms as regularizers, which can be optimized using computationally efficient optimization methods. We derive excess risk bounds for proposed coupled completion models and show that proposed norms lead to better performance. Through simulation and real-data experiments, we demonstrate that proposed norms achieve better performance for coupled completion compared to existing coupled norms.",https://proceedings.neurips.cc/paper_files/paper/2018/file/12092a75caa75e4644fd2869f0b6c45a-Paper.pdf,2018,"coupled, norms, completion, proposed, using, better, models, new, nuclear, performance","coupled, norms, rank, tensor, completion, ccp, nuclear, tensors, norm, al","{'coupled': 0.6431722895105704, 'norms': 0.355714531570326, 'nuclear': 0.355714531570326, 'tensors': 0.3215861447552852, 'completion': 0.30162231825838, 'convex': 0.23764971112928962, 'using': 0.2035213243142488, 'efficient': 0.2024148315824481}","coupled, norms, completion, nuclear, tensors, rank, proposed, rankness, using, better"
Stein Variational Gradient Descent as Moment Matching,"Qiang Liu, Dilin Wang","Stein variational gradient descent (SVGD) is a non-parametric inference algorithm that evolves a set of particles to fit a given distribution of interest. We analyze the non-asymptotic properties of SVGD, showing that there exists a set of functions, which we call the Stein matching set, whose expectations are exactly estimated by any set of particles that satisfies the fixed point equation of SVGD. This set is the image of Stein operator applied on the feature maps of the positive definite kernel used in SVGD. Our results provide a theoretical framework for analyzing the properties of SVGD with different kernels, shedding insight into optimal kernel choice. In particular, we show that SVGD with linear kernels yields exact estimation of means and variances on Gaussian distributions, while random Fourier features enable probabilistic bounds for distributional approximation. Our results offer a refreshing view of the classical inference problem as fitting Stein’s identity or solving the Stein equation, which may motivate more efficient algorithms.",https://proceedings.neurips.cc/paper_files/paper/2018/file/125b93c9b50703fe9dac43ec231f5f83-Paper.pdf,2018,"svgd, set, stein, equation, inference, kernel, kernels, non, particles, properties","stein, svgd, set, ﬁxed, al, et, features, point, functions, kernel","{'moment': 0.5269560185240793, 'stein': 0.4763981772255771, 'matching': 0.42584033592707493, 'descent': 0.34570805335153754, 'variational': 0.3203027208826198, 'gradient': 0.30317362751069693}","svgd, stein, set, particles, equation, kernels, kernel, refreshing, properties, expectations"
Data-Driven Clustering via Parameterized Lloyd's Families,"Maria-Florina F. Balcan, Travis Dick, Colin White","Algorithms for clustering points in metric spaces is a long-studied area of research. Clustering has seen a multitude of work both theoretically, in understanding the approximation guarantees possible for many objective functions such as k-median and k-means clustering, and experimentally, in finding the fastest algorithms and seeding procedures for Lloyd's algorithm. The performance of a given clustering algorithm depends on the specific application at hand, and this may not be known up front. For example, a ""typical instance"" may vary depending on the application, and different clustering heuristics perform differently depending on the instance.In this paper, we define an infinite family of algorithms generalizing Lloyd's algorithm, with one parameter controlling the the initialization procedure, and another parameter controlling the local search procedure. This family of algorithms includes the celebrated k-means++ algorithm, as well as the classic farthest-first traversal algorithm. We design efficient learning algorithms which receive samples from an application-specific distribution over clustering instances and learn a near-optimal clustering algorithm from the class. We show the best parameters vary significantly across datasets such as MNIST, CIFAR, and mixtures of Gaussians. Our learned algorithms never perform worse than k-means++, and on some datasets we see significant improvements.",https://proceedings.neurips.cc/paper_files/paper/2018/file/128ac9c427302b7a64314fc4593430b2-Paper.pdf,2018,"clustering, algorithm, algorithms, application, means, controlling, datasets, depending, family, instance","clustering, algorithm, αℓ, number, cost, log, centers, instance, dn, function","{'lloyd': 0.47229109168175654, 'families': 0.42697797783437863, 'parameterized': 0.42697797783437863, 'driven': 0.400471505442754, 'clustering': 0.3450810984885636, 'data': 0.2673144137712776, 'via': 0.24780103024693642}","clustering, algorithms, lloyd, means, application, algorithm, vary, depending, controlling, instance"
Semi-Supervised Learning with Declaratively Specified Entropy Constraints,"Haitian Sun, William W. Cohen, Lidong Bing","We propose a technique for declaratively specifying strategies for semi-supervised learning (SSL). SSL methods based on different assumptions perform differently on different tasks, which leads to difficulties applying them in practice. In this paper, we propose to use entropy to unify many types of constraints. Our method can be used to easily specify ensembles of semi-supervised learners, as well as agreement constraints and entropic regularization constraints between these learners, and can be used to model both well-known heuristics such as co-training, and novel domain-specific heuristics. Besides, our model is flexible as to the underlying learning mechanism. Compared to prior frameworks for specifying SSL techniques, our technique achieves consistent improvements on a suite of well-studied SSL benchmarks, and obtains a new state-of-the-art result on a difficult relation extraction task.",https://proceedings.neurips.cc/paper_files/paper/2018/file/12b1e42dc0746f22cf361267de07073f-Paper.pdf,2018,"ssl, constraints, well, different, heuristics, learners, learning, model, propose, semi","ssl, examples, relation, constraints, learner, training, xi, supervised, unlabeled, entropy","{'declaratively': 0.4704996793925804, 'specified': 0.4704996793925804, 'constraints': 0.3802171993163451, 'semi': 0.3802171993163451, 'entropy': 0.35947788633833794, 'supervised': 0.33927903325166947, 'learning': 0.15425710000048976}","ssl, constraints, learners, specifying, heuristics, semi, well, supervised, technique, declaratively"
Bayesian Inference of Temporal Task Specifications from Demonstrations,"Ankit Shah, Pritish Kamath, Julie A. Shah, Shen Li","When observing task demonstrations, human apprentices are able to identify whether a given task is executed correctly long before they gain expertise in actually performing that task. Prior research into learning from demonstrations (LfD) has failed to capture this notion of the acceptability of an execution; meanwhile, temporal logics provide a flexible language for expressing task specifications. Inspired by this, we present Bayesian specification inference, a probabilistic model for inferring task specification as a temporal logic formula. We incorporate methods from probabilistic programming to define our priors, along with a domain-independent likelihood function to enable sampling-based inference. We demonstrate the efficacy of our model for inferring true specifications with over 90% similarity between the inferred specification and the ground truth, both within a synthetic domain and a real-world table setting task.",https://proceedings.neurips.cc/paper_files/paper/2018/file/13168e6a2e6c84b4b7de9390c0ef5ec5-Paper.pdf,2018,"task, specification, demonstrations, domain, inference, inferring, model, probabilistic, specifications, temporal","prior, demonstrations, formula, task, set, speciﬁcation, formulas, temporal, number, deﬁned","{'specifications': 0.49842072966543216, 'demonstrations': 0.4610834960864576, 'task': 0.4181210310474868, 'temporal': 0.4104200270255751, 'inference': 0.32315392288636263, 'bayesian': 0.3021266833825429}","specification, task, specifications, demonstrations, inferring, temporal, probabilistic, domain, acceptability, apprentices"
Stochastic Nested Variance Reduction for Nonconvex Optimization,"Dongruo Zhou, Pan Xu, Quanquan Gu","We study finite-sum nonconvex optimization problems, where the objective function is an average of $n$ nonconvex functions. We propose a new stochastic gradient descent algorithm based on nested variance reduction. Compared with conventional stochastic variance reduced gradient (SVRG) algorithm that uses two reference points to construct a semi-stochastic gradient with diminishing variance in each iteration, our algorithm uses $K+1$ nested reference points to build a semi-stochastic gradient to further reduce its variance in each iteration. For smooth nonconvex functions, the proposed algorithm converges to an $\epsilon$-approximate first-order stationary point (i.e., $\|\nabla F(\mathbf{x})\|_2\leq \epsilon$) within $\tilde O(n\land \epsilon^{-2}+\epsilon^{-3}\land n^{1/2}\epsilon^{-2})$\footnote{$\tilde O(\cdot)$ hides the logarithmic factors, and $a\land b$ means $\min(a,b)$.} number of stochastic gradient evaluations. This improves the best known gradient complexity of SVRG $O(n+n^{2/3}\epsilon^{-2})$ and that of SCSG $O(n\land \epsilon^{-2}+\epsilon^{-10/3}\land n^{2/3}\epsilon^{-2})$. For gradient dominated functions, our algorithm also achieves better gradient complexity than the state-of-the-art algorithms. Thorough experimental results on different nonconvex optimization problems back up our theory.",https://proceedings.neurips.cc/paper_files/paper/2018/file/136f951362dab62e64eb8e841183c2a9-Paper.pdf,2018,"epsilon, gradient, algorithm, land, stochastic, nonconvex, variance, functions, complexity, iteration","gradient, algorithm, stochastic, complexity, snvrg, reference, svrg, nonconvex, point, variance","{'nested': 0.5465862252957998, 'variance': 0.43279269799517894, 'reduction': 0.417610616068112, 'nonconvex': 0.39936509149930666, 'stochastic': 0.3127281641631068, 'optimization': 0.28678209716974684}","epsilon, land, gradient, nonconvex, variance, stochastic, nested, svrg, reference, tilde"
On Markov Chain Gradient Descent,"Tao Sun, Yuejiao Sun, Wotao Yin","Stochastic gradient methods are the workhorse (algorithms) of large-scale optimization problems in machine learning, signal processing, and other computational sciences and engineering. This paper studies Markov chain gradient descent, a variant of stochastic gradient descent where the random samples are taken on the trajectory of a Markov chain. Existing results of this method assume convex objectives and a reversible Markov chain and thus have their limitations. We establish new non-ergodic convergence under wider step sizes, for nonconvex problems, and for non-reversible finite-state Markov chains. Nonconvexity makes our method applicable to broader problem classes. Non-reversible finite-state Markov chains, on the other hand, can mix substatially faster. To obtain these results, we introduce a new technique that varies the mixing levels of the Markov chains. The reported numerical results validate our contributions.",https://proceedings.neurips.cc/paper_files/paper/2018/file/1371bccec2447b5aa6d96d2a540fb401-Paper.pdf,2018,"markov, chain, chains, gradient, non, results, reversible, descent, finite, method","markov, xk, chain, mcgd, convergence, reversible, non, time, results, state","{'chain': 0.6251978015545901, 'markov': 0.5244732689390732, 'descent': 0.4345474879294651, 'gradient': 0.38108264173780143}","markov, chains, reversible, chain, gradient, non, finite, nonconvexity, substatially, workhorse"
The Limit Points of (Optimistic) Gradient Descent in Min-Max Optimization,"Constantinos Daskalakis, Ioannis Panageas","Motivated by applications in Optimization, Game Theory, and the training of Generative Adversarial Networks, the convergence properties of first order methods in min-max problems have received extensive study. It has been recognized that they may cycle, and there is no good understanding of their limit points when they do not. When they converge, do they converge to local min-max solutions? We characterize the limit points of two basic first order methods, namely Gradient Descent/Ascent (GDA) and Optimistic Gradient Descent Ascent (OGDA).  We show that both dynamics avoid unstable critical points for almost all initializations. Moreover, for small step sizes and under mild assumptions, the set of  OGDA-stable critical points is a superset of GDA-stable critical points, which is a superset of local min-max solutions (strict in some cases). The connecting thread is that the behavior of these dynamics can be studied from a dynamical systems perspective.",https://proceedings.neurips.cc/paper_files/paper/2018/file/139c3c1b7ca46a9d4fd6d163d98af635-Paper.pdf,2018,"points, critical, max, min, ascent, converge, descent, dynamics, first, gda","gda, ogda, stable, max, min, points, dynamics, point, critical, local","{'limit': 0.4278075765374727, 'max': 0.4278075765374727, 'min': 0.4037976599497185, 'optimistic': 0.3867623530263002, 'points': 0.362752436438546, 'descent': 0.2806619894162011, 'gradient': 0.24613055036110013, 'optimization': 0.22446148165941746}","points, max, min, gda, ogda, superset, critical, ascent, limit, converge"
Empirical Risk Minimization in Non-interactive Local Differential Privacy Revisited,"Di Wang, Marco Gaboardi, Jinhui Xu","In this paper, we revisit the Empirical Risk Minimization problem in the non-interactive local model of differential privacy. In the case of constant or low dimensions ($p\ll n$), we first show that if the  loss function is $(\infty, T)$-smooth,  we can avoid a dependence of the  sample complexity, to achieve error $\alpha$, on the exponential of the dimensionality $p$ with base $1/\alpha$  ({\em i.e.,} $\alpha^{-p}$),
 which answers a question in \cite{smith2017interaction}.  Our approach is based on polynomial approximation. Then, we propose player-efficient algorithms with $1$-bit communication complexity and $O(1)$ computation cost for each player. The error bound is asymptotically the same as the original one. With some additional assumptions, we also give an efficient algorithm for the server. 
 In the case of high dimensions ($n\ll p$), we show that if the loss function is a convex generalized linear function,  the error  can be bounded by using the Gaussian width of the constrained set, instead of $p$, which improves the one in    
  \cite{smith2017interaction}.",https://proceedings.neurips.cc/paper_files/paper/2018/file/13f320e7b5ead1024ac95c3b208610db-Paper.pdf,2018,"alpha, error, function, case, cite, complexity, dimensions, efficient, loss, one","log, loss, algorithm, function, complexity, ldp, 19, functions, sample, convex","{'revisited': 0.41491026361458866, 'interactive': 0.3518163709201348, 'empirical': 0.3429634147388514, 'privacy': 0.3352946352754371, 'risk': 0.3352946352754371, 'differential': 0.32247938149819544, 'minimization': 0.32247938149819544, 'local': 0.29548682110586133, 'non': 0.25750235170068575}","alpha, smith2017interaction, player, cite, dimensions, error, function, case, complexity, loss"
Greedy Hash: Towards Fast Optimization for Accurate Hash Coding in CNN,"Shupeng Su, Chao Zhang, Kai Han, Yonghong Tian","To convert the input into binary code, hashing algorithm has been widely used for approximate nearest neighbor search on large-scale image sets due to its computation and storage efficiency. Deep hashing further improves the retrieval quality by combining the hash coding with deep neural network. However, a major difficulty in deep hashing lies in the discrete constraints imposed on the network output, which generally makes the optimization NP hard. In this work, we adopt the greedy principle to tackle this NP hard problem by iteratively updating the network toward the probable optimal discrete solution in each iteration. A hash coding layer is designed to implement our approach which strictly uses the sign function in forward propagation to maintain the discrete constraints, while in back propagation the gradients are transmitted intactly to the front layer to avoid the vanishing gradients. In addition to the theoretical derivation, we provide a new perspective to visualize and understand the effectiveness and efficiency of our algorithm. Experiments on benchmark datasets show that our scheme outperforms state-of-the-art hashing methods in both supervised and unsupervised tasks.",https://proceedings.neurips.cc/paper_files/paper/2018/file/13f3cf8c531952d72e5847c4183e6910-Paper.pdf,2018,"hashing, deep, discrete, network, algorithm, coding, constraints, efficiency, gradients, hard","hash, network, function, method, discrete, loss, training, 10, layer, hashing","{'hash': 0.6919772260564133, 'cnn': 0.32657063589382473, 'accurate': 0.30210687789445984, 'coding': 0.30210687789445984, 'greedy': 0.2859930124436203, 'towards': 0.25634683832470867, 'fast': 0.22698488239453352, 'optimization': 0.1815328221771116}","hashing, hash, discrete, np, coding, propagation, hard, gradients, constraints, efficiency"
Which Neural Net Architectures Give Rise to Exploding and Vanishing Gradients?,Boris Hanin,"We give a rigorous analysis of the statistical behavior of gradients in a randomly initialized fully connected network N with ReLU activations. Our results show that the empirical variance of the squares of the entries in the input-output Jacobian of N is exponential in a simple architecture-dependent constant beta, given by the sum of the reciprocals of the hidden layer widths. When beta is large, the gradients computed by N at initialization vary wildly. Our approach complements the mean field theory analysis of random networks. From this point of view, we rigorously compute finite width corrections to the statistics of gradients at the edge of chaos.",https://proceedings.neurips.cc/paper_files/paper/2018/file/13f9896df61279c928f19721878fac41-Paper.pdf,2018,"gradients, analysis, beta, activations, approach, architecture, behavior, chaos, complements, compute","evgp, nj, layer, jn, input, relu, act, nd, n0, nets","{'exploding': 0.39670657142325616, 'give': 0.39670657142325616, 'rise': 0.39670657142325616, 'vanishing': 0.39670657142325616, 'architectures': 0.3586452775485503, 'gradients': 0.3279163045989256, 'net': 0.32058398367384455, 'neural': 0.17766810698754062}","gradients, beta, chaos, wildly, complements, corrections, reciprocals, jacobian, widths, entries"
Learning a High Fidelity Pose Invariant Model for High-resolution Face Frontalization,"Jie Cao, Yibo Hu, Hongwen Zhang, Ran He, Zhenan Sun","Face frontalization refers to the process of synthesizing the frontal view of a face from a given profile.  Due to self-occlusion and appearance distortion in the wild, it is extremely challenging to recover faithful results and preserve texture details in a high-resolution. This paper proposes a High Fidelity Pose Invariant Model (HF-PIM) to produce photographic and identity-preserving results. HF-PIM frontalizes the profiles through a novel texture warping procedure and leverages a dense correspondence field to bind the 2D and 3D surface spaces. We decompose the prerequisite of warping into dense correspondence field estimation and facial texture map recovering, which are both well addressed by deep networks. Different from those reconstruction methods relying on 3D data, we also propose Adversarial Residual Dictionary Learning (ARDL) to supervise facial texture map recovering with only monocular images. Exhaustive experiments on both controlled and uncontrolled environments demonstrate that the proposed method not only boosts the performance of pose-invariant face recognition but also dramatically improves high-resolution frontalization appearances.",https://proceedings.neurips.cc/paper_files/paper/2018/file/1415db70fe9ddb119e23e9b2808cde38-Paper.pdf,2018,"texture, face, high, 3d, also, correspondence, dense, facial, field, frontalization","face, gan, methods, results, texture, 3d, recognition, frontalization, high, pim","{'high': 0.5509900568676445, 'fidelity': 0.3544589869309998, 'frontalization': 0.3544589869309998, 'face': 0.3345656255771069, 'resolution': 0.3204510610735851, 'pose': 0.29299459474363126, 'invariant': 0.28644313521617043, 'model': 0.2215937412225145, 'learning': 0.11621222667712997}","texture, face, frontalization, hf, pim, facial, warping, correspondence, recovering, dense"
Provably Correct Automatic Sub-Differentiation for Qualified Programs,"Sham M. Kakade, Jason D. Lee","The \emph{Cheap Gradient Principle}~\citep{Griewank:2008:EDP:1455489} --- the computational cost of computing a $d$-dimensional vector of  partial derivatives of a scalar function is nearly the same (often within a factor of $5$)  as that of simply computing the scalar function itself --- is of central importance in optimization; it allows us to quickly obtain (high-dimensional) gradients of scalar loss functions which are subsequently used in black box gradient-based optimization procedures. The current state of affairs is markedly different with regards to computing sub-derivatives: widely used ML libraries, including TensorFlow and PyTorch, do \emph{not} correctly compute (generalized) sub-derivatives even on simple differentiable examples. This work considers the question: is there a \emph{Cheap Sub-gradient Principle}?  Our main result shows that, under certain restrictions on our library of non-smooth functions (standard in non-linear programming), provably correct generalized sub-derivatives can be computed at a computational cost that is within a (dimension-free) factor of $6$ of the cost of computing the scalar function itself.",https://proceedings.neurips.cc/paper_files/paper/2018/file/142c65e00f4f7cf2e6c4c996e34005df-Paper.pdf,2018,"computing, derivatives, scalar, sub, cost, emph, function, gradient, cheap, computational","functions, function, set, algorithm, program, ad, fpxq, input, library, nonsmooth","{'qualified': 0.41455971722708573, 'correct': 0.3912933124760856, 'sub': 0.3912933124760856, 'differentiation': 0.37478553559609784, 'provably': 0.37478553559609784, 'automatic': 0.3515191308450977, 'programs': 0.3426736542855066}","derivatives, scalar, sub, computing, emph, cheap, cost, principle, generalized, factor"
CatBoost: unbiased boosting with categorical features,"Liudmila Prokhorenkova, Gleb Gusev, Aleksandr Vorobev, Anna Veronika Dorogush, Andrey Gulin","This paper presents the key algorithmic techniques behind CatBoost, a new gradient boosting toolkit. Their combination leads to CatBoost outperforming other publicly available boosting implementations in terms of quality on a variety of datasets. Two critical algorithmic advances introduced in CatBoost are the implementation of ordered boosting, a permutation-driven alternative to the classic algorithm, and an innovative algorithm for processing categorical features. Both techniques were created to fight a prediction shift caused by a special kind of target leakage present in all currently existing implementations of gradient boosting algorithms. In this paper, we provide a detailed analysis of this problem and demonstrate that proposed algorithms solve it effectively, leading to excellent empirical results.",https://proceedings.neurips.cc/paper_files/paper/2018/file/14491b756b3a51daac41c24863285549-Paper.pdf,2018,"boosting, catboost, algorithm, algorithmic, algorithms, gradient, implementations, paper, techniques, advances","boosting, ts, ordered, catboost, section, training, one, gradient, xk, xi","{'catboost': 0.48245154088024444, 'categorical': 0.48245154088024444, 'unbiased': 0.48245154088024444, 'boosting': 0.42126221286600435, 'features': 0.35250486538212594}","boosting, catboost, implementations, algorithmic, fight, innovative, leakage, toolkit, techniques, created"
Constrained Generation of Semantically Valid Graphs via Regularizing Variational Autoencoders,"Tengfei Ma, Jie Chen, Cao Xiao","Deep generative models have achieved remarkable success in various data domains, including images, time series, and natural languages. There remain, however, substantial challenges for combinatorial structures, including graphs. One of the key challenges lies in the difficulty of ensuring semantic validity in context. For example, in molecular graphs, the number of bonding-electron pairs must not exceed the valence of an atom; whereas in protein interaction networks, two proteins may be connected only when they belong to the same or correlated gene ontology terms. These constraints are not easy to be incorporated into a generative model. In this work, we propose a regularization framework for variational autoencoders as a step toward semantic validity. We focus on the matrix representation of graphs and formulate penalty terms that regularize the output distribution of the decoder to encourage the satisfaction of validity constraints. Experimental results confirm a much higher likelihood of sampling valid graphs in our approach, compared with others reported in the literature.",https://proceedings.neurips.cc/paper_files/paper/2018/file/1458e7509aa5f47ecfb92536e7dd1dc7-Paper.pdf,2018,"graphs, validity, challenges, constraints, generative, including, semantic, terms, achieved, approach","graphs, node, graph, constraints, one, regularization, vae, nodes, training, valid","{'semantically': 0.41716423864673197, 'valid': 0.41716423864673197, 'regularizing': 0.3937516599019404, 'constrained': 0.3242312313269891, 'autoencoders': 0.31370352445418775, 'graphs': 0.30480247512134095, 'generation': 0.29709203547510304, 'variational': 0.25356734906970047, 'via': 0.21887715000242955}","validity, graphs, challenges, semantic, constraints, atom, bonding, electron, ontology, valence"
"Deep, complex, invertible  networks for inversion of transmission effects in multimode optical fibres","Oisín Moran, Piergiorgio Caramazza, Daniele Faccio, Roderick Murray-Smith","We use complex-weighted, deep networks to invert the effects of multimode optical fibre distortion of a coherent input image. We generated experimental data based on collections of optical fibre responses to greyscale input images generated with coherent light, by measuring only image amplitude  (not amplitude and phase as is typical) at the output of \SI{1}{\metre} and \SI{10}{\metre} long, \SI{105}{\micro\metre} diameter multimode fibre. This data is made available as the {\it Optical fibre inverse problem} Benchmark collection. The experimental data is used to train complex-weighted models with a range of regularisation approaches. A {\it unitary regularisation} approach for complex-weighted networks is proposed which performs well in robustly inverting the fibre transmission matrix, which fits well with the physical theory. A key benefit of the unitary constraint is that it allows us to learn a forward unitary model and analytically invert it to solve the inverse problem. We demonstrate this approach, and show how it can improve performance by incorporating knowledge of the phase shift induced by the spatial light modulator.",https://proceedings.neurips.cc/paper_files/paper/2018/file/148510031349642de5ca0c544f31b2ef-Paper.pdf,2018,"fibre, complex, data, metre, optical, si, unitary, weighted, amplitude, approach","complex, image, phase, ﬁbre, images, layer, al, et, speckle, output","{'fibres': 0.36021254168456557, 'multimode': 0.36021254168456557, 'optical': 0.36021254168456557, 'transmission': 0.36021254168456557, 'invertible': 0.3399962726093221, 'complex': 0.3256526014314376, 'effects': 0.3256526014314376, 'inversion': 0.31452678570632703, 'deep': 0.1687795441200847, 'networks': 0.16086443605124576}","fibre, metre, si, optical, unitary, amplitude, multimode, weighted, regularisation, coherent"
Scalable Hyperparameter Transfer Learning,"Valerio Perrone, Rodolphe Jenatton, Matthias W. Seeger, Cedric Archambeau","Bayesian optimization (BO) is a model-based approach for gradient-free black-box function optimization, such as hyperparameter optimization. Typically, BO relies on conventional Gaussian process (GP) regression, whose algorithmic complexity is cubic in the number of evaluations. As a result, GP-based BO cannot leverage large numbers of past function evaluations, for example, to warm-start related BO runs. We propose a multi-task adaptive Bayesian linear regression model for transfer learning in BO, whose complexity is linear in the function evaluations: one Bayesian linear regression model is associated to each black-box function optimization problem (or task), while transfer learning is achieved by coupling the models through a shared deep neural net. Experiments show that the neural net learns a representation suitable for warm-starting the black-box optimization problems and that BO runs can be accelerated when the target black-box function (e.g., validation loss) is learned together with other related signals (e.g., training loss). The proposed method was found to be at least one order of magnitude faster that methods recently published in the literature.",https://proceedings.neurips.cc/paper_files/paper/2018/file/14c879f3f5d8ed93a09f6090d77c2cc3-Paper.pdf,2018,"bo, function, optimization, black, box, bayesian, evaluations, linear, model, regression","ablr, task, learning, transfer, nn, model, data, gp, bo, linear","{'hyperparameter': 0.6651133253145262, 'scalable': 0.5169439095217768, 'transfer': 0.4927899117249625, 'learning': 0.21806274739051199}","bo, black, box, optimization, warm, evaluations, function, regression, runs, net"
Wasserstein Distributionally Robust Kalman Filtering,"Soroosh Shafieezadeh Abadeh, Viet Anh Nguyen, Daniel Kuhn, Peyman Mohajerin Mohajerin Esfahani","We study a distributionally robust mean square error estimation problem over a nonconvex Wasserstein ambiguity set containing only normal distributions. We show that the optimal estimator and the least favorable distribution form a Nash equilibrium. Despite the non-convex nature of the ambiguity set, we prove that the estimation problem is equivalent to a tractable convex program. We further devise a Frank-Wolfe algorithm for this convex program whose direction-searching subproblem can be solved in a quasi-closed form. Using these ingredients, we introduce a distributionally robust Kalman filter that hedges against model risk.",https://proceedings.neurips.cc/paper_files/paper/2018/file/15212f24321aa2c3dc8e9acf820f3c15-Paper.pdf,2018,"convex, ambiguity, distributionally, estimation, form, problem, program, robust, set, algorithm","set, distribution, wasserstein, robust, ambiguity, yt, ﬁlter, algorithm, estimator, xt","{'distributionally': 0.4980102720839715, 'kalman': 0.4980102720839715, 'filtering': 0.47700034885405007, 'wasserstein': 0.40312137088177585, 'robust': 0.3375431310650679}","distributionally, ambiguity, convex, program, form, estimation, robust, hedges, subproblem, ingredients"
SPIDER: Near-Optimal Non-Convex Optimization via Stochastic Path-Integrated Differential Estimator,"Cong Fang, Chris Junchi Li, Zhouchen Lin, Tong Zhang","In this paper, we propose a new technique named \textit{Stochastic Path-Integrated Differential EstimatoR} (SPIDER), which can be used to track many deterministic quantities of interests with significantly reduced computational cost. 
Combining SPIDER with the method of normalized gradient descent, we propose SPIDER-SFO that solve non-convex stochastic optimization problems using stochastic gradients only. 
We provide a few error-bound results on its convergence rates.
Specially, we prove that the SPIDER-SFO algorithm achieves a gradient computation cost of $\mathcal{O}\left(  \min( n^{1/2} \epsilon^{-2}, \epsilon^{-3} ) \right)$ to find an $\epsilon$-approximate first-order stationary point. 
In addition, we prove that SPIDER-SFO nearly matches the algorithmic lower bound for finding stationary point under the gradient Lipschitz assumption in the finite-sum setting.
Our SPIDER technique can be further applied to find an $(\epsilon, \mathcal{O}(\ep^{0.5}))$-approximate second-order stationary point at a gradient computation cost of $\tilde{\mathcal{O}}\left(  \min( n^{1/2} \epsilon^{-2}+\epsilon^{-2.5}, \epsilon^{-3} ) \right)$.",https://proceedings.neurips.cc/paper_files/paper/2018/file/1543843a4723ed2ab08e18053ae6dc5b-Paper.pdf,2018,"epsilon, spider, gradient, cost, mathcal, point, sfo, stationary, stochastic, approximate","gradient, stochastic, xk, order, spider, stationary, point, algorithm, cost, bound","{'spider': 0.3799732554368582, 'estimator': 0.3586479524993651, 'integrated': 0.3435174092737233, 'path': 0.33178124817091853, 'near': 0.3221921063362302, 'differential': 0.2953254020077836, 'convex': 0.2538567485359321, 'optimal': 0.24713339474184393, 'non': 0.2358196830465586, 'stochastic': 0.2174009023727972}","spider, epsilon, sfo, mathcal, stationary, left, gradient, min, point, cost"
Generalized Zero-Shot Learning with Deep Calibration Network,"Shichen Liu, Mingsheng Long, Jianmin Wang, Michael I. Jordan","A technical challenge of deep learning is recognizing target classes without seen data. Zero-shot learning leverages semantic representations such as attributes or class prototypes to bridge source and target classes. Existing standard zero-shot learning methods may be prone to overfitting the seen data of source classes as they are blind to the semantic representations of target classes. In this paper, we study generalized zero-shot learning that assumes accessible to target classes for unseen data during training, and prediction on unseen data is made by searching on both source and target classes. We propose a novel Deep Calibration Network (DCN) approach towards this generalized zero-shot learning paradigm, which enables simultaneous calibration of deep networks on the confidence of source classes and uncertainty of target classes. Our approach maps visual features of images and semantic representations of class prototypes to a common embedding space such that the compatibility of seen data to both source and target classes are maximized. We show superior accuracy of our approach over the state of the art on benchmark datasets for generalized zero-shot learning, including AwA, CUB, SUN, and aPY.",https://proceedings.neurips.cc/paper_files/paper/2018/file/1587965fb4d4b5afe8428a4a024feb0d-Paper.pdf,2018,"classes, target, learning, data, shot, source, zero, approach, deep, generalized","classes, target, learning, shot, zero, source, deep, data, class, calibration","{'calibration': 0.5349870959005091, 'zero': 0.4323303589415458, 'generalized': 0.4087484835290059, 'shot': 0.4023051881578512, 'network': 0.3230377356506921, 'deep': 0.25067111137758846, 'learning': 0.17539981761058313}","classes, target, shot, source, zero, seen, prototypes, generalized, semantic, calibration"
Dual Policy Iteration,"Wen Sun, Geoffrey J. Gordon, Byron Boots, J. Bagnell","Recently, a novel class of Approximate Policy Iteration (API) algorithms have demonstrated impressive practical performance (e.g., ExIt from [1], AlphaGo-Zero from [2]). This new family of algorithms maintains, and alternately optimizes, two policies: a fast, reactive policy (e.g., a deep neural network) deployed at test time, and a slow, non-reactive policy (e.g., Tree Search), that can plan multiple steps ahead. The reactive policy is updated under supervision from the non-reactive policy, while the non-reactive policy is improved with guidance from the reactive policy. In this work we study this Dual Policy Iteration (DPI) strategy in an alternating optimization framework and provide a convergence analysis that extends existing API theory. We also develop a special instance of this framework which reduces the update of non-reactive policies to model-based optimal control using learned local models, and provides a theoretically sound way of unifying model-free and model-based RL approaches with unknown dynamics. We demonstrate the efficacy of our approach on various continuous control Markov Decision Processes.",https://proceedings.neurips.cc/paper_files/paper/2018/file/15e122e839dfdaa7ce969536f94aecf6-Paper.pdf,2018,"policy, reactive, non, model, algorithms, api, based, control, framework, iteration","policy, model, eq, improvement, reactive, based, local, search, using, mboc","{'iteration': 0.6544046715517212, 'dual': 0.5849558593795364, 'policy': 0.4791462912606462}","reactive, policy, api, non, iteration, policies, control, alternately, dpi, exit"
Recurrently Controlled Recurrent Networks,"Yi Tay, Anh Tuan Luu, Siu Cheung Hui","Recurrent neural networks (RNNs) such as long short-term memory and gated recurrent units are pivotal building blocks across a broad spectrum of sequence modeling problems. This paper proposes a recurrently controlled recurrent network (RCRN) for expressive and powerful sequence encoding. More concretely, the key idea behind our approach is to learn the recurrent gating functions using recurrent networks. Our architecture is split into two components - a controller cell and a listener cell whereby the recurrent controller actively influences the compositionality of the listener cell. We conduct extensive experiments on a myriad of tasks in the NLP domain such as sentiment analysis (SST, IMDb, Amazon reviews, etc.), question classification (TREC), entailment classification (SNLI, SciTail), answer selection (WikiQA, TrecQA) and reading comprehension (NarrativeQA). Across all 26 datasets, our results demonstrate that RCRN not only consistently outperforms BiLSTMs but also stacked BiLSTMs, suggesting that our controller architecture might be a suitable replacement for the widely adopted stacked architecture. Additionally, RCRN achieves state-of-the-art results on several well-established datasets.",https://proceedings.neurips.cc/paper_files/paper/2018/file/16026d60ff9b54410b3435b403afd226-Paper.pdf,2018,"recurrent, architecture, cell, controller, rcrn, across, bilstms, classification, datasets, listener","al, et, rcrn, model, bilstm, 2017, cell, recurrent, 2016, architecture","{'controlled': 0.616645936749172, 'recurrently': 0.616645936749172, 'recurrent': 0.4045488786669022, 'networks': 0.2753829736037148}","recurrent, rcrn, controller, cell, bilstms, listener, stacked, architecture, sequence, amazon"
"Modelling sparsity, heterogeneity, reciprocity and community structure in temporal interaction data","Xenia Miscouridou, Francois Caron, Yee Whye Teh","We propose a novel class of network models for temporal dyadic interaction data. Our objective is to capture important features often observed in social interactions: sparsity, degree heterogeneity, community structure and reciprocity. We use mutually-exciting Hawkes processes to model the interactions between each (directed) pair of individuals. The intensity of each process allows interactions to arise as responses to opposite interactions (reciprocity), or due to shared interests between individuals (community structure). For sparsity and degree heterogeneity, we build the non time dependent part of the intensity function on compound random measures following Todeschini et al., 2016.  We conduct experiments on real-world temporal interaction data and show that the proposed model outperforms competing approaches for link prediction, and leads to interpretable parameters.",https://proceedings.neurips.cc/paper_files/paper/2018/file/160c88652d47d0be60bfbfed25111412-Paper.pdf,2018,"interactions, community, data, degree, heterogeneity, individuals, intensity, interaction, model, reciprocity","model, interactions, interaction, hawkes, al, et, data, nodes, number, models","{'heterogeneity': 0.39080174731991174, 'reciprocity': 0.39080174731991174, 'community': 0.35330698110485503, 'interaction': 0.35330698110485503, 'modelling': 0.3412363624489948, 'sparsity': 0.33137394889574034, 'temporal': 0.3037415962339381, 'structure': 0.2783174486747416, 'data': 0.2211918493182294}","interactions, reciprocity, heterogeneity, intensity, degree, individuals, community, interaction, sparsity, temporal"
Decentralize and Randomize: Faster Algorithm for Wasserstein Barycenters,"Pavel Dvurechenskii, Darina Dvinskikh, Alexander Gasnikov, Cesar Uribe, Angelia Nedich","We study the decentralized distributed computation of discrete approximations for the regularized Wasserstein barycenter of a finite set of continuous probability measures distributedly stored over a network. We assume there is a network of agents/machines/computers, and each agent holds a private continuous probability measure and seeks to compute the barycenter of all the measures in the network by getting samples from its local measure and exchanging information with its neighbors. Motivated by this problem, we develop, and analyze, a novel accelerated primal-dual stochastic gradient method for general stochastic convex optimization problems with linear equality constraints. Then, we apply this method to the decen- tralized distributed optimization setting to obtain a new algorithm for the distributed semi-discrete regularized Wasserstein barycenter problem. Moreover, we show explicit non-asymptotic complexity for the proposed algorithm. Finally, we show the effectiveness of our method on the distributed computation of the regularized Wasserstein barycenter of univariate Gaussian and von Mises distributions, as well as some applications to image aggregation.",https://proceedings.neurips.cc/paper_files/paper/2018/file/161882dd2d19c716819081aee2c08b98-Paper.pdf,2018,"barycenter, distributed, method, network, regularized, wasserstein, algorithm, computation, continuous, discrete","problem, barycenter, algorithm, wasserstein, dual, stochastic, network, optimization, distributed, regularized","{'barycenters': 0.45867122408493854, 'decentralize': 0.45867122408493854, 'randomize': 0.45867122408493854, 'faster': 0.35649157329915343, 'wasserstein': 0.3504405409396587, 'algorithm': 0.3449163812025836}","barycenter, distributed, regularized, wasserstein, measures, measure, probability, discrete, computation, network"
Heterogeneous Multi-output Gaussian Process Prediction,"Pablo Moreno-Muñoz, Antonio Artés, Mauricio Álvarez","We present a novel extension of multi-output Gaussian processes for handling heterogeneous outputs. We assume that each output has its own likelihood function and use a vector-valued Gaussian process prior to jointly model the parameters in all likelihoods as latent functions. Our multi-output Gaussian process uses a covariance function with a linear model of coregionalisation form. Assuming conditional independence across the underlying latent functions together with an inducing variable framework, we are able to obtain tractable variational bounds amenable to stochastic variational inference.  We illustrate the performance of the model on synthetic data and two real datasets: a human behavioral study and a demographic high-dimensional dataset.",https://proceedings.neurips.cc/paper_files/paper/2018/file/165a59f7cf3b5c4396ba65953d679f17-Paper.pdf,2018,"gaussian, model, output, function, functions, latent, multi, process, variational, able","output, fd, al, et, function, gaussian, model, functions, outputs, uq","{'heterogeneous': 0.510351901005035, 'output': 0.45847565227018144, 'process': 0.40060887868124245, 'prediction': 0.36836839306333247, 'gaussian': 0.3516679711382182, 'multi': 0.33088956587538937}","gaussian, output, coregionalisation, variational, latent, demographic, inducing, multi, process, functions"
SNIPER: Efficient Multi-Scale Training,"Bharat Singh, Mahyar Najibi, Larry S. Davis","We present SNIPER, an algorithm for performing efficient multi-scale training in instance level visual recognition tasks. Instead of processing every pixel in an image pyramid, SNIPER processes context regions around ground-truth instances (referred to as chips) at the appropriate scale. For background sampling, these context-regions are generated using proposals extracted from a region proposal network trained with a short learning schedule. Hence, the number of chips generated per image during training adaptively changes based on the scene complexity. SNIPER only processes 30% more pixels compared to the commonly used single scale training at 800x1333 pixels on the COCO dataset. But, it also observes samples from extreme resolutions of the image pyramid, like 1400x2000 pixels. As SNIPER operates on resampled low resolution chips (512x512 pixels), it can have a batch size as large as 20 on a single GPU even with a ResNet-101 backbone. Therefore it can benefit from batch-normalization during training without the need for synchronizing batch-normalization statistics across GPUs. SNIPER brings training of instance level recognition tasks like object detection closer to the protocol for image classification and suggests that the commonly accepted guideline that it is important to train on high resolution images for instance level visual recognition tasks might not be correct. Our implementation based on Faster-RCNN with a ResNet-101 backbone obtains an mAP of 47.6% on the COCO dataset for bounding box detection and can process 5 images per second during inference with a single GPU. Code is available at https://github.com/MahyarNajibi/SNIPER/ .",https://proceedings.neurips.cc/paper_files/paper/2018/file/166cee72e93a992007a89b39eb29628b-Paper.pdf,2018,"sniper, training, image, pixels, batch, chips, instance, level, recognition, scale","training, sniper, image, scale, chips, proposals, scales, chip, negative, resolution","{'sniper': 0.6108721186672883, 'scale': 0.459370000742096, 'training': 0.3939929746191741, 'multi': 0.3738341443693798, 'efficient': 0.3476090124083291}","sniper, pixels, chips, pyramid, instance, batch, backbone, 101, image, recognition"
Soft-Gated Warping-GAN for Pose-Guided Person Image Synthesis,"Haoye Dong, Xiaodan Liang, Ke Gong, Hanjiang Lai, Jia Zhu, Jian Yin","Despite remarkable advances in image synthesis research, existing works often fail in manipulating images under the context of large geometric transformations. Synthesizing person images conditioned on arbitrary poses is one of the most representative examples where the generation quality largely relies on the capability of identifying and modeling arbitrary transformations on different body parts. Current generative models are often built on local convolutions and overlook the key challenges (e.g. heavy occlusions, different views or dramatic appearance changes) when distinct geometric changes happen for each part, caused by arbitrary pose manipulations. This paper aims to resolve these challenges induced by geometric variability and spatial displacements via a new Soft-Gated Warping Generative Adversarial Network (Warping-GAN), which is composed of two stages: 1) it first synthesizes a target part segmentation map given a target pose, which depicts the region-level spatial layouts for guiding image synthesis with higher-level structure constraints; 2) the Warping-GAN equipped with a soft-gated warping-block learns feature-level mapping to render textures from the original image into the generated segmentation map. Warping-GAN is capable of controlling different transformation degrees given distinct target poses. Moreover, the proposed warping-block is light-weight and flexible enough to be injected into any networks. Human perceptual studies and quantitative evaluations demonstrate the superiority of our Warping-GAN that significantly outperforms all existing methods on two large datasets.",https://proceedings.neurips.cc/paper_files/paper/2018/file/1700002963a49da13542e0726b7bb758-Paper.pdf,2018,"warping, gan, arbitrary, different, geometric, image, level, target, block, challenges","image, pose, warping, target, parsing, condition, images, person, transformation, results","{'soft': 0.3939226165849441, 'warping': 0.3718144313050466, 'person': 0.3561284242176692, 'gan': 0.3439614118710659, 'gated': 0.3256150969847579, 'pose': 0.3256150969847579, 'guided': 0.3119120536578741, 'synthesis': 0.287820904617483, 'image': 0.2631761925087329}","warping, gan, geometric, arbitrary, soft, gated, level, target, transformations, synthesis"
Delta-encoder: an effective sample synthesis method for few-shot object recognition,"Eli Schwartz, Leonid Karlinsky, Joseph Shtok, Sivan Harary, Mattias Marder, Abhishek Kumar, Rogerio Feris, Raja Giryes, Alex Bronstein","Learning to classify new categories based on just one or a few examples is a long-standing challenge in modern computer vision. In this work, we propose a simple yet effective method for few-shot (and one-shot) object recognition. Our approach is based on a modified auto-encoder, denoted delta-encoder, that learns to synthesize new samples for an unseen category just by seeing few examples from it. The synthesized samples are then used to train a classifier. The proposed approach learns to both extract transferable intra-class deformations, or ""deltas"", between same-class pairs of training examples, and to apply those deltas to the few provided examples of a novel class (unseen during training) in order to efficiently synthesize samples from that new class. The proposed method improves the state-of-the-art of one-shot object-recognition and performs comparably in the few-shot case.",https://proceedings.neurips.cc/paper_files/paper/2018/file/1714726c817af50457d810aae9d27a2e-Paper.pdf,2018,"class, examples, shot, new, one, samples, approach, based, deltas, encoder","shot, examples, samples, class, training, encoder, using, learning, sample, categories","{'delta': 0.413029919179213, 'effective': 0.37340251125267915, 'encoder': 0.36064533531356574, 'recognition': 0.3155690191600573, 'sample': 0.3155690191600573, 'shot': 0.31059455568834266, 'synthesis': 0.3017817204882736, 'method': 0.29783737974922925, 'object': 0.2906817697109705}","shot, examples, deltas, class, synthesize, unseen, samples, encoder, recognition, object"
A Linear Speedup Analysis of Distributed Deep Learning with Sparse and Quantized Communication,"Peng Jiang, Gagan Agrawal","The large communication overhead has imposed a bottleneck on the performance of distributed Stochastic Gradient Descent (SGD) for training deep neural networks.  Previous works have demonstrated the potential of using gradient sparsification and quantization to reduce the communication cost.  However,  there is still a lack of understanding about how sparse and quantized communication affects the convergence rate of the training algorithm. In this paper, we study the convergence rate of distributed SGD for non-convex optimization with two communication reducing strategies: sparse parameter averaging and gradient quantization.  We show that $O(1/\sqrt{MK})$ convergence rate can be achieved if the sparsification and quantization hyperparameters are configured properly.  We also propose a strategy called periodic quantized averaging (PQASGD) that further reduces the communication cost while preserving the $O(1/\sqrt{MK})$ convergence rate. Our evaluation validates our theoretical results and shows that our PQASGD can converge as fast as full-communication SGD with only $3\%-5\%$ communication data size.",https://proceedings.neurips.cc/paper_files/paper/2018/file/17326d10d511828f6b34fa6d751739e2-Paper.pdf,2018,"communication, convergence, rate, gradient, quantization, sgd, averaging, cost, distributed, mk","rate, convergence, training, gradient, quantization, sgd, distributed, communication, data, mk","{'quantized': 0.4586633248782633, 'speedup': 0.4586633248782633, 'communication': 0.35648543382393105, 'analysis': 0.3191688021318606, 'distributed': 0.3157362530401037, 'sparse': 0.3093822852071291, 'linear': 0.29342716318913725, 'deep': 0.21490919365418806, 'learning': 0.15037645607675013}","communication, quantization, mk, pqasgd, sgd, rate, convergence, sparsification, averaging, quantized"
Almost Optimal Algorithms for Linear Stochastic Bandits with Heavy-Tailed Payoffs,"Han Shao, Xiaotian Yu, Irwin King, Michael R. Lyu","In linear stochastic bandits, it is commonly assumed that payoffs are with sub-Gaussian noises. In this paper, under a weaker assumption on noises, we study the problem of \underline{lin}ear stochastic {\underline b}andits with h{\underline e}avy-{\underline t}ailed payoffs (LinBET), where the distributions have finite moments of order $1+\epsilon$, for some $\epsilon\in (0,1]$. We rigorously analyze the regret lower bound of LinBET as $\Omega(T^{\frac{1}{1+\epsilon}})$, implying that finite moments of order 2 (i.e., finite variances) yield the bound of $\Omega(\sqrt{T})$, with $T$ being the total number of rounds to play bandits. The provided lower bound also indicates that the state-of-the-art algorithms for LinBET are far from optimal. By adopting median of means with a well-designed allocation of decisions and truncation based on historical information, we develop two novel bandit algorithms, where the regret upper bounds match the lower bound up to polylogarithmic factors. To the best of our knowledge, we are the first to solve LinBET optimally in the sense of the polynomial order on $T$.  Our proposed algorithms are evaluated based on synthetic datasets, and outperform the state-of-the-art results.",https://proceedings.neurips.cc/paper_files/paper/2018/file/173f0f6bb0ee97cf5098f73ee94029d4-Paper.pdf,2018,"bound, linbet, underline, algorithms, epsilon, finite, lower, order, art, bandits","payoffs, bound, algorithms, regret, linbet, et, al, algorithm, menu, lower","{'payoffs': 0.4164667295203332, 'almost': 0.39309329719758396, 'tailed': 0.39309329719758396, 'heavy': 0.3765095830469542, 'bandits': 0.31819486009318315, 'algorithms': 0.27086863401757594, 'optimal': 0.27086863401757594, 'linear': 0.2664321396053274, 'stochastic': 0.23828056714642523}","linbet, underline, payoffs, bound, epsilon, noises, finite, bandits, moments, lower"
Learning towards Minimum Hyperspherical Energy,"Weiyang Liu, Rongmei Lin, Zhen Liu, Lixin Liu, Zhiding Yu, Bo Dai, Le Song","Neural networks are a powerful class of nonlinear functions that can be trained end-to-end on various applications. While the over-parametrization nature in many neural networks renders the ability to fit complex functions and the strong representation power to handle challenging tasks, it also leads to highly correlated neurons that can hurt the generalization ability and incur unnecessary computation cost. As a result, how to regularize the network to avoid undesired representation redundancy becomes an important issue. To this end, we draw inspiration from a well-known problem in physics -- Thomson problem, where one seeks to find a state that distributes N electrons on a unit sphere as evenly as possible with minimum potential energy. In light of this intuition, we reduce the redundancy regularization problem to generic energy minimization, and propose a minimum hyperspherical energy (MHE) objective as generic regularization for neural networks. We also propose a few novel variants of MHE, and provide some insights from a theoretical point of view. Finally, we apply neural networks with MHE regularization to several challenging tasks. Extensive experiments demonstrate the effectiveness of our intuition, by showing the superior performance with MHE regularization.",https://proceedings.neurips.cc/paper_files/paper/2018/file/177540c7bcb8db31697b601642eac8d4-Paper.pdf,2018,"mhe, networks, neural, regularization, end, energy, problem, ability, also, challenging","mhe, neurons, energy, space, half, hyperspherical, regularization, layer, network, sphereface","{'hyperspherical': 0.530943564576234, 'minimum': 0.530943564576234, 'energy': 0.501145329581292, 'towards': 0.3933820333470696, 'learning': 0.17407411337918158}","mhe, energy, regularization, intuition, redundancy, end, generic, networks, minimum, neural"
Learning a latent manifold of odor representations from neural responses in piriform cortex,"Anqi Wu, Stan Pashkovski, Sandeep R. Datta, Jonathan W. Pillow","A major difficulty in studying the neural mechanisms underlying olfactory perception is the lack of obvious structure in the relationship between odorants and the neural activity patterns they elicit. Here we use odor-evoked responses in piriform cortex to identify a latent manifold specifying latent distance relationships between olfactory stimuli. Our approach is based on the Gaussian process latent variable model, and seeks to map odorants to points in a low-dimensional embedding space, where distances between points in the embedding space relate to the similarity of population responses they elicit. The model is specified by an explicit continuous mapping from a latent embedding space to the space of high-dimensional neural population firing rates via nonlinear tuning curves, each parametrized by a Gaussian process. Population responses are then generated by the addition of correlated, odor-dependent Gaussian noise. We fit this model to large-scale calcium fluorescence imaging measurements of population activity in layers 2 and 3 of mouse piriform cortex following the presentation of a diverse set of odorants. The model identifies a low-dimensional embedding of each odor, and a smooth tuning curve over the latent embedding space that accurately captures each neuron's response to different odorants. The model captures both signal and noise correlations across more than 500 neurons. We validate the model using a cross-validation analysis known as co-smoothing to show that the model can accurately predict the responses of a population of held-out neurons to test odorants.",https://proceedings.neurips.cc/paper_files/paper/2018/file/17b3c7061788dbe82de5abe9f6fe22b3-Paper.pdf,2018,"model, embedding, latent, odorants, population, space, responses, dimensional, gaussian, neural","latent, noise, odor, neural, covariance, model, trial, neurons, odors, matrix","{'cortex': 0.4151824834216371, 'odor': 0.4151824834216371, 'piriform': 0.4151824834216371, 'responses': 0.3753485516154479, 'manifold': 0.33551461980925873, 'latent': 0.30761335184248007, 'representations': 0.28580476033967966, 'neural': 0.18594268710816703, 'learning': 0.13612091286928327}","odorants, population, embedding, odor, responses, latent, space, olfactory, piriform, model"
Global Gated Mixture of Second-order Pooling for Improving Deep Convolutional Neural Networks,"Qilong Wang, Zilin Gao, Jiangtao Xie, Wangmeng Zuo, Peihua Li","In most of existing deep convolutional neural networks (CNNs) for classification, global average (first-order) pooling (GAP) has become a standard module to summarize activations of the last convolution layer as final representation for prediction. Recent researches show integration of higher-order pooling (HOP) methods clearly improves performance of deep CNNs. However, both GAP and existing HOP methods assume unimodal distributions, which cannot fully capture statistics of convolutional activations, limiting representation ability of deep CNNs, especially for samples with complex contents. To overcome the above limitation, this paper proposes a global Gated Mixture of Second-order Pooling (GM-SOP) method to further improve representation ability of deep CNNs. To this end, we introduce a sparsity-constrained gating mechanism and propose a novel parametric SOP as component of mixture model. Given a bank of SOP candidates, our method can adaptively choose Top-K (K > 1) candidates for each input sample through the sparsity-constrained gating module, and performs weighted sum of outputs of K selected candidates as representation of the sample. The proposed GM-SOP can flexibly accommodate a large number of personalized SOP candidates in an efficient way, leading to richer representations. The deep networks with our GM-SOP can be end-to-end trained, having potential to characterize complex, multi-modal distributions. The proposed method is evaluated on two large scale image benchmarks (i.e., downsampled ImageNet-1K and Places365), and experimental results show our GM-SOP is superior to its counterparts and achieves very competitive performance. The source code will be available at http://www.peihuali.org/GM-SOP.",https://proceedings.neurips.cc/paper_files/paper/2018/file/17c276c8e723eb46aef576537e9d56d0-Paper.pdf,2018,"sop, deep, gm, candidates, cnns, representation, end, method, order, pooling","sop, gm, 16, mixture, sr, gap, resnet, cms, 18, parametric","{'pooling': 0.3873692193275534, 'second': 0.3710269910324427, 'gated': 0.33923714439359165, 'mixture': 0.33923714439359165, 'global': 0.32496083678144394, 'improving': 0.3189756685521676, 'order': 0.30861860848633327, 'convolutional': 0.2741861198874937, 'deep': 0.19229622649241057, 'neural': 0.18380184339411432}","sop, gm, candidates, cnns, pooling, hop, representation, gating, deep, end"
Neural Code Comprehension: A Learnable Representation of Code Semantics,"Tal Ben-Nun, Alice Shoshana Jakobovits, Torsten Hoefler","With the recent success of embeddings in natural language processing, research has been conducted into applying similar methods to code analysis. Most works attempt to process the code directly or use a syntactic tree representation, treating it like sentences written in a natural language. However, none of the existing methods are sufficient to comprehend program semantics robustly, due to structural features such as function calls, branching, and interchangeable order of statements. In this paper, we propose a novel processing technique to learn code semantics, and apply it to a variety of program analysis tasks. In particular, we stipulate that a robust distributional hypothesis of code applies to both human- and machine-generated programs. Following this hypothesis, we define an embedding space, inst2vec, based on an Intermediate Representation (IR) of the code that is independent of the source programming language. We provide a novel definition of contextual flow for this IR, leveraging both the underlying data- and control-flow of the program. We then analyze the embeddings qualitatively using analogies and clustering, and evaluate the learned representation on three different high-level tasks. We show that even without fine-tuning, a single RNN architecture and fixed inst2vec embeddings outperform specialized approaches for performance prediction (compute device mapping, optimal thread coarsening); and algorithm classification from raw code (104 classes), where we set a new state-of-the-art.",https://proceedings.neurips.cc/paper_files/paper/2018/file/17c3433fecc21b57000debdf7ad5c930-Paper.pdf,2018,"code, embeddings, language, program, representation, analysis, flow, hypothesis, inst2vec, ir","code, statements, inst2vec, data, ir, llvm, id, context, statement, different","{'code': 0.7063154880313061, 'learnable': 0.368712904735444, 'semantics': 0.368712904735444, 'comprehension': 0.35315774401565303, 'representation': 0.2718312802479771, 'neural': 0.17494965576051025}","code, inst2vec, embeddings, program, ir, language, semantics, flow, hypothesis, representation"
PAC-Bayes Tree: weighted subtrees with guarantees,"Tin D. Nguyen, Samory Kpotufe","We present a weighted-majority classification approach over subtrees of a fixed tree, which provably achieves excess-risk of the same order as the best tree-pruning. Furthermore, the computational efficiency of pruning is maintained at both training and testing time despite having to aggregate over an exponential number of subtrees. We believe this is the first subtree aggregation approach with such guarantees.",https://proceedings.neurips.cc/paper_files/paper/2018/file/1819020b02e926785cf3be594d957696-Paper.pdf,2018,"approach, pruning, subtrees, tree, achieves, aggregate, aggregation, believe, best, classification","t0, ht, tree, bayes, pruning, log, hq, pac, risk, subtrees","{'subtrees': 0.476322485632651, 'weighted': 0.4306225869903068, 'guarantees': 0.39372654888387665, 'bayes': 0.3849226883479627, 'pac': 0.37715713300524883, 'tree': 0.37715713300524883}","subtrees, pruning, tree, subtree, maintained, aggregate, believe, excess, majority, aggregation"
Amortized Inference Regularization,"Rui Shu, Hung H. Bui, Shengjia Zhao, Mykel J. Kochenderfer, Stefano Ermon","The variational autoencoder (VAE) is a popular model for density estimation and representation learning. Canonically, the variational principle suggests to prefer an expressive inference model so that the variational approximation is accurate. However, it is often overlooked that an overly-expressive inference model can be detrimental to the test set performance of both the amortized posterior approximator and, more importantly, the generative density estimator. In this paper, we leverage the fact that VAEs rely on amortized inference and propose techniques for amortized inference regularization (AIR) that control the smoothness of the inference model. We demonstrate that, by applying AIR, it is possible to improve VAE generalization on both inference and generative performance. Our paper challenges the belief that amortized inference is simply a mechanism for approximating maximum likelihood training and illustrates that regularization of the amortization family provides a new direction for understanding and improving generalization in VAEs.",https://proceedings.neurips.cc/paper_files/paper/2018/file/1819932ff5cf474f4f19e7c7024640c2-Paper.pdf,2018,"inference, amortized, model, variational, air, density, expressive, generalization, generative, paper","inference, model, variational, amortized, regularization, likelihood, ln, importance, air, training","{'amortized': 0.7089417835027007, 'regularization': 0.5349081132676877, 'inference': 0.45964644887610323}","amortized, inference, air, vae, vaes, variational, expressive, density, regularization, model"
Structure-Aware Convolutional Neural Networks,"Jianlong Chang, Jie Gu, Lingfeng Wang, GAOFENG MENG, SHIMING XIANG, Chunhong Pan","Convolutional neural networks (CNNs) are inherently subject to invariable filters that can only aggregate local inputs with the same topological structures. It causes that CNNs are allowed to manage data with Euclidean or grid-like structures (e.g., images), not ones with non-Euclidean or graph structures (e.g., traffic networks). To broaden the reach of CNNs, we develop structure-aware convolution to eliminate the invariance, yielding a unified mechanism of dealing with both Euclidean and non-Euclidean structured data. Technically, filters in the structure-aware convolution are generalized to univariate functions, which are capable of aggregating local inputs with diverse topological structures. Since infinite parameters are required to determine a univariate function, we parameterize these filters with numbered learnable parameters in the context of the function approximation theory. By replacing the classical convolution in CNNs with the structure-aware convolution, Structure-Aware Convolutional Neural Networks (SACNNs) are readily established. Extensive experiments on eleven datasets strongly evidence that SACNNs outperform current models on various machine learning tasks, including image classification and clustering, text categorization, skeleton-based action recognition, molecular activity detection, and taxi flow prediction.",https://proceedings.neurips.cc/paper_files/paper/2018/file/182be0c5cdcd5072bb1864cdee4d3d6e-Paper.pdf,2018,"aware, cnns, convolution, euclidean, structure, structures, filters, networks, convolutional, data","sacnns, structure, convolution, local, aware, euclidean, ﬁlters, networks, data, parameters","{'aware': 0.5554985296658123, 'structure': 0.5090015178562646, 'convolutional': 0.4774971932612064, 'neural': 0.3200922948723217, 'networks': 0.31918049995289544}","aware, euclidean, convolution, cnns, structures, filters, sacnns, structure, univariate, topological"
"Alternating optimization of decision trees, with application to learning sparse oblique trees","Miguel A. Carreira-Perpinan, Pooya Tavallali","Learning a decision tree from data is a difficult optimization problem. The most widespread algorithm in practice, dating to the 1980s, is based on a greedy growth of the tree structure by recursively splitting nodes, and possibly pruning back the final tree. The parameters (decision function) of an internal node are approximately estimated by minimizing an impurity measure. We give an algorithm that, given an input tree (its structure and the parameter values at its nodes), produces a new tree with the same or smaller structure but new parameter values that provably lower or leave unchanged the misclassification error. This can be applied to both axis-aligned and oblique trees and our experiments show it consistently outperforms various other algorithms while being highly scalable to large datasets and trees. Further, the same algorithm can handle a sparsity penalty, so it can learn sparse oblique trees, having a structure that is a subset of the original tree and few nonzero parameters. This combines the best of axis-aligned and oblique trees: flexibility to model correlated data, low generalization error, fast inference and interpretable nodes that involve only a few features in their decision.",https://proceedings.neurips.cc/paper_files/paper/2018/file/185c29dc24325934ee377cfda20e414c-Paper.pdf,2018,"tree, structure, trees, algorithm, decision, nodes, oblique, aligned, axis, data","01, tree, trees, 02, 03, node, tao, oblique, nodes, error","{'trees': 0.6714110375059815, 'oblique': 0.37133232664505467, 'alternating': 0.33570551875299076, 'application': 0.32423624836138054, 'decision': 0.27923836422367293, 'sparse': 0.2504748855148144, 'optimization': 0.1948301264353296, 'learning': 0.12174428666699749}","tree, oblique, trees, nodes, axis, structure, aligned, decision, values, parameter"
Gradient Descent for Spiking Neural Networks,"Dongsung Huh, Terrence J. Sejnowski","Most large-scale network models use neurons with static nonlinearities that produce analog output, despite the fact that information processing in the brain is predominantly carried out by dynamic neurons that produce discrete pulses called spikes. Research in spike-based computation has been impeded by the lack of efficient supervised learning algorithm for spiking neural networks. Here, we present a gradient descent method for optimizing spiking network models by introducing a differentiable formulation of spiking dynamics and deriving the exact gradient calculation. For demonstration, we trained recurrent spiking networks on two dynamic tasks: one that requires optimizing fast (~ millisecond) spike-based interactions for efficient encoding of information, and a delayed-memory task over extended duration (~ second). The results show that the gradient descent approach indeed optimizes networks dynamics on the time scale of individual spikes as well as on behavioral time scales. In conclusion, our method yields a general purpose supervised learning algorithm for spiking neural networks, which can facilitate further investigations on spike-based computations.",https://proceedings.neurips.cc/paper_files/paper/2018/file/185e65bc40581880c4f2c82958de8cfe-Paper.pdf,2018,"spiking, networks, based, gradient, spike, algorithm, descent, dynamic, dynamics, efficient","time, input, synaptic, dynamics, current, network, spike, output, learning, neuron","{'spiking': 0.642690792893039, 'descent': 0.4663816814160556, 'gradient': 0.40900009354327904, 'neural': 0.3183804826097851, 'networks': 0.3174735638518566}","spiking, spike, spikes, neurons, networks, optimizing, produce, gradient, dynamic, dynamics"
Statistical and Computational Trade-Offs in Kernel K-Means,"Daniele Calandriello, Lorenzo Rosasco","We investigate the efficiency of k-means  in terms of both statistical and computational requirements.
More precisely,  we study  a Nystr\""om approach to kernel k-means. We analyze the statistical properties of the proposed method and show that it achieves  the same accuracy of exact kernel k-means with only a fraction of computations.
Indeed, we prove under basic assumptions  that sampling  $\sqrt{n}$ Nystr\""om  landmarks allows to greatly reduce computational costs without incurring in any loss of accuracy. To the best of our knowledge this is the first result showing in this kind for unsupervised learning.",https://proceedings.neurips.cc/paper_files/paper/2018/file/18903e4430783a191b0cfab439daaef8-Paper.pdf,2018,"means, accuracy, computational, kernel, nystr, om, statistical, achieves, allows, analyze","cn, φi, kernel, cj, eff, risk, µn, means, sampling, γπn","{'offs': 0.4514197951580649, 'trade': 0.4514197951580649, 'computational': 0.42608468605240174, 'means': 0.3941662233948765, 'kernel': 0.35743892186298615, 'statistical': 0.35743892186298615}","nystr, om, means, kernel, statistical, computational, incurring, landmarks, accuracy, kind"
The Spectrum of the Fisher Information Matrix of a Single-Hidden-Layer Neural Network,"Jeffrey Pennington, Pratik Worah","An important factor contributing to the success of deep learning has been the remarkable ability to optimize large neural networks using simple first-order optimization algorithms like stochastic gradient descent. While the efficiency of such methods depends crucially on the local curvature of the loss surface, very little is actually known about how this geometry depends on network architecture and hyperparameters. In this work, we extend a recently-developed framework for studying spectra of nonlinear random matrices to characterize an important measure of curvature, namely the eigenvalues of the Fisher information matrix. We focus on a single-hidden-layer neural network with Gaussian data and weights and provide an exact expression for the spectrum in the limit of infinite width. We find that linear networks suffer worse conditioning than nonlinear networks and that nonlinear networks are generically non-degenerate. We also predict and demonstrate empirically that by adjusting the nonlinearity, the spectrum can be tuned so as to improve the efficiency of first-order optimization methods.",https://proceedings.neurips.cc/paper_files/paper/2018/file/18bb68e2b38e4a8ce7cf4f6b2625768c-Paper.pdf,2018,"networks, nonlinear, curvature, depends, efficiency, first, important, methods, network, neural","fisher, spectrum, matrix, m2, order, function, networks, loss, network, neural","{'spectrum': 0.4115185009707418, 'fisher': 0.38842277891131793, 'single': 0.38842277891131793, 'hidden': 0.3593256102736065, 'layer': 0.3593256102736065, 'matrix': 0.30489866620949463, 'information': 0.2863622615668941, 'network': 0.24848450691729862, 'neural': 0.18430174422247028}","nonlinear, curvature, spectrum, depends, networks, efficiency, adjusting, generically, important, contributing"
Parameters as interacting particles: long time convergence and asymptotic error scaling of neural networks,"Grant Rotskoff, Eric Vanden-Eijnden","The performance of neural networks on high-dimensional data
  distributions suggests that it may be possible to parameterize a
  representation of a given high-dimensional function with
  controllably small errors, potentially outperforming standard
  interpolation methods.  We demonstrate, both theoretically and
  numerically, that this is indeed the case.  We map the parameters of
  a neural network to a system of particles relaxing with an
  interaction potential determined by the loss function.  We show that
  in the limit that the number of parameters $n$ is large, the
  landscape of the mean-squared error becomes convex and the
  representation error in the function scales as $O(n^{-1})$.
  In this limit, we prove a dynamical variant of the universal
  approximation theorem showing that the optimal
  representation can be attained by stochastic gradient
  descent, the algorithm ubiquitously used for parameter optimization
  in machine learning.  In the asymptotic regime, we study the
  fluctuations around the optimal representation and show that they
  arise at a scale $O(n^{-1})$.  These fluctuations in the landscape
  identify the natural scale for the noise in stochastic gradient
  descent.  Our results apply to both single and multi-layer neural
  networks, as well as standard kernel methods like radial basis
  functions.",https://proceedings.neurips.cc/paper_files/paper/2018/file/196f5641aa9dc87067da4ff90fd81e7b-Paper.pdf,2018,"representation, function, neural, descent, dimensional, error, fluctuations, gradient, high, landscape","function, fn, gradient, descent, neural, parameters, error, stochastic, 10, networks","{'interacting': 0.3638110501979363, 'parameters': 0.3638110501979363, 'particles': 0.3638110501979363, 'asymptotic': 0.3433928214240226, 'error': 0.3176688953917763, 'scaling': 0.3176688953917763, 'long': 0.30848762878561997, 'convergence': 0.2735824361472173, 'time': 0.245401557065234, 'neural': 0.16293559332258445}","representation, fluctuations, landscape, limit, function, controllably, radial, ubiquitously, descent, neural"
Uplift Modeling from Separate Labels,"Ikko Yamane, Florian Yger, Jamal Atif, Masashi Sugiyama","Uplift modeling is aimed at estimating the incremental impact of an action on an individual's behavior, which is useful in various application domains such as targeted marketing (advertisement campaigns) and personalized medicine (medical treatments). Conventional methods of uplift modeling require every instance to be jointly equipped with two types of labels: the taken action and its outcome. However, obtaining two labels for each instance at the same time is difficult or expensive in many real-world problems. In this paper, we propose a novel method of uplift modeling that is applicable to a more practical setting where only one type of labels is available for each instance. We show a mean squared error bound for the proposed estimator and demonstrate its effectiveness through experiments.",https://proceedings.neurips.cc/paper_files/paper/2018/file/198dd5fb9c43b2d29a548f8c77e85cf9-Paper.pdf,2018,"instance, labels, modeling, uplift, action, two, advertisement, aimed, applicable, application","uplift, data, treatment, individual, method, proposed, µw, samples, two, eq","{'uplift': 0.559573055751727, 'separate': 0.5281680430072253, 'labels': 0.4744808191154633, 'modeling': 0.42753299979546777}","uplift, instance, labels, modeling, action, advertisement, campaigns, marketing, treatments, aimed"
A Bridging Framework for Model Optimization and Deep Propagation,"Risheng Liu, Shichao Cheng, xiaokun liu, Long Ma, Xin Fan, Zhongxuan Luo","Optimizing task-related mathematical model is one of the most fundamental methodologies in statistic and learning areas. However, generally designed schematic iterations may hard to investigate complex data distributions in real-world applications. Recently, training deep propagations (i.e., networks) has gained promising performance in some particular tasks. Unfortunately, existing networks are often built in heuristic manners, thus lack of principled interpretations and solid theoretical supports. In this work, we provide a new paradigm, named Propagation and Optimization based Deep Model (PODM), to bridge the gaps between these different mechanisms (i.e., model optimization and deep propagation). On the one hand, we utilize PODM as a deeply trained solver for model optimization. Different from these existing network based iterations, which often lack theoretical investigations, we provide strict convergence analysis for PODM in the challenging nonconvex and nonsmooth scenarios. On the other hand, by relaxing the model constraints and performing end-to-end training, we also develop a PODM based strategy to integrate domain knowledge (formulated as models) and real data distributions (learned by networks), resulting in a generic ensemble framework for challenging real-world applications. Extensive experiments verify our theoretical results and demonstrate the superiority of PODM against these state-of-the-art approaches.",https://proceedings.neurips.cc/paper_files/paper/2018/file/1a0a283bfe7c549dee6c638a05200e32-Paper.pdf,2018,"model, podm, based, deep, networks, optimization, real, theoretical, applications, challenging","xl, podm, model, optimization, based, error, deep, network, eq, iterations","{'bridging': 0.5616436847125852, 'propagation': 0.47623657507142575, 'framework': 0.4223506884322786, 'model': 0.35111742096607435, 'optimization': 0.2946824239429968, 'deep': 0.2631612009409845}","podm, model, iterations, lack, propagation, theoretical, hand, optimization, real, deep"
Learning filter widths of spectral decompositions with wavelets,"Haidar Khan, Bulent Yener","Time series classification using deep neural networks, such as convolutional neural networks (CNN), operate on the spectral decomposition of the time series computed using a preprocessing step. This step can include a large number of hyperparameters, such as window length, filter widths, and filter shapes, each with a range of possible values that must be chosen using time and data intensive cross-validation procedures. We propose the wavelet deconvolution (WD) layer as an efficient alternative to this preprocessing step that eliminates a significant number of hyperparameters. The WD layer uses wavelet functions with adjustable scale parameters to learn the spectral decomposition directly from the signal. Using backpropagation, we show the scale parameters can be optimized with gradient descent. Furthermore, the WD layer adds interpretability to the learned time series classifier by exploiting the properties of the wavelet transform. In our experiments, we show that the WD layer can automatically extract the frequency content used to generate a dataset. The WD layer combined with a CNN applied to the phone recognition task on the TIMIT database achieves a phone error rate of 18.1\%, a relative improvement of 4\% over the baseline CNN. Experiments on a dataset where engineered features are not available showed WD+CNN is the best performing method. Our results show that the WD layer can improve neural network based time series classifiers both in accuracy and interpretability by learning directly from the input signal.",https://proceedings.neurips.cc/paper_files/paper/2018/file/1a3f91fead97497b1a96d6104ad339f6-Paper.pdf,2018,"wd, layer, time, cnn, series, using, neural, show, step, wavelet","layer, wd, signal, wavelet, network, parameters, time, using, frequency, scale","{'decompositions': 0.4606987831003705, 'filter': 0.4606987831003705, 'wavelets': 0.4606987831003705, 'widths': 0.4606987831003705, 'spectral': 0.3580674465290693, 'learning': 0.15104379740824636}","wd, layer, cnn, wavelet, series, phone, preprocessing, time, filter, step"
Learning a Warping Distance from Unlabeled Time Series Using Sequence Autoencoders,"Abubakar Abid, James Y. Zou","Measuring similarities between unlabeled time series trajectories is an important problem in many domains such as medicine, economics, and vision. It is often unclear what is the appropriate metric to use because of the complex nature of noise in the trajectories (e.g. different sampling rates or outliers). Experts typically hand-craft or manually select a specific metric, such as Dynamic Time Warping (DTW), to apply on their data. In this paper, we propose an end-to-end framework, autowarp, that optimizes and learns a good metric given unlabeled trajectories. We define a flexible and differentiable family of warping metrics, which encompasses common metrics such as DTW, Edit Distance, Euclidean, etc. Autowarp then leverages the representation power of sequence autoencoders to optimize for a member of this warping family. The output is an metric which is easy to interpret and can be robustly learned from relatively few  trajectories. In systematic experiments across different domains, we show that autowarp often outperforms hand-crafted trajectory similarity metrics.",https://proceedings.neurips.cc/paper_files/paper/2018/file/1a9dcba2349fef2bb823c39e45dd6c96-Paper.pdf,2018,"metric, trajectories, autowarp, metrics, warping, different, domains, dtw, end, family","distance, trajectories, warping, distances, latent, trajectory, betacv, time, data, autowarp","{'unlabeled': 0.4065278712412983, 'warping': 0.4065278712412983, 'sequence': 0.38937738289923673, 'distance': 0.3560152621204777, 'series': 0.3480546388776444, 'autoencoders': 0.32388238319811347, 'time': 0.2905202624193544, 'using': 0.2464241756620847, 'learning': 0.14120849696310692}","autowarp, trajectories, warping, metric, metrics, dtw, unlabeled, hand, family, domains"
Gen-Oja: Simple & Efficient Algorithm for Streaming Generalized Eigenvector Computation,"Kush Bhatia, Aldo Pacchiano, Nicolas Flammarion, Peter L. Bartlett, Michael I. Jordan","In this paper, we study the problems of principle Generalized Eigenvector computation and Canonical Correlation Analysis in the stochastic setting. We propose a simple and efficient algorithm for these problems. We prove the global convergence of our algorithm, borrowing ideas from the theory of fast-mixing Markov chains and two-Time-Scale Stochastic Approximation, showing that it achieves the optimal rate of convergence. In the process, we develop tools for understanding stochastic processes with Markovian noise which might be of independent interest.",https://proceedings.neurips.cc/paper_files/paper/2018/file/1b318124e37af6d74a03501474f44ea1-Paper.pdf,2018,"stochastic, algorithm, convergence, problems, achieves, analysis, approximation, borrowing, canonical, chains","oja, problem, algorithm, gen, step, eigenvector, generalized, stochastic, vt, convergence","{'gen': 0.3998837763286043, 'oja': 0.3998837763286043, 'eigenvector': 0.3615176512806493, 'streaming': 0.33054257138290527, 'computation': 0.3231515262326943, 'simple': 0.3166321623785846, 'generalized': 0.30552491530107134, 'algorithm': 0.30070878178166194, 'efficient': 0.2275487787377761}","stochastic, borrowing, chains, eigenvector, markovian, convergence, mixing, canonical, problems, correlation"
Heterogeneous Bitwidth Binarization in Convolutional Neural Networks,"Joshua Fromm, Shwetak Patel, Matthai Philipose","Recent work has shown that fast, compact low-bitwidth neural networks can
be surprisingly accurate. These networks use homogeneous binarization: all
parameters in each layer or (more commonly) the whole model have the same low
bitwidth (e.g., 2 bits). However, modern hardware allows efficient designs where
each arithmetic instruction can have a custom bitwidth, motivating heterogeneous
binarization, where every parameter in the network may have a different bitwidth.
In this paper, we show that it is feasible and useful to select bitwidths at the
parameter granularity during training. For instance a heterogeneously quantized
version of modern networks such as AlexNet and MobileNet, with the right mix
of 1-, 2- and 3-bit parameters that average to just 1.4 bits can equal the accuracy
of homogeneous 2-bit versions of these networks. Further, we provide analyses
to show that the heterogeneously binarized systems yield FPGA- and ASIC-based
implementations that are correspondingly more efficient in both circuit area and
energy efficiency than their homogeneous counterparts.",https://proceedings.neurips.cc/paper_files/paper/2018/file/1b36ea1c9b7a1c3ad668b8bb5df7963f-Paper.pdf,2018,"bitwidth, networks, homogeneous, binarization, bit, bits, efficient, heterogeneously, low, modern","bit, bits, accuracy, binarization, alexnet, bitwidth, al, et, weights, average","{'binarization': 0.5172771243811457, 'bitwidth': 0.5172771243811457, 'heterogeneous': 0.48824589330836743, 'convolutional': 0.34558824077352895, 'neural': 0.2316665618798194, 'networks': 0.23100665098066067}","bitwidth, homogeneous, binarization, heterogeneously, bits, bit, modern, networks, parameter, alexnet"
BRUNO: A Deep Recurrent Model for Exchangeable Data,"Iryna Korshunova, Jonas Degrave, Ferenc Huszar, Yarin Gal, Arthur Gretton, Joni Dambre","We present a novel model architecture which leverages deep learning tools to perform exact Bayesian inference on sets of high dimensional, complex observations. Our model is provably exchangeable, meaning that the joint distribution over observations is invariant under permutation: this property lies at the heart of Bayesian inference. The model does not require variational approximations to train, and new samples can be generated conditional on previous samples, with cost linear in the size of the conditioning set. The advantages of our architecture are demonstrated on learning tasks that require generalisation from short observed sequences while modelling sequence variability, such as conditional image generation, few-shot learning, and anomaly detection.",https://proceedings.neurips.cc/paper_files/paper/2018/file/1b9f38268c50805669fd8caf8f3cc84a-Paper.pdf,2018,"learning, model, architecture, bayesian, conditional, inference, observations, require, samples, advantages","x1, model, exchangeable, distribution, xn, bruno, learning, sequence, shot, one","{'bruno': 0.5545277517250888, 'exchangeable': 0.5234058974272418, 'recurrent': 0.3637964134373488, 'model': 0.3466688210684084, 'data': 0.3138599551908986, 'deep': 0.2598269918654148}","observations, conditional, require, architecture, bayesian, exchangeable, heart, samples, lies, inference"
Asymptotic optimality of adaptive importance sampling,"François Portier, Bernard Delyon","\textit{Adaptive importance sampling} (AIS) uses past samples to update the \textit{sampling policy} $q_t$ at each stage $t$. Each stage $t$ is formed with two steps : (i) to explore the space with $n_t$ points according to $q_t$ and (ii) to exploit the current amount of information to update the sampling policy. The very fundamental question raised in this paper concerns the behavior of empirical sums based on AIS. Without making any assumption on the \textit{allocation policy} $n_t$, the theory developed involves no restriction on the split of computational resources between the explore (i) and the exploit (ii) step. It is shown that AIS is asymptotically optimal : the asymptotic behavior of AIS is the same as some ``oracle'' strategy that knows the targeted sampling policy from the beginning. From a practical perspective, weighted AIS is introduced, a new method that allows to forget poor samples from early stages.",https://proceedings.neurips.cc/paper_files/paper/2018/file/1bc0249a6412ef49b07fe6f62e6dc8de-Paper.pdf,2018,"ais, policy, sampling, textit, behavior, exploit, explore, ii, n_t, q_t","ais, policy, sampling, nt, 04, xs, qt, estimate, xt, update","{'asymptotic': 0.513621480360454, 'optimality': 0.513621480360454, 'importance': 0.44980208959079787, 'adaptive': 0.37459181870151725, 'sampling': 0.36020133173867225}","ais, textit, n_t, q_t, policy, sampling, stage, update, ii, explore"
Phase Retrieval Under a Generative Prior,"Paul Hand, Oscar Leong, Vlad Voroninski","We introduce a novel deep-learning inspired formulation of the \textit{phase retrieval problem}, which asks to recover a signal $y_0 \in \R^n$ from $m$ quadratic observations, under structural assumptions on the underlying signal. As is common in many imaging problems, previous methodologies have considered natural signals as being sparse with respect to a known basis, resulting in the decision to enforce a generic sparsity prior. However, these methods for phase retrieval have encountered possibly fundamental limitations, as no computationally efficient algorithm for sparse phase retrieval has been proven to succeed with fewer than $O(k^2\log n)$ generic measurements, which is larger than the theoretical optimum of $O(k \log n)$. In this paper, we sidestep this issue by considering a prior that a natural signal is in the range of a generative neural network $G : \R^k \rightarrow \R^n$.  We introduce an empirical risk formulation that has favorable global geometry for gradient methods, as soon as $m = O(k)$, under the model of a multilayer fully-connected neural network with random weights.  Specifically, we show that there exists a descent direction outside of a small neighborhood around the true $k$-dimensional latent code and a negative multiple thereof.  This formulation for structured phase retrieval thus benefits from two effects: generative priors can more tightly represent natural signals than sparsity priors, and this empirical risk formulation can exploit those generative priors at an information theoretically optimal sample complexity, unlike for a sparsity prior. We corroborate these results with experiments showing that exploiting generative models in phase retrieval tasks outperforms both sparse and general phase retrieval methods.",https://proceedings.neurips.cc/paper_files/paper/2018/file/1bc2029a8851ad344a8d503930dfd7f7-Paper.pdf,2018,"phase, retrieval, formulation, generative, methods, natural, prior, priors, signal, sparse","x0, measurements, phase, retrieval, generative, rk, descent, methods, function, objective","{'phase': 0.5535583350063402, 'retrieval': 0.5375593916742106, 'prior': 0.5019793242271702, 'generative': 0.3906658779779653}","phase, retrieval, formulation, priors, generative, sparsity, signal, sparse, natural, prior"
Gaussian Process Prior Variational Autoencoders,"Francesco Paolo Casale, Adrian Dalca, Luca Saglietti, Jennifer Listgarten, Nicolo Fusi","Variational autoencoders (VAE) are a powerful and widely-used class of models to learn complex data distributions in an unsupervised fashion. One important limitation of VAEs is the prior assumption that latent sample representations are independent and identically distributed. However, for many important datasets, such as time-series of images, this assumption is too strong: accounting for covariances between samples, such as those in time, can yield to a more appropriate model specification and improve performance in downstream tasks. In this work, we introduce a new model, the Gaussian Process (GP) Prior Variational Autoencoder (GPPVAE), to specifically address this issue. The GPPVAE aims to combine the power of VAEs with the ability to model correlations afforded by GP priors. To achieve efficient inference in this new class of models, we leverage structure in the covariance matrix, and introduce a new stochastic backpropagation strategy that allows for computing stochastic gradients in a distributed and low-memory fashion. We show that our method outperforms conditional VAEs (CVAEs) and an adaptation of standard VAEs in two image data applications.",https://proceedings.neurips.cc/paper_files/paper/2018/file/1c336b8080f82bcc2cd2499b4c57261d-Paper.pdf,2018,"vaes, model, new, assumption, class, data, distributed, fashion, gp, gppvae","latent, al, et, object, gppvae, view, gp, data, model, sample","{'prior': 0.4974253846064615, 'autoencoders': 0.4724099418979319, 'process': 0.4654498148695835, 'gaussian': 0.4085875295641813, 'variational': 0.38185014608824785}","vaes, gppvae, gp, fashion, assumption, distributed, covariances, cvaes, important, variational"
Deep Predictive Coding Network with Local Recurrent Processing for Object Recognition,"Kuan Han, Haiguang Wen, Yizhen Zhang, Di Fu, Eugenio Culurciello, Zhongming Liu","Inspired by ""predictive coding"" - a theory in neuroscience, we develop a bi-directional and dynamic neural network with local recurrent processing, namely predictive coding network (PCN). Unlike feedforward-only convolutional neural networks, PCN includes both feedback connections, which carry top-down predictions, and feedforward connections, which carry bottom-up errors of prediction. Feedback and feedforward connections enable adjacent layers to interact locally and recurrently to refine representations towards minimization of layer-wise prediction errors. When unfolded over time, the recurrent processing gives rise to an increasingly deeper hierarchy of non-linear transformation, allowing a shallow network to dynamically extend itself into an arbitrarily deep network. We train and test PCN for image classification with SVHN, CIFAR and ImageNet datasets. Despite notably fewer layers and parameters, PCN achieves competitive performance compared to classical and state-of-the-art models. Further analysis shows that the internal representations in PCN converge over time and yield increasingly better accuracy in object recognition. Errors of top-down prediction also reveal visual saliency or bottom-up attention.",https://proceedings.neurips.cc/paper_files/paper/2018/file/1c63926ebcabda26b5cdb31b5cc91efb-Paper.pdf,2018,"pcn, network, connections, errors, feedforward, prediction, bottom, carry, coding, feedback","pcn, recurrent, processing, rl, layer, pcconv3, local, error, feedforward, connections","{'processing': 0.4247830893548927, 'coding': 0.3929621307075902, 'predictive': 0.3720022010772907, 'recognition': 0.3438466049938563, 'local': 0.3205057540158086, 'object': 0.3167292528105502, 'recurrent': 0.29524803819702566, 'network': 0.27174517622524974, 'deep': 0.21086906518419943}","pcn, feedforward, errors, connections, carry, bottom, coding, increasingly, prediction, network"
Exponentiated Strongly Rayleigh Distributions,"Zelda E. Mariet, Suvrit Sra, Stefanie Jegelka","Strongly Rayleigh (SR) measures are discrete probability distributions over the subsets of a ground set. They enjoy strong negative dependence properties, as a result of which they assign higher probability to subsets of diverse elements. We introduce in this paper Exponentiated Strongly Rayleigh (ESR) measures, which sharpen (or smoothen) the negative dependence property of SR measures via a single parameter (the exponent) that can intuitively understood as an inverse temperature. We develop efficient MCMC procedures for approximate sampling from ESRs, and obtain explicit mixing time bounds for two concrete instances: exponentiated versions of Determinantal Point Processes and Dual Volume Sampling. We illustrate some of the potential of ESRs, by applying them to a few machine learning tasks; empirical results confirm that beyond their theoretical appeal, ESR-based models hold significant promise for these tasks.",https://proceedings.neurips.cc/paper_files/paper/2018/file/1c6a0198177bfcc9bd93f6aab94aad3c-Paper.pdf,2018,"measures, dependence, esr, esrs, exponentiated, negative, probability, rayleigh, sampling, sr","sr, measures, esr, measure, mixing, sampling, dpps, time, dpp, distribution","{'exponentiated': 0.5250835199596374, 'rayleigh': 0.5250835199596374, 'strongly': 0.5250835199596374, 'distributions': 0.41576663069610503}","esr, esrs, exponentiated, rayleigh, sr, measures, subsets, dependence, negative, strongly"
Variational Inference with Tail-adaptive f-Divergence,"Dilin Wang, Hao Liu, Qiang Liu","Variational inference with α-divergences has been widely used in modern probabilistic
machine learning. Compared to Kullback-Leibler (KL) divergence, a major
advantage of using α-divergences (with positive α values) is their mass-covering
property. However, estimating and optimizing α-divergences require to use importance
sampling, which could have extremely large or infinite variances due
to heavy tails of importance weights. In this paper, we propose a new class of
tail-adaptive f-divergences that adaptively change the convex function f with the
tail of the importance weights, in a way that theoretically guarantee finite moments,
while simultaneously achieving mass-covering properties. We test our methods
on Bayesian neural networks, as well as deep reinforcement learning in which our
method is applied to improve a recent soft actor-critic (SAC) algorithm (Haarnoja
et al., 2018). Our results show that our approach yields significant advantages
compared with existing methods based on classical KL and α-divergences.",https://proceedings.neurips.cc/paper_files/paper/2018/file/1cd138d0499a68f4bb72bee04bbec2d7-Paper.pdf,2018,"divergences, importance, compared, covering, kl, learning, mass, methods, tail, weights","divergence, function, tail, distribution, kl, gradient, qθ, al, et, adaptive","{'divergence': 0.5671624828485715, 'tail': 0.5353315274103502, 'adaptive': 0.39042528034492224, 'inference': 0.34708524912983973, 'variational': 0.34474164835947974}","divergences, importance, covering, mass, kl, tail, weights, haarnoja, sac, compared"
Modelling and unsupervised learning of symmetric deformable object categories,"James Thewlis, Hakan Bilen, Andrea Vedaldi","We propose a new approach to model and learn, without manual supervision, the symmetries of natural objects, such as faces or flowers, given only images as input. It is well known that objects that have a symmetric structure do not usually result in symmetric images due to articulation and perspective effects. This is often tackled by seeking the intrinsic symmetries of the underlying 3D shape, which is very difficult to do when the latter cannot be recovered reliably from data. We show that, if only raw images are given, it is possible to look instead for symmetries in the space of object deformations. We can then learn symmetries from an unstructured collection of images of the object as an extension of the recently-introduced object frame representation, modified so that object symmetries reduce to the obvious symmetry groups in the normalized space. We also show that our formulation provides an explanation of the ambiguities that arise in recovering the pose of symmetric objects from their shape or images and we provide a way of discounting such ambiguities in learning.",https://proceedings.neurips.cc/paper_files/paper/2018/file/1d6408264d31d453d556c60fe7d0459e-Paper.pdf,2018,"images, symmetries, object, objects, symmetric, ambiguities, given, learn, shape, show","object, symmetry, pose, symmetries, images, symmetric, objects, 38, bilateral, shape","{'deformable': 0.4684123331236501, 'categories': 0.4421235489511027, 'symmetric': 0.42347135974477884, 'modelling': 0.40900359780253515, 'object': 0.32965874776676435, 'unsupervised': 0.32595281802081266, 'learning': 0.15357274675596103}","symmetries, images, symmetric, object, objects, ambiguities, shape, articulation, discounting, flowers"
Generalizing to Unseen Domains via Adversarial Data Augmentation,"Riccardo Volpi, Hongseok Namkoong, Ozan Sener, John C. Duchi, Vittorio Murino, Silvio Savarese","We are concerned with learning models that generalize well to different unseen
domains. We consider a worst-case formulation over data distributions that are
near the source domain in the feature space. Only using training data from a single
source distribution, we propose an iterative procedure that augments the dataset
with examples from a fictitious target domain that is ""hard"" under the current model. We show that our iterative scheme is an adaptive data augmentation method where we append adversarial examples at each iteration. For softmax losses, we show that our method is a data-dependent regularization scheme that behaves differently from classical regularizers that regularize towards zero (e.g., ridge or lasso). On digit recognition and semantic segmentation tasks, our method learns models improve performance across a range of a priori unknown target domains.",https://proceedings.neurips.cc/paper_files/paper/2018/file/1d94108e907bb8311d8802b48fd54b4a-Paper.pdf,2018,"data, method, domain, domains, examples, iterative, models, scheme, show, source","models, data, method, different, semantic, training, model, space, y0, trained","{'domains': 0.47253445812511574, 'augmentation': 0.44601432723764917, 'unseen': 0.44601432723764917, 'generalizing': 0.41260291369788793, 'adversarial': 0.2749784620246042, 'data': 0.2674521579703192, 'via': 0.24792871941248304}","iterative, source, scheme, data, domains, examples, target, domain, append, method"
Information-theoretic Limits for Community Detection in Network Models,"Chuyang Ke, Jean Honorio","We analyze the information-theoretic limits for the recovery of node labels in several network models. This includes the Stochastic Block Model, the Exponential Random Graph Model, the Latent Space Model, the Directed Preferential Attachment Model, and the Directed Small-world Model. For the Stochastic Block Model, the non-recoverability condition depends on the probabilities of having edges inside a community, and between different communities. For the Latent Space Model, the non-recoverability condition depends on the dimension of the latent space, and how far and spread are the communities in the latent space. For the Directed Preferential Attachment Model and the Directed Small-world Model, the non-recoverability condition depends on the ratio between homophily and neighborhood size. We also consider dynamic versions of the Stochastic Block Model and the Latent Space Model.",https://proceedings.neurips.cc/paper_files/paper/2018/file/1dba3025b159cd9354da65e2d0436a31-Paper.pdf,2018,"model, latent, space, directed, block, condition, depends, non, recoverability, stochastic","model, latent, models, directed, probability, space, dynamic, labels, block, stochastic","{'community': 0.4523079769717264, 'limits': 0.4523079769717264, 'theoretic': 0.43685502133536724, 'information': 0.34814883300746013, 'detection': 0.3444046137963286, 'network': 0.30209843514412626, 'models': 0.26250155394660024}","recoverability, model, directed, latent, space, depends, block, attachment, preferential, condition"
Dendritic cortical microcircuits approximate the backpropagation algorithm,"João Sacramento, Rui Ponte Costa, Yoshua Bengio, Walter Senn","Deep learning has seen remarkable developments over the last years, many of them inspired by neuroscience. However, the main learning mechanism behind these advances – error backpropagation – appears to be at odds with neurobiology. Here, we introduce a multilayer neuronal network model with simplified dendritic compartments in which error-driven synaptic plasticity adapts the network towards a global desired output. In contrast to previous work our model does not require separate phases and synaptic learning is driven by local dendritic prediction errors continuously in time. Such errors originate at apical dendrites and occur due to a mismatch between predictive input from lateral interneurons and activity from actual top-down feedback. Through the use of simple dendritic compartments and different cell-types our model can represent both error and normal activity within a pyramidal neuron. We demonstrate the learning capabilities of the model in regression and classification tasks, and show analytically that it approximates the error backpropagation algorithm. Moreover, our framework is consistent with recent observations of learning between brain areas and the architecture of cortical microcircuits. Overall, we introduce a novel view of learning on dendritic cortical circuits and on how the brain may solve the long-standing synaptic credit assignment problem.",https://proceedings.neurips.cc/paper_files/paper/2018/file/1dc3a89d0d440ba31729b0ba74b93a33-Paper.pdf,2018,"learning, dendritic, error, model, synaptic, activity, backpropagation, brain, compartments, cortical","layer, learning, error, al, et, network, input, pyramidal, dendritic, apical","{'microcircuits': 0.4568363654722388, 'cortical': 0.4311972612796078, 'dendritic': 0.4311972612796078, 'backpropagation': 0.4130060273547996, 'approximate': 0.3617278189695374, 'algorithm': 0.34353658504472917}","dendritic, synaptic, compartments, error, cortical, activity, errors, learning, driven, brain"
Structured Local Minima in Sparse Blind Deconvolution,"Yuqian Zhang, Han-wen Kuo, John Wright","Blind deconvolution is a ubiquitous problem of recovering two unknown signals from their convolution. Unfortunately, this is an ill-posed problem in general. This paper focuses on the {\em short and sparse} blind deconvolution problem, where the one unknown signal is short and the other one is sparsely and randomly supported. This variant captures the structure of the unknown signals in several important applications. We assume the short signal to have unit $\ell^2$ norm and cast the blind deconvolution problem as a nonconvex optimization problem over the sphere. We demonstrate that (i) in a certain region of the sphere, every local optimum is close to some shift truncation of the ground truth, and (ii) for a generic short signal of length $k$, when the sparsity of activation signal $\theta\lesssim k^{-2/3}$ and number of measurements $m\gtrsim\poly\paren{k}$, a simple initialization method together with a descent algorithm which escapes strict saddle points recovers a near shift truncation of the ground truth kernel.",https://proceedings.neurips.cc/paper_files/paper/2018/file/1e1d184167ca7676cf665225e236a3d2-Paper.pdf,2018,"problem, short, signal, blind, deconvolution, unknown, ground, one, shift, signals","a0, x0, sparse, shift, local, kernel, point, problem, rc, signal","{'deconvolution': 0.4714608935497988, 'blind': 0.44219295513763135, 'minima': 0.4310658013748114, 'local': 0.37139315116944016, 'structured': 0.3670170466363935, 'sparse': 0.35176399177246453}","short, deconvolution, blind, signal, truncation, sphere, unknown, shift, problem, truth"
Solving Non-smooth Constrained Programs with Lower Complexity than $\mathcal{O}(1/\varepsilon)$: A Primal-Dual Homotopy Smoothing Approach,"Xiaohan Wei, Hao Yu, Qing Ling, Michael Neely","We propose a new primal-dual homotopy smoothing algorithm for a linearly constrained convex program, where neither the primal nor the dual function has to be smooth or strongly convex. The best known iteration complexity solving such a non-smooth problem is $\mathcal{O}(\varepsilon^{-1})$. In this paper, 
we show that by leveraging a local error bound condition on the dual function, the proposed algorithm can achieve a better primal convergence time of  $\mathcal{O}\l(\varepsilon^{-2/(2+\beta)}\log_2(\varepsilon^{-1})\r)$, where $\beta\in(0,1]$ is a local error bound parameter. 
As an example application, we show that the distributed geometric median problem, which can be formulated as a constrained convex program, has its dual function non-smooth but satisfying the aforementioned local error bound condition with $\beta=1/2$, therefore enjoying a convergence time of $\mathcal{O}\l(\varepsilon^{-4/5}\log_2(\varepsilon^{-1})\r)$. This result improves upon the $\mathcal{O}(\varepsilon^{-1})$ convergence time bound achieved by existing distributed optimization algorithms. Simulation experiments also demonstrate the performance of our proposed algorithm.",https://proceedings.neurips.cc/paper_files/paper/2018/file/1e4d36177d71bbb3558e43af9577d70e-Paper.pdf,2018,"varepsilon, bound, dual, mathcal, algorithm, beta, convergence, convex, error, function","dual, function, λt, convergence, time, al, algorithm, et, problem, primal","{'homotopy': 0.30783350859189107, 'mathcal': 0.30783350859189107, 'smoothing': 0.30783350859189107, 'varepsilon': 0.30783350859189107, 'lower': 0.2782989798520153, 'primal': 0.2782989798520153, 'smooth': 0.268790985281391, 'solving': 0.2610223880627167, 'programs': 0.2544541327032248, 'dual': 0.24876445111213952}","varepsilon, mathcal, dual, primal, beta, log_2, bound, smooth, local, convex"
Unsupervised Cross-Modal Alignment of Speech and Text Embedding Spaces,"Yu-An Chung, Wei-Hung Weng, Schrasing Tong, James Glass","Recent research has shown that word embedding spaces learned from text corpora of different languages can be aligned without any parallel data supervision. Inspired by the success in unsupervised cross-lingual word embeddings, in this paper we target learning a cross-modal alignment between the embedding spaces of speech and text learned from corpora of their respective modalities in an unsupervised fashion. The proposed framework learns the individual speech and text embedding spaces, and attempts to align the two spaces via adversarial training, followed by a refinement procedure. We show how our framework could be used to perform the tasks of spoken word classification and translation, and the experimental results on these two tasks demonstrate that the performance of our unsupervised alignment approach is comparable to its supervised counterpart. Our framework is especially useful for developing automatic speech recognition (ASR) and speech-to-text translation systems for low- or zero-resource languages, which have little parallel audio-text data for training modern supervised ASR and speech-to-text translation models, but account for the majority of the languages spoken across the world.",https://proceedings.neurips.cc/paper_files/paper/2018/file/1ea97de85eb634d580161c603422437f-Paper.pdf,2018,"text, speech, spaces, embedding, framework, languages, translation, unsupervised, word, alignment","word, speech, text, embedding, alignment, unsupervised, audio, training, spoken, words","{'modal': 0.3988483486527497, 'spaces': 0.3820218419413334, 'speech': 0.3820218419413334, 'alignment': 0.3583062127236988, 'cross': 0.34147970601228256, 'text': 0.34147970601228256, 'embedding': 0.3177640767946479, 'unsupervised': 0.29404844757701326}","text, speech, spaces, languages, word, translation, asr, spoken, embedding, unsupervised"
Isolating Sources of Disentanglement in Variational Autoencoders,"Ricky T. Q. Chen, Xuechen Li, Roger B. Grosse, David K. Duvenaud","We decompose the evidence lower bound to show the existence of a term measuring the total correlation between latent variables. We use this to motivate the beta-TCVAE (Total Correlation Variational Autoencoder) algorithm, a refinement and plug-in replacement of the beta-VAE for learning disentangled representations, requiring no additional hyperparameters during training. We further propose a principled classifier-free measure of disentanglement called the mutual information gap (MIG). We perform extensive quantitative and qualitative experiments, in both restricted and non-restricted settings, and show a strong relation between total correlation and disentanglement, when the model is trained using our framework.",https://proceedings.neurips.cc/paper_files/paper/2018/file/1ee3dfcd8a0645a25a35977997223d22-Paper.pdf,2018,"correlation, total, beta, disentanglement, restricted, show, additional, algorithm, autoencoder, bound","factors, information, latent, tcvae, vae, disentangled, disentanglement, vk, mutual, variables","{'isolating': 0.5187017187126294, 'disentanglement': 0.4895905348925268, 'sources': 0.4895905348925268, 'autoencoders': 0.3900587400023803, 'variational': 0.31528546214441733}","correlation, total, disentanglement, beta, restricted, mig, tcvae, refinement, replacement, motivate"
Learning to Share and Hide Intentions using Information Regularization,"DJ Strouse, Max Kleiman-Weiner, Josh Tenenbaum, Matt Botvinick, David J. Schwab","Learning to cooperate with friends and compete with foes is a key component of multi-agent reinforcement learning. Typically to do so, one requires access to either a model of or interaction with the other agent(s). Here we show how to learn effective strategies for cooperation and competition in an asymmetric information game with no such model or interaction. Our approach is to encourage an agent to reveal or hide their intentions using an information-theoretic regularizer. We consider both the mutual information between goal and action given state, as well as the mutual information between goal and state. We show how to stochastically optimize these regularizers in a way that is easy to integrate with policy gradient reinforcement learning. Finally, we demonstrate that cooperative (competitive) policies learned with our approach lead to more (less) reward for a second agent in two simple asymmetric information games.",https://proceedings.neurips.cc/paper_files/paper/2018/file/1ef03ed0cd5863c550128836b28ec3e9-Paper.pdf,2018,"information, agent, learning, approach, asymmetric, goal, interaction, model, mutual, reinforcement","goal, information, alice, state, bob, st, reward, policy, πg, action","{'hide': 0.4753149077809296, 'intentions': 0.4753149077809296, 'share': 0.4753149077809296, 'regularization': 0.33850522255307786, 'information': 0.33075609389985616, 'using': 0.2719504291006302, 'learning': 0.15583581131435445}","agent, information, asymmetric, mutual, interaction, goal, foes, friends, hide, stochastically"
Why Is My Classifier Discriminatory?,"Irene Chen, Fredrik D. Johansson, David Sontag","Recent attempts to achieve fairness in predictive models focus on the balance between fairness and accuracy. In sensitive applications such as healthcare or criminal justice, this trade-off is often undesirable as any increase in prediction error could have devastating consequences. In this work, we argue that the fairness of predictions should be evaluated in context of the data, and that unfairness induced by inadequate samples sizes or unmeasured predictive variables should be addressed through data collection, rather than by constraining the model. We decompose cost-based metrics of discrimination into bias, variance, and noise, and propose actions aimed at estimating and reducing each term. Finally, we perform case-studies on prediction of income, mortality, and review ratings, confirming the value of this analysis. We find that data collection is often a means to reduce discrimination without sacrificing accuracy.",https://proceedings.neurips.cc/paper_files/paper/2018/file/1f1baa5b8edac74eb4eaa329f14a0361-Paper.pdf,2018,"data, fairness, accuracy, collection, discrimination, often, prediction, predictive, achieve, actions","discrimination, data, fairness, al, et, training, noise, prediction, groups, may","{'discriminatory': 0.7221761581984358, 'classifier': 0.6917091849395582}","fairness, discrimination, collection, predictive, confirming, income, justice, unmeasured, prediction, criminal"
Unsupervised Learning of Object Landmarks through Conditional Image Generation,"Tomas Jakab, Ankush Gupta, Hakan Bilen, Andrea Vedaldi","We propose a method for learning landmark detectors for visual objects (such as the eyes and the nose in a face) without any manual supervision. We cast this as the problem of generating images that combine the appearance of the object as seen in a first example image with the geometry of the object as seen in a second example image, where the two examples differ by a viewpoint change and/or an object deformation. In order to factorize appearance and geometry, we introduce a tight bottleneck in the geometry-extraction process that selects and distils geometry-related features. Compared to standard image generation problems, which often use generative adversarial networks, our generation task is conditioned on both appearance and geometry and thus is significantly less ambiguous, to the point that adopting a simple perceptual loss formulation is sufficient. We demonstrate that our approach can learn object landmarks from synthetic image deformations or videos, all without manual supervision, while outperforming state-of-the-art unsupervised landmark detectors. We further show that our method is applicable to a large variety of datasets - faces, people, 3D objects, and digits - without any modifications.",https://proceedings.neurips.cc/paper_files/paper/2018/file/1f36c15d6a3d18d52e8d493bc8187cb9-Paper.pdf,2018,"geometry, image, object, appearance, without, detectors, example, generation, landmark, manual","image, landmarks, 10, keypoints, unsupervised, target, images, object, supervised, training","{'landmarks': 0.5203249536580875, 'conditional': 0.4204816820910355, 'generation': 0.37056004630750944, 'object': 0.36619375820201755, 'unsupervised': 0.3620771122749911, 'image': 0.3476244683744624, 'learning': 0.1705927164771019}","geometry, appearance, object, landmark, manual, image, detectors, seen, supervision, without"
Scalable Coordinated Exploration in Concurrent Reinforcement Learning,"Maria Dimakopoulou, Ian Osband, Benjamin Van Roy","We consider a team of reinforcement learning agents that concurrently operate in a common environment, and we develop an approach to efficient coordinated exploration that is suitable for problems of practical scale. Our approach builds on the seed sampling concept introduced in Dimakopoulou and Van Roy (2018) and on a randomized value function learning algorithm from Osband et al. (2016). We demonstrate that, for simple tabular contexts, the approach is competitive with those previously proposed in Dimakopoulou and Van Roy (2018) and with a higher-dimensional problem and a neural network value function representation, the approach learns quickly with far fewer agents than alternative exploration schemes.",https://proceedings.neurips.cc/paper_files/paper/2018/file/1f4fe6a4411edc2ff625888b4093e917-Paper.pdf,2018,"approach, 2018, agents, dimakopoulou, exploration, function, learning, roy, value, van","seed, agent, agents, learning, sampling, time, θk, value, function, state","{'concurrent': 0.5225376362998332, 'coordinated': 0.5225376362998332, 'exploration': 0.40613026126542495, 'scalable': 0.40613026126542495, 'reinforcement': 0.3076935916617572, 'learning': 0.17131816225844176}","dimakopoulou, roy, van, 2018, approach, agents, exploration, value, osband, seed"
Bayesian Semi-supervised Learning with Graph Gaussian Processes,"Yin Cheng Ng, Nicolò Colombo, Ricardo Silva","We propose a data-efficient Gaussian process-based Bayesian approach to the semi-supervised learning problem on graphs. The proposed model shows extremely competitive performance when compared to the state-of-the-art graph neural networks on semi-supervised learning benchmark experiments, and outperforms the neural networks in active learning experiments where labels are scarce. Furthermore, the model does not require a validation data set for early stopping to control over-fitting. Our model can be viewed as an instance of empirical distribution regression weighted locally by network connectivity. We further motivate the intuitive construction of the model with a Bayesian linear model interpretation where the node features are filtered by an operator related to the graph Laplacian. The method can be easily implemented by adapting off-the-shelf scalable variational inference algorithms for Gaussian processes.",https://proceedings.neurips.cc/paper_files/paper/2018/file/1fc214004c9481e4c8073e85323bfd4b-Paper.pdf,2018,"model, learning, bayesian, data, experiments, gaussian, graph, networks, neural, semi","data, graph, model, ggp, learning, function, models, gaussian, points, gp","{'semi': 0.47306977146026763, 'supervised': 0.4221341249428534, 'processes': 0.3988234814571815, 'gaussian': 0.3807423416541995, 'graph': 0.3807423416541995, 'bayesian': 0.33493542519261893, 'learning': 0.19192811680946556}","model, semi, supervised, graph, gaussian, filtered, bayesian, laplacian, scarce, connectivity"
"Simple, Distributed, and Accelerated Probabilistic Programming","Dustin Tran, Matthew W. Hoffman, Dave Moore, Christopher Suter, Srinivas Vasudevan, Alexey Radul","We describe a simple, low-level approach for embedding probabilistic programming in a deep learning ecosystem. In particular, we distill probabilistic programming down to a single abstraction—the random variable. Our lightweight implementation in TensorFlow enables numerous applications: a model-parallel variational auto-encoder (VAE) with 2nd-generation tensor processing units (TPUv2s); a data-parallel autoregressive model (Image Transformer) with TPUv2s; and multi-GPU No-U-Turn Sampler (NUTS). For both a state-of-the-art VAE on 64x64 ImageNet and Image Transformer on 256x256 CelebA-HQ, our approach achieves an optimal linear speedup from 1 to 256 TPUv2 chips. With NUTS, we see a 100x speedup on GPUs over Stan and 37x over PyMC3.",https://proceedings.neurips.cc/paper_files/paper/2018/file/201e5bacd665709851b77148e225b332-Paper.pdf,2018,"approach, image, model, nuts, parallel, probabilistic, programming, speedup, tpuv2s, transformer","model, figure, tf, probabilistic, program, learning, variational, def, return, data","{'accelerated': 0.5228653245755871, 'programming': 0.45789735157419603, 'simple': 0.43862739970160136, 'probabilistic': 0.4232406402932894, 'distributed': 0.3813334923829025}","tpuv2s, nuts, transformer, vae, speedup, programming, parallel, probabilistic, 256, 256x256"
When do random forests fail?,"Cheng Tang, Damien Garreau, Ulrike von Luxburg","Random forests are learning algorithms that build large collections of random trees and make predictions by averaging the individual tree predictions.
In this paper, we consider various tree constructions and examine how the choice of parameters affects the generalization error of the resulting random forests as the sample size goes to infinity. 
We show that subsampling of data points during the tree construction phase is important: Forests can become inconsistent with either no subsampling or too severe subsampling. 
As a consequence, even highly randomized trees can lead to inconsistent forests if no subsampling is used, which implies that some of the commonly used setups for random forests can be inconsistent.As a second consequence we can show that trees that have good performance in nearest-neighbor search can be a poor choice for random forests.",https://proceedings.neurips.cc/paper_files/paper/2018/file/204da255aea2cd4a75ace6018fad6b4d-Paper.pdf,2018,"forests, random, subsampling, inconsistent, tree, trees, choice, consequence, predictions, show","trees, random, forests, tree, subsampling, local, data, forest, aj, average","{'fail': 0.6263533324660814, 'forests': 0.6263533324660814, 'random': 0.46407219892735346}","forests, subsampling, inconsistent, random, trees, tree, consequence, choice, predictions, infinity"
Contextual Combinatorial Multi-armed Bandits with Volatile Arms and Submodular Reward,"Lixing Chen, Jie Xu, Zhuo Lu","In this paper, we study the stochastic contextual combinatorial multi-armed bandit (CC-MAB) framework that is tailored for volatile arms and submodular reward functions. CC-MAB inherits properties from both contextual bandit and combinatorial bandit: it aims to select a set of arms in each round based on the side information (a.k.a. context) associated with the arms. By ``volatile arms'', we mean that the available arms to select from in each round may change; and by ``submodular rewards'', we mean that the total reward achieved by selected arms is not a simple sum of individual rewards but demonstrates a feature of diminishing returns determined by the relations between selected arms (e.g. relevance and redundancy). Volatile arms and submodular rewards are often seen in many real-world applications, e.g. recommender systems and crowdsourcing, in which multi-armed bandit (MAB) based strategies are extensively applied. Although there exist works that investigate these issues separately based on standard MAB, jointly considering all these issues in a single MAB problem requires very different algorithm design and regret analysis. Our algorithm CC-MAB provides an online decision-making policy in a contextual and combinatorial bandit setting and effectively addresses the issues raised by volatile arms and submodular reward functions. The proposed algorithm is proved to achieve $O(cT^{\frac{2\alpha+D}{3\alpha + D}}\log(T))$ regret after a span of $T$ rounds. The performance of CC-MAB is evaluated by experiments conducted on a real-world crowdsourcing dataset, and the result shows that our algorithm outperforms the prior art.",https://proceedings.neurips.cc/paper_files/paper/2018/file/207f88018f72237565570f8a9e5ca240-Paper.pdf,2018,"arms, mab, bandit, algorithm, cc, submodular, volatile, based, combinatorial, contextual","arms, mab, cc, reward, algorithm, arm, context, time, regret, bandit","{'arms': 0.3929272785010713, 'volatile': 0.3708749547478999, 'armed': 0.3552285820444869, 'combinatorial': 0.3552285820444869, 'reward': 0.3552285820444869, 'contextual': 0.3111239345381441, 'bandits': 0.30020991245433776, 'submodular': 0.29112422126756593, 'multi': 0.24045889224458197}","arms, mab, cc, volatile, bandit, submodular, combinatorial, contextual, issues, rewards"
Learning to Reconstruct Shapes from Unseen Classes,"Xiuming Zhang, Zhoutong Zhang, Chengkai Zhang, Josh Tenenbaum, Bill Freeman, Jiajun Wu","From a single image, humans are able to perceive the full 3D shape of an object by exploiting learned shape priors from everyday life. Contemporary single-image 3D reconstruction algorithms aim to solve this task in a similar fashion, but often end up with priors that are highly biased by training classes. Here we present an algorithm, Generalizable Reconstruction (GenRe), designed to capture more generic, class-agnostic shape priors. We achieve this with an inference network and training procedure that combine 2.5D representations of visible surfaces (depth and silhouette), spherical shape representations of both visible and non-visible surfaces, and 3D voxel-based representations, in a principled manner that exploits the causal structure of how 3D shapes give rise to 2D images. Experiments demonstrate that GenRe performs well on single-view shape reconstruction, and generalizes to diverse novel objects from categories not seen during training.",https://proceedings.neurips.cc/paper_files/paper/2018/file/208e43f0e45c4c78cafadb83d2888cb6-Paper.pdf,2018,"shape, 3d, priors, reconstruction, representations, single, training, visible, genre, image","al, et, 3d, spherical, shape, depth, latexit, classes, model, map","{'classes': 0.5070658924511693, 'reconstruct': 0.5070658924511693, 'shapes': 0.4786077480658248, 'unseen': 0.4786077480658248, 'learning': 0.166245626733813}","shape, visible, 3d, priors, reconstruction, genre, surfaces, representations, single, training"
Mixture Matrix Completion,Daniel Pimentel-Alarcon,"Completing a data matrix X has become an ubiquitous problem in modern data science, with motivations in recommender systems, computer vision, and networks inference, to name a few. One typical assumption is that X is low-rank. A more general model assumes that each column of X corresponds to one of several low-rank matrices. This paper generalizes these models to what we call mixture matrix completion (MMC): the case where each entry of X corresponds to one of several low-rank matrices. MMC is a more accurate model for recommender systems, and brings more flexibility to other completion and clustering problems. We make four fundamental contributions about this new model. First, we show that MMC is theoretically possible (well-posed). Second, we give its precise information-theoretic identifiability conditions. Third, we derive the sample complexity of MMC. Finally, we give a practical algorithm for MMC with performance comparable to the state-of-the-art for simpler related problems, both on synthetic and real data.",https://proceedings.neurips.cc/paper_files/paper/2018/file/20d135f0f28185b84a4cf7aa51f29500-Paper.pdf,2018,"mmc, data, low, model, one, rank, completion, corresponds, give, matrices","mmc, xk, entries, ωk, rank, uk, matrices, column, lrmc, low","{'completion': 0.607030705698812, 'mixture': 0.5917556455184402, 'matrix': 0.530413968835528}","mmc, rank, recommender, corresponds, completion, low, matrices, give, one, matrix"
Fighting Boredom in Recommender Systems with Linear Reinforcement Learning,"Romain WARLOP, Alessandro Lazaric, Jérémie Mary","A common assumption in recommender systems (RS) is the existence of a best fixed recommendation strategy. Such strategy may be simple and work at the item level (e.g., in multi-armed bandit it is assumed one best fixed arm/item exists) or implement more sophisticated RS (e.g., the objective of A/B testing is to find the
best fixed RS and execute it thereafter). We argue that this assumption is rarely verified in practice, as the recommendation process itself may impact the user’s
preferences. For instance, a user may get bored by a strategy, while she may gain interest again, if enough time passed since the last time that strategy was used. In
this case, a better approach consists in alternating different solutions at the right frequency to fully exploit their potential. In this paper, we first cast the problem as
a Markov decision process, where the rewards are a linear function of the recent history of actions, and we show that a policy considering the long-term influence
of the recommendations may outperform both fixed-action and contextual greedy policies. We then introduce an extension of the UCRL algorithm ( L IN UCRL ) to
effectively balance exploration and exploitation in an unknown environment, and we derive a regret bound that is independent of the number of states. Finally,
we empirically validate the model assumptions and the algorithm in a number of realistic scenarios.",https://proceedings.neurips.cc/paper_files/paper/2018/file/210f760a89db30aa72ca258a3483cc7f-Paper.pdf,2018,"may, fixed, strategy, best, rs, algorithm, assumption, item, number, process","reward, action, policy, optimal, state, model, linucrl, user, greedy, algorithm","{'boredom': 0.47311168662642233, 'fighting': 0.47311168662642233, 'recommender': 0.47311168662642233, 'systems': 0.36771508796557106, 'linear': 0.30267041759936936, 'reinforcement': 0.27858937615683105, 'learning': 0.15511346755762162}","rs, may, fixed, strategy, ucrl, recommendation, item, best, user, assumption"
Diffusion Maps for Textual Network Embedding,"Xinyuan Zhang, Yitong Li, Dinghan Shen, Lawrence Carin","Textual network embedding leverages rich text information associated with the network to learn low-dimensional vectorial representations of vertices.
Rather than using typical natural language processing (NLP) approaches, recent research exploits the relationship of texts on the same edge to graphically embed text. However, these models neglect to measure the complete level of connectivity between any two texts in the graph. We present diffusion maps for textual network embedding (DMTE), integrating global structural information of the graph to capture the semantic relatedness between texts, with a diffusion-convolution operation applied on the text inputs. In addition, a new objective function is designed to efficiently preserve the high-order proximity using the graph diffusion. Experimental results show that the proposed approach outperforms state-of-the-art methods on the vertex-classification and link-prediction tasks.",https://proceedings.neurips.cc/paper_files/paper/2018/file/211a7a84d3d5ce4d80347da11e0c85ed-Paper.pdf,2018,"diffusion, graph, network, text, texts, embedding, information, textual, using, addition","text, diffusion, embedding, vertex, dmte, graph, information, network, vertices, embeddings","{'textual': 0.5296472140949822, 'diffusion': 0.47883117090904725, 'maps': 0.47883117090904725, 'embedding': 0.39828964802431366, 'network': 0.3198133899789592}","texts, diffusion, text, textual, graph, embedding, network, dmte, graphically, proximity"
Out-of-Distribution Detection using Multiple Semantic Label Representations,"Gabi Shalev, Yossi Adi, Joseph Keshet","Deep Neural Networks are powerful models that attained remarkable results on a variety of tasks. These models are shown to be extremely efficient when training and test data are drawn from the same distribution. However, it is not clear how a network will act when it is fed with an out-of-distribution example. In this work, we consider the problem of out-of-distribution detection in neural networks. We propose to use multiple semantic dense representations instead of sparse representation as the target label. Specifically, we propose to use several word representations obtained from different corpora or architectures as target labels. We evaluated the proposed model on computer vision, and speech commands detection tasks and compared it to previous methods. Results suggest that our method compares favorably with previous work. Besides, we present the efficiency of our approach for detecting wrongly classified and adversarial examples.",https://proceedings.neurips.cc/paper_files/paper/2018/file/2151b4c76b4dcb048d06a5c32942b6f6-Paper.pdf,2018,"distribution, detection, models, networks, neural, previous, propose, representations, results, target","latexit, distribution, sha1_base64, examples, model, cifar, embed, word, using, ensemble","{'distribution': 0.43700023854771874, 'label': 0.43700023854771874, 'semantic': 0.3962817837993236, 'multiple': 0.3763528365323708, 'detection': 0.34451909909578476, 'representations': 0.34451909909578476, 'using': 0.286346038907117}","distribution, detection, target, wrongly, representations, classified, commands, previous, fed, attained"
First-order Stochastic Algorithms for Escaping From Saddle Points in Almost Linear Time,"Yi Xu, Rong Jin, Tianbao Yang","(This is a theory paper) In this paper, we consider first-order methods for solving stochastic non-convex optimization problems. The key building block of the proposed algorithms is first-order procedures to extract negative curvature from the Hessian matrix through a principled sequence starting from noise, which are referred to {\it NEgative-curvature-Originated-from-Noise or NEON} and are of independent interest. Based on this building block, we design purely first-order stochastic algorithms for escaping from non-degenerate saddle points with a much better time complexity (almost linear time in  the problem's dimensionality). In particular, we develop a general framework of {\it first-order stochastic algorithms} with a second-order convergence guarantee based on our new technique and existing algorithms that may only converge to a first-order stationary point. For finding a nearly {\it second-order stationary point} $\x$ such that $\|\nabla F(\x)\|\leq \epsilon$ and $\nabla^2 F(\x)\geq -\sqrt{\epsilon}I$ (in high probability), the best time complexity of the presented algorithms is $\widetilde O(d/\epsilon^{3.5})$, where $F(\cdot)$ denotes the objective function and $d$ is the dimensionality of the problem. To the best of our knowledge, this is the first theoretical result of first-order stochastic algorithms with an almost linear time in terms of problem's dimensionality for finding second-order stationary points, which is  even competitive with  existing stochastic algorithms hinging on the second-order information.",https://proceedings.neurips.cc/paper_files/paper/2018/file/217e342fc01668b10cb1188d40d3370e-Paper.pdf,2018,"order, algorithms, first, stochastic, second, time, dimensionality, epsilon, problem, stationary","neon, order, stochastic, algorithms, method, ﬁrst, 2f, nc, sgd, complexity","{'almost': 0.37341694331928943, 'escaping': 0.37341694331928943, 'first': 0.37341694331928943, 'saddle': 0.3576633298866586, 'points': 0.3354598588148681, 'order': 0.29750277431044675, 'time': 0.2668579353088458, 'algorithms': 0.25731025707384203, 'linear': 0.2530958321668657, 'stochastic': 0.2263533916007757}","order, first, algorithms, stochastic, second, stationary, dimensionality, epsilon, nabla, curvature"
cpSGD: Communication-efficient and differentially-private distributed SGD,"Naman Agarwal, Ananda Theertha Suresh, Felix Xinnan X. Yu, Sanjiv Kumar, Brendan McMahan","Distributed stochastic gradient descent is an important subroutine in distributed learning. A setting of particular interest is when the clients are mobile devices, where two important concerns are communication efficiency and the privacy of the clients. Several recent works have focused on reducing the communication cost or introducing privacy guarantees, but none of the proposed communication efficient methods are known to be privacy preserving and none of the known privacy mechanisms are known to be communication efficient. To this end, we study algorithms that achieve both communication efficiency and differential privacy. For $d$ variables and $n \approx d$ clients, the proposed method uses $\cO(\log \log(nd))$ bits of communication per client per coordinate and ensures constant privacy.

We also improve previous analysis of the \emph{Binomial mechanism} showing that it achieves nearly the same utility as the Gaussian mechanism, while requiring fewer representation bits, which can be of independent interest.",https://proceedings.neurips.cc/paper_files/paper/2018/file/21ce689121e39821d07d04faab328370-Paper.pdf,2018,"communication, privacy, clients, known, bits, distributed, efficiency, efficient, important, interest","mechanism, privacy, log, communication, binomial, gaussian, server, xmax, clients, distributed","{'cpsgd': 0.48672468213748044, 'differentially': 0.3853937011477496, 'sgd': 0.3853937011477496, 'communication': 0.3782954730698102, 'private': 0.3660123138087882, 'distributed': 0.3350532276392752, 'efficient': 0.27696449208007934}","privacy, communication, clients, none, bits, known, per, mechanism, interest, distributed"
Factored Bandits,"Julian Zimmert, Yevgeny Seldin","We introduce the factored bandits model, which is a framework for learning with
limited (bandit) feedback, where actions can be decomposed into a Cartesian
product of atomic actions. Factored bandits incorporate rank-1 bandits as a special
case, but significantly relax the assumptions on the form of the reward function. We
provide an anytime algorithm for stochastic factored bandits and up to constants
matching upper and lower regret bounds for the problem. Furthermore, we show
that with a slight modification the proposed algorithm can be applied to utility
based dueling bandits. We obtain an improvement in the additive terms of the regret
bound compared to state of the art algorithms (the additive terms are dominating
up to time horizons which are exponential in the number of arms).",https://proceedings.neurips.cc/paper_files/paper/2018/file/226d1f15ecd35f784d2a20c3ecf56d7f-Paper.pdf,2018,"bandits, factored, actions, additive, algorithm, regret, terms, algorithms, anytime, applied","bandits, algorithm, factored, regret, arms, dueling, actions, bound, problem, bandit","{'factored': 0.777267616957603, 'bandits': 0.6291701293204001}","bandits, factored, additive, actions, regret, relax, terms, decomposed, dominating, dueling"
Poison Frogs! Targeted Clean-Label Poisoning Attacks on Neural Networks,"Ali Shafahi, W. Ronny Huang, Mahyar Najibi, Octavian Suciu, Christoph Studer, Tudor Dumitras, Tom Goldstein","Data poisoning is an attack on machine learning models wherein the attacker adds examples to the training set to manipulate the behavior of the model at test time. This paper explores poisoning attacks on neural nets. The proposed attacks use ``clean-labels''; they don't require the attacker to have any control over the labeling of training data.  They are also targeted; they control the behavior of the classifier on a specific test instance without degrading overall classifier performance. For example, an attacker could add a seemingly innocuous image (that is properly labeled) to a training set for a face recognition engine, and control the identity of a chosen person at test time. Because the attacker does not need to control the labeling function, poisons could be entered into the training set simply by putting them online and waiting for them to be scraped by a data collection bot.We present an optimization-based method for crafting poisons, and show that just one single poison image can control classifier behavior when transfer learning is used. For full end-to-end training, we present a ``watermarking'' strategy that makes poisoning reliable using multiple (approx. 50) poisoned training instances. We demonstrate our method by generating poisoned frog images from the CIFAR dataset and using them to manipulate image classifiers.",https://proceedings.neurips.cc/paper_files/paper/2018/file/22722a343513ed45f14905eb07621686-Paper.pdf,2018,"training, control, attacker, behavior, classifier, data, image, poisoning, set, test","poison, target, instance, training, instances, base, attacks, end, poisoning, attack","{'clean': 0.38690277831625763, 'frogs': 0.38690277831625763, 'poison': 0.38690277831625763, 'poisoning': 0.38690277831625763, 'targeted': 0.3651885686004556, 'label': 0.3378318996766746, 'attacks': 0.3063536721927165, 'neural': 0.1732774024011033, 'networks': 0.17278381521487085}","attacker, control, poisoning, training, poisons, poisoned, classifier, behavior, manipulate, labeling"
Implicit Probabilistic Integrators for ODEs,"Onur Teymur, Han Cheng Lie, Tim Sullivan, Ben Calderhead","We introduce a family of implicit probabilistic integrators for initial value problems (IVPs), taking as a starting point the multistep Adams–Moulton method. The implicit construction allows for dynamic feedback from the forthcoming time-step, in contrast to previous probabilistic integrators, all of which are based on explicit methods. We begin with a concise survey of the rapidly-expanding field of probabilistic ODE solvers. We then introduce our method, which builds on and adapts the work of Conrad et al. (2016) and Teymur et al. (2016), and provide a rigorous proof of its well-definedness and convergence. We discuss the problem of the calibration of such integrators and suggest one approach. We give an illustrative example highlighting the effect of the use of probabilistic integrators—including our new method—in the setting of parameter inference within an inverse problem.",https://proceedings.neurips.cc/paper_files/paper/2018/file/228b25587479f2fc7570428e8bcbabdc-Paper.pdf,2018,"integrators, probabilistic, method, 2016, al, et, implicit, introduce, problem, adams","zi, method, al, et, step, model, probabilistic, 2016, data, methods","{'integrators': 0.550253503844597, 'odes': 0.550253503844597, 'implicit': 0.4665784575252619, 'probabilistic': 0.420412542613604}","integrators, probabilistic, 2016, implicit, al, et, adams, conrad, definedness, ivps"
Non-metric Similarity Graphs for Maximum Inner Product Search,"Stanislav Morozov, Artem Babenko","In this paper we address the problem of Maximum Inner Product Search (MIPS) that is currently the computational bottleneck in a large number of machine learning applications. 
While being similar to the nearest neighbor search (NNS), the MIPS problem was shown to be more challenging, as the inner product is not a proper metric function. We propose to solve the MIPS problem with the usage of similarity graphs, i.e., graphs where each vertex is connected to the vertices that are the most similar in terms of some similarity function. Originally, the framework of similarity graphs was proposed for metric spaces and in this paper we naturally extend it to the non-metric MIPS scenario. We demonstrate that, unlike existing approaches, similarity graphs do not require any data transformation to reduce MIPS to the NNS problem and should be used for the original data. Moreover, we explain why such a reduction is detrimental for similarity graphs. By an extensive comparison to the existing approaches, we show that the proposed method is a game-changer in terms of the runtime/accuracy trade-off for the MIPS problem.",https://proceedings.neurips.cc/paper_files/paper/2018/file/229754d7799160502a143a72f6789927-Paper.pdf,2018,"mips, graphs, problem, similarity, metric, approaches, data, existing, function, inner","similarity, mips, nsw, graph, graphs, ip, reduction, search, database, nns","{'inner': 0.4120185908351562, 'product': 0.39463646149368065, 'maximum': 0.37013772616225754, 'similarity': 0.3608237392599875, 'metric': 0.35275559682078206, 'search': 0.3282568614893589, 'graphs': 0.3189428745870889, 'non': 0.27091216560119147}","mips, similarity, graphs, nns, metric, inner, problem, product, similar, search"
Learning convex polytopes with margin,"Lee-Ad Gottlieb, Eran Kaufman, Aryeh Kontorovich, Gabriel Nivasch","We present improved algorithm for properly learning convex polytopes in the
realizable PAC setting from data with a margin. Our learning algorithm constructs
a consistent polytope as an intersection of about t log t halfspaces with margins
in time polynomial in t (where t is the number of halfspaces forming an optimal
polytope).
We also identify distinct generalizations of the notion of margin from hyperplanes
to polytopes and investigate how they relate geometrically; this result may be of
interest beyond the learning setting.",https://proceedings.neurips.cc/paper_files/paper/2018/file/22b1f2e0983160db6f7bb9f62f4dbb39-Paper.pdf,2018,"learning, algorithm, halfspaces, margin, polytope, polytopes, setting, also, beyond, consistent","margin, log, polytope, points, polytopes, lemma, rd, time, negative, set","{'polytopes': 0.6633102499569853, 'margin': 0.5624430760063482, 'convex': 0.44315167163815844, 'learning': 0.2174715946481856}","halfspaces, polytope, polytopes, margin, hyperplanes, setting, forming, intersection, geometrically, margins"
Distilled Wasserstein Learning for Word Embedding and Topic Modeling,"Hongteng Xu, Wenlin Wang, Wei Liu, Lawrence Carin","We propose a novel Wasserstein method with a distillation mechanism, yielding joint learning of word embeddings and topics. 
The proposed method is based on the fact that the Euclidean distance between word embeddings may be employed as the underlying distance in the Wasserstein topic model. 
The word distributions of topics, their optimal transport to the word distributions of documents, and the embeddings of words are learned in a unified framework. 
When learning the topic model, we leverage a distilled ground-distance matrix to update the topic distributions and smoothly calculate the corresponding optimal transports. 
Such a strategy provides the updating of word embeddings with robust guidance, improving algorithm convergence. 
As an application, we focus on patient admission records, in which the proposed method embeds the codes of diseases and procedures and learns the topics of admissions, obtaining superior performance on clinically-meaningful disease network construction, mortality prediction as a function of admission codes, and procedure recommendation.",https://proceedings.neurips.cc/paper_files/paper/2018/file/22fb0cee7e1f3bde58293de743871417-Paper.pdf,2018,"word, embeddings, distance, distributions, method, topic, topics, admission, codes, learning","distance, word, topic, embeddings, model, wasserstein, learning, method, admission, diseases","{'distilled': 0.47862073462510046, 'topic': 0.41791726776093013, 'word': 0.41791726776093013, 'modeling': 0.3656826509698295, 'wasserstein': 0.3656826509698295, 'embedding': 0.35991822265449297, 'learning': 0.15691965320505225}","word, embeddings, topics, topic, admission, distance, codes, distributions, wasserstein, admissions"
Inferring Latent Velocities from Weather Radar Data using Gaussian Processes,"Rico Angell, Daniel R. Sheldon","Archived data from the US network of weather radars hold detailed information about bird migration over the last 25 years, including very high-resolution partial measurements of velocity. Historically, most of this spatial resolution is discarded and velocities are summarized at a very small number of locations due to modeling and algorithmic limitations. This paper presents a Gaussian process (GP) model to reconstruct high-resolution full velocity fields across the entire US. The GP faithfully models all aspects of the problem in a single joint framework, including spatially random velocities, partial velocity measurements, station-specific geometries, measurement noise, and an ambiguity known as aliasing. We develop fast inference algorithms based on the FFT; to do so, we employ a creative use of Laplace's method to sidestep the fact that the kernel of the joint process is non-stationary.",https://proceedings.neurips.cc/paper_files/paper/2018/file/23451391cd1399019fa0421129066bc6-Paper.pdf,2018,"resolution, velocity, gp, high, including, joint, measurements, partial, process, us","velocity, radial, method, radar, measurements, data, velocities, inference, grid, likelihood","{'radar': 0.41165388702084305, 'velocities': 0.41165388702084305, 'weather': 0.41165388702084305, 'inferring': 0.3721585000088427, 'latent': 0.3049989752502835, 'processes': 0.2804530512872297, 'gaussian': 0.2677383765896174, 'using': 0.23552690938921192, 'data': 0.2329940568936101}","velocity, resolution, velocities, gp, partial, measurements, joint, us, aliasing, archived"
Generating Informative and Diverse Conversational Responses via Adversarial Information Maximization,"Yizhe Zhang, Michel Galley, Jianfeng Gao, Zhe Gan, Xiujun Li, Chris Brockett, Bill Dolan","Responses generated by neural conversational models tend to lack informativeness and diversity. We present Adversarial Information Maximization (AIM), an adversarial learning framework that addresses these two related but distinct problems. To foster response diversity, we leverage adversarial training that allows distributional matching of synthetic and real responses. To improve informativeness, our framework explicitly optimizes a variational lower bound on pairwise mutual information between query and response. Empirical results from automatic and human evaluations demonstrate that our methods significantly boost informativeness and diversity.",https://proceedings.neurips.cc/paper_files/paper/2018/file/23ce1851341ec1fa9e0c259de10bf87c-Paper.pdf,2018,"adversarial, diversity, informativeness, framework, information, response, responses, addresses, aim, allows","tq, objective, responses, aim, information, training, mmi, model, response, adversarial","{'generating': 0.40266170060596973, 'diverse': 0.38006305024339077, 'informative': 0.38006305024339077, 'conversational': 0.36402905264183727, 'responses': 0.36402905264183727, 'maximization': 0.32539640467770486, 'information': 0.2801991039525468, 'adversarial': 0.23431792802615958, 'via': 0.2112679786015897}","informativeness, diversity, response, adversarial, responses, foster, conversational, information, tend, boost"
Multi-Agent Generative Adversarial Imitation Learning,"Jiaming Song, Hongyu Ren, Dorsa Sadigh, Stefano Ermon","Imitation learning algorithms can be used to learn a policy from expert demonstrations without access to a reward signal. However, most existing approaches are not applicable in multi-agent settings due to the existence of multiple (Nash) equilibria and non-stationary environments.
We propose a new framework for multi-agent imitation learning for general Markov games, where we build upon a generalized notion of inverse reinforcement learning. We further introduce a practical multi-agent actor-critic algorithm with good empirical performance. Our method can be used to imitate complex behaviors in high-dimensional environments with multiple cooperative or competing agents.",https://proceedings.neurips.cc/paper_files/paper/2018/file/240c945bb72980130446fc2b40fbb8e0-Paper.pdf,2018,"agent, learning, multi, environments, imitation, multiple, used, access, actor, agents","agent, agents, policy, reward, learning, expert, multi, πi, policies, function","{'imitation': 0.5256014746140062, 'agent': 0.512375464660558, 'generative': 0.38197558209723836, 'multi': 0.379335397413871, 'adversarial': 0.36071142955311986, 'learning': 0.20322659431664095}","agent, imitation, multi, environments, multiple, imitate, cooperative, equilibria, actor, competing"
Discovery of Latent 3D Keypoints via End-to-end Geometric Reasoning,"Supasorn Suwajanakorn, Noah Snavely, Jonathan J. Tompson, Mohammad Norouzi","This paper presents KeypointNet, an end-to-end geometric reasoning framework to learn an optimal set of category-specific keypoints, along with their detectors to predict 3D keypoints in a single 2D input image. We demonstrate this framework on 3D pose estimation task by proposing a differentiable pose objective that seeks the optimal set of keypoints for recovering the relative pose between two views of an object. Our network automatically discovers a consistent set of keypoints across viewpoints of a single object as well as across all object instances of a given object class. Importantly, we find that our end-to-end approach using no ground-truth keypoint annotations outperforms a fully supervised baseline using the same neural network architecture for the pose estimation task. 
The discovered 3D keypoints across the car, chair, and plane
categories of ShapeNet are visualized at https://keypoints.github.io/",https://proceedings.neurips.cc/paper_files/paper/2018/file/24146db4eb48c718b84cae0a0799dcfc-Paper.pdf,2018,"keypoints, end, object, pose, 3d, across, set, estimation, framework, network","keypoints, 3d, keypoint, object, network, pose, image, supervised, two, one","{'end': 0.6166003959486298, 'keypoints': 0.3635900060005765, 'geometric': 0.32870602104755403, 'discovery': 0.31747588622954814, 'reasoning': 0.3083001979743149, '3d': 0.27341621302129243, 'latent': 0.26938790750639846, 'via': 0.1907678964547214}","keypoints, pose, object, end, 3d, across, set, estimation, chair, keypointnet"
Variational Learning on Aggregate Outputs with Gaussian Processes,"Ho Chung Law, Dino Sejdinovic, Ewan Cameron, Tim Lucas, Seth Flaxman, Katherine Battle, Kenji Fukumizu","While a typical supervised learning framework assumes that the inputs and the outputs are measured at the same levels of granularity, many applications, including global mapping of disease, only have access to outputs at a much coarser level than that of the inputs. Aggregation of outputs makes generalization to new inputs much more difficult. We consider an approach to this problem based on variational learning with a model of output aggregation and Gaussian processes, where aggregation leads to intractability of the standard evidence lower bounds. We propose new bounds and tractable approximations, leading to improved prediction accuracy and scalability to large datasets, while explicitly taking uncertainty into account. We develop a framework which extends to several types of likelihoods, including the Poisson model for aggregated count data. We apply our framework to a challenging and important problem, the fine-scale spatial modelling of malaria incidence, with over 1 million observations.",https://proceedings.neurips.cc/paper_files/paper/2018/file/24b43fb034a10d78bec71274033b4096-Paper.pdf,2018,"aggregation, framework, inputs, outputs, bounds, including, learning, model, much, new","bag, model, xa, vbagg, ya, models, poisson, bags, log, na","{'aggregate': 0.5542721150829638, 'outputs': 0.5231646079233884, 'processes': 0.3776165143087135, 'gaussian': 0.36049681773973496, 'variational': 0.33690642165485657, 'learning': 0.18172256609019916}","aggregation, outputs, inputs, framework, much, coarser, intractability, malaria, bounds, incidence"
Adaptation to Easy Data in Prediction with Limited Advice,"Tobias Sommer Thune, Yevgeny Seldin","We derive an online learning algorithm with improved regret guarantees for ``easy'' loss sequences. We consider two types of ``easiness'': (a) stochastic loss sequences and (b) adversarial loss sequences with small effective range of the losses. While a number of algorithms have been proposed for exploiting small effective range in the full information setting, Gerchinovitz and Lattimore [2016] have shown the impossibility of regret scaling with the effective range of the losses in the bandit setting. We show that just one additional observation per round is sufficient to circumvent the impossibility result. The proposed Second Order Difference Adjustments (SODA) algorithm requires no prior knowledge of the effective range of the losses, $\varepsilon$, and achieves an $O(\varepsilon \sqrt{KT \ln K}) + \tilde{O}(\varepsilon K \sqrt[4]{T})$ expected regret guarantee, where $T$ is the time horizon and $K$ is the number of actions. The scaling with the effective loss range is achieved under significantly weaker assumptions than those made by Cesa-Bianchi and Shamir [2018] in an earlier attempt to circumvent the impossibility result. We also provide a regret lower bound of $\Omega(\varepsilon\sqrt{T K})$, which almost matches the upper bound. In addition, we show that in the stochastic setting SODA achieves an $O\left(\sum_{a:\Delta_a>0} \frac{K\varepsilon^2}{\Delta_a}\right)$ pseudo-regret bound that holds simultaneously with the adversarial regret guarantee. In other words, SODA is safe against an unrestricted oblivious adversary and provides improved regret guarantees for at least two different types of ``easiness'' simultaneously.",https://proceedings.neurips.cc/paper_files/paper/2018/file/253f7b5d921338af34da817c00f42753-Paper.pdf,2018,"regret, effective, range, varepsilon, loss, bound, impossibility, losses, sequences, setting","loss, regret, ln, ηt, st, algorithm, bound, losses, range, effective","{'easy': 0.5048089479114728, 'advice': 0.47647747040421373, 'limited': 0.4172733618546192, 'adaptation': 0.3796115090055553, 'prediction': 0.34391806860723373, 'data': 0.28571935900154366}","regret, varepsilon, soda, range, impossibility, effective, sequences, losses, delta_a, easiness"
Maximum Causal Tsallis Entropy Imitation Learning,"Kyungjae Lee, Sungjoon Choi, Songhwai Oh","In this paper, we propose a novel maximum causal Tsallis entropy (MCTE) framework for imitation learning which can efficiently learn a sparse multi-modal policy distribution from demonstrations. We provide the full mathematical analysis of the proposed framework. First, the optimal solution of an MCTE problem is shown to be a sparsemax distribution, whose supporting set can be adjusted. 
The proposed method has advantages over a softmax distribution in that it can exclude unnecessary actions by assigning zero probability. Second, we prove that an MCTE problem is equivalent to robust Bayes estimation in the sense of the Brier score. Third, we propose a maximum causal Tsallis entropy imitation learning
(MCTEIL) algorithm with a sparse mixture density network (sparse MDN) by modeling mixture weights using a sparsemax distribution. In particular, we show that the causal Tsallis entropy of an MDN encourages exploration and efficient mixture utilization while Boltzmann Gibbs entropy is less effective. We validate the proposed method in two simulation studies and MCTEIL outperforms existing imitation learning methods in terms of average returns and learning multi-modal policies.",https://proceedings.neurips.cc/paper_files/paper/2018/file/2596a54cdbb555cfd09cd5d991da0f55-Paper.pdf,2018,"distribution, entropy, learning, causal, imitation, mcte, mixture, proposed, sparse, tsallis","policy, entropy, distribution, learning, expert, sparse, tsallis, causal, demonstrations, maximum","{'tsallis': 0.5252599986204882, 'imitation': 0.4453856236148254, 'maximum': 0.4453856236148254, 'entropy': 0.4013166477093863, 'causal': 0.36966693904597336, 'learning': 0.17221071061740556}","mcte, tsallis, entropy, imitation, mcteil, mdn, causal, sparsemax, mixture, distribution"
Importance Weighting and Variational Inference,"Justin Domke, Daniel R. Sheldon","Recent work used importance sampling ideas for better variational bounds on likelihoods. We clarify the applicability of these ideas to pure probabilistic inference, by showing the resulting Importance Weighted Variational Inference (IWVI) technique is an instance of augmented variational inference, thus identifying the looseness in previous work. Experiments confirm IWVI's practicality for probabilistic inference. As a second contribution, we investigate inference with elliptical distributions, which improves accuracy in low dimensions, and convergence in high dimensions.",https://proceedings.neurips.cc/paper_files/paper/2018/file/25db67c5657914454081c6a18e93d6dd-Paper.pdf,2018,"inference, variational, dimensions, ideas, importance, iwvi, probabilistic, work, accuracy, applicability","iwvi, z1, elbo, distribution, vi, 10, inference, log, eq, importance","{'weighting': 0.6418661057581763, 'importance': 0.5305643430417485, 'inference': 0.39280147041136004, 'variational': 0.39014918302385937}","inference, iwvi, variational, ideas, dimensions, importance, probabilistic, looseness, elliptical, pure"
Mapping Images to Scene Graphs with Permutation-Invariant Structured Prediction,"Roei Herzig, Moshiko Raboh, Gal Chechik, Jonathan Berant, Amir Globerson","Machine understanding of complex images is a key goal of artificial intelligence. One challenge underlying this task is that visual scenes contain multiple inter-related objects, and that global context plays an important role in interpreting the scene. A natural modeling framework for capturing such effects is structured prediction, which optimizes over complex labels, while modeling within-label interactions. However, it is unclear what principles should guide the design of a structured prediction model that utilizes the power of deep learning components. Here we propose a design principle for such architectures that follows from a natural requirement of permutation invariance. We prove a necessary and sufficient characterization for architectures that follow this invariance, and discuss its implication on model design. Finally, we show that the resulting model achieves new state of the art results on the Visual Genome scene graph labeling benchmark, outperforming all recent approaches.",https://proceedings.neurips.cc/paper_files/paper/2018/file/2668a7105966cae6e23901495176b8f9-Paper.pdf,2018,"design, model, architectures, complex, invariance, modeling, natural, prediction, scene, structured","graph, features, zi, gpi, scene, function, input, prediction, model, permutation","{'permutation': 0.4336663026839833, 'mapping': 0.4093275758230995, 'images': 0.3504516459123835, 'invariant': 0.3504516459123835, 'scene': 0.3433815205764157, 'graphs': 0.3168597645464409, 'structured': 0.30520522236912556, 'prediction': 0.29544974956599007}","design, invariance, scene, structured, architectures, visual, modeling, complex, natural, prediction"
Are ResNets Provably Better than Linear Predictors?,Ohad Shamir,"A residual network (or ResNet) is a standard deep neural net architecture, with state-of-the-art performance across numerous applications. The main premise of ResNets is that they allow the training of each layer to focus on fitting just the residual of the previous layer's output and the target output. Thus, we should expect that the trained network is no worse than what we can obtain if we remove the residual layers and train a shallower network instead. However, due to the non-convexity of the optimization problem, it is not at all clear that ResNets indeed achieve this behavior, rather than getting stuck at some arbitrarily poor local minimum. In this paper, we rigorously prove that arbitrarily deep, nonlinear residual units indeed exhibit this behavior, in the sense that the optimization landscape contains no local minima with value above what can be obtained with a linear predictor (namely a 1-layer network). Notably, we show this under minimal or no assumptions on the precise network architecture, data distribution, or loss function used. We also provide a quantitative analysis of approximate stationary points for this problem. Finally, we show that with a certain tweak to the architecture, training the network with standard stochastic gradient descent achieves an objective value close or better than any linear predictor.",https://proceedings.neurips.cc/paper_files/paper/2018/file/26e359e83860db1d11b6acca57d8ea88-Paper.pdf,2018,"network, residual, architecture, layer, arbitrarily, behavior, deep, indeed, linear, local","fθ, linear, network, residual, local, networks, gradient, al, et, minima","{'better': 0.4928144964186056, 'resnets': 0.4928144964186056, 'predictors': 0.46515618553029814, 'provably': 0.44553230165533503, 'linear': 0.31527517422714196}","residual, network, resnets, layer, architecture, predictor, arbitrarily, indeed, value, behavior"
Meta-Gradient Reinforcement Learning,"Zhongwen Xu, Hado P. van Hasselt, David Silver","The goal of reinforcement learning algorithms is to estimate and/or optimise
the value function. However, unlike supervised learning, no teacher or oracle is
available to provide the true value function. Instead, the majority of reinforcement
learning algorithms estimate and/or optimise a proxy for the value function. This
proxy is typically based on a sampled and bootstrapped approximation to the true
value function, known as a return. The particular choice of return is one of the
chief components determining the nature of the algorithm: the rate at which future
rewards are discounted; when and how values should be bootstrapped; or even the
nature of the rewards themselves. It is well-known that these decisions are crucial
to the overall success of RL algorithms. We discuss a gradient-based meta-learning
algorithm that is able to adapt the nature of the return, online, whilst interacting
and learning from the environment. When applied to 57 games on the Atari 2600
environment over 200 million frames, our algorithm achieved a new state-of-the-art
performance.",https://proceedings.neurips.cc/paper_files/paper/2018/file/2715518c875999308842e3455eda2fe3-Paper.pdf,2018,"learning, function, value, algorithm, algorithms, nature, return, based, bootstrapped, environment","meta, al, et, parameters, gradient, function, learning, return, algorithm, value","{'meta': 0.673830231876151, 'reinforcement': 0.49099711490165104, 'gradient': 0.4797280541297782, 'learning': 0.2733782102671025}","return, value, nature, bootstrapped, optimise, proxy, function, rewards, true, environment"
GPyTorch: Blackbox Matrix-Matrix Gaussian Process Inference with GPU Acceleration,"Jacob Gardner, Geoff Pleiss, Kilian Q. Weinberger, David Bindel, Andrew G. Wilson","Despite advances in scalable models, the inference tools used for Gaussian processes (GPs) have yet to fully capitalize on developments in computing hardware. We present an efficient and general approach to GP inference based on Blackbox Matrix-Matrix multiplication (BBMM). BBMM inference uses a modified batched version of the conjugate gradients algorithm to derive all terms for training and inference in a single call. BBMM reduces the asymptotic complexity of exact GP inference from O(n^3) to O(n^2). Adapting this algorithm to scalable approximations and complex GP models simply requires a routine for efficient matrix-matrix multiplication with the kernel and its derivative. In addition, BBMM uses a specialized preconditioner to substantially speed up convergence. In experiments we show that BBMM effectively uses GPU hardware to dramatically accelerate both exact GP inference and scalable approximations. Additionally, we provide GPyTorch, a software platform for scalable GP inference via BBMM, built on PyTorch.",https://proceedings.neurips.cc/paper_files/paper/2018/file/27e8e17134dd7083b050476733207ea1-Paper.pdf,2018,"inference, bbmm, gp, matrix, scalable, uses, algorithm, approximations, efficient, exact","matrix, kxx, inference, cholesky, log, bbmm, gp, kernel, time, exact","{'matrix': 0.5524223450491467, 'blackbox': 0.37279929453867766, 'gpu': 0.37279929453867766, 'gpytorch': 0.37279929453867766, 'acceleration': 0.30126418759646983, 'process': 0.27621117252457333, 'gaussian': 0.2424674734298972, 'inference': 0.2281412116163059}","bbmm, gp, inference, scalable, matrix, multiplication, uses, hardware, approximations, exact"
Spectral Signatures in Backdoor Attacks,"Brandon Tran, Jerry Li, Aleksander Madry","A recent line of work has uncovered a new form of data poisoning: so-called backdoor attacks. These attacks are particularly dangerous because they do not affect a network's behavior on typical, benign data. Rather, the network only deviates from its expected output when triggered by an adversary's planted perturbation.In this paper, we identify a new property of all known backdoor attacks, which we call spectral signatures. This property allows us to utilize tools from robust statistics to thwart the attacks. We demonstrate the efficacy of these signatures in detecting and removing poisoned examples on real image sets and state of the art neural network architectures. We believe that understanding spectral signatures is a crucial first step towards a principled understanding of backdoor attacks.",https://proceedings.neurips.cc/paper_files/paper/2018/file/280cf18baf4311c92aa5a042336587d3-Paper.pdf,2018,"attacks, backdoor, network, signatures, data, new, property, spectral, understanding, adversary","backdoor, poisoned, examples, 92, 10, attacks, 94, inputs, clean, data","{'backdoor': 0.5563248680220149, 'signatures': 0.5563248680220149, 'attacks': 0.4405038573058783, 'spectral': 0.43239060366665893}","attacks, backdoor, signatures, spectral, property, understanding, network, dangerous, thwart, triggered"
Uncertainty-Aware Attention for Reliable Interpretation and Prediction,"Jay Heo, Hae Beom Lee, Saehoon Kim, Juho Lee, Kwang Joon Kim, Eunho Yang, Sung Ju Hwang","Attention mechanism is effective in both focusing the deep learning models on relevant features and interpreting them. However, attentions may be unreliable since the networks that generate them are often trained in a weakly-supervised manner. To overcome this limitation, we introduce the notion of input-dependent uncertainty to the attention mechanism, such that it generates attention for each feature with varying degrees of noise based on the given input, to learn larger variance on instances it is uncertain about. We learn this Uncertainty-aware Attention (UA) mechanism using variational inference, and validate it on various risk prediction tasks from electronic health records on which our model significantly outperforms existing attention models. The analysis of the learned attentions shows that our model generates attentions that comply with clinicians' interpretation, and provide richer interpretation via learned variance. Further evaluation of both the accuracy of the uncertainty calibration and the prediction performance with ""I don't know'' decision show that UA yields networks with high reliability as well.",https://proceedings.neurips.cc/paper_files/paper/2018/file/285e19f20beded7d215102b49d5c09a0-Paper.pdf,2018,"attention, attentions, mechanism, uncertainty, generates, input, interpretation, learn, learned, model","attention, model, uncertainty, ua, prediction, input, retain, predictions, models, ratio","{'interpretation': 0.48196129864885967, 'reliable': 0.48196129864885967, 'uncertainty': 0.3894794899493141, 'aware': 0.374593243706123, 'attention': 0.3682349564166455, 'prediction': 0.3283523393563501}","attentions, attention, ua, uncertainty, mechanism, generates, interpretation, variance, clinicians, comply"
Attention in Convolutional LSTM for Gesture Recognition,"Liang Zhang, Guangming Zhu, Lin Mei, Peiyi Shen, Syed Afaq Ali Shah, Mohammed Bennamoun","Convolutional long short-term memory (LSTM) networks have been widely used for action/gesture recognition, and different attention mechanisms have also been embedded into the LSTM or the convolutional LSTM (ConvLSTM) networks. Based on the previous gesture recognition architectures which combine the three-dimensional convolution neural network (3DCNN) and ConvLSTM, this paper explores the effects of attention mechanism in ConvLSTM. Several variants of ConvLSTM are evaluated: (a) Removing the convolutional structures of the three gates in ConvLSTM, (b) Applying the attention mechanism on the input of ConvLSTM, (c) Reconstructing the input and (d) output gates respectively with the modified channel-wise attention mechanism. The evaluation results demonstrate that the spatial convolutions in the three gates scarcely contribute to the spatiotemporal feature fusion, and the attention mechanisms embedded into the input and output gates cannot improve the feature fusion. In other words, ConvLSTM mainly contributes to the temporal fusion along with the recurrent steps to learn the long-term spatiotemporal features, when taking as input the spatial or spatiotemporal features. On this basis, a new variant of LSTM is derived, in which the convolutional structures are only embedded into the input-to-state transition of LSTM. The code of the LSTM variants is publicly available.",https://proceedings.neurips.cc/paper_files/paper/2018/file/287e03db1d99e0ec2edb90d079e142f3-Paper.pdf,2018,"convlstm, lstm, attention, input, convolutional, gates, embedded, fusion, mechanism, spatiotemporal","convlstm, attention, input, feature, spatial, variant, convolutional, gates, spatiotemporal, mechanism","{'gesture': 0.5260359071342594, 'lstm': 0.5260359071342594, 'attention': 0.40190946841626224, 'recognition': 0.40190946841626224, 'convolutional': 0.35143990553946547}","convlstm, lstm, gates, spatiotemporal, attention, fusion, input, embedded, convolutional, gesture"
Forward Modeling for Partial Observation Strategy Games - A StarCraft Defogger,"Gabriel Synnaeve, Zeming Lin, Jonas Gehring, Dan Gant, Vegard Mella, Vasil Khalidov, Nicolas Carion, Nicolas Usunier","We formulate the problem of defogging as state estimation and future state prediction from previous, partial observations in the context of real-time strategy games. We propose to employ encoder-decoder neural networks for this task, and introduce proxy tasks and baselines for evaluation to assess their ability of capturing basic game rules and high-level dynamics. By combining convolutional neural networks and recurrent networks, we exploit spatial and sequential correlations and train well-performing models on a large dataset of human games of StarCraft: Brood War. Finally, we demonstrate the relevance of our models to downstream tasks by applying them for enemy unit prediction in a state-of-the-art, rule-based StarCraft bot. We observe improvements in win rates against several strong community bots.",https://proceedings.neurips.cc/paper_files/paper/2018/file/287e041302f34b11ddfb57afc8048cd8-Paper.pdf,2018,"networks, state, games, models, neural, prediction, starcraft, tasks, ability, applying","units, state, models, model, game, games, unit, starcraft, future, prediction","{'defogger': 0.39192017283192504, 'starcraft': 0.39192017283192504, 'forward': 0.36992437103964776, 'observation': 0.3543181013056984, 'partial': 0.3543181013056984, 'strategy': 0.3543181013056984, 'games': 0.2994404491940621, 'modeling': 0.2994404491940621}","starcraft, games, bots, brood, defogging, enemy, win, state, networks, prediction"
PacGAN: The power of two samples in generative adversarial networks,"Zinan Lin, Ashish Khetan, Giulia Fanti, Sewoong Oh","Generative adversarial networks (GANs) are a technique for learning generative models of complex data distributions from samples. Despite remarkable advances in generating realistic images, a major shortcoming of GANs is the fact that they tend to produce samples with little diversity, even when trained on diverse datasets. This phenomenon, known as mode collapse, has been the focus of much recent work. We study a principled approach to handling mode collapse, which we call packing. The main idea is to modify the discriminator to make decisions based on multiple samples from the same class, either real or artificially generated. We draw analysis tools from binary hypothesis testing---in particular the seminal result of Blackwell---to prove a fundamental connection between packing and mode collapse. We show that packing naturally penalizes generators with mode collapse, thereby favoring generator distributions with less mode collapse during the training process. Numerical experiments on benchmark datasets suggest that packing provides significant improvements.",https://proceedings.neurips.cc/paper_files/paper/2018/file/288cc0ff022877bd3df94bc9360b9c5d-Paper.pdf,2018,"collapse, mode, packing, samples, datasets, distributions, gans, generative, advances, adversarial","mode, collapse, discriminator, packing, samples, generator, distribution, number, total, gan","{'pacgan': 0.4848205411877966, 'power': 0.4576108763937248, 'two': 0.4233307528690456, 'samples': 0.4007510127437066, 'generative': 0.29875962439038073, 'adversarial': 0.28212801094488205, 'networks': 0.21651212525667707}","collapse, packing, mode, gans, samples, artificially, blackwell, favoring, seminal, generative"
Bayesian multi-domain learning for cancer subtype discovery from next-generation sequencing count data,"Ehsan Hajiramezanali, Siamak Zamani Dadaneh, Alireza Karbalayghareh, Mingyuan Zhou, Xiaoning Qian","Precision medicine aims for personalized prognosis and therapeutics by utilizing recent genome-scale high-throughput profiling techniques, including next-generation sequencing (NGS). However, translating NGS data faces several challenges. First, NGS count data are often overdispersed, requiring appropriate modeling. Second, compared to the number of involved molecules and system complexity, the number of available samples for studying complex disease, such as cancer, is often limited, especially considering disease heterogeneity. The key question is whether we may integrate available data from all different sources or domains to achieve reproducible disease prognosis based on NGS count data. In this paper, we develop a Bayesian Multi-Domain Learning (BMDL) model that derives domain-dependent latent representations of overdispersed count data based on hierarchical negative binomial factorization for accurate cancer subtyping even if the number of samples for a specific cancer type is small. Experimental results from both our simulated and NGS datasets from The Cancer Genome Atlas (TCGA) demonstrate the promising potential of BMDL for effective multi-domain learning without ``negative transfer'' effects often seen in existing multi-task learning and transfer learning methods.",https://proceedings.neurips.cc/paper_files/paper/2018/file/28a543c2a9eee8c0d6fbfaff7ca7e224-Paper.pdf,2018,"data, ngs, cancer, learning, count, disease, domain, multi, number, often","domain, domains, latent, data, samples, bmdl, cancer, factors, target, model","{'cancer': 0.359522456812687, 'next': 0.359522456812687, 'sequencing': 0.359522456812687, 'subtype': 0.359522456812687, 'count': 0.33934491748680046, 'discovery': 0.3139242242974392, 'generation': 0.2560412628848136, 'domain': 0.24019375207654597, 'multi': 0.22001621275065938, 'bayesian': 0.20570002076725483}","ngs, cancer, count, disease, bmdl, overdispersed, prognosis, genome, domain, data"
Multilingual Anchoring: Interactive Topic Modeling and Alignment Across Languages,"Michelle Yuan, Benjamin Van Durme, Jordan L. Ying","Multilingual topic models can reveal patterns in cross-lingual document collections. However, existing models lack speed and interactivity, which prevents adoption in everyday corpora exploration or quick moving situations (e.g., natural disasters, political instability). First, we propose a multilingual anchoring algorithm that builds an anchor-based topic model for documents in different languages. Then, we incorporate interactivity to develop MTAnchor (Multilingual Topic Anchors), a system that allows users to refine the topic model. We test our algorithms on labeled English, Chinese, and Sinhalese documents. Within minutes, our methods can produce interpretable topics that are useful for specific classification tasks.",https://proceedings.neurips.cc/paper_files/paper/2018/file/28b9f8aa9f07db88404721af4a5b6c11-Paper.pdf,2018,"topic, multilingual, documents, interactivity, model, models, adoption, algorithm, algorithms, allows","topic, words, anchor, multilingual, topics, anchoring, word, language, models, chinese","{'anchoring': 0.3892059577491856, 'languages': 0.3892059577491856, 'multilingual': 0.3892059577491856, 'across': 0.35186429667578484, 'topic': 0.33984296686649534, 'alignment': 0.33002082523320647, 'interactive': 0.33002082523320647, 'modeling': 0.29736669581282615}","topic, multilingual, interactivity, documents, anchoring, chinese, disasters, mtanchor, political, sinhalese"
Generalized Inverse Optimization through Online Learning,"Chaosheng Dong, Yiran Chen, Bo Zeng","Inverse optimization is a powerful paradigm for learning preferences and restrictions that explain the behavior of a decision maker, based on a set of external signal and the corresponding decision pairs. However, most inverse optimization algorithms are designed specifically in batch setting, where all the data is available in advance. As a consequence, there has been rare use of these methods in an online setting suitable for real-time applications. In this paper, we propose a general framework for inverse optimization through online learning. Specifically,  we develop an online learning algorithm that uses an implicit update rule which can handle noisy data. Moreover, under additional regularity assumptions in terms of the data and the model, we prove that our algorithm converges at a rate of $\mathcal{O}(1/\sqrt{T})$ and is statistically consistent. In our experiments, we show the online learning approach can learn the parameters with great accuracy and is very robust to noises, and achieves a dramatic improvement in computational efficacy over the batch learning approach.",https://proceedings.neurips.cc/paper_files/paper/2018/file/28dd2c7955ce926456240b2ff0100bde-Paper.pdf,2018,"learning, online, data, inverse, optimization, algorithm, approach, batch, decision, setting","learning, decision, algorithm, loss, online, inverse, optimization, yt, function, error","{'inverse': 0.5633489888296253, 'generalized': 0.5326205761849866, 'online': 0.46144777311823443, 'optimization': 0.36576188429361334, 'learning': 0.2285551034022635}","online, inverse, batch, optimization, learning, specifically, decision, maker, setting, dramatic"
Sanity Checks for Saliency Maps,"Julius Adebayo, Justin Gilmer, Michael Muelly, Ian Goodfellow, Moritz Hardt, Been Kim","Saliency methods have emerged as a popular tool to highlight features in an input
deemed relevant for the prediction of a learned model. Several saliency methods
have been proposed, often guided by visual appeal on image data. In this work, we
propose an actionable methodology to evaluate what kinds of explanations a given
method can and cannot provide. We find that reliance, solely, on visual assessment
can be misleading. Through extensive experiments we show that some existing
saliency methods are independent both of the model and of the data generating
process. Consequently, methods that fail the proposed tests are inadequate for
tasks that are sensitive to either data or model, such as, finding outliers in the data,
explaining the relationship between inputs and outputs that the model learned,
and debugging the model. We interpret our findings through an analogy with
edge detection in images, a technique that requires neither training data nor model.
Theory in the case of a linear model and a single-layer convolutional neural network
supports our experimental findings.",https://proceedings.neurips.cc/paper_files/paper/2018/file/294a8ed24b1ad22ec2e7efea049b8737-Paper.pdf,2018,"model, data, methods, saliency, findings, learned, proposed, visual, actionable, analogy","model, methods, input, gradient, saliency, explanation, data, randomization, guided, edge","{'checks': 0.5118241680562461, 'saliency': 0.5118241680562461, 'sanity': 0.5118241680562461, 'maps': 0.4627181247591181}","saliency, model, data, findings, methods, visual, actionable, debugging, misleading, analogy"
Differentially Private Uniformly Most Powerful Tests for Binomial Data,"Jordan Awan, Aleksandra Slavković","We derive uniformly most powerful (UMP) tests for simple and one-sided hypotheses for a population proportion within the framework of Differential Privacy (DP), optimizing finite sample performance. We show that in general, DP hypothesis tests can be written in terms of linear constraints, and for exchangeable data can always be expressed as a function of the empirical distribution. Using this structure, we prove a ‘Neyman-Pearson lemma’ for binomial data under DP, where the DP-UMP only depends on the sample sum. Our tests can also be stated as a post-processing of a random variable, whose distribution we coin “Truncated-Uniform-Laplace” (Tulap), a generalization of the Staircase and discrete Laplace distributions. Furthermore, we obtain exact p-values, which are easily computed in terms of the Tulap random variable. We show that our results also apply to distribution-free hypothesis tests for continuous data. Our simulation results demonstrate that our tests have exact type I error, and are more powerful than current techniques.",https://proceedings.neurips.cc/paper_files/paper/2018/file/296472c9542ad4d4788d543508116cbc-Paper.pdf,2018,"tests, dp, data, distribution, also, exact, hypothesis, laplace, powerful, random","dp, tests, ump, test, θ0, tulap, distribution, let, hypothesis, data","{'binomial': 0.43459325292677425, 'powerful': 0.43459325292677425, 'tests': 0.4102025026814741, 'uniformly': 0.4102025026814741, 'differentially': 0.3441154894873001, 'private': 0.3268099767837263, 'data': 0.24597762414149876}","tests, dp, tulap, ump, laplace, hypothesis, powerful, distribution, variable, exact"
Bayesian Alignments of Warped Multi-Output Gaussian Processes,"Markus Kaiser, Clemens Otte, Thomas Runkler, Carl Henrik Ek","We propose a novel Bayesian approach to modelling nonlinear alignments of time series based on latent shared information. We apply the method to the real-world problem of finding common structure in the sensor data of wind turbines introduced by the underlying latent and turbulent wind field. The proposed model allows for both arbitrary alignments of the inputs and non-parametric output warpings to transform the observations. This gives rise to multiple deep Gaussian process models connected via latent generating processes. We present an efficient variational approximation based on nested variational compression and show how the model can be used to extract shared information between dependent time series, recovering an interpretable functional decomposition of the learning problem. We show results for an artificial data set and real-world data of two wind turbines.",https://proceedings.neurips.cc/paper_files/paper/2018/file/2974788b53f73e7950e8aa49f3a306db-Paper.pdf,2018,"data, latent, wind, alignments, based, information, model, problem, real, series","model, gp, data, wind, latent, time, different, function, shared, series","{'alignments': 0.4817941322352146, 'warped': 0.4817941322352146, 'output': 0.4085294532290978, 'processes': 0.32823845161653353, 'gaussian': 0.31335736861031627, 'multi': 0.29484255653913194, 'bayesian': 0.2756575038036063}","wind, turbines, alignments, latent, shared, series, turbulent, warpings, variational, sensor"
Semidefinite relaxations for certifying robustness to adversarial examples,"Aditi Raghunathan, Jacob Steinhardt, Percy S. Liang","Despite their impressive performance on diverse tasks, neural networks fail catastrophically in the presence of adversarial inputs—imperceptibly but adversarially perturbed versions of natural inputs. We have witnessed an arms race between defenders who attempt to train robust networks and attackers who try to construct adversarial examples. One promise of ending the arms race is developing certified defenses, ones which are provably robust against all attackers in some family. These certified defenses are based on convex relaxations which construct an upper bound on the worst case loss over all attackers in the family. Previous relaxations are loose on networks that are not trained against the respective relaxation. In this paper, we propose a new semidefinite relaxation for certifying robustness that applies to arbitrary ReLU networks. We show that our proposed relaxation is tighter than previous relaxations and produces meaningful robustness guarantees on three different foreign networks whose training objectives are agnostic to our proposed relaxation.",https://proceedings.neurips.cc/paper_files/paper/2018/file/29c0605a3bab4229e46723f89cf59d83-Paper.pdf,2018,"networks, relaxation, attackers, relaxations, adversarial, arms, certified, construct, defenses, family","sdp, constraints, relaxation, networks, input, layer, lp, relu, network, bounds","{'certifying': 0.47548044530376044, 'relaxations': 0.4487949762822656, 'semidefinite': 0.4487949762822656, 'examples': 0.4031758656750106, 'robustness': 0.36328336226216545, 'adversarial': 0.2766927984282225}","relaxation, attackers, relaxations, defenses, certified, race, networks, arms, construct, inputs"
Compact Representation of Uncertainty in Clustering,"Craig Greenberg, Nicholas Monath, Ari Kobren, Patrick Flaherty, Andrew McGregor, Andrew McCallum","For many classic structured prediction problems, probability distributions over the dependent variables can be efficiently computed using widely-known algorithms and data structures (such as forward-backward, and its corresponding trellis for exact probability distributions in Markov models). However, we know of no previous work studying efficient representations of exact distributions over clusterings.  This paper presents definitions and proofs for a dynamic-programming inference procedure that computes the partition function, the marginal probability of a cluster, and the MAP clustering---all exactly.  Rather than the Nth Bell number, these exact solutions take time and space proportional to the substantially smaller powerset of N.  Indeed, we improve upon the time complexity of the algorithm introduced by Kohonen and Corander (2016) for this problem by a factor of N.  While still large, this previously unknown result is intellectually interesting in its own right, makes feasible exact inference for important real-world small data applications (such as medicine), and provides a natural stepping stone towards sparse-trellis approximations that enable further scalability (which we also explore). In experiments, we demonstrate the superiority of our approach over approximate methods in analyzing real-world gene expression data used in cancer treatment.",https://proceedings.neurips.cc/paper_files/paper/2018/file/29c4a0e4ef7d1969a94a5f4aadd20690-Paper.pdf,2018,"exact, data, distributions, probability, inference, real, time, trellis, world, 2016","clustering, trellis, cluster, partition, function, pairwise, energy, clusterings, probability, algorithm","{'compact': 0.5484683608227933, 'uncertainty': 0.5227116024226791, 'clustering': 0.4726080679066098, 'representation': 0.4501073783206118}","exact, trellis, probability, distributions, bell, clusterings, corander, intellectually, kohonen, nth"
DeepPINK: reproducible feature selection in deep neural networks,"Yang Lu, Yingying Fan, Jinchi Lv, William Stafford Noble","Deep learning has become increasingly popular in both supervised and unsupervised machine learning thanks to its outstanding empirical performance. However, because of their intrinsic complexity, most deep learning methods are largely treated as black box tools with little interpretability. Even though recent attempts have been made to facilitate the interpretability of deep neural networks (DNNs), existing methods are susceptible to noise and lack of robustness. Therefore, scientists are justifiably cautious about the reproducibility of the discoveries, which is often related to the interpretability of the underlying statistical models. In this paper, we describe a method to increase the interpretability and reproducibility of DNNs by incorporating the idea of feature selection with controlled error rate. By designing a new DNN architecture and integrating it with the recently proposed knockoffs framework, we perform feature selection with a controlled error rate, while maintaining high power. This new method, DeepPINK (Deep feature selection using Paired-Input Nonlinear Knockoffs), is applied to both simulated and real data sets to demonstrate its empirical utility.",https://proceedings.neurips.cc/paper_files/paper/2018/file/29daf9442f3c0b60642b14c081b4a556-Paper.pdf,2018,"deep, interpretability, feature, learning, selection, controlled, dnns, empirical, error, knockoffs","knockoff, feature, deeppink, fdr, features, model, zj, importance, 10, power","{'deeppink': 0.5132952548229267, 'reproducible': 0.5132952548229267, 'selection': 0.4064325588935481, 'feature': 0.3803062029969343, 'deep': 0.24050728134802302, 'neural': 0.2298832507938901, 'networks': 0.22922841972331193}","interpretability, knockoffs, reproducibility, selection, feature, controlled, dnns, deep, cautious, deeppink"
Supervised autoencoders: Improving generalization performance with unsupervised regularizers,"Lei Le, Andrew Patterson, Martha White","Generalization performance is a central goal in machine learning, particularly when learning representations with large neural networks. A common strategy to improve generalization has been through the use of regularizers, typically as a norm constraining the parameters. Regularizing hidden layers in a neural network architecture, however, is not straightforward. There have been a few effective layer-wise suggestions, but without theoretical guarantees for improved performance. In this work, we theoretically and empirically analyze one such model, called a supervised auto-encoder: a neural network that predicts both inputs (reconstruction error) and targets jointly. We provide a novel generalization result for linear auto-encoders, proving uniform stability based on the inclusion of the reconstruction error---particularly as an improvement on simplistic regularization such as norms or even on more advanced regularizations such as the use of auxiliary tasks. Empirically, we then demonstrate that, across an array of architectures with a different number of hidden units and activation functions, the supervised auto-encoder compared to the corresponding standard neural network never harms performance and can significantly improve generalization.",https://proceedings.neurips.cc/paper_files/paper/2018/file/2a38a4a9316c49e5a833517c45d31070-Paper.pdf,2018,"generalization, neural, auto, network, performance, empirically, encoder, error, hidden, improve","sae, error, nn, reconstruction, supervised, performance, hidden, accuracy, generalization, layer","{'regularizers': 0.4358488907574611, 'performance': 0.42095825447435964, 'improving': 0.3747037134150853, 'autoencoders': 0.3625371776904442, 'generalization': 0.357195831995883, 'supervised': 0.3476465414073427, 'unsupervised': 0.33548000568270164}","auto, generalization, reconstruction, particularly, encoder, neural, hidden, supervised, empirically, network"
Understanding Regularized Spectral Clustering via Graph Conductance,"Yilin Zhang, Karl Rohe","This paper uses the relationship between graph conductance and spectral clustering to study (i) the failures of spectral clustering and (ii) the benefits of regularization.  The explanation is simple.  Sparse and stochastic graphs create severaldangling sets'', or small trees that are connected to the core of the graph by only one edge.  Graph conductance is sensitive to these noisy dangling sets and spectral clustering inherits this sensitivity.  The second part of the paper starts from a previously proposed form of regularized spectral clustering and shows that it is related to the graph conductance on aregularized graph''.  When graph conductance is computed on the regularized graph, we call it CoreCut.  Based upon previous arguments that relate graph conductance to spectral clustering (e.g. Cheeger inequality), minimizing CoreCut relaxes to regularized spectral clustering.  Simple inspection of CoreCut reveals why it is less sensitive to dangling sets.   Together, these results show that unbalanced partitions from spectral clustering can be understood as overfitting to noise in the periphery of a sparse and stochastic graph.  Regularization fixes this overfitting.  In addition to this statistical benefit, these results also demonstrate how regularization can improve the computational speed of spectral clustering.  We provide  simulations and data examples to illustrate these results.",https://proceedings.neurips.cc/paper_files/paper/2018/file/2a845d4d23b883acb632fefd814e175f-Paper.pdf,2018,"graph, clustering, spectral, conductance, corecut, regularization, regularized, results, sets, dangling","graph, sc, sets, nodes, conductance, regularized, spectral, set, clustering, corecut","{'conductance': 0.4942646681844303, 'regularized': 0.3994220104479635, 'spectral': 0.38415575238838606, 'understanding': 0.38415575238838606, 'clustering': 0.3611361671756829, 'graph': 0.32146816546057516, 'via': 0.25933009567178567}","spectral, clustering, conductance, graph, corecut, regularized, dangling, regularization, sets, overfitting"
Plug-in Estimation in High-Dimensional Linear Inverse Problems: A Rigorous Analysis,"Alyson K. Fletcher, Parthe Pandit, Sundeep Rangan, Subrata Sarkar, Philip Schniter","Estimating a vector $\mathbf{x}$ from noisy linear measurements $\mathbf{Ax+w}$ often requires use of prior knowledge or structural constraints
on $\mathbf{x}$ for accurate reconstruction. Several recent works have considered combining linear least-squares estimation with a generic or plug-in ``denoiser"" function that can be designed in a modular manner based on the prior knowledge about $\mathbf{x}$. While these methods have shown excellent performance, it has been difficult to obtain rigorous performance guarantees. This work considers plug-in denoising combined with the recently-developed Vector Approximate Message Passing (VAMP) algorithm, which is itself derived via Expectation Propagation techniques. It shown that the mean squared error of this ``plug-in""  VAMP can be exactly predicted for a large class of high-dimensional random $\Abf$ and denoisers. The method is illustrated in image reconstruction and parametric bilinear estimation.",https://proceedings.neurips.cc/paper_files/paper/2018/file/2ad9e5e943e43cad612a7996c12a8796-Paper.pdf,2018,"mathbf, plug, estimation, knowledge, linear, performance, prior, reconstruction, shown, vamp","vamp, amp, x0, 10, separable, denoiser, denoisers, γ1, estimation, se","{'plug': 0.40992248868782966, 'rigorous': 0.40992248868782966, 'problems': 0.3388405369244354, 'dimensional': 0.33126394642157664, 'inverse': 0.33126394642157664, 'high': 0.3186027490923807, 'analysis': 0.2852516488344285, 'estimation': 0.2792734779592541, 'linear': 0.26224550004085445}","mathbf, plug, vamp, reconstruction, vector, estimation, shown, abf, knowledge, ax"
Multitask Boosting for Survival Analysis with Competing Risks,"Alexis Bellot, Mihaela van der Schaar","The co-occurrence of multiple diseases among the general population is an important problem as those patients have more risk of complications and represent a large share of health care expenditure. Learning to predict time-to-event probabilities for these patients is a challenging problem because the risks of events are correlated (there are competing risks) with often only few patients experiencing individual events of interest, and of those only a fraction are actually observed in the data. We introduce in this paper a survival model with the flexibility to leverage a common representation of related events that is designed to correct for the strong imbalance in observed outcomes. The procedure is sequential: outcome-specific survival distributions form the components of nonparametric multivariate estimators which we combine into an ensemble in such a way as to ensure accurate predictions on all outcome types simultaneously. Our algorithm is general and represents the first boosting-like method for time-to-event data with multiple outcomes. We demonstrate the performance of our algorithm on synthetic and real data.",https://proceedings.neurips.cc/paper_files/paper/2018/file/2afe4567e1bf64d32a5527244d104cea-Paper.pdf,2018,"data, events, patients, algorithm, event, general, multiple, observed, outcome, outcomes","data, event, survival, task, time, boosting, speciﬁc, tasks, 01, performance","{'competing': 0.4336886096171848, 'risks': 0.4336886096171848, 'survival': 0.4336886096171848, 'multitask': 0.41539227135970397, 'boosting': 0.4012005288572072, 'analysis': 0.3197342118139101}","patients, events, survival, risks, outcome, event, outcomes, observed, expenditure, experiencing"
Deep Dynamical Modeling and Control of Unsteady Fluid Flows,"Jeremy Morton, Antony Jameson, Mykel J. Kochenderfer, Freddie Witherden","The design of flow control systems remains a challenge due to the nonlinear nature of the equations that govern fluid flow. However, recent advances in computational fluid dynamics (CFD) have enabled the simulation of complex fluid flows with high accuracy, opening the possibility of using learning-based approaches to facilitate controller design. We present a method for learning the forced and unforced dynamics of airflow over a cylinder directly from CFD data. The proposed approach, grounded in Koopman theory, is shown to produce stable dynamical models that can predict the time evolution of the cylinder system over extended time horizons. Finally, by performing model predictive control with the learned dynamical models, we are able to find a straightforward, interpretable control law for suppressing vortex shedding in the wake of the cylinder.",https://proceedings.neurips.cc/paper_files/paper/2018/file/2b0aa0d9e30ea3a55fc271ced8364536-Paper.pdf,2018,"control, cylinder, fluid, cfd, design, dynamical, dynamics, flow, learning, models","time, control, model, koopman, deep, models, xt, dynamical, system, ﬂow","{'fluid': 0.4503320408974756, 'unsteady': 0.4503320408974756, 'flows': 0.42505798000717154, 'dynamical': 0.3639194559569175, 'control': 0.3440691190719806, 'modeling': 0.3440691190719806, 'deep': 0.21100552526541866}","cylinder, fluid, cfd, control, flow, dynamical, dynamics, design, forced, koopman"
Learning Deep Disentangled Embeddings With the F-Statistic Loss,"Karl Ridgeway, Michael C. Mozer","Deep-embedding methods aim to discover representations of a domain that make explicit the domain's class structure and thereby support few-shot learning. Disentangling methods aim to make explicit compositional or factorial structure. We combine these two active but independent lines of research and propose a new paradigm suitable for both goals. We propose and evaluate a novel loss function based on the $F$ statistic, which describes the separation of two or more distributions. By ensuring that distinct classes are well separated on a subset of embedding dimensions, we obtain embeddings that are useful for few-shot learning. By not requiring separation on all dimensions, we encourage the discovery of disentangled representations. Our embedding method matches or beats state-of-the-art, as evaluated by performance on recall@$k$ and few-shot learning tasks. Our method also obtains performance superior to a variety of alternatives on disentangling, as evaluated by two key properties of a disentangled representation: modularity and explicitness. The goal of our work is to obtain more interpretable, manipulable, and generalizable deep representations of concepts and categories.",https://proceedings.neurips.cc/paper_files/paper/2018/file/2b24d495052a8ce66358eb576b8912c8-Paper.pdf,2018,"embedding, learning, representations, shot, two, aim, deep, dimensions, disentangled, disentangling","factor, statistic, class, loss, al, et, embedding, instances, explicitness, modularity","{'statistic': 0.5486831310600513, 'disentangled': 0.4790936505890013, 'embeddings': 0.4535396126347919, 'loss': 0.40652547683087814, 'deep': 0.2570884630879696, 'learning': 0.17989017277499736}","shot, embedding, separation, disentangling, disentangled, representations, evaluated, dimensions, aim, explicit"
Disconnected Manifold Learning for Generative Adversarial Networks,"Mahyar Khayatkhoei, Maneesh K. Singh, Ahmed Elgammal","Natural images may lie on a union of disjoint manifolds rather than one globally connected manifold, and this can cause several difficulties for the training of common Generative Adversarial Networks (GANs). In this work, we first show that single generator GANs are unable to correctly model a distribution supported on a disconnected manifold, and investigate how sample quality, mode dropping and local convergence are affected by this. Next, we show how using a collection of generators can address this problem, providing new insights into the success of such multi-generator GANs. Finally, we explain the serious issues caused by considering a fixed prior over the collection of generators and propose a novel approach for learning the prior and inferring the necessary number of generators without any supervision. Our proposed modifications can be applied on top of any other GAN model to enable learning of distributions supported on disconnected manifolds. We conduct several experiments to illustrate the aforementioned shortcoming of GANs, its consequences in practice, and the effectiveness of our proposed modifications in alleviating these issues.",https://proceedings.neurips.cc/paper_files/paper/2018/file/2b346a0aa375a07f5a90a344a61416c4-Paper.pdf,2018,"gans, generators, collection, disconnected, generator, issues, learning, manifold, manifolds, model","generator, real, data, generators, learning, prior, manifold, distribution, model, disconnected","{'disconnected': 0.6110360482056317, 'manifold': 0.49378655311733305, 'generative': 0.3765370580290342, 'adversarial': 0.35557566202439445, 'networks': 0.2728776983774646, 'learning': 0.20033307762004526}","gans, generators, disconnected, modifications, manifolds, supported, generator, issues, manifold, collection"
Automating Bayesian optimization with Bayesian optimization,"Gustavo Malkomes, Roman Garnett","Bayesian optimization is a powerful tool for global optimization of expensive functions. One of its key components is the underlying probabilistic model used for the objective function f. In practice, however, it is often unclear how one should appropriately choose a model, especially when gathering data is expensive. In this work, we introduce a novel automated Bayesian optimization approach that dynamically selects promising models for explaining the observed data using Bayesian Optimization in the model space. Crucially, we account for the uncertainty in the choice of model; our method is capable of using multiple models to represent its current belief about f and subsequently using this information for decision making. We argue, and demonstrate empirically, that our approach automatically finds suitable models for the objective function, which ultimately results in more-efficient optimization.",https://proceedings.neurips.cc/paper_files/paper/2018/file/2b64c2f19d868305aa8bbc2d72902cc5-Paper.pdf,2018,"optimization, model, bayesian, models, using, approach, data, expensive, function, objective","model, models, function, optimization, bayesian, functions, objective, abo, log, search","{'bayesian': 0.6196197347959346, 'optimization': 0.5682118445202181, 'automating': 0.541485626769859}","optimization, bayesian, expensive, model, objective, using, models, gathering, ultimately, unclear"
Leveraged volume sampling for linear regression,"Michal Derezinski, Manfred K. K. Warmuth, Daniel J. Hsu","Suppose an n x d design matrix in a linear regression problem is given, 
but the response for each point is hidden unless explicitly requested. 
The goal is to sample only a small number k << n of the responses, 
and then produce a weight vector whose sum of squares loss overallpoints is at most 1+epsilon times the minimum. 
When k is very small (e.g., k=d), jointly sampling diverse subsets of
points is crucial. One such method called ""volume sampling"" has
a unique and desirable property that the weight vector it produces is an unbiased
estimate of the optimum. It is therefore natural to ask if this method
offers the optimal unbiased estimate in terms of the number of
responses k needed to achieve a 1+epsilon loss approximation.Surprisingly we show that volume sampling can have poor behavior when
we require a very accurate approximation -- indeed worse than some
i.i.d. sampling techniques whose estimates are biased, such as
leverage score sampling. 
We then develop a new rescaled variant of volume sampling that
produces an unbiased estimate which avoids
this bad behavior and has at least as good a tail bound as leverage
score sampling: sample size k=O(d log d + d/epsilon) suffices to
guarantee total loss at most 1+epsilon times the minimum
with high probability. Thus, we improve on the best previously known
sample size for an unbiased estimator, k=O(d^2/epsilon).Our rescaling procedure leads to a new efficient algorithm
for volume sampling which is based
on a ""determinantal rejection sampling"" technique with
potentially broader applications to determinantal point processes.
Other contributions include introducing the
combinatorics needed for rescaled volume sampling and developing tail
bounds for sums of dependent random matrices which arise in the
process.",https://proceedings.neurips.cc/paper_files/paper/2018/file/2ba8698b79439589fdd2b0f7218d8b07-Paper.pdf,2018,"sampling, epsilon, volume, unbiased, estimate, loss, sample, approximation, behavior, determinantal","sampling, volume, matrix, det, size, loss, leverage, sample, bounds, vector","{'leveraged': 0.5510013659750636, 'volume': 0.5510013659750636, 'regression': 0.3681191062893912, 'sampling': 0.3647288669915431, 'linear': 0.3524998816382685}","sampling, volume, epsilon, unbiased, rescaled, estimate, determinantal, tail, needed, score"
Scalable Robust Matrix Factorization with Nonconvex Loss,"Quanming Yao, James Kwok","Robust matrix factorization (RMF), which uses the $\ell_1$-loss, often outperforms standard matrix factorization using the $\ell_2$-loss, particularly when outliers are present. The state-of-the-art RMF solver is the RMF-MM algorithm, which, however, cannot utilize data sparsity. Moreover, sometimes even the (convex) $\ell_1$-loss is not robust enough. In this paper, we propose the use of nonconvex loss to enhance robustness. To address the resultant difficult optimization problem, we use majorization-minimization (MM) optimization and propose a new MM surrogate. To improve scalability, we exploit data sparsity and optimize the surrogate via its dual with the accelerated proximal gradient algorithm. The resultant algorithm has low time and space complexities and is guaranteed to converge to a critical point. Extensive experiments demonstrate its superiority over the state-of-the-art in terms of both accuracy and scalability.",https://proceedings.neurips.cc/paper_files/paper/2018/file/2c3ddf4bf13852db711dd1901fb517fa-Paper.pdf,2018,"loss, algorithm, mm, rmf, art, data, ell_1, factorization, matrix, optimization","rmf, mm, matrix, loss, time, data, nnz, algorithm, apg, rmfnl","{'factorization': 0.4530831049554941, 'scalable': 0.42602199877378977, 'loss': 0.40611629095861135, 'matrix': 0.40611629095861135, 'nonconvex': 0.40049368208898706, 'robust': 0.35066349055400725}","mm, rmf, loss, ell_1, resultant, surrogate, factorization, scalability, sparsity, matrix"
Pipe-SGD: A Decentralized Pipelined SGD Framework for Distributed Deep Net Training,"Youjie Li, Mingchao Yu, Songze Li, Salman Avestimehr, Nam Sung Kim, Alexander Schwing","Distributed training of deep nets is an important technique to address some of the present day computing challenges like memory consumption and computational demands. Classical distributed approaches, synchronous or asynchronous, are based on the parameter server architecture, i.e., worker nodes compute gradients which are communicated to the parameter server while updated parameters are returned. Recently, distributed training with AllReduce operations gained popularity as well. While many of those operations seem appealing, little is reported about wall-clock training time improvements. In this paper, we carefully analyze the AllReduce based setup, propose timing models which include network latency, bandwidth, cluster size and compute time, and demonstrate that a pipelined training with a width of two combines the best of both synchronous and asynchronous training. Specifically, for a setup consisting of a four-node GPU cluster we show wall-clock time training improvements of up to 5.4x compared to conventional approaches.",https://proceedings.neurips.cc/paper_files/paper/2018/file/2c6a0bae0f071cbbf0bb3d5b11d90a82-Paper.pdf,2018,"training, distributed, time, allreduce, approaches, asynchronous, based, clock, cluster, compute","sgd, communicate, compute, pipe, time, update, sync, decentralized, bit, communication","{'sgd': 0.5724429761965276, 'pipe': 0.36147726960938564, 'pipelined': 0.36147726960938564, 'decentralized': 0.32679598732492854, 'net': 0.29211470504047143, 'framework': 0.2718274554271391, 'distributed': 0.24883497867619378, 'training': 0.23314127516791752, 'deep': 0.16937213926291014}","training, allreduce, synchronous, clock, wall, server, setup, distributed, asynchronous, cluster"
Wasserstein Variational Inference,"Luca Ambrogioni, Umut Güçlü, Yağmur Güçlütürk, Max Hinne, Marcel A. J. van Gerven, Eric Maris","This paper introduces Wasserstein variational inference, a new form of approximate Bayesian inference based on optimal transport theory. Wasserstein variational inference uses a new family of divergences that includes both f-divergences and the Wasserstein distance as special cases. The gradients of the Wasserstein variational loss are obtained by backpropagating through the Sinkhorn iterations. This technique results in a very stable likelihood-free training method that can be used with implicit distributions and probabilistic programs. Using the Wasserstein variational inference framework, we introduce several new forms of autoencoders and test their robustness and performance against existing variational autoencoding techniques.",https://proceedings.neurips.cc/paper_files/paper/2018/file/2c89109d42178de8a367c0228f169bf8-Paper.pdf,2018,"variational, wasserstein, inference, new, divergences, approximate, autoencoders, autoencoding, backpropagating, based","variational, wasserstein, x2, x1, inference, divergence, loss, cost, divergences, cp","{'wasserstein': 0.6630709687142596, 'inference': 0.5310994207788657, 'variational': 0.5275133132885717}","wasserstein, variational, divergences, inference, new, autoencoding, backpropagating, sinkhorn, introduces, forms"
Bayesian Control of Large MDPs with Unknown Dynamics in Data-Poor Environments,"Mahdi Imani, Seyede Fatemeh Ghoreishi, Ulisses M. Braga-Neto","We propose a Bayesian decision making framework for control of Markov Decision Processes (MDPs) with unknown dynamics and large, possibly continuous, state, action, and parameter spaces in data-poor environments. Most of the existing adaptive controllers for MDPs with unknown dynamics are based on the reinforcement learning framework and rely on large data sets acquired by sustained direct interaction with the system or via a simulator. This is not feasible in many applications, due to ethical, economic, and physical constraints. The proposed framework addresses the data poverty issue by decomposing the problem into an offline planning stage that does not rely on sustained direct interaction with the system or simulator and an online execution stage. In the offline process, parallel Gaussian process temporal difference (GPTD) learning techniques are employed for near-optimal Bayesian approximation of the expected discounted reward over a sample drawn from the prior distribution of unknown parameters. In the online stage, the action with the maximum expected return with respect to the posterior distribution of the parameters is selected. This is achieved by an approximation of the posterior distribution using a Markov Chain Monte Carlo (MCMC) algorithm, followed by constructing multiple Gaussian processes over the parameter space for efficient prediction of the means of the expected return at the MCMC sample. The effectiveness of the proposed framework is demonstrated using a simple dynamical system model with continuous state and action spaces, as well as a more complex model for a metastatic melanoma gene regulatory network observed through noisy synthetic gene expression data.",https://proceedings.neurips.cc/paper_files/paper/2018/file/2cbd9c540641923027adb8ab89decc05-Paper.pdf,2018,"data, framework, action, distribution, expected, stage, system, unknown, approximation, bayesian","state, action, sk, expected, process, sample, spaces, ofﬂine, parameter, distribution","{'poor': 0.421901173314495, 'mdps': 0.3982227427884537, 'environments': 0.357744198442705, 'unknown': 0.357744198442705, 'control': 0.3223469614741487, 'dynamics': 0.3223469614741487, 'large': 0.29042979542979164, 'bayesian': 0.24138987278265142, 'data': 0.23879397007549963}","stage, expected, sustained, action, unknown, mdps, gene, simulator, mcmc, offline"
A Smoothed Analysis of the Greedy Algorithm for the Linear Contextual Bandit Problem,"Sampath Kannan, Jamie H. Morgenstern, Aaron Roth, Bo Waggoner, Zhiwei  Steven Wu","Bandit learning is characterized by the tension between long-term exploration and short-term exploitation.  However, as has recently been noted, in settings in which the choices of the learning algorithm correspond to important decisions about individual people (such as criminal recidivism prediction, lending, and sequential drug trials), exploration corresponds to explicitly sacrificing the well-being of one individual for the potential future benefit of others. In such settings, one might like to run a ``greedy'' algorithm, which always makes the optimal decision for the individuals at hand --- but doing this can result in a catastrophic failure to learn. In this paper, we consider the linear contextual bandit problem and revisit the performance of the greedy algorithm.We give a smoothed analysis, showing that even when contexts may be chosen by an adversary, small perturbations of the adversary's choices suffice for the algorithm to achieve ``no regret'', perhaps (depending on the specifics of the setting) with a constant amount of initial training data.  This suggests that in slightly perturbed environments, exploration and exploitation need not be in conflict in the linear setting.",https://proceedings.neurips.cc/paper_files/paper/2018/file/2cfd4560539f887a5e420412b370b361-Paper.pdf,2018,"algorithm, exploration, adversary, bandit, choices, exploitation, greedy, individual, learning, linear","parameter, greedy, regret, setting, arm, adversary, xt, βt, contexts, et","{'smoothed': 0.40599173288460183, 'problem': 0.38078807082803606, 'greedy': 0.3712060832230269, 'bandit': 0.36290578801549633, 'contextual': 0.3555844087714702, 'algorithm': 0.33770212595893045, 'analysis': 0.31249846390236463, 'linear': 0.2872948018457988}","exploration, exploitation, adversary, choices, bandit, greedy, term, individual, algorithm, settings"
Lifelong Inverse Reinforcement Learning,"Jorge Mendez, Shashank Shivkumar, Eric Eaton","Methods for learning from demonstration (LfD) have shown success in acquiring behavior policies by imitating a user. However, even for a single task, LfD may require numerous demonstrations. For versatile agents that must learn many tasks via demonstration, this process would substantially burden the user if each task were learned in isolation. To address this challenge, we introduce the novel problem of lifelong learning from demonstration, which allows the agent to continually build upon knowledge learned from previously demonstrated tasks to accelerate the learning of new tasks, reducing the amount of demonstrations required. As one solution to this problem, we propose the first lifelong learning approach to inverse reinforcement learning, which learns consecutive tasks via demonstration, continually transferring knowledge between tasks to improve performance.",https://proceedings.neurips.cc/paper_files/paper/2018/file/2d969e2cee8cfa07ce7ca0bb13c7a36d-Paper.pdf,2018,"learning, tasks, demonstration, continually, demonstrations, knowledge, learned, lfd, lifelong, problem","tasks, task, reward, irl, elirl, learning, function, lifelong, transfer, performance","{'lifelong': 0.6677255533603452, 'inverse': 0.571682762204179, 'reinforcement': 0.41656573659468843, 'learning': 0.23193618062636445}","demonstration, tasks, lfd, continually, lifelong, demonstrations, user, learning, via, knowledge"
Recurrent World Models Facilitate Policy Evolution,"David Ha, Jürgen Schmidhuber","A generative recurrent neural network is quickly trained in an unsupervised manner to model popular reinforcement learning environments through compressed spatio-temporal representations. The world model's extracted features are fed into compact and simple policies trained by evolution, achieving state of the art results in various environments. We also train our agent entirely inside of an environment generated by its own internal world model, and transfer this policy back into the actual environment. Interactive version of this paper is available at https://worldmodels.github.io",https://proceedings.neurips.cc/paper_files/paper/2018/file/2de5d16682c3c35007e4e92982f1a2ba-Paper.pdf,2018,"model, environment, environments, trained, world, achieving, actual, agent, also, art","environment, model, agent, train, world, actual, time, zt, inside, policy","{'facilitate': 0.5059628897709847, 'world': 0.5059628897709847, 'evolution': 0.44179161742155204, 'policy': 0.33491617792883993, 'recurrent': 0.3319355687041136, 'models': 0.26546790223274114}","environments, environment, worldmodels, actual, io, trained, fed, world, entirely, extracted"
Algorithms and Theory for Multiple-Source Adaptation,"Judy Hoffman, Mehryar Mohri, Ningshan Zhang","We present a number of novel contributions to the multiple-source adaptation problem. We derive new normalized solutions with strong theoretical guarantees for the cross-entropy loss and other similar losses. We also provide new guarantees that hold in the case where the conditional probabilities for the source domains are distinct. Moreover, we give new algorithms for determining the distribution-weighted combination solution for the cross-entropy loss and other losses. We report the results of a series of experiments with real-world datasets. We find that our algorithm outperforms competing approaches by producing a single robust model that performs well on any target mixture distribution. Altogether, our theory, algorithms, and empirical results provide a full solution for the multiple-source adaptation problem with very practical benefits.",https://proceedings.neurips.cc/paper_files/paper/2018/file/2e2079d63348233d91cad1fa9b1361e9-Paper.pdf,2018,"new, source, adaptation, algorithms, cross, distribution, entropy, guarantees, loss, losses","distribution, source, dk, target, loss, domain, al, et, model, problem","{'source': 0.5198509738435773, 'theory': 0.4646816967747222, 'adaptation': 0.4324097384975464, 'multiple': 0.4324097384975464, 'algorithms': 0.37399133917968796}","source, losses, adaptation, cross, entropy, solution, guarantees, new, altogether, loss"
On preserving non-discrimination when combining expert advice,"Avrim Blum, Suriya Gunasekar, Thodoris Lykouris, Nati Srebro","We study the interplay between sequential decision making and avoiding discrimination against protected groups, when examples arrive online and do not follow distributional assumptions. We consider the most basic extension of classical online learning: Given a class of predictors that are individually non-discriminatory with respect to a particular metric, how can we combine them to perform as well as the best predictor, while preserving non-discrimination? Surprisingly we show that this task is unachievable for the prevalent notion of ""equalized odds"" that requires equal false negative rates and equal false positive rates across groups. On the positive side, for another notion of non-discrimination, ""equalized error rates"", we show that running separate instances of the classical multiplicative weights algorithm for each group achieves this guarantee. Interestingly, even for this notion, we show that algorithms with stronger performance guarantees than  multiplicative weights cannot preserve non-discrimination.",https://proceedings.neurips.cc/paper_files/paper/2018/file/2e855f9489df0712b4bd8ea9e2848c5a-Paper.pdf,2018,"discrimination, non, notion, rates, show, classical, equal, equalized, false, groups","group, algorithm, non, negative, respect, equalized, discrimination, fairness, positive, false","{'combining': 0.4607302204273145, 'advice': 0.43487258075797947, 'discrimination': 0.43487258075797947, 'expert': 0.43487258075797947, 'preserving': 0.37232236646246336, 'non': 0.28593921544879647}","discrimination, notion, equalized, rates, false, non, equal, multiplicative, groups, positive"
A Likelihood-Free Inference Framework for Population Genetic Data using Exchangeable Neural Networks,"Jeffrey Chan, Valerio Perrone, Jeffrey Spence, Paul Jenkins, Sara Mathieson, Yun Song","An explosion of high-throughput DNA sequencing in the past decade has led to a surge of interest in population-scale inference with whole-genome data. Recent work in population genetics has centered on designing inference methods for relatively simple model classes, and few scalable general-purpose inference techniques exist for more realistic, complex models. To achieve this, two inferential challenges need to be addressed: (1) population data are exchangeable, calling for methods that efficiently exploit the symmetries of the data, and (2) computing likelihoods is intractable as it requires integrating over a set of correlated, extremely high-dimensional latent variables. These challenges are traditionally tackled by likelihood-free methods that use scientific simulators to generate datasets and reduce them to hand-designed, permutation-invariant summary statistics, often leading to inaccurate inference. In this work, we develop an exchangeable neural network that performs summary statistic-free, likelihood-free inference. Our framework can be applied in a black-box fashion  across a variety of simulation-based tasks, both within and outside biology. We demonstrate the power of our approach on the recombination hotspot testing problem, outperforming the state-of-the-art.",https://proceedings.neurips.cc/paper_files/paper/2018/file/2e9f978b222a956ba6bdf427efbd9ab3-Paper.pdf,2018,"inference, data, free, methods, population, challenges, exchangeable, high, likelihood, summary","recombination, data, training, population, method, neural, posterior, exchangeable, inference, network","{'free': 0.40111811821508236, 'exchangeable': 0.3786060985867028, 'genetic': 0.3786060985867028, 'likelihood': 0.3502443080431094, 'population': 0.3315628741940571, 'framework': 0.30163699509502195, 'inference': 0.24547142344805925, 'using': 0.2294988913305984, 'data': 0.22703086404169115, 'neural': 0.179643852346582}","inference, population, exchangeable, summary, free, challenges, likelihood, calling, dna, genetics"
The Price of Privacy for Low-rank Factorization,Jalaj Upadhyay,"In this paper, we study what  price one has to pay to release \emph{differentially private low-rank factorization} of a matrix. We consider various settings that are close to the real world applications of low-rank factorization: (i) the manner in which matrices are updated (row by row or in an arbitrary manner), (ii) whether matrices are distributed or not, and (iii) how the output is produced (once at the end of all updates, also known as \emph{one-shot algorithms}  or continually). Even though these settings are well studied without privacy, surprisingly, there are no  private algorithm for these settings (except when a matrix is updated row by row). We present the first set of differentially private algorithms for all these settings.Our algorithms when private matrix is updated in an arbitrary manner promise differential privacy with respect to two stronger privacy guarantees than previously studied, use space and time \emph{comparable} to the non-private algorithm, and achieve \emph{optimal accuracy}. To complement our positive results, we also prove that the space required by our algorithms is optimal up to logarithmic factors. When data matrices are distributed over multiple servers, we give a non-interactive differentially private algorithm  with communication cost independent of dimension. In concise, we give algorithms that incur {\em optimal cost across all parameters of interest}. We also perform experiments  to verify that all our algorithms  perform well in practice and outperform the best known algorithm until now for large range of parameters.",https://proceedings.neurips.cc/paper_files/paper/2018/file/2eace51d8f796d04991c831a07059758-Paper.pdf,2018,"algorithms, private, algorithm, emph, row, settings, also, differentially, manner, matrices","private, matrix, algorithm, error, privacy, matrices, additive, algorithms, space, rank","{'price': 0.49060105568788476, 'factorization': 0.46443323237816125, 'privacy': 0.4540483461730226, 'rank': 0.4162898586780565, 'low': 0.405160745532008}","private, row, emph, differentially, updated, settings, algorithms, privacy, matrices, manner"
Efficient Formal Safety Analysis of Neural Networks,"Shiqi Wang, Kexin Pei, Justin Whitehouse, Junfeng Yang, Suman Jana","Neural networks are increasingly deployed in real-world safety-critical domains such as autonomous driving, aircraft collision avoidance, and malware detection. However, these networks have been shown to often mispredict on inputs with minor adversarial or even accidental perturbations. Consequences of such errors can be disastrous and even potentially fatal as shown by the recent Tesla autopilot crash. Thus, there is an urgent need for formal analysis systems that can rigorously check neural networks for violations of different safety properties such as robustness against adversarial perturbations within a certain L-norm of a given image. An effective safety analysis system for a neural network must be able to either ensure that a safety property is satisfied by the network or find a counterexample, i.e., an input for which the network will violate the property. Unfortunately, most existing techniques for performing such analysis struggle to scale beyond very small networks and the ones that can scale to larger networks suffer from high false positives and cannot produce concrete counterexamples in case of a property violation. In this paper, we present a new efficient approach for rigorously checking different safety properties of neural networks that significantly outperforms existing approaches by multiple orders of magnitude. Our approach can check different safety properties and find concrete counterexamples for networks that are 10x larger than the ones supported by existing analysis techniques. We believe that our approach to estimating tight output bounds of a network for a given input range can also help improve the explainability of neural networks and guide the training process of more robust neural networks.",https://proceedings.neurips.cc/paper_files/paper/2018/file/2ecd2bd94734e5dd392d8678bc64cdab-Paper.pdf,2018,"networks, neural, safety, analysis, network, approach, different, existing, properties, property","linear, input, relaxation, safety, properties, network, output, symbolic, networks, 10","{'formal': 0.5583154632500771, 'safety': 0.5583154632500771, 'analysis': 0.3885134650007569, 'efficient': 0.31770231585632625, 'neural': 0.25004589942040445, 'networks': 0.24933363428823113}","safety, networks, check, neural, counterexamples, property, concrete, analysis, rigorously, properties"
Inferring Networks From Random Walk-Based Node Similarities,"Jeremy Hoskins, Cameron Musco, Christopher Musco, Babis Tsourakakis","Digital presence in the world of online social media entails significant privacy risks. In this work we consider a privacy threat to a social network in which an attacker has access to a subset of random walk-based node similarities, such as effective resistances (i.e., commute times) or personalized PageRank scores. Using these similarities, the attacker seeks to infer as much information as possible about the network, including unknown pairwise node similarities and edges.For the effective resistance metric, we show that with just a small subset of measurements, one  can learn a large fraction of edges in a social network. We also show that it is possible to  learn a graph which accurately matches the underlying network on all other effective resistances. This second observation is interesting from a data mining perspective, since it can be expensive to compute all effective resistances or other random walk-based similarities. As an alternative, our graphs learned from just a subset of effective resistances can be used as surrogates in a range of applications that use effective resistances to probe graph structure, including for graph clustering, node centrality evaluation, and anomaly detection.We obtain our results by formalizing the graph learning objective mathematically, using two optimization problems. One formulation is convex and can be solved provably in polynomial time. The other is not, but we solve it efficiently with projected gradient and coordinate descent. We demonstrate the effectiveness of these methods on a number of social networks obtained from Facebook. We also discuss how our methods can be generalized to other random walk-based similarities, such as personalized PageRank scores. Our code is available at https://github.com/cnmusco/graph-similarity-learning.",https://proceedings.neurips.cc/paper_files/paper/2018/file/2f25f6e326adb93c5787175dda209ab6-Paper.pdf,2018,"effective, graph, resistances, similarities, network, social, based, node, random, subset","effective, graph, problem, resistances, resistance, 10, edges, set, graphs, given","{'similarities': 0.456727851604928, 'node': 0.43109483755432726, 'walk': 0.43109483755432726, 'inferring': 0.41290792465405546, 'random': 0.33839478042075044, 'based': 0.3051358107847028, 'networks': 0.20396643585403487}","resistances, similarities, effective, social, graph, walk, node, subset, pagerank, attacker"
Unsupervised Learning of View-invariant Action Representations,"Junnan Li, Yongkang Wong, Qi Zhao, Mohan S. Kankanhalli","The recent success in human action recognition with deep learning methods mostly adopt the supervised learning paradigm, which requires significant amount of manually labeled data to achieve good performance. However, label collection is an expensive and time-consuming process. In this work, we propose an unsupervised learning framework, which exploits unlabeled data to learn video representations. Different from previous works in video representation learning, our unsupervised learning task is to predict 3D motion in multiple target views using video representation from a source view. By learning to extrapolate cross-view motions, the representation can capture view-invariant motion dynamics which is discriminative for the action. In addition, we propose a view-adversarial training method to enhance learning of view-invariant features. We demonstrate the effectiveness of the learned representations for action recognition on multiple datasets.",https://proceedings.neurips.cc/paper_files/paper/2018/file/2f37d10131f2a483a8dd005b3d14b0d9-Paper.pdf,2018,"learning, view, action, representation, video, data, invariant, motion, multiple, propose","view, action, cross, learning, recognition, unsupervised, representations, ﬂow, input, methods","{'view': 0.4799790560551732, 'action': 0.46790106906418466, 'invariant': 0.45743864084255853, 'unsupervised': 0.39390077897236786, 'representations': 0.3896645135476998, 'learning': 0.18558644451491343}","view, video, action, learning, motion, representation, invariant, unsupervised, recognition, representations"
Extracting Relationships by Multi-Domain Matching,"Yitong Li, michael Murias, geraldine Dawson, David E. Carlson","In many biological and medical contexts, we construct a large labeled corpus by aggregating many sources to use in target prediction tasks.  Unfortunately, many of the sources may be irrelevant to our target task, so ignoring the structure of the dataset is detrimental.  This work proposes a novel approach, the Multiple Domain Matching Network (MDMN), to exploit this structure. MDMN embeds all data into a shared feature space while learning which domains share strong statistical relationships. These relationships are often insightful in their own right, and they allow domains to share strength without interference from irrelevant data. This methodology builds on existing distribution-matching approaches by assuming that source domains are varied and outcomes multi-factorial. Therefore, each domain should only match a relevant subset. Theoretical analysis shows that the proposed approach can have a tighter generalization bound than existing multiple-domain adaptation approaches.  Empirically, we show that the proposed methodology handles higher numbers of source domains (up to 21 empirically), and provides state-of-the-art performance on image, text, and multi-channel time series classification, including clinically relevant data of a novel treatment of Autism Spectrum Disorder.",https://proceedings.neurips.cc/paper_files/paper/2018/file/2fd0fd3efa7c4cfb034317b21f3c2d93-Paper.pdf,2018,"domains, data, domain, many, approach, approaches, empirically, existing, irrelevant, matching","domain, domains, source, target, adaptation, data, mdmn, feature, ds, used","{'extracting': 0.5365268536911039, 'relationships': 0.5365268536911039, 'matching': 0.43357465818426, 'domain': 0.3584488135188517, 'multi': 0.32833722664964327}","domains, mdmn, irrelevant, share, domain, sources, methodology, relationships, relevant, matching"
Distributed $k$-Clustering for Data with Heavy Noise,"Shi Li, Xiangyu Guo","In this paper, we consider the $k$-center/median/means clustering with outliers problems (or the $(k, z)$-center/median/means problems) in the distributed setting.  Most previous distributed algorithms have their communication costs linearly depending on $z$, the number of outliers.  Recently Guha et al.[10] overcame this dependence issue by considering bi-criteria approximation algorithms that output solutions with $2z$ outliers.  For the case where $z$ is large, the extra $z$ outliers discarded by the algorithms might be too large, considering that the data gathering process might be costly. In this paper, we improve the number of outliers to the best possible $(1+\epsilon)z$, while maintaining the $O(1)$-approximation ratio and independence of communication cost on $z$.  The problems we consider include the $(k, z)$-center problem, and $(k, z)$-median/means problems in Euclidean metrics. Implementation of the our algorithm for $(k, z)$-center shows that it outperforms many previous algorithms, both in terms of the communication cost and quality of the output solution.",https://proceedings.neurips.cc/paper_files/paper/2018/file/2fe5a27cde066c0b65acb8f2c1717464-Paper.pdf,2018,"outliers, algorithms, center, problems, communication, means, median, approximation, consider, considering","algorithm, points, set, center, outliers, cost, approximation, communication, median, means","{'heavy': 0.5341704899435737, 'noise': 0.5010095868545575, 'clustering': 0.43171345834937563, 'distributed': 0.4067378659163291, 'data': 0.3344235037540279}","outliers, center, median, communication, means, problems, algorithms, considering, might, distributed"
"Interpreting Neural Network Judgments via Minimal, Stable, and Symbolic Corrections","Xin Zhang, Armando Solar-Lezama, Rishabh Singh","We present a new algorithm to generate minimal, stable, and symbolic corrections to an input that will cause a neural network with ReLU activations to change its output. We argue that such a correction is a useful way to provide feedback to a user when the network's output is different from a desired output. Our algorithm generates such a correction by solving a series of linear constraint satisfaction problems. The technique is evaluated on three neural network models: one predicting whether an applicant will pay a mortgage, one predicting whether a first-order theorem can be proved efficiently by a solver using certain heuristics, and the final one judging whether a drawing is an accurate rendition of a canonical drawing of a cat.",https://proceedings.neurips.cc/paper_files/paper/2018/file/300891a62162b960cf02ce3827bb363c-Paper.pdf,2018,"network, one, output, whether, algorithm, correction, drawing, neural, predicting, accurate","corrections, correction, network, features, regions, algorithm, symbolic, drawing, input, neural","{'corrections': 0.39600382639246084, 'interpreting': 0.39600382639246084, 'judgments': 0.39600382639246084, 'minimal': 0.39600382639246084, 'symbolic': 0.3580099561176216, 'stable': 0.3357849667495765, 'network': 0.239116383118557, 'via': 0.20777473445953476, 'neural': 0.17735337718896846}","whether, drawing, correction, output, predicting, one, network, applicant, cat, judging"
Diverse Ensemble Evolution: Curriculum Data-Model Marriage,"Tianyi Zhou, Shengjie Wang, Jeff A. Bilmes","We study a new method (``Diverse Ensemble Evolution (DivE$^2$)'') to train an ensemble of machine learning models that assigns data to models at each training epoch based on each model's current expertise and an intra- and inter-model diversity reward.  DivE$^2$ schedules, over the course of training epochs, the relative importance of these characteristics; it starts by selecting easy samples for each model, and then gradually adjusts towards the models having specialized and complementary expertise on subsets of the training data, thereby encouraging high accuracy of the ensemble.  We utilize an intra-model diversity term on data assigned to each model, and an inter-model diversity term on data assigned to pairs of models, to penalize both within-model and cross-model redundancy.  We formulate the data-model marriage problem as a generalized bipartite matching, represented as submodular maximization subject to two matroid constraints. DivE$^2$ solves a sequence of continuous-combinatorial optimizations with slowly varying objectives and constraints. The combinatorial part handles the data-model marriage while the continuous part updates model parameters based on the assignments. In experiments, DivE$^2$ outperforms other ensemble training methods under a variety of model aggregation techniques, while also maintaining competitive efficiency.",https://proceedings.neurips.cc/paper_files/paper/2018/file/3070e6addcd702cb58de5d7897bfdae1-Paper.pdf,2018,"model, data, dive, ensemble, models, training, diversity, assigned, based, combinatorial","models, model, training, ensemble, diversity, data, sample, different, dive2, samples","{'curriculum': 0.4392974377732457, 'marriage': 0.4392974377732457, 'diverse': 0.41464267377042563, 'ensemble': 0.3971498403248193, 'evolution': 0.383581344574168, 'model': 0.2746313856744952, 'data': 0.24864016941633865}","dive, model, ensemble, diversity, marriage, assigned, expertise, intra, data, inter"
Q-learning with Nearest Neighbors,"Devavrat Shah, Qiaomin Xie","We consider model-free reinforcement learning for infinite-horizon discounted Markov Decision Processes (MDPs) with a continuous state space and unknown transition kernel, when only a single sample path under an arbitrary policy of the system is available.  We consider the Nearest Neighbor Q-Learning (NNQL) algorithm to learn the optimal Q function using nearest neighbor regression method. As the main contribution, we provide tight finite sample analysis of the convergence rate. In particular, for MDPs with a $d$-dimensional state space and the discounted factor $\gamma \in (0,1)$, given an arbitrary sample path with ``covering time'' $L$, we establish that the algorithm is guaranteed to output an $\varepsilon$-accurate estimate of the optimal Q-function using  $\Ot(L/(\varepsilon^3(1-\gamma)^7))$ samples. For instance, for a well-behaved MDP, the covering time of the sample path under the purely random policy scales as $\Ot(1/\varepsilon^d),$ so the sample complexity scales as $\Ot(1/\varepsilon^{d+3}).$ Indeed, we establish a lower bound that argues that the dependence of $ \Omegat(1/\varepsilon^{d+2})$ is necessary.",https://proceedings.neurips.cc/paper_files/paper/2018/file/309fee4e541e51de2e41f21bebb342aa-Paper.pdf,2018,"sample, varepsilon, ot, path, algorithm, arbitrary, consider, covering, discounted, establish","state, policy, ci, yes, space, learning, time, action, function, mdp","{'neighbors': 0.7113082047570791, 'nearest': 0.6580233411730875, 'learning': 0.2470747261794858}","varepsilon, ot, sample, path, covering, discounted, mdps, neighbor, nearest, gamma"
Modular Networks: Learning to Decompose Neural Computation,"Louis Kirsch, Julius Kunze, David Barber","Scaling model capacity has been vital in the success of deep learning. For a typical network, necessary compute resources and training time grow dramatically with model size. Conditional computation is a promising way to increase the number of parameters with a relatively small increase in resources. We propose a training algorithm that flexibly chooses neural modules based on the data to be processed. Both the decomposition and modules are learned end-to-end. In contrast to existing approaches, training does not rely on regularization to enforce diversity in module use. We apply modular networks both to image recognition and language modeling tasks, where we achieve superior performance compared to several baselines. Introspection reveals that modules specialize in interpretable contexts.",https://proceedings.neurips.cc/paper_files/paper/2018/file/310ce61c90f3a46e340ee8257bc70e93-Paper.pdf,2018,"modules, training, end, increase, model, resources, achieve, algorithm, apply, approaches","modules, module, training, modular, networks, layer, controller, number, selection, use","{'modular': 0.5724618963839361, 'decompose': 0.5403335210683532, 'computation': 0.46261425563440317, 'neural': 0.25638148893812124, 'networks': 0.25565117664134457, 'learning': 0.18768623203095722}","modules, resources, increase, training, end, introspection, processed, specialize, grow, vital"
The Convergence of Sparsified Gradient Methods,"Dan Alistarh, Torsten Hoefler, Mikael Johansson, Nikola Konstantinov, Sarit Khirirat, Cedric Renggli","Distributed training of massive machine learning models, in particular deep neural networks, via Stochastic Gradient Descent (SGD) is becoming commonplace. Several families of communication-reduction methods, such as quantization, large-batch methods, and gradient sparsification, have been proposed. To date, gradient sparsification methods--where each node sorts gradients by magnitude, and only communicates a subset of the components, accumulating the rest locally--are known to yield some of the largest practical gains. Such methods can reduce the amount of communication per step by up to \emph{three orders of magnitude}, while preserving model accuracy. Yet, this family of methods currently has no theoretical justification.This is the question we address in this paper. We prove that, under analytic assumptions, sparsifying gradients by magnitude with local error correction provides convergence guarantees, for both convex and non-convex smooth objectives, for data-parallel SGD. The main insight is that sparsification methods implicitly maintain bounds on the maximum impact of stale updates, thanks to selection by magnitude. Our analysis and empirical validation also reveal that these methods do require analytical conditions to converge well, justifying existing heuristics.",https://proceedings.neurips.cc/paper_files/paper/2018/file/314450613369e0ee72d0da7f6fee773c-Paper.pdf,2018,"methods, magnitude, gradient, sparsification, communication, convex, gradients, sgd, accumulating, accuracy","gradient, xt, convergence, vt, gradients, sgd, topk, algorithm, bound, convex","{'sparsified': 0.6103543080154193, 'methods': 0.5025907163994068, 'convergence': 0.4862717217190083, 'gradient': 0.3720349488053251}","magnitude, sparsification, methods, sgd, communication, gradient, gradients, accumulating, communicates, rest"
Deepcode: Feedback Codes via Deep Learning,"Hyeji Kim, Yihan Jiang, Sreeram Kannan, Sewoong Oh, Pramod Viswanath","The design of codes for communicating reliably over a statistically well defined channel is an important endeavor involving deep mathematical research and wide- ranging practical applications. In this work, we present the first family of codes obtained via deep learning, which significantly beats state-of-the-art codes designed over several decades of research. The communication channel under consideration is the Gaussian noise channel with feedback, whose study was initiated by Shannon; feedback is known theoretically to improve reliability of communication, but no practical codes that do so have ever been successfully constructed.We break this logjam by integrating information theoretic insights harmoniously with recurrent-neural-network based encoders and decoders to create novel codes that outperform known codes by 3 orders of magnitude in reliability. We also demonstrate several desirable properties in the codes: (a) generalization to larger block lengths; (b) composability with known codes; (c) adaptation to practical constraints. This result also presents broader ramifications to coding theory: even when the channel has a clear mathematical model, deep learning methodologies, when combined with channel specific information-theoretic insights, can potentially beat state-of-the-art codes, constructed over decades of mathematical research.",https://proceedings.neurips.cc/paper_files/paper/2018/file/31f81674a348511b990af268ca3a8391-Paper.pdf,2018,"codes, channel, deep, known, mathematical, practical, research, also, art, communication","feedback, bits, codes, information, encoder, code, rnn, noise, ber, deepcode","{'deepcode': 0.5579528720714156, 'codes': 0.5266387891681663, 'feedback': 0.47310701086674606, 'via': 0.2927459335220711, 'deep': 0.2614318506188218, 'learning': 0.18292933184098165}","codes, channel, mathematical, reliability, research, decades, practical, constructed, insights, theoretic"
Chain of Reasoning for Visual Question Answering,"Chenfei Wu, Jinlai Liu, Xiaojie Wang, Xuan Dong","Reasoning plays an essential role in Visual Question Answering (VQA). Multi-step and dynamic reasoning is often necessary for answering complex questions. For example, a question ""What is placed next to the bus on the right of the picture?"" talks about a compound object ""bus on the right,"" which is generated by the relation. Furthermore, a new relation including this compound objectis then required to infer the answer. However, previous methods support either one-step or static reasoning, without updating relations or generating compound objects. This paper proposes a novel reasoning model for addressing these problems. A chain of reasoning (CoR) is constructed for supporting multi-step and dynamic reasoning on changed relations and objects. In detail, iteratively, the relational reasoning operations form new relations between objects, and the object refining operations generate new compound objects from relations. We achieve new state-of-the-art results on four publicly available datasets. The visualization of the chain of reasoning illustrates the progress that the CoR generates new compound objects that lead to the answer of the question step by step.",https://proceedings.neurips.cc/paper_files/paper/2018/file/31fefc0e570cb3860f2a6d4b38c6490d-Paper.pdf,2018,"reasoning, compound, new, objects, step, relations, question, answer, answering, bus","reasoning, objects, cor, compound, relations, step, chain, question, vqa, object","{'chain': 0.5057091823749247, 'reasoning': 0.4543048566915719, 'answering': 0.44287292423568364, 'question': 0.44287292423568364, 'visual': 0.3815658082244851}","reasoning, compound, objects, relations, step, cor, bus, question, new, answering"
Hamiltonian Variational Auto-Encoder,"Anthony L. Caterini, Arnaud Doucet, Dino Sejdinovic","Variational Auto-Encoders (VAE) have become very popular techniques to perform
inference and learning in latent variable models as they allow us to leverage the rich
representational power of neural networks to obtain flexible approximations of the
posterior of latent variables as well as tight evidence lower bounds (ELBO). Com-
bined with stochastic variational inference, this provides a methodology scaling to
large datasets. However, for this methodology to be practically efficient, it is neces-
sary to obtain low-variance unbiased estimators of the ELBO and its gradients with
respect to the parameters of interest. While the use of Markov chain Monte Carlo
(MCMC) techniques such as Hamiltonian Monte Carlo (HMC) has been previously
suggested to achieve this [23, 26], the proposed methods require specifying reverse
kernels which have a large impact on performance. Additionally, the resulting
unbiased estimator of the ELBO for most MCMC kernels is typically not amenable
to the reparameterization trick. We show here how to optimally select reverse
kernels in this setting and, by building upon Hamiltonian Importance Sampling
(HIS) [17], we obtain a scheme that provides low-variance unbiased estimators of
the ELBO and its gradients using the reparameterization trick. This allows us to
develop a Hamiltonian Variational Auto-Encoder (HVAE). This method can be
re-interpreted as a target-informed normalizing flow [20] which, within our context,
only requires a few evaluations of the gradient of the sampled likelihood and trivial
Jacobian calculations at each iteration.",https://proceedings.neurips.cc/paper_files/paper/2018/file/3202111cf90e7c816a472aaceb72b0df-Paper.pdf,2018,"elbo, hamiltonian, kernels, obtain, unbiased, variational, auto, carlo, estimators, gradients","pθ, zk, variational, elbo, log, hamiltonian, hvae, z0, dynamics, using","{'auto': 0.565569019447828, 'encoder': 0.5232016351732415, 'hamiltonian': 0.5232016351732415, 'variational': 0.3642139236011101}","elbo, hamiltonian, kernels, unbiased, reparameterization, reverse, trick, obtain, mcmc, variational"
Unorganized Malicious Attacks Detection,"Ming Pang, Wei Gao, Min Tao, Zhi-Hua Zhou","Recommender systems have attracted much attention during the past decade. Many attack detection algorithms have been developed for better recommendations, mostly focusing on shilling attacks, where an attack organizer produces a large number of user profiles by the same strategy to promote or demote an item. This work considers another different attack style: unorganized malicious attacks, where attackers individually utilize a small number of user profiles to attack different items without organizer. This attack style occurs in many real applications, yet relevant study remains open. We formulate the unorganized malicious attacks detection as a matrix completion problem, and propose the Unorganized Malicious Attacks detection (UMA) algorithm, based on the alternating splitting augmented Lagrangian method. We verify, both theoretically and empirically, the effectiveness of the proposed approach.",https://proceedings.neurips.cc/paper_files/paper/2018/file/322f62469c5e3c7dc3e58f5a4d1ea399-Paper.pdf,2018,"attack, attacks, detection, malicious, unorganized, different, many, number, organizer, profiles","attack, attacks, proﬁles, matrix, malicious, detection, rating, user, users, items","{'malicious': 0.5678853000247236, 'unorganized': 0.5678853000247236, 'attacks': 0.4496575284466658, 'detection': 0.3909228557438095}","attack, unorganized, attacks, malicious, organizer, profiles, detection, style, user, demote"
Differentially Private k-Means with Constant Multiplicative Error,"Uri Stemmer, Haim Kaplan","We design new differentially private algorithms for the Euclidean k-means problem, both in the centralized model and in the local model of differential privacy. In both models, our algorithms achieve significantly improved error guarantees than the previous state-of-the-art. In addition, in the local model, our algorithm significantly reduces the number of interaction rounds.Although the problem has been widely studied in the context of differential privacy, all of the existing constructions achieve only super constant approximation factors. We present, for the first time, efficient private algorithms for the problem with constant multiplicative error. Furthermore, we show how to modify our algorithms so they compute private coresets for k-means clustering in both models.",https://proceedings.neurips.cc/paper_files/paper/2018/file/32b991e5d77ad140559ffb95522992d0-Paper.pdf,2018,"algorithms, model, private, problem, achieve, constant, differential, error, local, means","private, centers, algorithm, log, means, points, set, every, error, assign","{'constant': 0.4361245473139321, 'error': 0.4212244933137995, 'means': 0.4212244933137995, 'multiplicative': 0.4212244933137995, 'differentially': 0.38197597819418877, 'private': 0.3627664675936975}","private, algorithms, privacy, differential, means, constant, local, centralized, coresets, constructions"
Multi-value Rule Sets for Interpretable Classification with Feature-Efficient Representations,Tong Wang,"We present the Multi-value Rule Set (MRS) for interpretable
classification with feature efficient presentations. Compared to
rule sets built from single-value rules, MRS adopts a more
generalized form of association rules that allows multiple values
in a condition. Rules of this form are more concise than classical
single-value rules in capturing and describing patterns in data.
Our formulation also pursues a higher efficiency of feature utilization,
which reduces possible cost in data collection and storage.
We propose a Bayesian framework for formulating an MRS model
and develop an efficient inference method for learning a maximum
a posteriori, incorporating theoretically grounded bounds to iteratively
reduce the search space and improve the search efficiency.
Experiments on synthetic and real-world data demonstrate that
MRS models have significantly smaller complexity and fewer features
than baseline models while being competitive in predictive
accuracy.",https://proceedings.neurips.cc/paper_files/paper/2018/file/32bbf7b2bc4ed14eb1e9c2580056a989-Paper.pdf,2018,"mrs, rules, data, value, efficiency, efficient, feature, form, models, rule","model, rule, rules, set, mrs, data, models, value, accuracy, features","{'rule': 0.4267406840087709, 'interpretable': 0.4027906448098875, 'sets': 0.36184778237108645, 'value': 0.3448549591311687, 'feature': 0.31617695210464536, 'classification': 0.3039120966923676, 'representations': 0.29376123461465287, 'multi': 0.2611516119315816, 'efficient': 0.2428313605904422}","mrs, rules, value, rule, efficiency, search, form, feature, formulating, presentations"
Provable Gaussian Embedding with One Observation,"Ming Yu, Zhuoran Yang, Tuo Zhao, Mladen Kolar, Zhaoran Wang","The success of machine learning methods heavily relies on having an appropriate representation for data at hand. Traditionally, machine learning approaches relied on user-defined heuristics to extract features encoding structural information about data. However, recently there has been a surge in approaches that learn how to encode the data automatically in a low dimensional space. Exponential family embedding provides a probabilistic framework for learning low-dimensional representation for various types of high-dimensional data. Though successful in practice, theoretical underpinnings for exponential family embeddings have not been established. In this paper, we study the Gaussian embedding model and develop the first theoretical results for exponential family embedding models. First, we show that, under a mild condition, the embedding structure can be learned from one observation by leveraging the parameter sharing between different contexts even though the data are dependent with each other.  Second, we study properties of two algorithms used for learning the embedding structure and establish convergence results for each of them. The first algorithm is based on a convex relaxation, while the other solved the non-convex formulation of the problem directly. Experiments demonstrate the effectiveness of our approach.",https://proceedings.neurips.cc/paper_files/paper/2018/file/32fdab6559cdfa4f167f8c31b9199643-Paper.pdf,2018,"data, embedding, learning, dimensional, exponential, family, first, approaches, convex, low","embedding, convex, structure, xk, conditional, xj, exponential, family, graph, k2cj","{'observation': 0.509234475333546, 'provable': 0.4776215813155603, 'one': 0.4460086872975745, 'embedding': 0.4235789820395167, 'gaussian': 0.36635361472606326}","embedding, exponential, family, though, dimensional, data, first, convex, relied, representation"
Contamination Attacks and Mitigation in Multi-Party Machine Learning,"Jamie Hayes, Olga Ohrimenko","Machine learning is data hungry; the more data a model has access to in training, the more likely it is to perform well at inference time. Distinct parties may want to combine their local data to gain the benefits of a model trained on a large corpus of data. We consider such a case: parties get access to the model trained on their joint data but do not see each others individual datasets. We show that one needs to be careful when using this multi-party model since a potentially malicious party can taint the model by providing contaminated data. We then show how adversarial training can defend against such attacks by preventing the model from learning trends specific to individual parties data, thereby also guaranteeing party-level membership privacy.",https://proceedings.neurips.cc/paper_files/paper/2018/file/331316d4efb44682092a006307b9ae3a-Paper.pdf,2018,"data, model, parties, party, access, individual, learning, show, trained, training","model, party, training, data, contaminated, parties, set, contamination, records, accuracy","{'contamination': 0.4606270012762078, 'mitigation': 0.4606270012762078, 'party': 0.4606270012762078, 'attacks': 0.36472928410128125, 'machine': 0.35801165577070837, 'multi': 0.281888951277071, 'learning': 0.1510202631604816}","parties, party, data, model, access, individual, contaminated, taint, guaranteeing, trends"
Gradient Sparsification for Communication-Efficient Distributed Optimization,"Jianqiao Wangni, Jialei Wang, Ji Liu, Tong Zhang","Modern large-scale machine learning applications require stochastic optimization algorithms to be implemented on distributed computational architectures. A key bottleneck is the communication overhead for exchanging information such as stochastic gradients among different workers. In this paper,  to reduce the communication cost, we propose a convex optimization formulation to minimize the coding length of stochastic gradients. The key idea is to randomly drop out coordinates of the stochastic gradient vectors and amplify the remaining coordinates appropriately to ensure the sparsified gradient to be unbiased. To solve the optimal sparsification efficiently, several simple and fast algorithms are proposed for an approximate solution, with a theoretical guarantee for sparseness.  Experiments on $\ell_2$ regularized logistic regression, support vector machines, and convolutional neural networks validate our sparsification approaches.",https://proceedings.neurips.cc/paper_files/paper/2018/file/3328bdf9a4b9504b9398284244fe97c2-Paper.pdf,2018,"stochastic, algorithms, communication, coordinates, gradient, gradients, key, optimization, sparsification, among","10, spa, var, gspar, gradient, pi, rho, bits, unisp, wt","{'sparsification': 0.5543636871560312, 'communication': 0.456485747728232, 'distributed': 0.40430571877197014, 'gradient': 0.33790646393768886, 'efficient': 0.3342105635982953, 'optimization': 0.30815754259872424}","stochastic, sparsification, coordinates, communication, gradients, key, sparseness, amplify, ell_2, exchanging"
The promises and pitfalls of Stochastic Gradient Langevin Dynamics,"Nicolas Brosse, Alain Durmus, Eric Moulines","Stochastic Gradient Langevin Dynamics (SGLD) has emerged as a key MCMC algorithm for Bayesian learning from large scale datasets. While SGLD with decreasing step sizes converges weakly to the posterior distribution, the algorithm is often used with a constant step size in practice and has demonstrated spectacular successes in machine learning tasks. The current practice is to set the step size inversely proportional to N where N is the number of training samples. As N becomes large, we show that the SGLD algorithm has an invariant probability measure which significantly departs from the target posterior and behaves like as Stochastic Gradient Descent (SGD). This difference is inherently due to the high variance of the stochastic gradients. Several strategies have been suggested to reduce this effect; among them, SGLD Fixed Point (SGLDFP) uses carefully designed control variates to reduce the variance of the stochastic gradients. We show that SGLDFP gives approximate samples from the posterior distribution, with an accuracy comparable to the Langevin Monte Carlo (LMC) algorithm for a computational cost sublinear in the number of data points. We provide a detailed analysis of the Wasserstein distances between LMC, SGLD, SGLDFP and SGD and explicit expressions of the means and covariance matrices of their invariant distributions. Our findings are supported by limited numerical experiments.",https://proceedings.neurips.cc/paper_files/paper/2018/file/335cd1b90bfa4ee70b39d08a4ae0cf2d-Paper.pdf,2018,"sgld, algorithm, stochastic, posterior, sgldfp, step, distribution, gradient, gradients, invariant","rd, sgld, sgldfp, lmc, sgd, distribution, 10, algorithm, 105, stochastic","{'pitfalls': 0.499716982774734, 'promises': 0.499716982774734, 'langevin': 0.4363378787977823, 'dynamics': 0.3818009078500252, 'gradient': 0.28750219196821586, 'stochastic': 0.28591202520644693}","sgld, sgldfp, lmc, stochastic, posterior, langevin, step, invariant, sgd, algorithm"
Training Deep Neural Networks with 8-bit Floating Point Numbers,"Naigang Wang, Jungwook Choi, Daniel Brand, Chia-Yu Chen, Kailash Gopalakrishnan","The state-of-the-art hardware platforms for training deep neural networks are moving from traditional single precision (32-bit) computations towards 16 bits of precision - in large part due to the high energy efficiency and smaller bit storage associated with using reduced-precision representations. However, unlike inference, training with numbers represented with less than 16 bits has been challenging due to the need to maintain fidelity of the gradient computations during back-propagation. Here we demonstrate, for the first time, the successful training of deep neural networks using 8-bit floating point numbers while fully maintaining the accuracy on a spectrum of deep learning models and datasets. In addition to reducing the data and computation precision to 8 bits, we also successfully reduce the arithmetic precision for additions (used in partial product accumulation and weight updates) from 32 bits to 16 bits through the introduction of a number of key ideas including chunk-based accumulation and floating point stochastic rounding. The use of these novel techniques lays the foundation for a new generation of hardware training platforms with the potential for 2-4 times improved throughput over today's systems.",https://proceedings.neurips.cc/paper_files/paper/2018/file/335d3d1cd7ef05ec77714a215134914c-Paper.pdf,2018,"bits, precision, training, 16, bit, deep, 32, accumulation, computations, due","precision, bit, training, accumulation, 16, fp16, chunk, fp8, rounding, accuracy","{'numbers': 0.478567715398077, 'bit': 0.4517089789977098, 'floating': 0.4517089789977098, 'point': 0.3545761801027094, 'training': 0.3086608669548491, 'deep': 0.22423550401032258, 'neural': 0.21433025360553418, 'networks': 0.21371972583136573}","bits, precision, bit, 16, accumulation, 32, floating, platforms, numbers, hardware"
ATOMO: Communication-efficient Learning via Atomic Sparsification,"Hongyi Wang, Scott Sievert, Shengchao Liu, Zachary Charles, Dimitris Papailiopoulos, Stephen Wright","Distributed model training suffers from communication overheads due to frequent gradient updates transmitted between compute nodes. To mitigate these overheads, several studies propose the use of sparsified stochastic gradients. We argue that these are facets of a general sparsification method that can operate on any possible atomic decomposition. Notable examples include element-wise, singular value, and Fourier decompositions. We present ATOMO, a general framework for atomic sparsification of stochastic gradients. Given a gradient, an atomic decomposition, and a sparsity budget, ATOMO gives a random unbiased sparsification of the atoms minimizing variance. We show that recent methods such as QSGD and TernGrad are special cases of ATOMO, and that sparsifiying the singular value decomposition of neural networks gradients, rather than their coordinates, can lead to significantly faster distributed training.",https://proceedings.neurips.cc/paper_files/paper/2018/file/33b3214d792caf311e1f00fd22b392c5-Paper.pdf,2018,"atomic, atomo, decomposition, gradients, sparsification, distributed, general, gradient, overheads, singular","atomo, qsgd, decomposition, communication, sparsiﬁcation, atomic, gradient, sparsity, pi, training","{'atomo': 0.49882380914543706, 'sparsification': 0.47082823658801953, 'atomic': 0.4509650617506828, 'communication': 0.3876992389834399, 'efficient': 0.28384934647385046, 'via': 0.2617221793826081, 'learning': 0.16354339350307817}","atomo, atomic, sparsification, decomposition, overheads, singular, gradients, distributed, value, sparsifiying"
Depth-Limited Solving for Imperfect-Information Games,"Noam Brown, Tuomas Sandholm, Brandon Amos","A fundamental challenge in imperfect-information games is that states do not have well-defined values. As a result, depth-limited search algorithms used in single-agent settings and perfect-information games do not apply. This paper introduces a principled way to conduct depth-limited solving in imperfect-information games by allowing the opponent to choose among a number of strategies for the remainder of the game at the depth limit. Each one of these strategies results in a different set of values for leaf nodes. This forces an agent to be robust to the different strategies an opponent may employ. We demonstrate the effectiveness of this approach by building a master-level heads-up no-limit Texas hold'em poker AI that defeats two prior top agents using only a 4-core CPU and 16 GB of memory. Developing such a powerful agent would have previously required a supercomputer.",https://proceedings.neurips.cc/paper_files/paper/2018/file/34306d99c63613fad5b2a140398c0420-Paper.pdf,2018,"agent, depth, games, information, strategies, different, imperfect, limit, limited, opponent","strategy, game, depth, state, p2, action, limited, solving, player, subgame","{'imperfect': 0.48294237169612697, 'depth': 0.4366073400687781, 'solving': 0.40950308409705033, 'limited': 0.39919852422074326, 'games': 0.368984529861266, 'information': 0.3360637964979736}","depth, games, strategies, opponent, agent, imperfect, limit, values, information, limited"
Causal Inference and Mechanism Clustering of A Mixture of Additive Noise Models,"Shoubo Hu, Zhitang Chen, Vahid Partovi Nia, Laiwan CHAN, Yanhui Geng","The inference of the causal relationship between a pair of observed variables is a fundamental problem in science, and most existing approaches are based on one single causal model. In practice, however, observations are often collected from multiple sources with heterogeneous causal models due to certain uncontrollable factors, which renders causal analysis results obtained by a single model skeptical. In this paper, we generalize the Additive Noise Model (ANM) to a mixture model, which consists of a finite number of ANMs, and provide the condition of its causal identifiability. To conduct model estimation, we propose Gaussian Process Partially Observable Model (GPPOM), and incorporate independence enforcement into it to learn latent parameter associated with each observation. Causal inference and clustering according to the underlying generating mechanisms of the mixture model are addressed in this work. Experiments on synthetic and real data demonstrate the effectiveness of our proposed approach.",https://proceedings.neurips.cc/paper_files/paper/2018/file/347665597cbfaef834886adbb848011f-Paper.pdf,2018,"model, causal, inference, mixture, single, according, additive, addressed, analysis, anm","anm, mm, causal, model, clustering, data, inference, direction, different, given","{'mechanism': 0.43129411968222575, 'additive': 0.4130987995212147, 'noise': 0.3874539361021236, 'mixture': 0.37770421152391026, 'clustering': 0.33386402794380815, 'causal': 0.32158404534415813, 'inference': 0.2796320024383567, 'models': 0.23974610736209148}","causal, model, mixture, anm, anms, gppom, skeptical, uncontrollable, single, enforcement"
Adversarial Risk and Robustness: General Definitions and Implications for the Uniform Distribution,"Dimitrios Diochnos, Saeed Mahloujifar, Mohammad Mahmoody","We study adversarial perturbations when the instances are uniformly distributed over {0,1}^n. We study both ""inherent"" bounds that apply to any problem and any classifier for such a problem as well as bounds that apply to specific problems and specific hypothesis classes.As the current literature contains multiple  definitions of adversarial risk and robustness, we start by giving a taxonomy for these definitions based on their direct goals; we identify one of them as the one guaranteeing misclassification by pushing the instances to the error region. We then study some classic algorithms for learning monotone conjunctions and compare their adversarial risk and robustness under different definitions by attacking the hypotheses using instances  drawn from the uniform distribution. We observe that sometimes these definitions lead to significantly different bounds. Thus, this study advocates for the use of the error-region definition, even though other definitions, in other contexts with context-dependent assumptions, may coincide with the error-region definition.Using the error-region definition of adversarial perturbations, we then study inherent bounds on risk and robustness of any classifier for any classification problem whose instances are uniformly distributed over {0,1}^n. Using the isoperimetric inequality for the Boolean hypercube, we show that for initial error 0.01, there always exists an adversarial perturbation that changes O(√n) bits of the instances to increase the risk to 0.5, making classifier's decisions meaningless. Furthermore, by also using the central limit theorem we show that when n→∞, at most c√n bits of perturbations, for a universal constant c<1.17, suffice for increasing the risk to 0.5, and the same c√n bits of perturbations on average suffice to increase the risk to 1, hence bounding the robustness by c√n.",https://proceedings.neurips.cc/paper_files/paper/2018/file/3483e5ec0489e5c394b028ec4e81f3e1-Paper.pdf,2018,"risk, adversarial, definitions, error, instances, study, bounds, perturbations, region, robustness","risk, adversarial, robustness, error, deﬁnitions, instance, region, deﬁnition, distribution, instances","{'definitions': 0.41478040690744283, 'implications': 0.41478040690744283, 'uniform': 0.39150161635238556, 'distribution': 0.3621738086865462, 'risk': 0.3351896963016908, 'general': 0.3169064518006441, 'robustness': 0.3169064518006441, 'adversarial': 0.2413700766329065}","definitions, risk, instances, perturbations, region, bits, definition, robustness, adversarial, error"
Analysis of Krylov Subspace Solutions of  Regularized Non-Convex Quadratic Problems,"Yair Carmon, John C. Duchi","We provide convergence rates for Krylov subspace solutions to the trust-region and cubic-regularized (nonconvex) quadratic problems. Such solutions may be efficiently computed by the Lanczos method and have long been used in practice. We prove error bounds of the form $1/t^2$ and $e^{-4t/\sqrt{\kappa}}$, where $\kappa$ is a condition number for the problem, and $t$ is the Krylov subspace order (number of Lanczos iterations). We also provide lower bounds showing that our analysis is sharp.",https://proceedings.neurips.cc/paper_files/paper/2018/file/349f36aa789af083b8e26839bd498af9-Paper.pdf,2018,"bounds, kappa, krylov, lanczos, number, provide, solutions, subspace, 4t, also","fa, str, λmin, problem, aλ, subspace, kstr, λmax, region, trust","{'krylov': 0.404320960613442, 'solutions': 0.38162918732232765, 'subspace': 0.38162918732232765, 'problems': 0.33421033284268714, 'quadratic': 0.33421033284268714, 'regularized': 0.32673727528954716, 'analysis': 0.2813537287073183, 'convex': 0.27012323356349777, 'non': 0.2509303994864612}","krylov, lanczos, kappa, subspace, solutions, 4t, bounds, trust, cubic, provide"
Efficient Anomaly Detection via Matrix Sketching,"Vatsal Sharan, Parikshit Gopalan, Udi Wieder","We consider the problem of finding anomalies in high-dimensional data using popular PCA based anomaly scores.  The naive algorithms for computing these scores explicitly compute the PCA of the covariance matrix which uses space quadratic in the dimensionality of the data. We give the first streaming algorithms that use space that is linear or sublinear in the dimension. We prove general results showing that \emph{any} sketch of a matrix that satisfies a certain operator norm guarantee can be used to approximate these scores. We instantiate these results with powerful matrix sketching techniques such as Frequent Directions and random projections to derive efficient and practical algorithms for these problems, which we validate over real-world data sets. Our main technical contribution is to prove matrix perturbation inequalities for operators arising in the computation of these measures.",https://proceedings.neurips.cc/paper_files/paper/2018/file/34adeb8e3242824038aa65460a47c29e-Paper.pdf,2018,"matrix, algorithms, data, scores, pca, prove, results, space, anomalies, anomaly","scores, matrix, algorithm, anomaly, projection, data, results, rank, subspace, leverage","{'anomaly': 0.5009759364999667, 'sketching': 0.5009759364999667, 'matrix': 0.4105700857401092, 'detection': 0.3814622618126197, 'efficient': 0.3153275147804185, 'via': 0.29074650131433577}","scores, matrix, pca, algorithms, anomalies, instantiate, prove, inequalities, frequent, space"
Backpropagation with Callbacks: Foundations for Efficient and Expressive Differentiable Programming,"Fei Wang, James Decker, Xilun Wu, Gregory Essertel, Tiark Rompf","Training of deep learning models depends on gradient descent and end-to-end
differentiation. Under the slogan of differentiable programming, there is an
increasing demand for efficient automatic gradient computation for emerging
network architectures that incorporate dynamic control flow, especially in NLP.In this paper we propose an implementation of backpropagation using functions
with callbacks, where the forward pass is executed as a sequence of function
calls, and the backward pass as a corresponding sequence of function returns.
A key realization is that this technique of chaining callbacks is well known in the
programming languages community as continuation-passing style (CPS). Any
program can be converted to this form using standard techniques, and hence,
any program can be mechanically converted to compute gradients.Our approach achieves the same flexibility as other reverse-mode automatic
differentiation (AD) techniques, but it can be implemented without any auxiliary
data structures besides the function call stack, and it can easily be combined
with graph construction and native code generation techniques through forms of
multi-stage programming, leading to a highly efficient implementation that
combines the performance benefits of define-then-run software frameworks such
as TensorFlow with the expressiveness of define-by-run frameworks such as PyTorch.",https://proceedings.neurips.cc/paper_files/paper/2018/file/34e157766f31db3d2099831d348a7933-Paper.pdf,2018,"function, programming, techniques, automatic, callbacks, converted, define, differentiation, efficient, end","shift, ad, mode, continuations, numr, figure, function, graph, tensorflow, al","{'callbacks': 0.4272789658748266, 'expressive': 0.4272789658748266, 'foundations': 0.4272789658748266, 'backpropagation': 0.3862844589567877, 'programming': 0.35318734201918445, 'differentiable': 0.3452899520387488, 'efficient': 0.2431376630425252}","programming, callbacks, converted, differentiation, pass, frameworks, techniques, automatic, run, define"
Constrained Cross-Entropy Method for Safe Reinforcement Learning,"Min Wen, Ufuk Topcu","We study a safe reinforcement learning problem in which the constraints are defined as the expected cost over finite-length trajectories. We propose a constrained cross-entropy-based method to solve this problem. The method explicitly tracks its performance with respect to constraint satisfaction and thus is well-suited for safety-critical applications. We show that the asymptotic behavior of the proposed algorithm can be almost-surely described by that of an ordinary differential equation. Then we give sufficient conditions on the properties of this differential equation to guarantee the convergence of the proposed algorithm. At last, we show with simulation experiments that the proposed algorithm can effectively learn feasible policies without assumptions on the feasibility of initial policies, even with non-Markovian objective functions and constraint functions.",https://proceedings.neurips.cc/paper_files/paper/2018/file/34ffeb359a192eb8174b6854643cc046-Paper.pdf,2018,"algorithm, proposed, constraint, differential, equation, functions, method, policies, problem, show","πθ, policy, policies, fv, function, algorithm, constraint, feasible, objective, distribution","{'safe': 0.46164442002094463, 'cross': 0.42724915285112036, 'constrained': 0.41091931710710144, 'entropy': 0.40394443671386987, 'method': 0.38124703405728166, 'reinforcement': 0.31132189009980715, 'learning': 0.1733383324452008}","equation, constraint, differential, policies, proposed, surely, tracks, markovian, ordinary, functions"
Graphical model inference: Sequential Monte Carlo meets deterministic approximations,"Fredrik Lindsten, Jouni Helske, Matti Vihola","Approximate inference in probabilistic graphical models (PGMs) can be grouped into deterministic methods and Monte-Carlo-based methods. The former can often provide accurate and rapid inferences, but are typically associated with biases that are hard to quantify. The latter enjoy asymptotic consistency, but can suffer from high computational costs. In this paper we present a way of bridging the gap between deterministic and stochastic inference. Specifically, we suggest an efficient sequential Monte Carlo (SMC) algorithm for PGMs which can leverage the output from deterministic inference methods. While generally applicable, we show explicitly how this can be done with loopy belief propagation, expectation propagation, and Laplace approximations. The resulting algorithm can be viewed as a post-correction of the biases associated with these methods and, indeed, numerical results show clear improvements over the baseline deterministic methods as well as over ""plain"" SMC.",https://proceedings.neurips.cc/paper_files/paper/2018/file/351869bde8b9d6ad1e3090bd173f600d-Paper.pdf,2018,"methods, deterministic, inference, algorithm, associated, biases, carlo, monte, pgms, propagation","smc, x1, model, twisting, inference, methods, fj, used, functions, variables","{'deterministic': 0.3970104317691632, 'meets': 0.38026146260022675, 'approximations': 0.34768040011448065, 'graphical': 0.34768040011448065, 'carlo': 0.32691463744999194, 'monte': 0.32691463744999194, 'sequential': 0.32691463744999194, 'model': 0.26295297590223954, 'inference': 0.25740397783842733}","deterministic, pgms, smc, methods, biases, propagation, carlo, monte, inference, associated"
Playing hard exploration games by watching YouTube,"Yusuf Aytar, Tobias Pfaff, David Budden, Thomas Paine, Ziyu Wang, Nando de Freitas","Deep reinforcement learning methods traditionally struggle with tasks where environment rewards are particularly sparse. One successful method of guiding exploration in these domains is to imitate trajectories provided by a human demonstrator. However, these demonstrations are typically collected under artificial conditions, i.e. with access to the agent’s exact environment setup and the demonstrator’s action and reward trajectories. Here we propose a method that overcomes these limitations in two stages. First, we learn to map unaligned videos from multiple sources to a common representation using self-supervised objectives constructed over both time and modality (i.e. vision and sound). Second, we embed a single YouTube video in this representation to learn a reward function that encourages an agent to imitate human gameplay. This method of one-shot imitation allows our agent to convincingly exceed human-level performance on the infamously hard exploration games Montezuma’s Revenge, Pitfall! and Private Eye for the first time, even if the agent is not presented with any environment rewards.",https://proceedings.neurips.cc/paper_files/paper/2018/file/35309226eb45ec366ca86a4329a2b7c3-Paper.pdf,2018,"agent, environment, human, method, demonstrator, exploration, first, imitate, learn, one","embedding, using, agent, learning, youtube, reward, domain, imitation, methods, environment","{'playing': 0.4494549637981289, 'watching': 0.4494549637981289, 'youtube': 0.4494549637981289, 'hard': 0.3924505915925923, 'exploration': 0.34932844869692015, 'games': 0.34339900209711693}","agent, demonstrator, environment, imitate, human, trajectories, rewards, exploration, reward, gameplay"
Improved Algorithms for Collaborative PAC Learning,"Huy Nguyen, Lydia Zakynthinou","We study a recent model of collaborative PAC learning where $k$ players with $k$ different tasks collaborate to learn a single classifier that works for all tasks. Previous work showed that when there is a classifier that has very small error on all tasks, there is a collaborative algorithm that finds a single classifier for all tasks and has $O((\ln (k))^2)$ times the worst-case sample complexity for learning a single task. In this work, we design new algorithms for both the realizable and the non-realizable setting, having sample complexity only $O(\ln (k))$ times the worst-case sample complexity for learning a single task. The sample complexity upper bounds of our algorithms match previous lower bounds and in some range of parameters are even better than previous algorithms that are allowed to output different classifiers for different tasks.",https://proceedings.neurips.cc/paper_files/paper/2018/file/3569df159ec477451530c4455b2a9e86-Paper.pdf,2018,"tasks, complexity, sample, single, algorithms, classifier, different, learning, previous, bounds","ln, algorithm, classiﬁer, error, sample, samples, probability, algorithms, complexity, setting","{'collaborative': 0.5707898169163967, 'pac': 0.4999214100713786, 'improved': 0.4613089837246804, 'algorithms': 0.4106377765157852, 'learning': 0.20699808378307674}","tasks, single, classifier, complexity, sample, realizable, ln, collaborative, worst, previous"
Scaling provable adversarial defenses,"Eric Wong, Frank Schmidt, Jan Hendrik Metzen, J. Zico Kolter","Recent work has developed methods for learning deep network classifiers that are \emph{provably} robust to norm-bounded adversarial perturbation; however, these methods are currently only possible for relatively small feedforward networks.  In this paper, in an effort to scale these approaches to substantially larger models, we extend previous work in three main directly.  First, we present a technique for extending these training procedures to much more general networks, with skip connections (such as ResNets) and general nonlinearities; the approach is fully modular, and can be implemented automatically analogously to automatic differentiation. Second, in the specific case of $\ell_\infty$ adversarial perturbations and networks with ReLU nonlinearities, we adopt a nonlinear random projection for training, which scales \emph{linearly} in the number of hidden units (previous approached scaled quadratically).  Third, we show how to further improve robust error through cascade models.  On both MNIST and CIFAR data sets, we train classifiers that improve substantially on the state of the art in provable robust adversarial error bounds: from 5.8% to 3.1% on MNIST  (with $\ell_\infty$ perturbations of $\epsilon=0.1$), and from 80% to 36.4% on CIFAR (with $\ell_\infty$ perturbations of $\epsilon=2/255$).",https://proceedings.neurips.cc/paper_files/paper/2018/file/358f9e7be09177c17d0d17ff73584307-Paper.pdf,2018,"adversarial, ell_, infty, networks, perturbations, robust, cifar, classifiers, emph, epsilon","dual, networks, robust, network, 2017, error, bound, adversarial, al, et","{'defenses': 0.5954859304406804, 'scaling': 0.519960451013008, 'provable': 0.5049325537443121, 'adversarial': 0.3465266976710359}","ell_, infty, perturbations, nonlinearities, robust, adversarial, substantially, classifiers, emph, cifar"
Understanding Batch Normalization,"Nils Bjorck, Carla P. Gomes, Bart Selman, Kilian Q. Weinberger","Batch normalization (BN) is a technique to normalize activations in intermediate layers of deep neural networks. Its tendency to improve accuracy and speed up training have established BN as a favorite technique in deep learning. Yet, despite its enormous success, there remains little consensus on the exact reason and mechanism behind these improvements. In this paper we take a step towards a better understanding of BN, following an empirical approach. We conduct several experiments, and show that BN primarily enables training with larger learning rates, which is the cause for faster convergence and better generalization. For networks without BN we demonstrate how large gradient updates can result in diverging loss and activations growing uncontrollably with network depth, which limits possible learning rates. BN avoids this problem by constantly correcting activations to be zero-mean and of unit standard deviation, which enables larger gradient steps, yields faster convergence and may help bypass sharp local minima. We further show various ways in which gradients and activations of deep unnormalized networks are ill-behaved. We contrast our results against recent findings in random matrix theory, shedding new light on classical initialization schemes and their consequences.",https://proceedings.neurips.cc/paper_files/paper/2018/file/36072923bfc3cf47745d704feb489480-Paper.pdf,2018,"bn, activations, deep, learning, networks, better, convergence, enables, faster, gradient","latexit, sha1_base64, bn, dj0vpw8plj, network, learning, 47bnqvtfwhh4z4adeynecg1c9sprk1vbg4vt0s7u3v7b, cwx, networks, batch","{'normalization': 0.5986323052761968, 'batch': 0.5835685788400161, 'understanding': 0.5487140210258438}","bn, activations, larger, enables, rates, faster, deep, technique, favorite, tendency"
Navigating with Graph Representations for Fast and Scalable Decoding of Neural Language Models,"Minjia Zhang, Wenhan Wang, Xiaodong Liu, Jianfeng Gao, Yuxiong He","Neural language models (NLMs) have recently gained a renewed interest by achieving state-of-the-art performance across many natural language processing (NLP) tasks. However, NLMs are very computationally demanding largely due to the computational cost of the decoding process, which consists of a softmax layer over a large vocabulary.We observe that in the decoding of many NLP tasks, only the probabilities of the top-K hypotheses need to be calculated preciously and K is often much smaller than the vocabulary size.
This paper proposes a novel softmax layer approximation algorithm, called Fast Graph Decoder (FGD), which quickly identifies, for a given context, a set of K words that are most likely to occur according to a NLM.  We demonstrate that FGD reduces the decoding time by an order of magnitude while attaining close to the full softmax baseline accuracy on neural machine translation and language modeling tasks. We also prove the theoretical guarantee on the softmax approximation quality.",https://proceedings.neurips.cc/paper_files/paper/2018/file/366f0bc7bd1d4bf414073cabbadfdfcd-Paper.pdf,2018,"softmax, decoding, language, tasks, approximation, fgd, layer, many, neural, nlms","fgd, softmax, graph, top, small, world, layer, vocabulary, decoding, word","{'navigating': 0.45866706356777404, 'decoding': 0.40049432076593616, 'scalable': 0.35648833963366094, 'language': 0.3504373621618999, 'representations': 0.3157388266921779, 'fast': 0.3009072714011245, 'graph': 0.29831559683182307, 'models': 0.24065279420730146, 'neural': 0.20541759272920165}","softmax, decoding, fgd, nlms, vocabulary, nlp, language, tasks, layer, nlm"
Learning from discriminative feature feedback,"Sanjoy Dasgupta, Akansha Dey, Nicholas Roberts, Sivan Sabato","We consider the problem of learning a multi-class classifier from labels as well as simple explanations that we call ""discriminative features"". We show that such explanations can be provided whenever the target concept is a decision tree, or more generally belongs to a particular subclass of DNF formulas. We present an efficient online algorithm for learning from such feedback and we give tight bounds on the number of mistakes made during the learning process. These bounds depend only on the size of the target concept and not on the overall number of available features, which could be infinite. We also demonstrate the learning procedure experimentally.",https://proceedings.neurips.cc/paper_files/paper/2018/file/36ac8e558ac7690b6f44e2cb5ef93322-Paper.pdf,2018,"learning, bounds, concept, explanations, features, number, target, algorithm, also, available","features, learning, feedback, feature, algorithm, label, number, instance, mistake, one","{'discriminative': 0.6269793698864711, 'feedback': 0.5632481724914966, 'feature': 0.4921574737585099, 'learning': 0.21778288946883498}","explanations, concept, target, features, bounds, dnf, formulas, learning, belongs, mistakes"
Zeroth-order (Non)-Convex Stochastic Optimization via Conditional Gradient and Gradient Updates,"Krishnakumar Balasubramanian, Saeed Ghadimi","In this paper, we propose and analyze zeroth-order stochastic approximation algorithms for nonconvex and convex optimization. Specifically, we propose generalizations of the conditional gradient algorithm achieving rates similar to the standard stochastic gradient algorithm using only zeroth-order information. Furthermore, under a structural sparsity assumption, we first illustrate an implicit regularization phenomenon where the standard stochastic gradient algorithm with zeroth-order information adapts to the sparsity of the problem at hand by just varying the step-size. Next, we propose a truncated stochastic gradient algorithm with zeroth-order information, whose rate of convergence depends only poly-logarithmically on the dimensionality.",https://proceedings.neurips.cc/paper_files/paper/2018/file/36d7534290610d9b7e9abed244dd2f28-Paper.pdf,2018,"algorithm, gradient, order, stochastic, zeroth, information, propose, sparsity, standard, achieving","order, optimization, stochastic, zeroth, gradient, information, algorithm, oracle, convex, function","{'gradient': 0.4684371037323365, 'updates': 0.38425503466195465, 'zeroth': 0.38425503466195465, 'conditional': 0.3289854319651302, 'order': 0.30613752509066494, 'convex': 0.2719818502463593, 'non': 0.25265695747471734, 'stochastic': 0.23292309546071444, 'optimization': 0.2135982026890725, 'via': 0.2135982026890725}","zeroth, order, stochastic, gradient, sparsity, information, algorithm, standard, poly, logarithmically"
Coordinate Descent with Bandit Sampling,"Farnood Salehi, Patrick Thiran, Elisa Celis","Coordinate descent methods minimize a cost function by updating a single decision variable (corresponding to one coordinate) at a time. Ideally, we would update the decision variable that yields the largest marginal decrease in the cost function. However, finding this coordinate would require checking all of them, which is not computationally practical. Therefore, we propose a new adaptive method for coordinate descent. First, we define a lower bound on the decrease of the cost function when a coordinate is updated and, instead of calculating this lower bound for all coordinates, we use a multi-armed bandit algorithm to learn which coordinates result in the largest marginal decrease and simultaneously perform coordinate descent. We show that our approach improves the convergence of the coordinate methods both theoretically and experimentally.",https://proceedings.neurips.cc/paper_files/paper/2018/file/36f4d832825380f102846560a5104c90-Paper.pdf,2018,"coordinate, cost, decrease, descent, function, bound, coordinates, decision, largest, lower","coordinate, xt, cost, rt, update, gi, function, max_r, convergence, b_max_r","{'coordinate': 0.6076879796781232, 'bandit': 0.5202807366474798, 'sampling': 0.4261699090311187, 'descent': 0.42237718104164296}","coordinate, decrease, largest, coordinates, cost, descent, marginal, would, variable, function"
PAC-Bayes bounds for stable algorithms with instance-dependent priors,"Omar Rivasplata, Emilio Parrado-Hernandez, John S. Shawe-Taylor, Shiliang Sun, Csaba Szepesvari","PAC-Bayes bounds have been proposed to get risk estimates based on a training sample. In this paper the PAC-Bayes approach is combined with stability of the hypothesis learned by a Hilbert space valued algorithm. The PAC-Bayes setting is used with a Gaussian prior centered at the expected output. Thus a novelty of our paper is using priors defined in terms of the data-generating distribution. Our main result estimates the risk of the randomized algorithm in terms of the hypothesis stability coefficients. We also provide a new bound for the SVM classifier, which is compared to other known bounds experimentally. Ours appears to be the first uniform hypothesis stability-based bound that evaluates to non-trivial values.",https://proceedings.neurips.cc/paper_files/paper/2018/file/386854131f58a556343e056f03626e00-Paper.pdf,2018,"bayes, hypothesis, pac, stability, algorithm, based, bound, bounds, estimates, paper","bound, stability, bounds, sn, z1, space, theorem, gaussian, svm, bayes","{'instance': 0.40231836631637197, 'dependent': 0.3773427440992821, 'stable': 0.3773427440992821, 'bayes': 0.3596222581277285, 'priors': 0.3596222581277285, 'pac': 0.3523671218821921, 'bounds': 0.29457209201981693, 'algorithms': 0.2894359964025356}","pac, bayes, hypothesis, stability, estimates, risk, terms, bounds, bound, centered"
DropMax: Adaptive Variational Softmax,"Hae Beom Lee, Juho Lee, Saehoon Kim, Eunho Yang, Sung Ju Hwang","We propose DropMax, a stochastic version of softmax classifier which at each iteration drops non-target classes according to dropout probabilities adaptively decided for each instance. Specifically, we overlay binary masking variables over class output probabilities, which are input-adaptively learned via variational inference. This stochastic regularization has an effect of building an ensemble classifier out of exponentially many classifiers with different decision boundaries. Moreover, the learning of dropout rates for non-target classes on each instance allows the classifier to focus more on classification against the most confusing classes. We validate our model on multiple public datasets for classification, on which it obtains significantly improved accuracy over the regular softmax classifier and other baselines. Further analysis of the learned dropout probabilities shows that our model indeed selects confusing classes more often when it performs classification.",https://proceedings.neurips.cc/paper_files/paper/2018/file/389bc7bb1e1c2a5e7e147703232a88f6-Paper.pdf,2018,"classes, classifier, classification, dropout, probabilities, adaptively, confusing, instance, learned, model","class, dropmax, classes, dropout, softmax, model, training, posterior, network, probability","{'dropmax': 0.619487613946061, 'softmax': 0.5409180010449518, 'adaptive': 0.4264450358746203, 'variational': 0.37654673506862757}","dropout, classifier, classes, probabilities, confusing, softmax, adaptively, classification, instance, target"
Multi-Layered Gradient Boosting Decision Trees,"Ji Feng, Yang Yu, Zhi-Hua Zhou","Multi-layered distributed representation is believed to be the key ingredient of deep neural networks especially in cognitive tasks like computer vision. While non-differentiable models such as gradient boosting decision trees (GBDTs) are still the dominant methods for modeling discrete or tabular data, they are hard to incorporate with such representation learning ability. In this work, we propose the multi-layered GBDT forest (mGBDTs), with an explicit emphasis on exploring the ability to learn hierarchical distributed representations by stacking several layers of regression GBDTs as its building block. The model can be jointly trained by a variant of target propagation across layers, without the need to derive backpropagation nor differentiability. Experiments confirmed the effectiveness of the model in terms of performance and representation learning ability.",https://proceedings.neurips.cc/paper_files/paper/2018/file/39027dfad5138c9ca0c474d71db915c3-Paper.pdf,2018,"ability, representation, distributed, gbdts, layered, layers, learning, model, multi, across","layer, training, output, fi, model, multi, layered, structure, loss, representation","{'layered': 0.5095979806655243, 'trees': 0.4607055248877421, 'boosting': 0.44496566974483603, 'decision': 0.38321281591178974, 'multi': 0.3118576200368685, 'gradient': 0.29318702688549575}","gbdts, layered, ability, representation, distributed, layers, gbdt, ingredient, mgbdts, stacking"
Rest-Katyusha: Exploiting the Solution's Structure via Scheduled Restart Schemes,"Junqi Tang, Mohammad Golbabaee, Francis Bach, Mike E. davies","We propose a structure-adaptive variant of the state-of-the-art stochastic variance-reduced gradient algorithm Katyusha for  regularized empirical risk minimization. The proposed method is able to exploit the intrinsic low-dimensional structure of the solution, such as sparsity or low rank which is enforced by a non-smooth regularization, to achieve even faster convergence rate. This provable algorithmic improvement is done by restarting the Katyusha algorithm according to restricted strong-convexity constants. We demonstrate the effectiveness of our approach via numerical experiments.",https://proceedings.neurips.cc/paper_files/paper/2018/file/39059724f73a9969845dfe4146c5660e-Paper.pdf,2018,"algorithm, katyusha, low, structure, able, according, achieve, adaptive, algorithmic, approach","katyusha, convergence, gradient, rest, restart, algorithm, rsc, convexity, strong, linear","{'katyusha': 0.37528343539656694, 'rest': 0.37528343539656694, 'restart': 0.37528343539656694, 'scheduled': 0.37528343539656694, 'schemes': 0.33927754552765876, 'solution': 0.33927754552765876, 'exploiting': 0.31020801311893176, 'structure': 0.26726576578984246, 'via': 0.19690318865582596}","katyusha, restarting, low, enforced, structure, constants, done, provable, convexity, according"
Automatic Program Synthesis of Long Programs with a Learned Garbage Collector,"Amit Zohar, Lior Wolf","We consider the problem of generating automatic code given sample input-output pairs. We train a neural network to map from the current state and the outputs to the program's next statement. The neural network optimizes multiple tasks concurrently: the next operation out of a set of high level commands, the operands of the next statement, and which variables can be dropped from memory. Using our method we are able to create programs that are more than twice as long as existing state-of-the-art solutions, while improving the success rate for comparable lengths, and cutting the run-time by two orders of magnitude. Our code, including an implementation of various literature baselines, is publicly available at https://github.com/amitz25/PCCoder",https://proceedings.neurips.cc/paper_files/paper/2018/file/390e982518a50e280d8e2b535462ec1f-Paper.pdf,2018,"next, code, network, neural, state, statement, able, amitz25, art, automatic","program, length, programs, network, variable, input, output, model, search, pccoder","{'collector': 0.408992917652257, 'garbage': 0.408992917652257, 'automatic': 0.34679885420746015, 'learned': 0.34679885420746015, 'long': 0.34679885420746015, 'programs': 0.3380721566635449, 'program': 0.31788026155977883, 'synthesis': 0.2988320715407116}","next, statement, code, amitz25, operands, pccoder, commands, cutting, dropped, concurrently"
Quantifying Learning Guarantees for Convex but Inconsistent Surrogates,"Kirill Struminsky, Simon Lacoste-Julien, Anton Osokin","We study consistency properties of machine learning methods based on minimizing convex surrogates. We extend the recent framework of Osokin et al. (2017) for the quantitative analysis of consistency properties to the case of inconsistent surrogates. Our key technical contribution consists in a new lower bound on the calibration function for the quadratic surrogate, which is non-trivial (not always zero) for inconsistent cases. The new bound allows to quantify the level of inconsistency of the setting and shows how learning with inconsistent surrogates can have guarantees on sample complexity and optimization difficulty. We apply our theory to two concrete cases: multi-class classification with the tree-structured loss and ranking with the mean average precision loss. The results show the approximation-computation trade-offs caused by inconsistent surrogates and their potential benefits.",https://proceedings.neurips.cc/paper_files/paper/2018/file/39461a19e9eddfb385ea76b26521ea48-Paper.pdf,2018,"inconsistent, surrogates, bound, cases, consistency, learning, loss, new, properties, 2017","surrogate, loss, function, bound, calibration, 14, consistency, consistent, inconsistent, score","{'inconsistent': 0.4921868988378013, 'surrogates': 0.4921868988378013, 'quantifying': 0.4645638107141016, 'guarantees': 0.406840018958747, 'convex': 0.3288256845006046, 'learning': 0.16136742913612903}","inconsistent, surrogates, consistency, cases, osokin, properties, inconsistency, bound, ranking, loss"
Unsupervised Text Style Transfer using Language Models as Discriminators,"Zichao Yang, Zhiting Hu, Chris Dyer, Eric P. Xing, Taylor Berg-Kirkpatrick","Binary classifiers are employed as discriminators in GAN-based unsupervised style transfer models to ensure that transferred sentences are similar to sentences in the target domain. One difficulty with the binary discriminator is that error signal is sometimes insufficient to train the model to produce rich-structured language. In this paper, we propose a technique of using a target domain language model as the discriminator to provide richer, token-level feedback during the learning process. Because our language model scores sentences directly using a product of locally normalized probabilities, it offers more stable and more useful training signal to the generator. We train the generator to minimize the negative log likelihood (NLL) of generated sentences evaluated by a language model. By using continuous approximation of the discrete samples, our model can be trained using back-propagation in an end-to-end way. Moreover, we find empirically with a language model as a structured discriminator, it is possible to eliminate the adversarial training steps using negative samples, thus making training more stable. We compare our model with previous work using convolutional neural networks (CNNs) as discriminators and show our model outperforms them significantly in three tasks including word substitution decipherment, sentiment modification and related language translation.",https://proceedings.neurips.cc/paper_files/paper/2018/file/398475c83b47075e8897a083e97eb9f0-Paper.pdf,2018,"model, language, using, sentences, discriminator, training, binary, discriminators, domain, end","al, et, model, language, sentences, 2017, training, use, style, data","{'discriminators': 0.4617602336565815, 'style': 0.41745748391940973, 'text': 0.373154734182238, 'language': 0.352800650160406, 'transfer': 0.34212332864345807, 'unsupervised': 0.3213238396320789, 'using': 0.26419515068606253, 'models': 0.24227571436869202}","language, sentences, discriminator, model, using, discriminators, generator, negative, binary, stable"
Multi-View Silhouette and Depth Decomposition for High Resolution 3D Object Representation,"Edward Smith, Scott Fujimoto, David Meger","We consider the problem of scaling deep generative shape models to high-resolution. Drawing motivation from the canonical view representation of objects, we introduce a novel method for the fast up-sampling of 3D objects in voxel space through networks that perform super-resolution on the six orthographic depth projections. This allows us to generate high-resolution objects with more efficient scaling than methods which work directly in 3D. We decompose the problem of 2D depth super-resolution into silhouette and depth prediction to capture both structure and fine detail. This allows our method to generate sharp edges more easily than an individual network. We evaluate our work on multiple experiments concerning high-resolution 3D objects, and show our system is capable of accurately predicting novel objects at resolutions as large as 512x512x512 -- the highest resolution reported for this task. We achieve state-of-the-art performance on 3D object reconstruction from RGB images on the ShapeNet dataset, and further demonstrate the first effective 3D super-resolution method.",https://proceedings.neurips.cc/paper_files/paper/2018/file/39ae2ed11b14a4ccb41d35e9d1ba5d11-Paper.pdf,2018,"resolution, 3d, objects, depth, high, method, super, allows, generate, novel","resolution, 3d, object, objects, high, depth, super, method, low, silhouette","{'silhouette': 0.39041944137562506, 'depth': 0.35296135481234053, 'resolution': 0.35296135481234053, 'view': 0.33104977882405207, 'decomposition': 0.32271938425461294, 'high': 0.3034444578036119, '3d': 0.29359169226076753, 'object': 0.2747689910071454, 'representation': 0.27168011627247907, 'multi': 0.2389241763566572}","resolution, 3d, objects, super, depth, scaling, high, generate, 512x512x512, orthographic"
Domain Adaptation by Using Causal Inference to Predict Invariant Conditional Distributions,"Sara Magliacane, Thijs van Ommen, Tom Claassen, Stephan Bongers, Philip Versteeg, Joris M. Mooij","An important goal common to domain adaptation and causal inference is to make accurate predictions when the distributions for the source (or training) domain(s) and target (or test) domain(s) differ. In many cases, these different distributions can be modeled as different contexts of a single underlying system, in which each distribution corresponds to a different perturbation of the system, or in causal terms, an intervention. We focus on a class of such causal domain adaptation problems, where data for one or more source domains are given, and the task is to predict the distribution of a certain target variable from measurements of other variables in one or more target domains. We propose an approach for solving these problems that exploits causal inference and does not rely on prior knowledge of the causal graph, the type of interventions or the intervention targets. We demonstrate our approach by evaluating a possible implementation on simulated and real world data.",https://proceedings.neurips.cc/paper_files/paper/2018/file/39e98420b5e98bfbdc8a619bef7b8f61-Paper.pdf,2018,"causal, domain, different, target, adaptation, approach, data, distribution, distributions, domains","c1, domains, causal, target, variables, source, domain, data, system, al","{'predict': 0.4207300736520729, 'conditional': 0.36021405716366717, 'invariant': 0.36021405716366717, 'distributions': 0.3529469817721556, 'adaptation': 0.3351973347398576, 'causal': 0.3137072195249666, 'domain': 0.2977994653650993, 'inference': 0.27278274294128974, 'using': 0.25503309590899165}","causal, domain, intervention, target, adaptation, source, different, domains, system, distributions"
Confounding-Robust Policy Improvement,"Nathan Kallus, Angela Zhou","We study the problem of learning personalized decision policies from observational data while accounting for possible unobserved confounding in the data-generating process. Unlike previous approaches that assume unconfoundedness, i.e., no unobserved confounders affected both treatment assignment and outcomes, we calibrate policy learning for realistic violations of this unverifiable assumption with uncertainty sets motivated by sensitivity analysis in causal inference. Our framework for confounding-robust policy improvement optimizes the minimax regret of a candidate policy against a baseline or reference ""status quo"" policy, over an uncertainty set around nominal propensity weights. We prove that if the uncertainty set is well-specified, robust policy learning can do no worse than the baseline, and only improve if the data supports it. We characterize the adversarial subproblem and use efficient algorithmic solutions to optimize over parametrized spaces of decision policies such as logistic treatment assignment. We assess our methods on synthetic data and a large clinical trial, demonstrating that confounded selection can hinder policy learning and lead to unwarranted harm, while our robust approach guarantees safety and focuses on well-evidenced improvement.",https://proceedings.neurips.cc/paper_files/paper/2018/file/3a09a524440d44d7f19870070a5ad42f-Paper.pdf,2018,"policy, data, learning, robust, uncertainty, assignment, baseline, confounding, decision, improvement","policy, xi, data, wi, treatment, pn, confounding, policies, regret, robust","{'improvement': 0.6043053731048338, 'confounding': 0.5703898409882401, 'policy': 0.4000128269362632, 'robust': 0.3866008065440332}","policy, uncertainty, confounding, unobserved, robust, treatment, assignment, improvement, baseline, policies"
Near Optimal Exploration-Exploitation in Non-Communicating Markov Decision Processes,"Ronan Fruit, Matteo Pirotta, Alessandro Lazaric","While designing the state space of an MDP, it is common to include states that are transient or not reachable by any policy (e.g., in mountain car, the product space of speed and position contains configurations that are not physically reachable). This results in weakly-communicating or multi-chain MDPs. In this paper, we introduce TUCRL, the first algorithm able to perform efficient exploration-exploitation in any finite Markov Decision Process (MDP) without requiring any form of prior knowledge. In particular, for any MDP with $S^c$ communicating states, $A$ actions and $\Gamma^c \leq S^c$ possible communicating next states, we derive a $O(D^c \sqrt{\Gamma^c S^c A T}) regret bound, where $D^c$ is the diameter (i.e., the length of the longest shortest path between any two states) of the communicating part of the MDP. This is in contrast with optimistic algorithms (e.g., UCRL, Optimistic PSRL) that suffer linear regret in weakly-communicating MDPs, as well as posterior sampling or regularised algorithms (e.g., REGAL), which require prior knowledge on the bias span of the optimal policy to bias the exploration to achieve sub-linear regret. We also prove that in weakly-communicating MDPs, no algorithm can ever achieve a logarithmic growth of the regret without first suffering a linear regret for a number of steps that is exponential in the parameters of the MDP. Finally, we report numerical simulations supporting our theoretical findings and showing how TUCRL overcomes the limitations of the state-of-the-art.",https://proceedings.neurips.cc/paper_files/paper/2018/file/3a20f62a0af1aa152670bab3c602feed-Paper.pdf,2018,"communicating, mdp, regret, states, linear, mdps, weakly, achieve, algorithm, algorithms","states, sc, regret, communicating, st, tucrl, state, mdp, ucrl, set","{'communicating': 0.4156625948967353, 'exploitation': 0.4156625948967353, 'near': 0.3524542979241782, 'markov': 0.32912599618418803, 'exploration': 0.323064113542264, 'decision': 0.31257430269161124, 'processes': 0.28318411830969703, 'optimal': 0.27034562742035967, 'non': 0.2579692648898728}","communicating, mdp, regret, states, mdps, weakly, reachable, tucrl, optimistic, gamma"
Mesh-TensorFlow: Deep Learning for Supercomputers,"Noam Shazeer, Youlong Cheng, Niki Parmar, Dustin Tran, Ashish Vaswani, Penporn Koanantakool, Peter Hawkins, HyoukJoong Lee, Mingsheng Hong, Cliff Young, Ryan Sepassi, Blake Hechtman","Batch-splitting (data-parallelism) is the dominant distributed Deep Neural Network (DNN) training strategy, due to its universal applicability and its amenability to Single-Program-Multiple-Data (SPMD) programming.  However, batch-splitting suffers from problems including the inability to train very large models (due to memory constraints), high latency, and inefficiency at small batch sizes.  All of these can be solved by more general distribution strategies (model-parallelism).  Unfortunately, efficient model-parallel algorithms tend to be complicated to discover, describe, and to implement, particularly on large clusters.  We introduce Mesh-TensorFlow, a language for specifying a general class of distributed tensor computations.  Where data-parallelism can be viewed as splitting tensors and operations along the ""batch"" dimension, in Mesh-TensorFlow, the user can specify any tensor-dimensions to be split across any dimensions of a multi-dimensional mesh of processors.  A Mesh-TensorFlow graph compiles into a SPMD program consisting of parallel operations coupled with collective communication primitives such as Allreduce.  We use Mesh-TensorFlow to implement an efficient data-parallel, model-parallel version of the Transformer sequence-to-sequence model.  Using TPU meshes of up to 512 cores, we train Transformer models with up to 5 billion parameters, surpassing SOTA results on WMT'14 English-to-French translation task and the one-billion-word Language modeling benchmark.  Mesh-Tensorflow is available at https://github.com/tensorflow/mesh",https://proceedings.neurips.cc/paper_files/paper/2018/file/3a37abdeefe1dab1b30f7c5c7e581b93-Paper.pdf,2018,"mesh, tensorflow, batch, data, model, parallel, parallelism, splitting, billion, dimensions","mesh, batch, dimension, dimensions, model, tensor, processor, tensorflow, layout, processors","{'mesh': 0.548240745753997, 'supercomputers': 0.548240745753997, 'tensorflow': 0.548240745753997, 'deep': 0.2568811810485045, 'learning': 0.17974513319817367}","mesh, tensorflow, parallelism, parallel, splitting, batch, spmd, transformer, billion, tensor"
Foreground Clustering for Joint Segmentation and Localization in Videos and Images,Abhishek Sharma,"This paper presents a novel framework in which video/image segmentation and localization are cast into a single optimization problem that integrates information from low level appearance cues with that of high level localization cues in a very weakly supervised manner. The proposed framework leverages two representations at different levels, exploits the spatial relationship between bounding boxes and superpixels as linear constraints and  simultaneously discriminates between foreground and background at bounding box and superpixel level. Different from previous approaches that mainly rely on discriminative clustering, we incorporate a foreground model that minimizes the histogram difference of an object across all image frames. Exploiting the geometric relation between the superpixels and bounding boxes enables the transfer of segmentation cues to improve localization output and vice-versa. Inclusion of the foreground model generalizes our discriminative framework to video data where the background tends to be similar and thus, not discriminative. We demonstrate the effectiveness of our unified framework on the YouTube Object video dataset, Internet Object Discovery dataset and Pascal VOC 2007.",https://proceedings.neurips.cc/paper_files/paper/2018/file/3b3dbaf68507998acd6a5a5254ab2d76-Paper.pdf,2018,"framework, bounding, cues, discriminative, foreground, level, localization, object, video, background","foreground, segmentation, bounding, image, video, box, object, superpixels, discriminative, model","{'foreground': 0.43006508816073963, 'localization': 0.40592847286837336, 'videos': 0.40592847286837336, 'joint': 0.364666656491067, 'segmentation': 0.364666656491067, 'images': 0.3475414554061269, 'clustering': 0.3142285248608715}","foreground, cues, localization, bounding, discriminative, superpixels, video, object, background, boxes"
Privacy Amplification by Subsampling: Tight Analyses via Couplings and Divergences,"Borja Balle, Gilles Barthe, Marco Gaboardi","Differential privacy comes equipped with multiple analytical tools for the
design of private data analyses. One important tool is the so-called ""privacy
amplification by subsampling"" principle, which ensures that a differentially
private mechanism run on a random subsample of a population provides higher
privacy guarantees than when run on the entire population. Several instances
of this principle have been studied for different random subsampling methods,
each with an ad-hoc analysis.  In this paper we present a general method that
recovers and improves prior analyses, yields lower bounds and derives new
instances of privacy amplification by subsampling. Our method leverages a
characterization of differential privacy as a divergence which emerged in the
program verification community. Furthermore, it introduces new tools,
including advanced joint convexity and privacy profiles, which might be of
independent interest.",https://proceedings.neurips.cc/paper_files/paper/2018/file/3b5020bb891119b9f5130f1fea9bd773-Paper.pdf,2018,"privacy, subsampling, amplification, analyses, differential, instances, method, new, population, principle","privacy, mechanism, subsampling, ampliﬁcation, δm, al, et, let, relation, dp","{'analyses': 0.4022332769880571, 'amplification': 0.3796586711162596, 'couplings': 0.3796586711162596, 'divergences': 0.3796586711162596, 'subsampling': 0.3636417333524104, 'tight': 0.35121803123102413, 'privacy': 0.32505018971676375, 'via': 0.2110431938962021}","privacy, subsampling, amplification, analyses, run, principle, private, population, tools, differential"
The Description Length of Deep Learning models,"Léonard Blier, Yann Ollivier","Deep learning models often have more parameters than observations, and still perform well. This is sometimes described as a paradox. In this work, we show experimentally that despite their huge number of parameters, deep neural networks can compress the data losslessly even when taking the cost of encoding the parameters into account. Such a compression viewpoint originally motivated the use of variational methods in neural networks. However, we show that these variational methods provide surprisingly poor compression bounds, despite being explicitly built to minimize such bounds. This might explain the relatively poor practical performance of variational methods in deep learning. Better encoding methods, imported from the Minimum Description Length (MDL) toolbox, yield much better compression values on deep networks.",https://proceedings.neurips.cc/paper_files/paper/2018/file/3b712de48137572f3849aabd5666a4e3-Paper.pdf,2018,"deep, methods, compression, networks, parameters, variational, better, bounds, despite, encoding","model, data, compression, codelength, encoding, x1, y1, learning, deep, variational","{'length': 0.6333141030900779, 'description': 0.5977705091404059, 'models': 0.3322863589419208, 'deep': 0.2967427649922488, 'learning': 0.20763711690135359}","compression, variational, deep, encoding, poor, parameters, methods, despite, imported, losslessly"
Semi-crowdsourced Clustering with Deep Generative Models,"Yucen Luo, TIAN TIAN, Jiaxin Shi, Jun Zhu, Bo Zhang","We consider the semi-supervised clustering problem where crowdsourcing provides noisy information about the pairwise comparisons on a small subset of data, i.e., whether a sample pair is in the same cluster. We propose a new approach that includes a deep generative model (DGM) to characterize low-level features of the data, and a statistical relational model for noisy pairwise annotations on its subset. The two parts share the latent variables. To make the model automatically trade-off between its complexity and fitting data, we also develop its fully Bayesian variant. The challenge of inference is addressed by fast (natural-gradient) stochastic variational inference algorithms, where we effectively combine variational message passing for the relational part and amortized learning of the DGM under a unified framework. Empirical results on synthetic and real-world datasets show that our model outperforms previous crowdsourced clustering methods.",https://proceedings.neurips.cc/paper_files/paper/2018/file/3c1e4bd67169b8153e0047536c9f541e-Paper.pdf,2018,"model, data, clustering, dgm, inference, noisy, pairwise, relational, subset, variational","model, annotations, clustering, log, data, ηθ, variational, αm, eq, zn","{'crowdsourced': 0.5715247002457853, 'semi': 0.46185689467016655, 'clustering': 0.41758647335880045, 'generative': 0.35218908909454777, 'models': 0.2998667813703074, 'deep': 0.26779100447124965}","dgm, relational, pairwise, subset, clustering, noisy, model, crowdsourced, variational, annotations"
Scalable Laplacian K-modes,"Imtiaz Ziko, Eric Granger, Ismail Ben Ayed","We advocate Laplacian K-modes for joint clustering and density mode finding,
and propose a concave-convex relaxation of the problem, which yields a parallel
algorithm that scales up to large datasets and high dimensions. We optimize a tight
bound (auxiliary function) of our relaxation, which, at each iteration, amounts to
computing an independent update for each cluster-assignment variable, with guar-
anteed convergence. Therefore, our bound optimizer can be trivially distributed
for large-scale data sets. Furthermore, we show that the density modes can be
obtained as byproducts of the assignment variables via simple maximum-value
operations whose additional computational cost is linear in the number of data
points. Our formulation does not need storing a full affinity matrix and computing
its eigenvalue decomposition, neither does it perform expensive projection steps
and Lagrangian-dual inner iterates for the simplex constraints of each point. Fur-
thermore, unlike mean-shift, our density-mode estimation does not require inner-
loop gradient-ascent iterates. It has a complexity independent of feature-space
dimension, yields modes that are valid data points in the input set and is appli-
cable to discrete domains as well as arbitrary kernels. We report comprehensive
experiments over various data sets, which show that our algorithm yields very
competitive performances in term of optimization quality (i.e., the value of the
discrete-variable objective at convergence) and clustering accuracy.",https://proceedings.neurips.cc/paper_files/paper/2018/file/3d387d2612f9027154ed3b99a7427da1-Paper.pdf,2018,"data, density, modes, yields, algorithm, assignment, bound, clustering, computing, convergence","modes, zp, relaxation, xp, convex, slk, variables, data, mode, updates","{'laplacian': 0.619687466528946, 'modes': 0.619687466528946, 'scalable': 0.4816377141107959}","modes, density, yields, inner, iterates, relaxation, assignment, mode, clustering, computing"
Early Stopping for Nonparametric Testing,"Meimei Liu, Guang Cheng","Early stopping of iterative algorithms is an algorithmic regularization method to avoid over-fitting in estimation and classification. In this paper, we show that early stopping can also be applied to obtain the minimax optimal testing in a general non-parametric setup. Specifically, a Wald-type test statistic is obtained based on an iterated estimate produced by functional gradient descent algorithms in a reproducing kernel Hilbert space. A notable contribution is to establish a ``sharp'' stopping rule: when the number of iterations achieves an optimal order, testing optimality is achievable; otherwise, testing optimality becomes impossible. As a by-product, a similar sharpness result is also derived for minimax optimal estimation under early stopping. All obtained results hold for various kernel classes, including Sobolev smoothness classes and Gaussian kernel classes.",https://proceedings.neurips.cc/paper_files/paper/2018/file/3d863b367aa379f71c7afc0c9cdca41d-Paper.pdf,2018,"stopping, classes, early, kernel, optimal, testing, algorithms, also, estimation, minimax","stopping, testing, size, dn, power, rule, kernel, step, optimal, early","{'early': 0.5563248680220149, 'stopping': 0.5563248680220149, 'nonparametric': 0.4405038573058783, 'testing': 0.43239060366665893}","stopping, early, testing, kernel, classes, optimality, minimax, optimal, obtained, estimation"
Non-Adversarial Mapping with VAEs,Yedid Hoshen,"The study of cross-domain mapping without supervision has recently attracted much attention. Much of the recent progress was enabled by the use of adversarial training as well as cycle constraints. The practical difficulty of adversarial training motivates research into non-adversarial methods. In a recent paper, it was shown that cross-domain mapping is possible without the use of cycles or GANs. Although promising, this approach suffers from several drawbacks including costly inference and an optimization variable for every training example preventing the method from using large training sets. We present an alternative approach which is able to achieve non-adversarial mapping using a novel form of Variational Auto-Encoder. Our method is much faster at inference time, is able to leverage large datasets and has a simple interpretation.",https://proceedings.neurips.cc/paper_files/paper/2018/file/3db11d259a9db7fb8965bdf25ec850b9-Paper.pdf,2018,"adversarial, training, mapping, much, able, approach, cross, domain, inference, large","nam, domain, vae, mapping, latent, training, image, domains, model, code","{'vaes': 0.6184268541814488, 'mapping': 0.5837187798067464, 'non': 0.3838091830249457, 'adversarial': 0.359876538765566}","mapping, adversarial, much, training, cross, domain, able, cycles, enabled, cycle"
Deep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models,"Kurtland Chua, Roberto Calandra, Rowan McAllister, Sergey Levine","Model-based reinforcement learning (RL) algorithms can attain excellent sample efficiency, but often lag behind the best model-free algorithms in terms of asymptotic performance. This is especially true with high-capacity parametric function approximators, such as deep networks. In this paper, we study how to bridge this gap, by employing uncertainty-aware dynamics models. We propose a new algorithm called probabilistic ensembles with trajectory sampling (PETS) that combines uncertainty-aware deep network dynamics models with sampling-based uncertainty propagation. Our comparison to state-of-the-art model-based and model-free deep RL algorithms shows that our approach matches the asymptotic performance of model-free algorithms on several challenging benchmark tasks, while requiring significantly fewer samples (e.g. 8 and 125 times fewer samples than Soft Actor Critic and Proximal Policy Optimization respectively on the half-cheetah task).",https://proceedings.neurips.cc/paper_files/paper/2018/file/3de568f8597b94bda53149c7d7f5958c-Paper.pdf,2018,"model, algorithms, based, deep, free, uncertainty, asymptotic, aware, dynamics, fewer","model, al, et, uncertainty, models, dynamics, state, st, based, pe","{'handful': 0.47437011935249734, 'trials': 0.47437011935249734, 'dynamics': 0.3624350351674011, 'probabilistic': 0.3624350351674011, 'reinforcement': 0.27933039777604424, 'using': 0.2714098703798424, 'models': 0.2488918515810568, 'deep': 0.22226869756971393, 'learning': 0.15552605483742735}","uncertainty, free, aware, model, algorithms, fewer, asymptotic, rl, dynamics, deep"
The challenge of realistic music generation: modelling raw audio at scale,"Sander Dieleman, Aaron van den Oord, Karen Simonyan","Realistic music generation is a challenging task. When building generative models of music that are learnt from data, typically high-level representations such as scores or MIDI are used that abstract away the idiosyncrasies of a particular performance. But these nuances are very important for our perception of musicality and realism, so in this work we embark on modelling music in the raw audio domain. It has been shown that autoregressive models excel at generating raw audio waveforms of speech, but when applied to music, we find them biased towards capturing local signal structure at the expense of modelling long-range correlations. This is problematic because music exhibits structure at many different timescales. In this work, we explore autoregressive discrete autoencoders (ADAs) as a means to enable autoregressive models to capture long-range correlations in waveforms. We find that they allow us to unconditionally generate piano music directly in the raw audio domain, which shows stylistic consistency across tens of seconds.",https://proceedings.neurips.cc/paper_files/paper/2018/file/3e441eec3456b703a4fe741005f3981f-Paper.pdf,2018,"music, audio, autoregressive, models, raw, correlations, domain, find, long, modelling","models, model, audio, music, structure, vq, also, vae, signal, wavenet","{'challenge': 0.39093159720650456, 'music': 0.39093159720650456, 'raw': 0.39093159720650456, 'realistic': 0.3689912774103601, 'audio': 0.3534243727791337, 'modelling': 0.34134974347471747, 'scale': 0.29397682855561835, 'generation': 0.278409923924392}","music, autoregressive, audio, raw, waveforms, modelling, correlations, range, long, domain"
Approximation algorithms for stochastic clustering,"David Harris, Shi Li, Aravind Srinivasan, Khoa Trinh, Thomas Pensyl","We consider stochastic settings for clustering, and develop provably-good (approximation) algorithms for a number of these notions. These algorithms allow one to obtain better approximation ratios compared to the usual deterministic clustering setting. Additionally, they offer a number of advantages including providing fairer clustering and clustering which has better long-term behavior for each user. In particular, they ensure thatevery useris guaranteed to get good service (on average). We also complement some of these with impossibility results.",https://proceedings.neurips.cc/paper_files/paper/2018/file/3e60e09c222f206c725385f53d7e567c-Paper.pdf,2018,"clustering, algorithms, approximation, better, good, number, additionally, advantages, allow, also","set, algorithm, approximation, center, fj, pj, problem, clustering, problems, rj","{'clustering': 0.5459009816074842, 'approximation': 0.532091590558967, 'algorithms': 0.4859380007627819, 'stochastic': 0.4274750483374255}","clustering, good, fairer, thatevery, useris, approximation, ratios, service, better, impossibility"
Inexact trust-region algorithms on Riemannian manifolds,"Hiroyuki Kasai, Bamdev Mishra","We consider an inexact variant of the popular Riemannian trust-region algorithm for structured big-data minimization problems. The proposed algorithm approximates the gradient and the Hessian in addition to the solution of a trust-region sub-problem. Addressing large-scale finite-sum problems, we specifically propose sub-sampled algorithms with a fixed bound on sub-sampled Hessian and gradient sizes, where the gradient and Hessian are computed by a random sampling technique. Numerical evaluations demonstrate that the proposed algorithms outperform state-of-the-art Riemannian deterministic and stochastic gradient algorithms across different applications.",https://proceedings.neurips.cc/paper_files/paper/2018/file/3e9e39fed3b8369ed940f52cf300cf88-Paper.pdf,2018,"gradient, algorithms, hessian, sub, algorithm, problems, proposed, region, riemannian, sampled","rtr, sub, 10, xk, case, riemannian, gradient, hessian, algorithms, algorithm","{'inexact': 0.4511538026568479, 'region': 0.4258336218930782, 'riemannian': 0.4258336218930782, 'trust': 0.4258336218930782, 'manifolds': 0.40786866774212366, 'algorithms': 0.293428995872596}","hessian, sub, riemannian, trust, gradient, region, sampled, algorithms, inexact, approximates"
Towards Robust Interpretability with Self-Explaining Neural Networks,"David Alvarez Melis, Tommi Jaakkola","Most recent work on interpretability of complex machine learning models has focused on estimating a-posteriori explanations for previously trained models around specific predictions. Self-explaining models where interpretability plays a key role already during learning have received much less attention. We propose three desiderata for explanations in general -- explicitness, faithfulness, and stability -- and show that existing methods do not satisfy them. In response, we design self-explaining models in stages, progressively generalizing linear classifiers to complex yet architecturally explicit models. Faithfulness and stability are enforced via regularization specifically tailored to such models. Experimental results across various benchmark datasets show that our framework offers a promising direction for reconciling model complexity and interpretability.",https://proceedings.neurips.cc/paper_files/paper/2018/file/3e9f0fc9b2f89e043bc6233994dfcf76-Paper.pdf,2018,"models, interpretability, complex, explaining, explanations, faithfulness, learning, self, show, stability","model, concepts, models, input, interpretability, explanations, interpretable, methods, inputs, senn","{'explaining': 0.4779553446255512, 'interpretability': 0.4482842355650723, 'self': 0.43700380343555495, 'towards': 0.39170377761820707, 'robust': 0.3382189214783052, 'neural': 0.23677272954668668, 'networks': 0.23609827353722393}","interpretability, faithfulness, models, explaining, explanations, stability, self, complex, architecturally, reconciling"
Predictive Uncertainty Estimation via Prior Networks,"Andrey Malinin, Mark Gales","Estimating how uncertain an AI system is in its predictions is important to improve the safety of such systems. Uncertainty in predictive can result from uncertainty in model parameters, irreducible \emph{data uncertainty} and uncertainty due to distributional mismatch between the test and training data distributions. Different actions might be taken depending on the source of the uncertainty so it is important to be able to distinguish between them. Recently, baseline tasks and metrics have been defined and several practical methods to estimate uncertainty developed. These methods, however, attempt to model uncertainty due to distributional mismatch either implicitly through \emph{model uncertainty} or as \emph{data uncertainty}. This work proposes a new framework for modeling predictive uncertainty called Prior Networks (PNs) which explicitly models \emph{distributional uncertainty}. PNs do this by parameterizing a prior distribution over predictive distributions. This work focuses on uncertainty for classification and evaluates PNs on the tasks of identifying out-of-distribution (OOD) samples and detecting misclassification on the MNIST and CIFAR-10 datasets, where they are found to outperform previous methods. Experiments on synthetic and MNIST and CIFAR-10 data show that unlike previous non-Bayesian methods PNs are able to distinguish between data and distributional uncertainty.",https://proceedings.neurips.cc/paper_files/paper/2018/file/3ea2db50e62ceefceaf70a9d9a56a6f4-Paper.pdf,2018,"uncertainty, data, distributional, emph, methods, pns, model, predictive, 10, able","uncertainty, distribution, data, model, entropy, prior, distributional, eq, dpn, measures","{'predictive': 0.485214919593349, 'uncertainty': 0.4743653477416311, 'prior': 0.46479534713615017, 'estimation': 0.3999157225476062, 'via': 0.307988062238357, 'networks': 0.2621450978455536}","uncertainty, pns, distributional, emph, predictive, mismatch, distinguish, data, methods, cifar"
"Graph Oracle Models, Lower Bounds, and Gaps for Parallel Stochastic Optimization","Blake E. Woodworth, Jialei Wang, Adam Smith, Brendan McMahan, Nati Srebro","We suggest a general oracle-based framework that captures parallel
  stochastic optimization in different parallelization settings
  described by a dependency graph, and derive generic lower bounds 
  in terms of this graph.  We then use the framework and derive lower
  bounds to study several specific parallel optimization settings,
  including delayed updates and parallel processing with intermittent
  communication.  We highlight gaps between lower and upper bounds on
  the oracle complexity, and cases where the ``natural'' algorithms
  are not known to be optimal.",https://proceedings.neurips.cc/paper_files/paper/2018/file/3ec27c2cff04bc5fd2586ca36c62044e-Paper.pdf,2018,"bounds, lower, parallel, derive, framework, graph, optimization, oracle, settings, algorithms","oracle, lower, bound, optimization, graph, stochastic, algorithm, bounds, gradient, sgd","{'gaps': 0.4228068686489147, 'lower': 0.40496960634863155, 'oracle': 0.40496960634863155, 'parallel': 0.3798293971402521, 'bounds': 0.29651329428184653, 'graph': 0.29134335227958724, 'stochastic': 0.25629198252248075, 'models': 0.23502824707932102, 'optimization': 0.23502824707932102}","parallel, lower, oracle, bounds, derive, settings, graph, intermittent, gaps, delayed"
An Off-policy Policy Gradient Theorem Using Emphatic Weightings,"Ehsan Imani, Eric Graves, Martha White","Policy gradient methods are widely used for control in reinforcement learning, particularly for the continuous action setting. There have been a host of theoretically sound algorithms proposed for the on-policy setting, due to the existence of the policy gradient theorem which provides a simplified form for the gradient. In off-policy learning, however, where the behaviour policy is not necessarily attempting to learn and follow the optimal policy for the given task, the existence of such a theorem has been elusive. In this work, we solve this open problem by providing the first off-policy policy gradient theorem. The key to the derivation is the use of emphatic weightings. We develop a new actor-critic algorithm—called Actor Critic with Emphatic weightings (ACE)—that approximates the simplified gradients provided by the theorem. We demonstrate in a simple counterexample that previous off-policy policy gradient methods—particularly OffPAC and DPG—converge to the wrong solution whereas ACE finds the optimal solution.",https://proceedings.neurips.cc/paper_files/paper/2018/file/3ef815416f775098fe977004015c6193-Paper.pdf,2018,"policy, gradient, theorem, ace, actor, critic, emphatic, existence, learning, methods","policy, gradient, emphatic, λa, actor, s0, ace, st, weightings, al","{'policy': 0.5749515817685422, 'emphatic': 0.4342939860191211, 'weightings': 0.4342939860191211, 'theorem': 0.40992003158082463, 'gradient': 0.2498623765912645, 'using': 0.24848039461905072}","policy, theorem, ace, emphatic, weightings, gradient, simplified, actor, critic, existence"
Multiple-Step Greedy Policies in Approximate and Online Reinforcement Learning,"Yonathan Efroni, Gal Dalal, Bruno Scherrer, Shie Mannor","Multiple-step lookahead policies have demonstrated high empirical competence in Reinforcement Learning, via the use of Monte Carlo Tree Search or Model Predictive Control. In a recent work (Efroni et al., 2018), multiple-step greedy policies and their use in vanilla Policy Iteration algorithms were proposed and analyzed. In this work, we study multiple-step greedy algorithms in more practical setups. We begin by highlighting a counter-intuitive difficulty, arising with soft-policy updates: even in the absence of approximations, and contrary to the 1-step-greedy case, monotonic policy improvement is not guaranteed unless the update stepsize is sufficiently large. Taking particular care about this difficulty, we formulate and analyze online and approximate algorithms that use such a multi-step greedy operator.",https://proceedings.neurips.cc/paper_files/paper/2018/file/3f998e713a6e02287c374fd26835d87e-Paper.pdf,2018,"step, greedy, algorithms, multiple, policy, use, difficulty, policies, work, 2018","policy, greedy, step, cπ, approximate, api, vπ, algorithm, let, policies","{'step': 0.4282356942380058, 'greedy': 0.405394332908229, 'approximate': 0.38833389510987154, 'policies': 0.38833389510987154, 'multiple': 0.3688046458887313, 'online': 0.3246396181167099, 'reinforcement': 0.2887918492166122, 'learning': 0.16079401789230127}","greedy, step, policy, difficulty, multiple, use, policies, algorithms, efroni, competence"
Scaling the Poisson GLM to massive neural datasets through polynomial approximations,"David Zoltowski, Jonathan W. Pillow","Recent advances in recording technologies have allowed neuroscientists to record simultaneous spiking activity from hundreds to thousands of neurons in multiple brain regions. Such large-scale recordings pose a major challenge to existing statistical methods for neural data analysis. Here we develop highly scalable approximate inference methods for Poisson generalized linear models (GLMs) that require only a single pass over the data. Our approach relies on a recently proposed method for obtaining approximate sufficient statistics for GLMs using polynomial approximations [Huggins et al., 2017], which we adapt to the Poisson GLM setting. We focus on inference using quadratic approximations to nonlinear terms in the Poisson GLM log-likelihood with Gaussian priors, for which we derive closed-form solutions to the approximate maximum likelihood and MAP estimates, posterior distribution, and marginal likelihood. We introduce an adaptive procedure to select the polynomial approximation interval and show that the resulting method allows for efficient and accurate inference and regularization of high-dimensional parameters. We use the quadratic estimator to fit a fully-coupled Poisson GLM to spike train data recorded from 831 neurons across five regions of the mouse brain for a duration of 41 minutes, binned at 1 ms resolution. Across all neurons, this model is fit to over 2 billion spike count bins and identifies fine-timescale statistical dependencies between neurons within and across cortical and subcortical areas.",https://proceedings.neurips.cc/paper_files/paper/2018/file/3fab5890d8113d0b5a4178201dc842ad-Paper.pdf,2018,"neurons, poisson, across, approximate, data, glm, inference, likelihood, approximations, brain","approximation, log, paglm, likelihood, exact, interval, estimates, poisson, data, approximate","{'glm': 0.3966977780785125, 'massive': 0.3966977780785125, 'polynomial': 0.3966977780785125, 'datasets': 0.37443384194324814, 'poisson': 0.3586373278653503, 'scaling': 0.3463845996376717, 'approximations': 0.32790903605002547, 'neural': 0.1776641688200707}","poisson, glm, neurons, glms, likelihood, spike, fit, approximate, across, quadratic"
Hybrid Macro/Micro Level Backpropagation for Training Deep Spiking Neural Networks,"Yingyezhe Jin, Wenrui Zhang, Peng Li","Spiking neural networks (SNNs) are positioned to enable spatio-temporal information processing and ultra-low power event-driven neuromorphic hardware. However, SNNs are yet to reach the same performances of conventional deep artificial neural networks (ANNs), a long-standing challenge due to complex dynamics and non-differentiable spike events encountered in training. The existing SNN error backpropagation (BP) methods are limited in terms of scalability, lack of proper handling of spiking discontinuities, and/or mismatch between the rate-coded loss function and computed gradient. We present a hybrid macro/micro level backpropagation (HM2-BP) algorithm for training multi-layer SNNs. The temporal effects are precisely captured by the proposed spike-train level post-synaptic potential (S-PSP) at the microscopic level.  The rate-coded errors are defined at the macroscopic level, computed and back-propagated across both macroscopic and microscopic levels.  Different from existing BP methods, HM2-BP directly computes the gradient of the rate-coded loss function w.r.t tunable parameters. We evaluate the proposed HM2-BP algorithm by training deep fully connected and convolutional SNNs based on the static MNIST [14] and dynamic neuromorphic N-MNIST [26]. HM2-BP achieves an accuracy level of 99.49% and 98.88% for MNIST and N-MNIST, respectively, outperforming the best reported performances obtained from the existing SNN BP algorithms. Furthermore, the HM2-BP produces the highest accuracies based on SNNs for the EMNIST [3] dataset, and leads to high recognition accuracy for the 16-speaker spoken English letters of TI46 Corpus [16], a challenging patio-temporal speech recognition benchmark for which no prior success based on SNNs was reported. It also achieves competitive performances surpassing those of conventional deep learning models when dealing with asynchronous spiking streams.",https://proceedings.neurips.cc/paper_files/paper/2018/file/3fb04953d95a94367bb133f862402bce-Paper.pdf,2018,"bp, snns, hm2, level, mnist, based, coded, deep, existing, performances","spiking, level, spike, synaptic, bp, ﬁring, ok, training, micro, mnist","{'macro': 0.4011528532540298, 'micro': 0.4011528532540298, 'level': 0.37863888418521596, 'backpropagation': 0.3626649689177551, 'spiking': 0.3626649689177551, 'hybrid': 0.3241770845814805, 'training': 0.25873075738885865, 'deep': 0.18796234961184718, 'neural': 0.1796594087024896, 'networks': 0.17914764213170287}","bp, snns, hm2, coded, level, spiking, mnist, performances, macroscopic, microscopic"
Differential Properties of Sinkhorn Approximation for Learning with Wasserstein Distance,"Giulia Luise, Alessandro Rudi, Massimiliano Pontil, Carlo Ciliberto","Applications of optimal transport have recently gained remarkable attention as a result of the computational advantages of entropic regularization. However, in most situations the  Sinkhorn approximation to the Wasserstein distance is replaced by a regularized version that is less accurate but easy to differentiate. In this work we characterize the differential properties of the original Sinkhorn approximation, proving that it enjoys the same smoothness as its regularized version and we explicitly provide an efficient algorithm to compute its gradient. We show that this result benefits both theory and applications: on one hand, high order smoothness confers statistical guarantees to learning with Wasserstein approximations. On the other hand, the gradient formula allows to efficiently solve learning and optimization problems in practice. Promising preliminary experiments complement our analysis.",https://proceedings.neurips.cc/paper_files/paper/2018/file/3fc2c60b5782f641f76bcefc39fb2392-Paper.pdf,2018,"applications, approximation, gradient, hand, learning, regularized, result, sinkhorn, smoothness, version","sinkhorn, sλ, wasserstein, gradient, eq, sharp, approximation, learning, regularized, distance","{'sinkhorn': 0.4820697464258404, 'properties': 0.4358184372504179, 'distance': 0.39847721513603257, 'differential': 0.3746775322261054, 'wasserstein': 0.36831781423643706, 'approximation': 0.3433158188995731, 'learning': 0.15805043943414485}","sinkhorn, smoothness, regularized, version, wasserstein, hand, confers, approximation, result, entropic"
"The Cluster Description Problem - Complexity Results, Formulations and Approximations","Ian Davidson, Antoine Gourru, S Ravi","Consider the situation where you are given an existing $k$-way clustering $\pi$. A challenge for explainable AI is to find a compact and distinct explanations of each cluster which in this paper is using instance-level descriptors/tags from a common dictionary. Since the descriptors/tags were not given to the clustering method, this is not a semi-supervised learning situation.  We show that the \emph{feasibility} problem of just testing whether any distinct description (not the most compact) exists is generally intractable for just two clusters. This means that unless \textbf{P} = \cnp,  there cannot exist an efficient algorithm for the cluster description problem. Hence, we explore ILP formulations for smaller problems and a relaxed but restricted setting that leads to a polynomial time algorithm for larger problems.  We explore several extension to the basic setting such as the ability to ignore some instances and composition constraints on the descriptions of the clusters.  We show our formulation's usefulness on Twitter data where the communities were found using social connectivity (i.e. \texttt{follower} relation) but the explanation of the communities is based on behavioral properties of the nodes (i.e. hashtag usage) not available to the clustering method.",https://proceedings.neurips.cc/paper_files/paper/2018/file/3fd60983292458bf7dee75f12d5e9e05-Paper.pdf,2018,"clustering, algorithm, cluster, clusters, communities, compact, description, descriptors, distinct, explore","cluster, tags, clustering, problem, clusters, number, constraints, description, using, descriptors","{'formulations': 0.4148333452701873, 'results': 0.4148333452701873, 'cluster': 0.39155158364649195, 'description': 0.39155158364649195, 'problem': 0.35175114926823176, 'approximations': 0.34289983429660803, 'complexity': 0.32846938764453637}","tags, descriptors, clustering, situation, communities, clusters, description, cluster, distinct, compact"
Global Non-convex Optimization with Discretized Diffusions,"Murat A. Erdogdu, Lester Mackey, Ohad Shamir",An Euler discretization of the Langevin diffusion is known to converge to the global minimizers of certain convex and non-convex optimization problems.  We show that this property holds for any suitably smooth diffusion and that different diffusions are suitable for optimizing different classes of convex and non-convex functions. This allows us to design diffusions suitable for globally optimizing convex and non-convex functions not covered by the existing Langevin theory. Our non-asymptotic analysis delivers computable optimization and integration error bounds based on easily accessed properties of the objective and chosen diffusion. Central to our approach are new explicit Stein factor bounds on the solutions of Poisson equations. We complement these results with improved optimization guarantees for targets other than the standard Gibbs measure.,https://proceedings.neurips.cc/paper_files/paper/2018/file/3ffebb08d23c609875d7177ee769a3e9-Paper.pdf,2018,"convex, non, diffusion, optimization, bounds, different, diffusions, functions, langevin, optimizing","diffusion, error, optimization, convex, langevin, bounds, non, stein, log, r2","{'diffusions': 0.5175190028541553, 'discretized': 0.5175190028541553, 'global': 0.40977696682314424, 'convex': 0.34574983762757194, 'non': 0.3211835714156416, 'optimization': 0.27153114750265067}","convex, diffusion, diffusions, langevin, non, suitable, optimizing, optimization, covered, euler"
Contextual Pricing for Lipschitz Buyers,"Jieming Mao, Renato Leme, Jon Schneider","We investigate the problem of learning a Lipschitz function from binary
  feedback. In this problem, a learner is trying to learn a Lipschitz function
  $f:[0,1]^d \rightarrow [0,1]$ over the course of $T$ rounds. On round $t$, an
  adversary provides the learner with an input $x_t$, the learner submits a
  guess $y_t$ for $f(x_t)$, and learns whether $y_t > f(x_t)$ or $y_t \leq
  f(x_t)$. The learner's goal is to minimize their total loss $\sum_t\ell(f(x_t),
  y_t)$ (for some loss function $\ell$). The problem is motivated by \textit{contextual dynamic pricing},
  where a firm must sell a stream of differentiated products to a collection of
  buyers with non-linear valuations for the items and observes only whether the
  item was sold or not at the posted price.

  For the symmetric loss $\ell(f(x_t), y_t) = \vert f(x_t) - y_t \vert$,  we
  provide an algorithm for this problem achieving total loss $O(\log T)$
  when $d=1$ and $O(T^{(d-1)/d})$ when $d>1$, and show that both bounds are
  tight (up to a factor of $\sqrt{\log T}$). For the pricing loss function
  $\ell(f(x_t), y_t) = f(x_t) - y_t {\bf 1}\{y_t \leq f(x_t)\}$ we show a regret
  bound of $O(T^{d/(d+1)})$ and show that this bound is tight. We present
  improved bounds in the special case of a population of linear buyers.",https://proceedings.neurips.cc/paper_files/paper/2018/file/403ea2e851b9ab04a996beab4a480a30-Paper.pdf,2018,"x_t, y_t, loss, ell, function, learner, problem, show, bound, bounds","xt, algorithm, lipschitz, regret, function, loss, problem, yt, pricing, learning","{'buyers': 0.5431743442153455, 'pricing': 0.5431743442153455, 'lipschitz': 0.47428354316935495, 'contextual': 0.4300911348204952}","x_t, y_t, ell, learner, loss, buyers, vert, pricing, leq, function"
Processing of missing data by neural networks,"Marek Śmieja, Łukasz Struski, Jacek Tabor, Bartosz Zieliński, Przemysław Spurek","We propose a general, theoretically justified mechanism for processing missing data by neural networks. Our idea is to replace typical neuron's response in the first hidden layer by its expected value. This approach can be applied for various types of networks at minimal cost in their modification. Moreover, in contrast to recent approaches, it does not require complete data for training. Experimental results performed on different types of architectures show that our method gives better results than typical imputation strategies and other methods dedicated for incomplete data.",https://proceedings.neurips.cc/paper_files/paper/2018/file/411ae1bf081d1674ca6091f8c59a266f-Paper.pdf,2018,"data, networks, results, types, typical, applied, approach, approaches, architectures, better","data, missing, density, network, function, attributes, incomplete, complete, neural, results","{'processing': 0.6056736883691478, 'missing': 0.5801216899298998, 'data': 0.36319177454104284, 'neural': 0.2873845800417815, 'networks': 0.28656595427602205}","typical, types, dedicated, imputation, justified, data, replace, incomplete, modification, minimal"
Invariant Representations without Adversarial Training,"Daniel Moyer, Shuyang Gao, Rob Brekelmans, Aram Galstyan, Greg Ver Steeg","Representations of data that are invariant to changes in specified factors are useful for a wide range of problems: removing potential biases in prediction problems, controlling the effects of covariates, and disentangling meaningful factors of variation. Unfortunately, learning representations that exhibit invariance to arbitrary nuisance factors yet remain useful for other tasks is challenging. Existing approaches cast the trade-off between task performance and invariance in an adversarial way, using an iterative minimax optimization. We show that adversarial training is unnecessary and sometimes counter-productive; we instead cast invariant representation learning as a single information-theoretic objective that can be directly optimized. We demonstrate that this approach matches or exceeds performance of state-of-the-art adversarial approaches for learning fair representations and for generative modeling with controllable transformations.",https://proceedings.neurips.cc/paper_files/paper/2018/file/415185ea244ea2b2bedeb0449b926802-Paper.pdf,2018,"adversarial, factors, learning, representations, approaches, cast, invariance, invariant, performance, problems","log, adversarial, invariant, information, al, et, also, methods, loss, using","{'invariant': 0.5075969759194346, 'without': 0.5075969759194346, 'representations': 0.4323913876965333, 'training': 0.40512101648850535, 'adversarial': 0.3655205465087785}","factors, cast, representations, invariance, adversarial, invariant, useful, controllable, productive, nuisance"
Inference in Deep Gaussian Processes using Stochastic Gradient Hamiltonian Monte Carlo,"Marton Havasi, José Miguel Hernández-Lobato, Juan José Murillo-Fuentes","Deep Gaussian Processes (DGPs) are hierarchical generalizations of Gaussian Processes that combine well calibrated uncertainty estimates with the high flexibility of multilayer models. One of the biggest challenges with these models is that exact inference is intractable. The current state-of-the-art inference method, Variational Inference (VI), employs a Gaussian approximation to the posterior distribution. This can be a potentially poor unimodal approximation of the generally multimodal posterior. In this work, we provide evidence for the non-Gaussian nature of the posterior and we apply the Stochastic Gradient Hamiltonian Monte Carlo method to generate samples. To efficiently optimize the hyperparameters, we introduce the Moving Window MCEM algorithm. This results in significantly better predictions at a lower computational cost than its VI counterpart. Thus our method establishes a new state-of-the-art for inference in DGPs.",https://proceedings.neurips.cc/paper_files/paper/2018/file/4172f3101212a2009c74b547b6ddf935-Paper.pdf,2018,"gaussian, inference, method, posterior, approximation, art, dgps, models, processes, state","dgp, posterior, gaussian, inference, layer, inducing, mcem, sghmc, samples, distribution","{'hamiltonian': 0.41450428148687424, 'carlo': 0.3689588975836173, 'monte': 0.3689588975836173, 'processes': 0.32341351368036025, 'gaussian': 0.30875117500947125, 'inference': 0.2905085212387646, 'gradient': 0.27311607655065767, 'stochastic': 0.2716054789998619, 'using': 0.2716054789998619, 'deep': 0.22242888950799647}","gaussian, dgps, vi, inference, posterior, processes, biggest, establishes, mcem, method"
Regret bounds for meta Bayesian optimization with an unknown Gaussian process prior,"Zi Wang, Beomjoon Kim, Leslie Pack Kaelbling","Bayesian optimization usually assumes that a Bayesian prior is given. However, the strong theoretical guarantees in Bayesian optimization are often regrettably compromised in practice because of unknown parameters in the prior. In this paper, we adopt a variant of empirical Bayes and show that,  by estimating the Gaussian process prior from offline data sampled from the same prior and constructing unbiased estimators of the posterior, variants of both GP-UCB and \emph{probability of improvement} achieve a near-zero regret bound, which decreases to a constant proportional to the observational noise as the number of offline data and the number of online evaluations increase. Empirically, we have verified our approach on challenging simulated robotic problems featuring task and motion planning.",https://proceedings.neurips.cc/paper_files/paper/2018/file/41f860e3b7f548abc1f8b812059137bf-Paper.pdf,2018,"prior, bayesian, data, number, offline, optimization, achieve, adopt, approach, assumes","bo, function, gp, prior, xt, functions, regret, ucb, kernel, training","{'unknown': 0.3946716078721909, 'meta': 0.3761373368413174, 'prior': 0.368549020033497, 'regret': 0.3617610393417837, 'process': 0.344857899201651, 'bounds': 0.3080998455906372, 'gaussian': 0.3027278829726826, 'bayesian': 0.2663068461484797, 'optimization': 0.24421220913537375}","prior, offline, bayesian, featuring, regrettably, compromised, robotic, decreases, ucb, verified"
Fully Neural Network Based Speech Recognition on Mobile and Embedded Devices,"Jinhwan Park, Yoonho Boo, Iksoo Choi, Sungho Shin, Wonyong Sung","Real-time automatic speech recognition (ASR) on mobile and embedded devices has been of great interests for many years.  We present real-time speech recognition on smartphones or embedded systems by employing recurrent neural network (RNN) based acoustic models, RNN based language models, and beam-search decoding. The acoustic model is end-to-end trained with connectionist temporal classification (CTC) loss. The RNN implementation on embedded devices can suffer from excessive DRAM accesses because the parameter size of a neural network usually exceeds that of the cache memory and the parameters are used only once for each time step. To remedy this problem, we employ a multi-time step parallelization approach that computes multiple output samples at a time with the parameters fetched from the DRAM. Since the number of DRAM accesses can be reduced in proportion to the number of parallelization steps, we can achieve a high processing speed. However, conventional RNNs, such as long short-term memory (LSTM) or gated recurrent unit (GRU), do not permit multi-time step parallelization. We construct an acoustic model by combining simple recurrent units (SRUs) and depth-wise 1-dimensional convolution layers for multi-time step parallelization. Both the character and word piece models are developed for acoustic modeling, and the corresponding RNN based language models are used for beam search decoding. We achieve a competitive WER for WSJ corpus using the entire model size of around 15MB and achieve real-time speed using only a single core ARM without GPU or special hardware.",https://proceedings.neurips.cc/paper_files/paper/2018/file/42299f06ee419aa5d9d07798b56779e2-Paper.pdf,2018,"time, acoustic, models, parallelization, rnn, step, achieve, based, dram, embedded","word, model, rnn, time, piece, sru, wer, based, models, lm","{'embedded': 0.3928000513114869, 'mobile': 0.3928000513114869, 'devices': 0.3762287085393201, 'speech': 0.3762287085393201, 'fully': 0.3633749764846536, 'recognition': 0.31795748811466107, 'based': 0.278030149498366, 'network': 0.2512847658955369, 'neural': 0.18637870515804972}","acoustic, parallelization, dram, rnn, time, embedded, step, accesses, beam, recurrent"
Large Margin Deep Networks for Classification,"Gamaleldin Elsayed, Dilip Krishnan, Hossein Mobahi, Kevin Regan, Samy Bengio","We present a formulation of deep learning that aims at  producing a large margin classifier. The notion of \emc{margin}, minimum distance to a decision boundary, has served as the foundation of several theoretically profound and empirically successful results for both classification and regression tasks. However, most large margin algorithms are applicable only to shallow models with a preset feature representation; and conventional margin methods for neural networks only enforce margin at the output layer.
Such methods are therefore not well suited for deep networks. In this work, we propose a novel loss function to impose a margin on any chosen set of layers of a deep network (including input and hidden layers). Our formulation allows choosing any $l_p$ norm ($p \geq 1$) on the metric measuring the margin. We demonstrate that the decision boundary obtained by our loss has nice properties compared to standard classification loss functions. Specifically, we show improved empirical results on the MNIST, CIFAR-10 and ImageNet datasets on multiple tasks:
generalization from small training sets, corrupted labels, and robustness against adversarial perturbations. The resulting loss is general and complementary to existing data augmentation (such as random/adversarial input transform) and regularization techniques such as weight decay, dropout, and batch norm. \footnote{Code for the large margin loss function is released at \url{https://github.com/google-research/google-research/tree/master/large_margin}}",https://proceedings.neurips.cc/paper_files/paper/2018/file/42998cf32d552343bc8e460416382dca-Paper.pdf,2018,"margin, loss, deep, large, adversarial, boundary, classification, decision, formulation, function","margin, models, cross, entropy, loss, al, et, model, training, xk","{'margin': 0.5824965372116894, 'classification': 0.48923263472843576, 'large': 0.4728919459138267, 'deep': 0.32187886504433766, 'networks': 0.3067839907502793}","margin, loss, google, boundary, norm, formulation, research, layers, large, decision"
Collaborative Learning for Deep Neural Networks,"Guocong Song, Wei Chai","We introduce collaborative learning in which multiple classifier heads of the same network are simultaneously trained on the same training data to improve generalization and robustness to label noise with no extra inference cost. It acquires the strengths from auxiliary training, multi-task learning and knowledge distillation. There are two important mechanisms involved in collaborative learning. First, the consensus of multiple views from different classifier heads on the same example provides supplementary information as well as regularization to each classifier, thereby improving generalization. Second, intermediate-level representation (ILR) sharing with backpropagation rescaling aggregates the gradient flows from all heads, which not only reduces training computational complexity, but also facilitates supervision to the shared layers. The empirical results on CIFAR and ImageNet datasets demonstrate that deep neural networks learned as a group in a collaborative way significantly reduce the generalization error and increase the robustness to label noise.",https://proceedings.neurips.cc/paper_files/paper/2018/file/430c3626b879b4005d41b8a46172e0c0-Paper.pdf,2018,"classifier, collaborative, generalization, heads, learning, training, label, multiple, noise, robustness","learning, training, heads, sharing, collaborative, ilr, network, head, classiﬁer, label","{'collaborative': 0.7274792820260195, 'deep': 0.3770386634021985, 'neural': 0.3603835739694672, 'networks': 0.3593570078288448, 'learning': 0.26382183582881064}","heads, collaborative, classifier, generalization, label, robustness, noise, training, ilr, rescaling"
Multi-Task Learning as Multi-Objective Optimization,"Ozan Sener, Vladlen Koltun","In multi-task learning, multiple tasks are solved jointly, sharing inductive bias between them. Multi-task learning is inherently a multi-objective problem because different tasks may conflict, necessitating a trade-off. A common compromise is to optimize a proxy objective that minimizes a weighted linear combination of per-task losses. However, this workaround is only valid when the tasks do not compete, which is rarely the case. In this paper, we explicitly cast multi-task learning as multi-objective optimization, with the overall objective of finding a Pareto optimal solution. To this end, we use algorithms developed in the gradient-based multi-objective optimization literature. These algorithms are not directly applicable to large-scale learning problems since they scale poorly with the dimensionality of the gradients and the number of tasks. We therefore propose an upper bound for the multi-objective loss and show that it can be optimized efficiently. We further prove that optimizing this upper bound yields a Pareto optimal solution under realistic assumptions. We apply our method to a variety of multi-task deep learning problems including digit classification, scene understanding (joint semantic segmentation, instance segmentation, and depth estimation), and multi-label classification. Our method produces higher-performing models than recent multi-task learning formulations or per-task training.",https://proceedings.neurips.cc/paper_files/paper/2018/file/432aca3a1e345e339f35a30c8f65edce-Paper.pdf,2018,"multi, task, learning, objective, tasks, algorithms, bound, classification, method, optimal","sh, task, et, al, multi, tasks, optimization, objective, mtl, problem","{'multi': 0.676818703222357, 'objective': 0.48285029485501035, 'task': 0.4378596606470245, 'optimization': 0.29013962649569963, 'learning': 0.18130071826069888}","multi, task, objective, pareto, tasks, learning, segmentation, per, upper, solution"
Learning to Exploit Stability for 3D Scene Parsing,"Yilun Du, Zhijian Liu, Hector Basevi, Ales Leonardis, Bill Freeman, Josh Tenenbaum, Jiajun Wu","Human scene understanding uses a variety of visual and non-visual cues to perform inference on object types, poses, and relations. Physics is a rich and universal cue which we exploit to enhance scene understanding. We integrate the physical cue of stability into the learning process using a REINFORCE approach coupled to a physics engine, and apply this to the problem of producing the 3D bounding boxes and poses of objects in a scene. We first show that applying physics supervision to an existing scene understanding model increases performance, produces more stable predictions, and allows training to an equivalent performance level with fewer annotated training examples. We then present a novel architecture for 3D scene parsing named Prim R-CNN, learning to predict bounding boxes as well as their 3D size, translation, and rotation. With physics supervision, Prim R-CNN outperforms existing scene understanding approaches on this problem. Finally, we show that applying physics supervision on unlabeled real images improves real domain transfer of models training on synthetic data.",https://proceedings.neurips.cc/paper_files/paper/2018/file/43feaeeecd7b2fe2ae2e26d917b6477d-Paper.pdf,2018,"scene, physics, understanding, 3d, supervision, training, applying, bounding, boxes, cnn","al, et, stability, physics, data, model, scene, factored3d, prediction, objects","{'exploit': 0.5029462325813372, 'stability': 0.47471929655991457, 'parsing': 0.4391575628575426, 'scene': 0.3982380946896145, '3d': 0.37821076486216, 'learning': 0.16489496314711488}","scene, physics, understanding, prim, supervision, 3d, cue, boxes, bounding, poses"
Direct Runge-Kutta Discretization Achieves Acceleration,"Jingzhao Zhang, Aryan Mokhtari, Suvrit Sra, Ali Jadbabaie","We study gradient-based optimization methods obtained by directly discretizing a second-order ordinary differential equation (ODE) related to the continuous limit of Nesterov's accelerated gradient method. When the function is smooth enough, we show that acceleration can be achieved by a stable discretization of this ODE using standard Runge-Kutta integrators. Specifically, we prove that under Lipschitz-gradient, convexity and order-$(s+2)$ differentiability assumptions, the sequence of iterates generated by discretizing the proposed second-order ODE converges to the optimal solution at a rate of $\mathcal{O}({N^{-2\frac{s}{s+1}}})$, where $s$ is the order of the Runge-Kutta numerical integrator. Furthermore, we introduce a new local flatness condition on the objective, under which rates even faster than $\mathcal{O}(N^{-2})$ can be achieved with low-order integrators and only gradient information. Notably, this flatness condition is satisfied by several standard loss functions used in machine learning. We provide numerical experiments that verify the theoretical rates predicted by our results.",https://proceedings.neurips.cc/paper_files/paper/2018/file/44968aece94f667e4095002d140b5896-Paper.pdf,2018,"order, gradient, ode, achieved, condition, discretizing, flatness, integrators, kutta, mathcal","order, ode, integrator, y0, function, step, rate, assumption, bound, gradient","{'discretization': 0.42894937194262117, 'kutta': 0.42894937194262117, 'runge': 0.42894937194262117, 'achieves': 0.40487537417926045, 'direct': 0.40487537417926045, 'acceleration': 0.3466398299337515}","ode, order, flatness, kutta, runge, discretizing, integrators, gradient, mathcal, condition"
Communication Compression for Decentralized Training,"Hanlin Tang, Shaoduo Gan, Ce Zhang, Tong Zhang, Ji Liu","Optimizing distributed learning systems is an art
of balancing between computation and communication.
There have been two lines of research that try to
deal with slower networks: {\em communication 
compression} for
low bandwidth networks, and {\em decentralization} for
high latency networks. In this paper, We explore
a natural question: {\em can the combination
of both techniques lead to
a system that is robust to both bandwidth
and latency?}

Although the system implication of such combination
is trivial, the underlying theoretical principle and
algorithm design is challenging:  unlike centralized algorithms, simply compressing
{\rc exchanged information,
even in an unbiased stochastic way, 
within the decentralized network would accumulate the error and cause divergence.} 
In this paper, we develop
a framework of quantized, decentralized training and
propose two different strategies, which we call
{\em extrapolation compression} and {\em difference compression}.
We analyze both algorithms and prove 
both converge at the rate of $O(1/\sqrt{nT})$ 
where $n$ is the number of workers and $T$ is the
number of iterations, matching the convergence rate for
full precision, centralized training. We validate 
our algorithms and find that our proposed algorithm outperforms
the best of merely decentralized and merely quantized
algorithm significantly for networks with {\em both} 
high latency and low bandwidth.",https://proceedings.neurips.cc/paper_files/paper/2018/file/44feb0096faa8326192570788b38c1d1-Paper.pdf,2018,"em, networks, algorithm, algorithms, bandwidth, compression, decentralized, latency, centralized, combination","psgd, decentralized, al, et, node, algorithms, training, compression, stochastic, algorithm","{'decentralized': 0.565435933913318, 'compression': 0.5303340955292329, 'communication': 0.48611100917897676, 'training': 0.4033906772767077}","em, bandwidth, latency, decentralized, compression, merely, centralized, quantized, networks, communication"
Neural Voice Cloning with a Few Samples,"Sercan Arik, Jitong Chen, Kainan Peng, Wei Ping, Yanqi Zhou","Voice cloning is a highly desired feature for personalized speech interfaces. We introduce a neural voice cloning system that learns to synthesize a person's voice from only a few audio samples. We study two approaches: speaker adaptation and speaker encoding. Speaker adaptation is based on fine-tuning a multi-speaker generative model. Speaker encoding is based on training a separate model to directly infer a new speaker embedding, which will be applied to a multi-speaker generative model. In terms of naturalness of the speech and similarity to the original speaker, both approaches can achieve good performance, even with a few cloning audios.  While speaker adaptation can achieve slightly better naturalness and similarity, cloning time and required memory for the speaker encoding approach are significantly less, making it more favorable for low-resource deployment.",https://proceedings.neurips.cc/paper_files/paper/2018/file/4559912e7a94a9c32b09d894f2bc3c82-Paper.pdf,2018,"speaker, cloning, adaptation, encoding, model, voice, achieve, approaches, based, generative","speaker, model, cloning, al, et, adaptation, audio, generative, embedding, speakers","{'cloning': 0.5888633597367565, 'voice': 0.5888633597367565, 'samples': 0.48675245319433846, 'neural': 0.2637270111846184}","speaker, cloning, voice, encoding, adaptation, naturalness, speech, similarity, audios, interfaces"
Low-Rank Tucker Decomposition of Large Tensors Using TensorSketch,"Osman Asif Malik, Stephen Becker","We propose two randomized algorithms for low-rank Tucker decomposition of tensors. The algorithms, which incorporate sketching, only require a single pass of the input tensor and can handle tensors whose elements are streamed in any order. To the best of our knowledge, ours are the only algorithms which can do this. We test our algorithms on sparse synthetic data and compare them to multiple other methods. We also apply one of our algorithms to a real dense 38 GB tensor representing a video and use the resulting decomposition to correctly classify frames containing disturbances.",https://proceedings.neurips.cc/paper_files/paper/2018/file/45a766fa266ea2ebeb6680fa139d2a3d-Paper.pdf,2018,"algorithms, decomposition, tensor, tensors, 38, also, apply, best, classify, compare","tucker, 1e, tensor, algorithm, 10, algorithms, rank, tensorsketch, als, ttmts","{'tensorsketch': 0.4315018983799014, 'tucker': 0.4315018983799014, 'tensors': 0.3901022298470395, 'decomposition': 0.35667800368548025, 'rank': 0.3197045891558854, 'low': 0.3111575912604155, 'large': 0.29703877590457484, 'using': 0.24688290752335387}","algorithms, tensors, tensor, decomposition, disturbances, streamed, tucker, 38, gb, classify"
But How Does It Work in Theory? Linear SVM with Random Features,"Yitong Sun, Anna Gilbert, Ambuj Tewari","We prove that, under low noise assumptions, the support vector machine with $N\ll m$ random features (RFSVM) can achieve the learning rate faster than $O(1/\sqrt{m})$ on a training set with $m$ samples when an optimized feature map is used. Our work extends the previous fast rate analysis of random features method from least square loss to 0-1 loss. We also show that the reweighted feature selection method, which approximates the optimized feature map, helps improve the performance of RFSVM in experiments on a synthetic data set.",https://proceedings.neurips.cc/paper_files/paper/2018/file/464d828b85b0bed98e80ade0a5c43b0f-Paper.pdf,2018,"feature, features, loss, map, method, optimized, random, rate, rfsvm, set","feature, features, rfsvm, rate, random, learning, map, kernel, data, function","{'work': 0.4977633819201783, 'svm': 0.4698273238983189, 'theory': 0.402249368671742, 'random': 0.36879846441261227, 'features': 0.3636925143109418, 'linear': 0.31844119460616344}","rfsvm, feature, optimized, map, features, rate, random, reweighted, loss, set"
Designing by Training: Acceleration Neural Network for Fast High-Dimensional Convolution,"Longquan Dai, Liang Tang, Yuan Xie, Jinhui Tang","The high-dimensional convolution is widely used in various disciplines but has a serious performance problem due to its high computational complexity. Over the decades, people took a handmade approach to design fast algorithms for the Gaussian convolution. Recently, requirements for various non-Gaussian convolutions have emerged and are continuously getting higher. However, the handmade acceleration approach is no longer feasible for so many different convolutions since it is a time-consuming and painstaking job. Instead, we propose an Acceleration Network (AccNet) which turns the work of designing new fast algorithms to training the AccNet. This is done by: 1, interpreting splatting, blurring, slicing operations as convolutions; 2, turning these convolutions to $g$CP layers to build AccNet. After training,  the activation function $g$ together with AccNet weights automatically define the new splatting, blurring and slicing operations. Experiments demonstrate AccNet is able to design acceleration algorithms for a ton of convolutions including Gaussian/non-Gaussian convolutions and produce state-of-the-art results.",https://proceedings.neurips.cc/paper_files/paper/2018/file/470e7a4f017a5476afb7eeb3f8b96f9b-Paper.pdf,2018,"convolutions, accnet, gaussian, acceleration, algorithms, approach, blurring, convolution, design, fast","accnet, convolution, ﬁltering, gaussian, gcp, layer, blurring, ki, bilateral, lattice","{'designing': 0.44540318159507714, 'convolution': 0.3776723419178201, 'acceleration': 0.35993637762155783, 'dimensional': 0.35993637762155783, 'high': 0.34617929493190447, 'fast': 0.29220553794430076, 'training': 0.2872708035919822, 'network': 0.2689448705148681, 'neural': 0.19947726057654633}","accnet, convolutions, acceleration, gaussian, blurring, handmade, slicing, splatting, convolution, operations"
A Probabilistic U-Net for Segmentation of Ambiguous Images,"Simon Kohl, Bernardino Romera-Paredes, Clemens Meyer, Jeffrey De Fauw, Joseph R. Ledsam, Klaus Maier-Hein, S. M. Ali Eslami, Danilo Jimenez Rezende, Olaf Ronneberger","Many real-world vision problems suffer from inherent ambiguities. In clinical applications for example, it might not be clear from a CT scan alone which particular region is cancer tissue. Therefore a group of graders typically produces a set of diverse but plausible segmentations. We consider the task of learning a distribution over segmentations given an input. To this end we propose a generative segmentation model based on a combination of a U-Net with a conditional variational autoencoder that is capable of efficiently producing an unlimited number of plausible hypotheses. We show on a lung abnormalities segmentation task and on a Cityscapes segmentation task that our model reproduces the possible segmentation variants as well as the frequencies with which they occur, doing so significantly better than published approaches. These models could have a high impact in real-world applications, such as being used as clinical decision-making algorithms accounting for multiple plausible semantic segmentation hypotheses to provide possible diagnoses and recommend further actions to resolve the present ambiguities.",https://proceedings.neurips.cc/paper_files/paper/2018/file/473447ac58e1cd7e96172575f48dca3b-Paper.pdf,2018,"segmentation, plausible, task, ambiguities, applications, clinical, hypotheses, model, possible, real","segmentation, net, image, model, set, training, distribution, models, space, modes","{'ambiguous': 0.5264007278399415, 'segmentation': 0.44635288629638536, 'images': 0.42539159796193565, 'net': 0.42539159796193565, 'probabilistic': 0.40218820394343696}","segmentation, plausible, segmentations, ambiguities, clinical, hypotheses, task, abnormalities, diagnoses, graders"
Bandit Learning in Concave N-Person Games,"Mario Bravo, David Leslie, Panayotis Mertikopoulos","This paper examines the long-run behavior of learning with bandit feedback in non-cooperative concave games. The bandit framework accounts for extremely low-information environments where the agents may not even know they are playing a game; as such, the agents’ most sensible choice in this setting would be to employ a no-regret learning algorithm. In general, this does not mean that the players' behavior stabilizes in the long run: no-regret learning may lead to cycles, even with perfect gradient information. However, if a standard monotonicity condition is satisfied, our analysis shows that no-regret learning based on mirror descent with bandit feedback converges to Nash equilibrium with probability 1. We also derive an upper bound for the convergence rate of the process that nearly matches the best attainable rate for single-agent bandit stochastic optimization.",https://proceedings.neurips.cc/paper_files/paper/2018/file/47fd3c87f42f55d4b233417d49c34783-Paper.pdf,2018,"bandit, learning, regret, agents, behavior, even, feedback, information, long, may","xi, vi, games, players, player, learning, xn, al, et, regret","{'concave': 0.5238005931626053, 'person': 0.5238005931626053, 'bandit': 0.4682121620409707, 'games': 0.4426730791532501, 'learning': 0.1899573465673431}","bandit, regret, run, agents, feedback, behavior, long, examines, rate, may"
Fast Approximate Natural Gradient Descent in a Kronecker Factored Eigenbasis,"Thomas George, César Laurent, Xavier Bouthillier, Nicolas Ballas, Pascal Vincent","Optimization algorithms that leverage gradient covariance information, such as variants of natural gradient descent (Amari, 1998), offer the prospect of yielding more effective descent directions. For models with many parameters, the covari- ance matrix they are based on becomes gigantic, making them inapplicable in their original form. This has motivated research into both simple diagonal approxima- tions and more sophisticated factored approximations such as KFAC (Heskes, 2000; Martens & Grosse, 2015; Grosse & Martens, 2016). In the present work we draw inspiration from both to propose a novel approximation that is provably better than KFAC and amendable to cheap partial updates. It consists in tracking a diagonal variance, not in parameter coordinates, but in a Kronecker-factored eigenbasis, in which the diagonal approximation is likely to be more effective. Experiments show improvements over KFAC in optimization speed for several deep network architectures.",https://proceedings.neurips.cc/paper_files/paper/2018/file/48000647b315f6f00f913caa757a70b3-Paper.pdf,2018,"diagonal, kfac, approximation, descent, effective, factored, gradient, grosse, martens, optimization","ekfac, kfac, diagonal, gradient, approximation, training, figure, matrix, basis, kronecker","{'eigenbasis': 0.436285430280206, 'factored': 0.4117997096806255, 'kronecker': 0.4117997096806255, 'natural': 0.3525681985964864, 'approximate': 0.3454553732391852, 'descent': 0.2862238621550461, 'fast': 0.2862238621550461, 'gradient': 0.25100811429876707}","kfac, diagonal, grosse, martens, factored, effective, 1998, 2000, 2015, amari"
A convex program for bilinear inversion of sparse vectors,"Alireza Aghasi, Ali Ahmed, Paul Hand, Babhru Joshi","We consider the bilinear inverse problem of recovering two vectors,  x in R^L and w in R^L, from their entrywise product. We consider the case where x and w have known signs and are sparse with respect to known dictionaries of size K and N, respectively.  Here,  K and N may be larger than, smaller than, or equal to L.  We introduce L1-BranchHull, which is a convex program posed in the natural parameter space and does not require an approximate solution or initialization in order to be stated or solved. We study the case where x and w are S1- and S2-sparse with respect to a random dictionary, with the sparse vectors satisfying an effective sparsity condition, and present a recovery guarantee that depends on the number of measurements as L > Omega(S1+S2)(log(K+N))^2. Numerical experiments verify that the scaling constant in the theorem is not too large.  One application of this problem is the sweep distortion removal task in dielectric imaging, where one of the signals is a nonnegative reflectivity, and the other signal lives in a known subspace, for example that given by dominant wavelet coefficients. We also introduce a variants of L1-BranchHull for the purposes of tolerating noise and outliers, and for the purpose of recovering piecewise constant signals.  We provide an ADMM implementation of these variants and show they can extract piecewise constant behavior from real images.",https://proceedings.neurips.cc/paper_files/paper/2018/file/482db0ecc10b8a9984ae850c9ada9899-Paper.pdf,2018,"constant, known, sparse, branchhull, case, consider, introduce, l1, one, piecewise","program, set, let, branchhull, k1, 10, image, al, et, convex","{'vectors': 0.4903641569046316, 'bilinear': 0.4628433668330856, 'inversion': 0.4281712718150351, 'program': 0.38112417044081426, 'sparse': 0.33076545522162343, 'convex': 0.32760792684541756}","branchhull, s1, s2, piecewise, l1, constant, sparse, recovering, signals, vectors"
Lipschitz-Margin Training: Scalable Certification of Perturbation Invariance for Deep Neural Networks,"Yusuke Tsuzuku, Issei Sato, Masashi Sugiyama","High sensitivity of neural networks against malicious perturbations on inputs causes security concerns. To take a steady step towards robust classifiers, we aim to create neural network models provably defended from perturbations. Prior certification work requires strong assumptions on network structures and massive computational costs, and thus the range of their applications was limited. From the relationship between the Lipschitz constants and prediction margins, we present a computationally efficient calculation technique to lower-bound the size of adversarial perturbations that can deceive networks, and that is widely applicable to various complicated networks. Moreover, we propose an efficient training procedure that robustifies networks and significantly improves the provably guarded areas around data points. In experimental evaluations, our method showed its ability to provide a non-trivial guarantee and enhance robustness for even large networks.",https://proceedings.neurips.cc/paper_files/paper/2018/file/485843481a7edacbfce101ecb1e4d2a8-Paper.pdf,2018,"networks, perturbations, efficient, network, neural, provably, ability, adversarial, aim, applicable","lipschitz, networks, constant, lmt, training, guarded, perturbations, norm, bounds, network","{'perturbation': 0.41418393537567616, 'invariance': 0.3909386206927402, 'certification': 0.3744458074541793, 'lipschitz': 0.36165298763802156, 'margin': 0.3512004927712434, 'scalable': 0.3219148597165248, 'training': 0.26713538849039775, 'deep': 0.19406813396237887, 'neural': 0.18549547964088084, 'networks': 0.1849670888029178}","perturbations, networks, provably, deceive, defended, guarded, robustifies, certification, steady, calculation"
Robust Learning of Fixed-Structure Bayesian Networks,"Yu Cheng, Ilias Diakonikolas, Daniel Kane, Alistair Stewart","We investigate the problem of learning Bayesian networks in a robust model where an $\epsilon$-fraction of the samples are adversarially corrupted.  In this work, we study the fully observable discrete case where the structure of the network is given.  Even in this basic setting, previous learning algorithms either run in exponential time or lose dimension-dependent factors in their error guarantees.  We provide the first computationally efficient robust learning algorithm for this problem with dimension-independent error guarantees.  Our algorithm has near-optimal sample complexity, runs in polynomial time, and achieves error that scales nearly-linearly with the fraction of adversarially corrupted samples.  Finally, we show on both synthetic and semi-synthetic data that our algorithm performs well in practice.",https://proceedings.neurips.cc/paper_files/paper/2018/file/486fbd761bfa5400722324fdc9822adc-Paper.pdf,2018,"algorithm, error, learning, adversarially, corrupted, dimension, fraction, guarantees, problem, robust","samples, algorithm, conditional, bc, ld, bayesian, error, noise, set, learning","{'fixed': 0.6261335878065274, 'structure': 0.44591382685201236, 'robust': 0.4005652784561953, 'bayesian': 0.35824102103858746, 'networks': 0.27961998775556784, 'learning': 0.20528292727559302}","corrupted, adversarially, fraction, error, dimension, robust, lose, guarantees, synthetic, samples"
3D Steerable CNNs: Learning Rotationally Equivariant Features in Volumetric Data,"Maurice Weiler, Mario Geiger, Max Welling, Wouter Boomsma, Taco S. Cohen","We present a convolutional network that is equivariant to rigid body motions. The model uses scalar-, vector-, and tensor fields over 3D Euclidean space to represent data, and equivariant convolutions to map between such representations. These SE(3)-equivariant convolutions utilize kernels which are parameterized as a linear combination of a complete steerable kernel basis, which is derived analytically in this paper. We prove that equivariant convolutions are the most general equivariant linear maps between fields over R^3. Our experimental results confirm the effectiveness of 3D Steerable CNNs for the problem of amino acid propensity prediction and protein structure classification, both of which have inherent SE(3) symmetry.",https://proceedings.neurips.cc/paper_files/paper/2018/file/488e4104520c6aab692863cc1dba45af-Paper.pdf,2018,"equivariant, convolutions, 3d, fields, linear, se, steerable, acid, amino, analytically","3d, equivariant, steerable, basis, space, ﬁelds, kernel, representation, fn, r3","{'cnns': 0.39472727755277565, 'rotationally': 0.39472727755277565, 'steerable': 0.39472727755277565, 'volumetric': 0.39472727755277565, 'equivariant': 0.372573932150013, '3d': 0.2968311439355497, 'features': 0.2884088329006556, 'data': 0.22341367994646094, 'learning': 0.12941450928295742}","equivariant, convolutions, steerable, se, fields, 3d, acid, amino, motions, rigid"
Toddler-Inspired Visual Object Learning,"Sven Bambach, David Crandall, Linda Smith, Chen Yu","Real-world learning systems have practical limitations on the quality and quantity of the training datasets that they can collect and consider. How should a system go about choosing a subset of the possible training examples that still allows for learning accurate, generalizable models? To help address this question, we draw inspiration from a highly efficient practical learning system: the human child. Using head-mounted cameras, eye gaze trackers, and a model of foveated vision, we collected first-person (egocentric) images that represents a highly accurate approximation of the ""training data"" that toddlers' visual systems collect in everyday, naturalistic learning contexts. We used state-of-the-art computer vision learning models (convolutional neural networks) to help characterize the structure of these data, and found that child data produce significantly better object models than egocentric data experienced by adults in exactly the same environment. By using the CNNs as a modeling tool to investigate the properties of the child data that may enable this rapid learning, we found that child data exhibit a unique combination of quality and diversity, with not only many similar large, high-quality object views but also  a greater number and diversity of rare views. This novel methodology of analyzing the visual ""training data"" used by children may not only reveal insights to improve machine learning, but also may suggest new experimental tools to better understand infant learning in developmental psychology.",https://proceedings.neurips.cc/paper_files/paper/2018/file/48ab2f9b45957ab574cf005eb8a76760-Paper.pdf,2018,"learning, data, child, training, may, models, quality, accurate, also, better","data, object, learning, training, visual, objects, instances, toddler, models, properties","{'inspired': 0.5670495773772284, 'toddler': 0.5670495773772284, 'visual': 0.4038359417019925, 'object': 0.39907756559967444, 'learning': 0.1859117597606889}","child, data, egocentric, learning, quality, collect, views, diversity, may, help"
Reducing Network Agnostophobia,"Akshay Raj Dhamija, Manuel Günther, Terrance Boult","Agnostophobia, the fear of the unknown, can be experienced by deep learning engineers while applying their networks to real-world applications. Unfortunately, network behavior is not well defined for inputs far from a networks training set. In an uncontrolled environment, networks face many instances that are not of interest to them and have to be rejected in order to avoid a false positive. This problem has previously been tackled by researchers by either a) thresholding softmax, which by construction cannot return ""none of the known classes"", or b) using an additional background or garbage class. In this paper, we show that both of these approaches help, but are generally insufficient when previously unseen classes are encountered. We also introduce a new evaluation metric that focuses on comparing the performance of multiple approaches in scenarios where such unseen classes or unknowns are encountered. Our major contributions are simple yet effective Entropic Open-Set and Objectosphere losses that train networks using negative samples from some classes. These novel losses are designed to maximize entropy for unknown inputs while increasing separation in deep feature space by modifying magnitudes of known and unknown samples. Experiments on networks trained to classify classes from MNIST and CIFAR-10 show that our novel loss functions are significantly better at dealing with unknown inputs from datasets such as Devanagari, NotMNIST, CIFAR-100 and SVHN.",https://proceedings.neurips.cc/paper_files/paper/2018/file/48db71587df6c7c442e5b76cc723169a-Paper.pdf,2018,"classes, networks, unknown, inputs, approaches, cifar, deep, encountered, known, losses","samples, unknown, softmax, classes, known, loss, set, open, network, feature","{'agnostophobia': 0.665852428810365, 'reducing': 0.6284826809323957, 'network': 0.40205728797693285}","classes, unknown, inputs, encountered, networks, unseen, losses, cifar, previously, agnostophobia"
Deep Functional Dictionaries: Learning Consistent Semantic Structures on 3D Models from Functions,"Minhyuk Sung, Hao Su, Ronald Yu, Leonidas J. Guibas","Various 3D semantic attributes such as segmentation masks, geometric features, keypoints, and materials can be encoded as per-point probe functions on 3D geometries. Given a collection of related 3D shapes, we consider how to jointly analyze such probe functions over different shapes, and how to discover common latent structures using a neural network — even in the absence of any correspondence information. Our network is trained on point cloud representations of shape geometry and associated semantic functions on that point cloud. These functions express a shared semantic understanding of the shapes but are not coordinated in any way. For example, in a segmentation task, the functions can be indicator functions of arbitrary sets of shape parts, with the particular combination involved not known to the network. Our network is able to produce a small dictionary of basis functions for each shape, a dictionary whose span includes the semantic functions provided for that shape. Even though our shapes have independent discretizations and no functional correspondences are provided, the network is able to generate latent bases, in a consistent order, that reflect the shared semantic structure among the shapes. We demonstrate the effectiveness of our technique in various segmentation and keypoint selection applications.",https://proceedings.neurips.cc/paper_files/paper/2018/file/49182f81e6a13cf5eaa496d51fea6406-Paper.pdf,2018,"functions, network, semantic, shapes, shape, 3d, point, segmentation, able, cloud","functions, shape, function, shapes, segmentation, point, network, semantic, dictionary, labels","{'dictionaries': 0.4093380350930375, 'functional': 0.4093380350930375, 'consistent': 0.3863646876868862, 'structures': 0.35742169287903336, 'semantic': 0.3241181434897794, '3d': 0.30781829410497535, 'functions': 0.2951751486819266, 'models': 0.21477090845418859, 'deep': 0.1917975610480372, 'learning': 0.1342047635289981}","shapes, functions, semantic, shape, segmentation, 3d, probe, cloud, network, dictionary"
Teaching Inverse Reinforcement Learners via Features and Demonstrations,"Luis Haug, Sebastian Tschiatschek, Adish Singla","Learning near-optimal behaviour from an expert's demonstrations typically relies on the assumption that the learner knows the features that the true reward function depends on. In this paper, we study the problem of learning from demonstrations in the setting where this is not the case, i.e., where there is a mismatch between the worldviews of the learner and the expert. We introduce a natural quantity, the teaching risk, which measures the potential suboptimality of policies that look optimal to the learner in this setting. We show that bounds on the teaching risk guarantee that the learner is able to find a near-optimal policy using standard algorithms based on inverse reinforcement learning. Based on these findings, we suggest a teaching scheme in which the expert can decrease the teaching risk by updating the learner's worldview, and thus ultimately enable her to find a near-optimal policy.",https://proceedings.neurips.cc/paper_files/paper/2018/file/4928e7510f45da6575b04a28519c09ed-Paper.pdf,2018,"learner, optimal, teaching, expert, learning, near, risk, based, demonstrations, find","teaching, al, learner, feature, policy, risk, reward, performance, worldview, features","{'learners': 0.45626268980712864, 'teaching': 0.43701400233445564, 'demonstrations': 0.4220835603914336, 'inverse': 0.3906358136018258, 'features': 0.35319215465242054, 'reinforcement': 0.2846429981654613, 'via': 0.2536255395654644}","teaching, learner, expert, near, risk, optimal, demonstrations, worldview, worldviews, find"
Learning to Decompose and Disentangle Representations for Video Prediction,"Jun-Ting Hsieh, Bingbin Liu, De-An Huang, Li F. Fei-Fei, Juan Carlos Niebles","Our goal is to predict future video frames given a sequence of input frames. Despite large amounts of video data, this remains a challenging task because of the high-dimensionality of video frames. We address this challenge by proposing the Decompositional Disentangled Predictive Auto-Encoder (DDPAE), a framework that combines structured probabilistic models and deep networks to automatically (i) decompose the high-dimensional video that we aim to predict into components, and (ii) disentangle each component to have low-dimensional temporal dynamics that are easier to predict. Crucially, with an appropriately specified generative model of video frames, our DDPAE is able to learn both the latent decomposition and disentanglement without explicit supervision. For the Moving MNIST dataset, we show that DDPAE is able to recover the underlying components (individual digits) and disentanglement (appearance and location) as we would intuitively do. We further demonstrate that DDPAE can be applied to the Bouncing Balls dataset involving complex interactions between multiple objects to predict the video frame directly from the pixels and recover physical states without explicit supervision.",https://proceedings.neurips.cc/paper_files/paper/2018/file/496e05e1aea0a9c4655800e8a7b9ea28-Paper.pdf,2018,"video, ddpae, frames, predict, able, components, dataset, dimensional, disentanglement, explicit","zi, model, prediction, video, components, ddpae, component, x1, frames, decomposition","{'disentangle': 0.5278194262331504, 'decompose': 0.4981965277800086, 'video': 0.4265380674567435, 'representations': 0.3633421703487289, 'prediction': 0.3595947306292939, 'learning': 0.1730498395232959}","ddpae, video, frames, predict, disentanglement, recover, supervision, components, explicit, bouncing"
Maximizing acquisition functions for Bayesian optimization,"James Wilson, Frank Hutter, Marc Deisenroth","Bayesian optimization is a sample-efficient approach to global optimization that relies on theoretically motivated value heuristics (acquisition functions) to guide its search process. Fully maximizing acquisition functions produces the Bayes' decision rule, but this ideal is difficult to achieve since these functions are frequently non-trivial to optimize. This statement is especially true when evaluating queries in parallel, where acquisition functions are routinely non-convex, high-dimensional, and intractable. We first show that acquisition functions estimated via Monte Carlo integration are consistently amenable to gradient-based optimization. Subsequently, we identify a common family of acquisition functions, including EI and UCB, whose characteristics not only facilitate but justify use of greedy approaches for their maximization.",https://proceedings.neurips.cc/paper_files/paper/2018/file/498f2c21688f6451d9f5fd09d53edda7-Paper.pdf,2018,"functions, acquisition, optimization, non, achieve, amenable, approach, approaches, based, bayes","16, ucb, 256, 64, acquisition, functions, budget, equivalent, 128, 32","{'acquisition': 0.5538399130905763, 'maximizing': 0.5538399130905763, 'functions': 0.42312298173838764, 'bayesian': 0.3357200174624614, 'optimization': 0.3078663890967416}","acquisition, functions, optimization, ei, statement, ideal, justify, routinely, characteristics, ucb"
Nonparametric Density Estimation under Adversarial Losses,"Shashank Singh, Ananya Uppal, Boyue Li, Chun-Liang Li, Manzil Zaheer, Barnabas Poczos","We study minimax convergence rates of nonparametric density estimation under a large class of loss functions called ``adversarial losses'', which, besides classical L^p losses, includes maximum mean discrepancy (MMD), Wasserstein distance, and total variation distance. These losses are closely related to the losses encoded by discriminator networks in generative adversarial networks (GANs). In a general framework, we study how the choice of loss and the assumed smoothness of the underlying density together determine the minimax rate. We also discuss implications for training GANs based on deep ReLU networks, and more general connections to learning implicit generative models in a minimax statistical sense.",https://proceedings.neurips.cc/paper_files/paper/2018/file/4996dcc43b5be197b5887a4e60817b1c-Paper.pdf,2018,"losses, minimax, networks, adversarial, density, distance, gans, general, generative, loss","fg, minimax, density, fd, x1, estimation, distribution, loss, theorem, bound","{'density': 0.5211338650921395, 'losses': 0.5033294952647717, 'nonparametric': 0.4564306666861387, 'estimation': 0.39271864700315956, 'adversarial': 0.3354430274354984}","losses, minimax, gans, density, distance, networks, mmd, generative, adversarial, discrepancy"
Weakly Supervised Dense Event Captioning in Videos,"Xuguang Duan, Wenbing Huang, Chuang Gan, Jingdong Wang, Wenwu Zhu, Junzhou Huang","Dense event captioning aims to detect and describe all events of interest contained in a video. Despite the advanced development in this area, existing methods tackle this task by making use of dense temporal annotations, which is dramatically source-consuming. This paper formulates a new problem: weakly supervised dense event captioning, which does not require temporal segment annotations for model training.  Our solution is based on the one-to-one correspondence assumption, each caption describes one temporal segment, and each temporal segment has one caption, which holds in current benchmark datasets and  most real world cases. We decompose the problem into a pair of dual problems: event captioning and sentence localization and present a cycle system to train our model. Extensive experimental results are provided to  demonstrate the ability of our model  on both dense event captioning and sentence localization in videos.",https://proceedings.neurips.cc/paper_files/paper/2018/file/49af6c4e558a7569d80eee2e035e2bd7-Paper.pdf,2018,"captioning, dense, event, one, temporal, model, segment, annotations, caption, localization","caption, event, video, segment, captioning, sentence, temporal, model, one, dense","{'dense': 0.43206077409954446, 'videos': 0.43206077409954446, 'weakly': 0.43206077409954446, 'captioning': 0.413833110528458, 'event': 0.39969463625111284, 'supervised': 0.33008607488947295}","captioning, event, dense, segment, temporal, caption, annotations, sentence, localization, one"
Moonshine: Distilling with Cheap Convolutions,"Elliot J. Crowley, Gavin Gray, Amos J. Storkey","Many engineers wish to deploy modern neural networks in memory-limited settings; but the development of flexible methods for reducing memory use is in its infancy, and there is little knowledge of the resulting cost-benefit. We propose structural model distillation for memory reduction using a strategy that produces a student architecture that is a simple transformation of the teacher architecture: no redesign is needed, and the same hyperparameters can be used. Using attention transfer, we provide Pareto curves/tables for distillation of residual networks with four benchmark datasets, indicating the memory versus accuracy payoff. We show that substantial memory savings are possible with very little loss of accuracy, and confirm that distillation provides student network performance that is better than training that student architecture directly on data.",https://proceedings.neurips.cc/paper_files/paper/2018/file/49b8b4f95f02e055801da3b4f58e28b7-Paper.pdf,2018,"memory, architecture, distillation, student, accuracy, little, networks, using, attention, benchmark","network, networks, student, bg, teacher, al, et, blocks, convolutions, parameters","{'cheap': 0.5263234485744512, 'moonshine': 0.5263234485744512, 'distilling': 0.49678450912711586, 'convolutions': 0.446287358607349}","memory, student, distillation, architecture, little, infancy, redesign, tables, engineers, payoff"
Exploiting Numerical Sparsity for Efficient Learning : Faster Eigenvector Computation and Regression,"Neha Gupta, Aaron Sidford","In this paper, we obtain improved running times for regression and top eigenvector computation for numerically sparse matrices. Given a data matrix $\mat{A} \in \R^{n \times d}$ where every row $a \in \R^d$ has $\|a\|_2^2 \leq L$ and numerical sparsity $\leq s$, i.e. $\|a\|_1^2 / \|a\|_2^2 \leq s$, we provide faster algorithms for these problems for many parameter settings.

For top eigenvector computation, when $\gap > 0$ is the relative gap between the top two eigenvectors of $\mat{A}^\top \mat{A}$ and $r$ is the stable rank of $\mat{A}$ we obtain a running time of $\otilde(nd + r(s + \sqrt{r s}) / \gap^2)$ improving upon the previous best unaccelerated running time of $O(nd + r d / \gap^2)$. As $r \leq d$ and $s \leq d$ our algorithm everywhere improves or matches the previous bounds for all parameter settings.

For regression, when $\mu > 0$ is the smallest eigenvalue of $\mat{A}^\top \mat{A}$ we obtain a running time of $\otilde(nd + (nL / \mu) \sqrt{s nL / \mu})$ improving upon the previous best unaccelerated running time of $\otilde(nd + n L d / \mu)$. This result expands when regression can be solved in nearly linear time from when $L/\mu = \otilde(1)$ to when $L / \mu = \otilde(d^{2/3} / (sn)^{1/3})$.

Furthermore, we obtain similar improvements even when row norms and numerical sparsities are non-uniform and we show how to achieve even faster running times by accelerating using approximate proximal point \cite{frostig2015regularizing} / catalyst \cite{lin2015universal}. Our running times depend only on the size of the input and natural numerical measures of the matrix, i.e. eigenvalues and $\ell_p$ norms, making progress on a key open problem regarding optimal running times for efficient large-scale learning.",https://proceedings.neurips.cc/paper_files/paper/2018/file/4a1590df1d5968d41b855005bb8b67bf-Paper.pdf,2018,"running, mat, mu, leq, otilde, time, times, top, gap, nd","nnz, running, ai, eigenvector, regression, time, si, top, computation, gap","{'numerical': 0.4321623959659934, 'eigenvector': 0.39069935718787047, 'sparsity': 0.36644503433672143, 'exploiting': 0.3572239687468876, 'computation': 0.3492363184097476, 'faster': 0.33588820132766567, 'regression': 0.28872384861217065, 'efficient': 0.2459165168472446, 'learning': 0.14168791361779778}","mat, running, mu, otilde, leq, nd, top, times, gap, obtain"
The Everlasting Database: Statistical Validity at a Fair Price,"Blake E. Woodworth, Vitaly Feldman, Saharon Rosset, Nati Srebro","The problem of handling adaptivity in data analysis, intentional or not,  permeates
  a variety of fields, including  test-set overfitting in ML challenges and the
  accumulation of invalid scientific discoveries.
  We propose a mechanism for answering an arbitrarily long sequence of
  potentially adaptive statistical queries, by charging a price for
  each query and using the proceeds to collect additional samples.
  Crucially, we guarantee statistical validity without any assumptions on
  how the queries are generated. We also ensure with high probability that
  the cost for $M$ non-adaptive queries is $O(\log M)$,
  while the cost to a potentially adaptive user who makes $M$
  queries that do not depend on any others is $O(\sqrt{M})$.",https://proceedings.neurips.cc/paper_files/paper/2018/file/4ad13f04ef4373992c9d3046200aa350-Paper.pdf,2018,"queries, adaptive, cost, potentially, statistical, accumulation, adaptivity, additional, also, analysis","queries, adaptive, cost, data, non, user, query, users, answers, database","{'database': 0.43518205913494695, 'everlasting': 0.43518205913494695, 'validity': 0.43518205913494695, 'fair': 0.4107582632197711, 'price': 0.37998791940075444, 'statistical': 0.34458171240993857}","queries, adaptive, potentially, statistical, charging, permeates, cost, accumulation, intentional, proceeds"
Learning Conditioned Graph Structures for Interpretable Visual Question Answering,"Will Norcliffe-Brown, Stathis Vafeias, Sarah Parisot","Visual Question answering is a challenging problem requiring a combination of concepts from Computer Vision and Natural Language Processing. Most existing approaches use a two streams strategy, computing image and question features that are consequently merged using a variety of techniques. Nonetheless, very few rely on  higher level image representations, which can capture semantic and spatial relationships. In this paper, we propose a novel graph-based approach for Visual Question Answering. Our method combines a graph learner module, which learns a question specific graph representation of the input image, with the recent concept of graph convolutions, aiming to learn image representations that capture question specific interactions. We test our approach on the VQA v2 dataset using a simple baseline architecture enhanced by the proposed graph learner module. We obtain promising results with 66.18% accuracy and demonstrate the interpretability of the proposed method. Code can be found at github.com/aimbrain/vqa-project.",https://proceedings.neurips.cc/paper_files/paper/2018/file/4aeae10ea1c6433c926cdfa558d31134-Paper.pdf,2018,"graph, question, image, answering, approach, capture, learner, method, module, proposed","graph, question, image, model, vqa, features, object, structure, using, set","{'interpretable': 0.4274979185224495, 'conditioned': 0.40946275147353883, 'structures': 0.39547358909876673, 'answering': 0.3743797025626105, 'question': 0.3743797025626105, 'visual': 0.32255413680499484, 'graph': 0.2945758120554506, 'learning': 0.1484924965786192}","question, graph, image, vqa, answering, learner, module, capture, visual, specific"
Exponentially Weighted Imitation Learning for Batched Historical Data,"Qing Wang, Jiechao Xiong, Lei Han, peng sun, Han Liu, Tong Zhang","We consider deep policy learning with only batched historical trajectories. The main challenge of this problem is that the learner no longer has a simulator or ``environment oracle'' as in most reinforcement learning settings. To solve this problem, we propose a monotonic advantage reweighted imitation learning strategy that is applicable to problems with complex nonlinear function approximation and works well with hybrid (discrete and continuous) action space. The method does not rely on the knowledge of the behavior policy, thus can be used to learn from data generated by an unknown policy. Under mild conditions, our algorithm, though surprisingly simple, has a policy improvement bound and outperforms most competing methods empirically. Thorough numerical results are also provided to demonstrate the efficacy of the proposed methodology.",https://proceedings.neurips.cc/paper_files/paper/2018/file/4aec1b3435c52abbdf8334ea0e7141e0-Paper.pdf,2018,"policy, learning, problem, action, advantage, algorithm, also, applicable, approximation, batched","policy, st, πθ, algorithm, learning, al, action, data, et, method","{'batched': 0.4538401628324155, 'historical': 0.4538401628324155, 'exponentially': 0.42836921502458813, 'weighted': 0.41029728995351095, 'imitation': 0.38482634214568356, 'data': 0.25687127961997624, 'learning': 0.14879514364953353}","policy, batched, monotonic, reweighted, thorough, historical, longer, simulator, imitation, competing"
Temporal Regularization for Markov Decision Process,"Pierre Thodoroff, Audrey Durand, Joelle Pineau, Doina Precup","Several applications of Reinforcement Learning suffer from instability due to high
variance. This is especially prevalent in high dimensional domains. Regularization
is a commonly used technique in machine learning to reduce variance, at the cost
of introducing some bias. Most existing regularization techniques focus on spatial
(perceptual) regularization. Yet in reinforcement learning, due to the nature of the
Bellman equation, there is an opportunity to also exploit temporal regularization
based on smoothness in value estimates over trajectories. This paper explores a
class of methods for temporal regularization. We formally characterize the bias
induced by this technique using Markov chain concepts. We illustrate the various
characteristics of temporal regularization via a sequence of simple discrete and
continuous MDPs, and show that the technique provides improvement even in
high-dimensional Atari games.",https://proceedings.neurips.cc/paper_files/paper/2018/file/4b0250793549726d5c1ea3906726ebfe-Paper.pdf,2018,"regularization, high, learning, technique, temporal, bias, dimensional, due, reinforcement, variance","regularization, state, temporal, st, value, markov, function, chain, using, states","{'markov': 0.4688084057966311, 'temporal': 0.46017383553954244, 'decision': 0.44523210635674443, 'process': 0.4386723967856626, 'regularization': 0.42165580691685783}","regularization, temporal, technique, bias, variance, high, due, dimensional, reinforcement, bellman"
Explaining Deep Learning Models -- A Bayesian Non-parametric Approach,"Wenbo Guo, Sui Huang, Yunzhe Tao, Xinyu Xing, Lin Lin","Understanding and interpreting how machine learning (ML) models make decisions have been a big challenge. While recent research has proposed various technical approaches to provide some clues as to how an ML model makes individual predictions, they cannot provide users with an ability to inspect a model as a complete entity. In this work, we propose a novel technical approach that augments a Bayesian non-parametric regression mixture model with multiple elastic nets. Using the enhanced mixture model, we can extract generalizable insights for a target model through a global approximation. To demonstrate the utility of our approach, we evaluate it on different ML models in the context of image recognition. The empirical results indicate that our proposed approach not only outperforms the state-of-the-art techniques in explaining individual decisions but also provides users with an ability to discover the vulnerabilities of the target ML models.",https://proceedings.neurips.cc/paper_files/paper/2018/file/4b4edc2630fe75800ddc29a7b4070add-Paper.pdf,2018,"model, ml, approach, models, ability, decisions, individual, mixture, proposed, provide","model, approach, models, data, mixture, samples, proposed, features, insights, target","{'parametric': 0.5039980050440604, 'explaining': 0.4827354729486908, 'approach': 0.3715693733362934, 'non': 0.33139084993326035, 'bayesian': 0.3055074490934458, 'models': 0.28016046199888145, 'deep': 0.2501926061598722, 'learning': 0.17506499750525906}","ml, technical, users, model, decisions, mixture, individual, ability, target, clues"
Optimization of Smooth Functions with Noisy Observations: Local Minimax Rates,"Yining Wang, Sivaraman Balakrishnan, Aarti Singh","We consider the problem of global optimization of an unknown non-convex smooth function with noisy zeroth-order feedback. We propose a local minimax framework to study the fundamental difficulty of optimizing smooth functions with adaptive function evaluations. We show that for functions with fast growth around their global minima, carefully designed optimization algorithms can identify a near global minimizer with many fewer queries than worst-case global minimax theory predicts. For the special case of strongly convex and smooth functions, our implied convergence rates match the ones developed for zeroth-order convex optimization problems. On the other hand, we show that in the worst case no algorithm can converge faster than the minimax rate of estimating an unknown functions in linf-norm. Finally, we show that non-adaptive algorithms, although optimal in a global minimax sense, do not attain the optimal local minimax rate.",https://proceedings.neurips.cc/paper_files/paper/2018/file/4ba3c163cd1efd4c14e3a415fa0a3010-Paper.pdf,2018,"global, minimax, functions, case, convex, optimization, show, smooth, adaptive, algorithms","f0, function, optimization, functions, xn, minimax, rate, local, global, level","{'observations': 0.4024800250978779, 'rates': 0.4024800250978779, 'smooth': 0.38872942530965654, 'minimax': 0.37749436722496665, 'noisy': 0.3525087093520553, 'functions': 0.321030503181159, 'local': 0.31705349658670545, 'optimization': 0.23358339317385915}","minimax, global, zeroth, smooth, functions, case, convex, worst, optimization, adaptive"
Measures of distortion for machine learning,"Leena Chennuru Vankadara, Ulrike von Luxburg","Given data from a general metric space, one of the standard machine learning pipelines is to first embed the data into a Euclidean space and subsequently apply out of the box machine learning algorithms to analyze the data. The quality of such an embedding is typically described in terms of a distortion measure. In this paper, we show that many of the existing distortion measures behave in an undesired way, when considered from a machine learning point of view. We investigate desirable properties of distortion measures and formally prove that most of the existing measures fail to satisfy these properties. These theoretical findings are supported by simulations, which for example demonstrate that existing distortion measures are not robust to noise or outliers and cannot serve as good indicators for classification accuracy. As an alternative, we suggest a new measure of distortion, called $\sigma$-distortion. We can show both in theory and in experiments that it satisfies all desirable properties and is a better candidate to evaluate distortion in the context of machine learning.",https://proceedings.neurips.cc/paper_files/paper/2018/file/4c5bcfec8584af0d967f1ab10179ca4b-Paper.pdf,2018,"distortion, learning, machine, measures, data, existing, properties, desirable, measure, show","distortion, measures, embedding, measure, space, properties, data, learning, let, machine","{'distortion': 0.6288324343155539, 'measures': 0.5685002447212076, 'machine': 0.4887454282703705, 'learning': 0.20616776578678256}","distortion, measures, machine, desirable, properties, measure, existing, indicators, sigma, undesired"
Sparse DNNs with Improved Adversarial Robustness,"Yiwen Guo, Chao Zhang, Changshui Zhang, Yurong Chen","Deep neural networks (DNNs) are computationally/memory-intensive and vulnerable to adversarial attacks, making them prohibitive in some real-world applications. By converting dense models into sparse ones, pruning appears to be a promising solution to reducing the computation/memory cost. This paper studies classification models, especially DNN-based ones, to demonstrate that there exists intrinsic relationships between their sparsity and adversarial robustness. Our analyses reveal, both theoretically and empirically, that nonlinear DNN-based classifiers behave differently under $l_2$ attacks from some linear ones. We further demonstrate that an appropriately higher model sparsity implies better robustness of nonlinear DNNs, whereas over-sparsified models can be more difficult to resist adversarial examples.",https://proceedings.neurips.cc/paper_files/paper/2018/file/4c5bde74a8f110656874902f07378009-Paper.pdf,2018,"adversarial, models, ones, attacks, based, demonstrate, dnn, dnns, memory, nonlinear","sparsity, robustness, dnns, adversarial, linear, models, classiﬁers, attacks, r2, nonlinear","{'dnns': 0.5340061265469066, 'robustness': 0.4672618028827448, 'improved': 0.4468470040808606, 'sparse': 0.41252365855765727, 'adversarial': 0.3558874126058703}","ones, dnn, dnns, attacks, adversarial, nonlinear, sparsity, robustness, memory, resist"
e-SNLI: Natural Language Inference with Natural Language Explanations,"Oana-Maria Camburu, Tim Rocktäschel, Thomas Lukasiewicz, Phil Blunsom","In order for machine learning to garner widespread public adoption, models must be able to provide interpretable and robust explanations for their decisions, as well as learn from human-provided explanations at train time. In this work, we extend the Stanford Natural Language Inference dataset with an additional layer of human-annotated natural language explanations of the entailment relations. We further implement models that incorporate these explanations into their training process and output them at test time. We show how our corpus of explanations, which we call e-SNLI, can be used for various goals, such as obtaining full sentence justifications of a model’s decisions, improving universal sentence representations and transferring to out-of-domain NLI datasets. Our dataset thus opens up a range of research directions for using natural language explanations, both for improving models and for asserting their trust",https://proceedings.neurips.cc/paper_files/paper/2018/file/4c7a167bb329bd92580a99ce422d6fa6-Paper.pdf,2018,"explanations, language, models, natural, dataset, decisions, human, improving, sentence, time","explanations, explanation, label, hypothesis, model, snli, premise, infersent, language, models","{'natural': 0.6048998863133703, 'language': 0.5719050399856107, 'snli': 0.3742666074638226, 'explanations': 0.33835827531025386, 'inference': 0.22903916006595917}","explanations, language, sentence, natural, decisions, improving, human, asserting, garner, nli"
Synaptic Strength For Convolutional Neural Network,"CHEN LIN, Zhao Zhong, Wu Wei, Junjie Yan","Convolutional Neural Networks(CNNs) are both computation and memory inten-sive which hindered their deployment in mobile devices. Inspired by the relevantconcept in neural science literature, we propose Synaptic Pruning: a data-drivenmethod to prune connections between input and output feature maps with a newlyproposed class of parameters called Synaptic Strength.  Synaptic Strength is de-signed to capture the importance of a connection based on the amount of informa-tion it transports. Experiment results show the effectiveness of our approach. OnCIFAR-10, we prune connections for various CNN models with up to96%, whichresults in significant size reduction and computation saving. Further evaluation onImageNet demonstrates that synaptic pruning is able to discover efficient modelswhich is competitive to state-of-the-art compact CNNs such as MobileNet-V2andNasNet-Mobile. Our contribution is summarized as following: (1) We introduceSynaptic Strength, a new class of parameters for CNNs to indicate the importanceof each connections. (2) Our approach can prune various CNNs with high com-pression without compromising accuracy.  (3) Further investigation shows, theproposed Synaptic Strength is a better indicator for kernel pruning compared withthe previous approach in both empirical result and theoretical analysis.",https://proceedings.neurips.cc/paper_files/paper/2018/file/4d19b37a2c399deace9082d464930022-Paper.pdf,2018,"synaptic, cnns, strength, approach, connections, prune, pruning, class, computation, mobile","pruning, synaptic, kernel, strength, 10, model, accuracy, layer, cnns, resnet","{'strength': 0.5762444624259326, 'synaptic': 0.5762444624259326, 'convolutional': 0.3849837942544141, 'network': 0.3479498996326099, 'neural': 0.2580755404024714}","synaptic, strength, prune, cnns, pruning, connections, mobile, computation, drivenmethod, importanceof"
Meta-Reinforcement Learning of Structured Exploration Strategies,"Abhishek Gupta, Russell Mendonca, YuXuan Liu, Pieter Abbeel, Sergey Levine","Exploration is a fundamental challenge in reinforcement learning (RL). Many
current exploration methods for deep RL use task-agnostic objectives, such as
information gain or bonuses based on state visitation. However, many practical
applications of RL involve learning more than a single task, and prior tasks can be
used to inform how exploration should be performed in new tasks. In this work, we
study how prior tasks can inform an agent about how to explore effectively in new
situations. We introduce a novel gradient-based fast adaptation algorithm – model
agnostic exploration with structured noise (MAESN) – to learn exploration strategies
from prior experience. The prior experience is used both to initialize a policy
and to acquire a latent exploration space that can inject structured stochasticity into
a policy, producing exploration strategies that are informed by prior knowledge
and are more effective than random action-space noise. We show that MAESN is
more effective at learning exploration strategies when compared to prior meta-RL
methods, RL without learned exploration strategies, and task-agnostic exploration
methods. We evaluate our method on a variety of simulated tasks: locomotion with
a wheeled robot, locomotion with a quadrupedal walker, and object manipulation.",https://proceedings.neurips.cc/paper_files/paper/2018/file/4de754248c196c85ee4fbdcee89179bd-Paper.pdf,2018,"exploration, prior, rl, strategies, tasks, agnostic, learning, methods, task, based","meta, exploration, tasks, latent, policy, task, learning, maesn, prior, training","{'strategies': 0.5362965469157827, 'meta': 0.459157942598495, 'exploration': 0.44160852504398473, 'structured': 0.39987656958632073, 'reinforcement': 0.33457273721947733, 'learning': 0.18628397872265937}","exploration, rl, prior, strategies, agnostic, maesn, inform, locomotion, tasks, experience"
Gamma-Poisson Dynamic Matrix Factorization Embedded with Metadata Influence,"Trong Dinh Thac Do, Longbing Cao","A conjugate Gamma-Poisson model for Dynamic Matrix Factorization incorporated with metadata influence (mGDMF for short) is proposed to effectively and efficiently model massive, sparse and dynamic data in recommendations. Modeling recommendation problems with a massive number of ratings and very sparse or even no ratings on some users/items in a dynamic setting is very demanding and poses critical challenges to well-studied matrix factorization models due to the large-scale, sparse and dynamic nature of the data. Our proposed mGDMF tackles these challenges by introducing three strategies: (1) constructing a stable Gamma-Markov chain model that smoothly drifts over time by combining both static and dynamic latent features of data; (2) incorporating the user/item metadata into the model to tackle sparse ratings; and (3) undertaking stochastic variational inference to  efficiently handle massive data. mGDMF is conjugate, dynamic and scalable. Experiments show that mGDMF significantly (both effectively and efficiently) outperforms the state-of-the-art static and dynamic models on large, sparse and dynamic data.",https://proceedings.neurips.cc/paper_files/paper/2018/file/4e2a6330465c8ffcaa696a5a16639176-Paper.pdf,2018,"dynamic, data, sparse, mgdmf, model, efficiently, massive, ratings, challenges, conjugate","user, dynamic, item, mgdmf, metadata, gamma, data, users, items, time","{'metadata': 0.3983556876539486, 'embedded': 0.3759987043806731, 'influence': 0.3759987043806731, 'gamma': 0.36013617230772854, 'poisson': 0.36013617230772854, 'factorization': 0.3292794584743017, 'dynamic': 0.3219166569615086, 'matrix': 0.2951461904049131}","dynamic, mgdmf, ratings, sparse, massive, metadata, conjugate, static, efficiently, gamma"
A Block Coordinate Ascent Algorithm for Mean-Variance Optimization,"Tengyang Xie, Bo Liu, Yangyang Xu, Mohammad Ghavamzadeh, Yinlam Chow, Daoming Lyu, Daesub Yoon","Risk management in dynamic decision problems is a primary concern in many fields, including financial investment, autonomous driving, and healthcare. The mean-variance function is one of the most widely used objective functions in risk management due to its simplicity and interpretability. Existing algorithms for mean-variance optimization are based on multi-time-scale stochastic approximation, whose learning rate schedules are often hard to tune, and have only asymptotic convergence proof. In this paper, we develop a model-free policy search framework for mean-variance optimization with finite-sample error bound analysis (to local optima). Our starting point is a reformulation of the original mean-variance function with its Fenchel dual, from which we propose a stochastic block coordinate ascent policy search algorithm. Both the asymptotic convergence guarantee of the last iteration's solution and the convergence rate of the randomly picked solution are provided, and their applicability is demonstrated on several benchmark domains.",https://proceedings.neurips.cc/paper_files/paper/2018/file/4e4b5fbbbb602b6d35bea8460aa8f8e5-Paper.pdf,2018,"mean, variance, convergence, asymptotic, function, management, optimization, policy, rate, risk","algorithm, al, et, algorithms, variance, mean, stochastic, gradient, analysis, block","{'ascent': 0.43762763724121956, 'coordinate': 0.43762763724121956, 'mean': 0.3931436642843954, 'block': 0.38325076582786216, 'variance': 0.3671222082215592, 'algorithm': 0.3486596913275712, 'optimization': 0.24326675861000785}","mean, variance, management, convergence, asymptotic, risk, solution, search, fenchel, investment"
MetaGAN: An Adversarial Approach to Few-Shot Learning,"Ruixiang ZHANG, Tong Che, Zoubin Ghahramani, Yoshua Bengio, Yangqiu Song","In this paper, we propose a conceptually simple and general framework called MetaGAN for few-shot learning problems. Most state-of-the-art few-shot classification models can be integrated with MetaGAN in a principled and straightforward way. By introducing an adversarial generator conditioned on tasks, we augment vanilla few-shot classification models with the ability to discriminate between real and fake data.  We argue that this GAN-based approach can help few-shot classifiers to learn sharper decision boundary, which could generalize better. We show that with our MetaGAN framework, we can extend supervised few-shot learning models to naturally cope with unsupervised data. Different from previous work in semi-supervised few-shot learning, our algorithms can deal with semi-supervision at both sample-level and task-level. We give theoretical justifications of the strength of MetaGAN, and validate the effectiveness of MetaGAN on challenging few-shot image classification benchmarks.",https://proceedings.neurips.cc/paper_files/paper/2018/file/4e4e53aa080247bc31d0eb4e7aeb07a0-Paper.pdf,2018,"shot, metagan, classification, learning, models, data, framework, level, semi, supervised","shot, learning, metagan, supervised, et, data, al, task, set, model","{'metagan': 0.6329816730626012, 'shot': 0.47599617454937365, 'approach': 0.44047123762605817, 'adversarial': 0.3683463162435214, 'learning': 0.20752812704601972}","metagan, shot, semi, classification, supervised, level, sharper, conceptually, fake, models"
Efficient High Dimensional Bayesian Optimization with Additivity and Quadrature Fourier Features,"Mojmir Mutny, Andreas Krause","We develop an efficient and provably no-regret Bayesian optimization (BO) algorithm for optimization of black-box functions in high dimensions. We assume a generalized additive model with possibly overlapping variable groups. When the groups do not overlap, we are able to provide the first provably no-regret \emph{polynomial time} (in the number of evaluations of the acquisition function) algorithm for solving high dimensional BO. To make the optimization efficient and feasible, we introduce a novel deterministic Fourier Features approximation based on numerical integration with detailed analysis for the squared exponential kernel. The error of this approximation decreases \emph{exponentially} with the number of features, and allows for a precise approximation of both posterior mean and variance. In addition, the kernel matrix inversion improves in its complexity from cubic to essentially linear in the number of data points measured in basic arithmetic operations.",https://proceedings.neurips.cc/paper_files/paper/2018/file/4e5046fc8d6a97d18a5f54beaed54dea-Paper.pdf,2018,"approximation, number, optimization, algorithm, bo, efficient, emph, features, groups, high","additive, function, approximation, kernel, bo, fourier, qff, sampling, features, acquisition","{'additivity': 0.4315014320884893, 'quadrature': 0.4072842046242309, 'fourier': 0.3767741062756139, 'dimensional': 0.3487021844977366, 'high': 0.3353744824802376, 'features': 0.3152779944552064, 'bayesian': 0.24688264073569893, 'efficient': 0.2455404037563443, 'optimization': 0.22639956863003105}","bo, groups, approximation, emph, provably, kernel, optimization, regret, number, features"
The Physical Systems Behind Optimization Algorithms,"Lin Yang, Raman Arora, Vladimir braverman, Tuo Zhao","We use differential equations based approaches to provide some {\it \textbf{physics}} insights into analyzing the dynamics of popular optimization algorithms in machine learning. In particular, we study gradient descent, proximal gradient descent, coordinate gradient descent, proximal coordinate gradient, and Newton's methods as well as their Nesterov's accelerated variants in a unified framework motivated by a natural connection of optimization algorithms to physical systems. Our analysis is applicable to more general algorithms and optimization problems {\it \textbf{beyond}} convexity and strong convexity, e.g. Polyak-\L ojasiewicz and error bound conditions (possibly nonconvex).",https://proceedings.neurips.cc/paper_files/paper/2018/file/4e62e752ae53fb6a6eebd0f6146aa702-Paper.pdf,2018,"gradient, algorithms, descent, optimization, convexity, coordinate, proximal, textbf, accelerated, analysis","convergence, system, algorithms, particle, convex, nag, vgd, condition, energy, analysis","{'physical': 0.5596041309758387, 'behind': 0.5281973741913065, 'systems': 0.4349393347583086, 'algorithms': 0.36396474388866523, 'optimization': 0.293612313737424}","textbf, gradient, descent, convexity, proximal, coordinate, optimization, algorithms, ojasiewicz, polyak"
Unsupervised Learning of Shape and Pose with Differentiable Point Clouds,"Eldar Insafutdinov, Alexey Dosovitskiy","We address the problem of learning accurate 3D shape and camera pose from a collection of unlabeled category-specific images. We train a convolutional network to predict both the shape and the pose from a single image by minimizing the reprojection error: given several views of an object, the projections of the predicted shapes to the predicted camera poses should match the provided views. To deal with pose ambiguity, we introduce an ensemble of pose predictors which we then distill to a single ""student"" model. To allow for efficient learning of high-fidelity shapes, we represent the shapes by point clouds and devise a formulation allowing for differentiable projection of these. Our experiments show that the distilled ensemble of pose predictors learns to estimate the pose accurately, while the point cloud representation allows to predict detailed shape models.",https://proceedings.neurips.cc/paper_files/paper/2018/file/4e8412ad48562e3c9934f45c3e144d48-Paper.pdf,2018,"pose, shape, shapes, camera, ensemble, learning, point, predict, predicted, predictors","pose, point, shape, cloud, camera, based, learning, predicted, projection, points","{'clouds': 0.4784806962542346, 'shape': 0.45162684364391636, 'pose': 0.3955105184537197, 'differentiable': 0.386666767746981, 'point': 0.3545117066444574, 'unsupervised': 0.33295906252634444, 'learning': 0.15687374050006073}","pose, shapes, shape, camera, predictors, views, ensemble, predicted, predict, clouds"
Unsupervised Attention-guided Image-to-Image Translation,"Youssef Alami Mejjati, Christian Richardt, James Tompkin, Darren Cosker, Kwang In Kim","Current unsupervised image-to-image translation techniques struggle to focus their attention on individual objects without altering the background or the way multiple objects interact within a scene. Motivated by the important role of attention in human perception, we tackle this limitation by introducing unsupervised attention mechanisms which are jointly adversarially trained with the generators and discriminators. We empirically demonstrate that our approach is able to attend to relevant regions in the image without requiring any additional supervision, and that by doing so it achieves more realistic mappings compared to recent approaches.",https://proceedings.neurips.cc/paper_files/paper/2018/file/4e87337f366f72daa424dae11df0538c-Paper.pdf,2018,"attention, image, objects, unsupervised, without, able, achieves, additional, adversarially, altering","image, attention, translation, source, background, target, discriminator, images, map, domain","{'image': 0.6593074548972698, 'guided': 0.3907001242943541, 'translation': 0.3907001242943541, 'attention': 0.3769946220448263, 'unsupervised': 0.3433592296981626}","attention, image, objects, unsupervised, altering, attend, mappings, generators, without, background"
Learning Concave Conditional Likelihood Models for Improved Analysis of Tandem Mass Spectra,"John T. Halloran, David M. Rocke","The most widely used technology to identify the proteins present in a complex biological sample is tandem mass spectrometry, which quickly produces a large collection of spectra representative of the peptides (i.e., protein subsequences) present in the original sample. In this work, we greatly expand the parameter learning capabilities of a dynamic Bayesian network (DBN) peptide-scoring algorithm, Didea, by deriving emission distributions for which its conditional log-likelihood scoring function remains concave. We show that this class of emission distributions, called Convex Virtual Emissions (CVEs), naturally generalizes the log-sum-exp function while rendering both maximum likelihood estimation and conditional maximum likelihood estimation concave for a wide range of Bayesian networks. Utilizing CVEs in Didea allows efficient learning of a large number of parameters while ensuring global convergence, in stark contrast to Didea’s previous parameter learning framework (which could only learn a single parameter using a costly grid search) and other trainable models (which only ensure convergence to local optima). The newly trained scoring function substantially outperforms the state-of-the-art in both scoring function accuracy and downstream Fisher kernel analysis. Furthermore, we significantly improve Didea’s runtime performance through successive optimizations to its message passing schedule and derive explicit connections between Didea’s new concave score and related MS/MS scoring functions.",https://proceedings.neurips.cc/paper_files/paper/2018/file/4ebccfb3e317c7789f04f7a558df4537-Paper.pdf,2018,"didea, scoring, function, concave, learning, likelihood, parameter, bayesian, conditional, convergence","didea, log, scoring, function, drip, spectra, using, ms, search, learning","{'mass': 0.38826077164961387, 'spectra': 0.38826077164961387, 'tandem': 0.38826077164961387, 'concave': 0.3510097947455552, 'likelihood': 0.3390176587181385, 'conditional': 0.3137588178414966, 'improved': 0.28368405828655074, 'analysis': 0.27017796863328825, 'models': 0.20371211931321984, 'learning': 0.1272944133690865}","didea, scoring, concave, cves, emission, likelihood, ms, parameter, function, conditional"
Beyond Grids: Learning Graph Representations for Visual Recognition,"Yin Li, Abhinav Gupta","We propose learning graph representations from 2D feature maps for visual recognition. Our method draws inspiration from region based recognition, and learns to transform a 2D image into a graph structure. The vertices of the graph define clusters of pixels (""regions""), and the edges measure the similarity between these clusters in a feature space. Our method further learns to propagate information across all vertices on the graph, and is able to project the learned graph representation back into 2D grids. Our graph representation facilitates reasoning beyond regular grids and can capture long range dependencies among regions. We demonstrate that our model can be trained from end-to-end, and is easily integrated into existing networks. Finally, we evaluate our method on three challenging recognition tasks: semantic segmentation, object detection and object instance segmentation. For all tasks, our method outperforms state-of-the-art methods.",https://proceedings.neurips.cc/paper_files/paper/2018/file/4efb80f630ccecb2d3b9b2087b0f9c89-Paper.pdf,2018,"graph, method, 2d, recognition, clusters, end, feature, grids, learns, object","graph, method, feature, features, gcu, vertices, segmentation, vertex, recognition, thus","{'grids': 0.5120263073010783, 'beyond': 0.4341643315375658, 'recognition': 0.39120565381859956, 'visual': 0.364649995757899, 'representations': 0.3524704482707698, 'graph': 0.3330202789534843, 'learning': 0.16787193859556426}","graph, 2d, grids, recognition, vertices, clusters, regions, segmentation, method, object"
Approximate Knowledge Compilation by Online Collapsed Importance Sampling,"Tal Friedman, Guy Van den Broeck","We introduce collapsed compilation, a novel approximate inference algorithm for discrete probabilistic graphical models. It is a collapsed sampling algorithm that incrementally selects which variable to sample next based on the partial compila- tion obtained so far. This online collapsing, together with knowledge compilation inference on the remaining variables, naturally exploits local structure and context- specific independence in the distribution. These properties are used implicitly in exact inference, but are difficult to harness for approximate inference. More- over, by having a partially compiled circuit available during sampling, collapsed compilation has access to a highly effective proposal distribution for importance sampling. Our experimental evaluation shows that collapsed compilation performs well on standard benchmarks. In particular, when the amount of exact inference is equally limited, collapsed compilation is competitive with the state of the art, and outperforms it on several benchmarks.",https://proceedings.neurips.cc/paper_files/paper/2018/file/4f164cf233807fc02da06599a1264dee-Paper.pdf,2018,"collapsed, compilation, inference, sampling, algorithm, approximate, benchmarks, distribution, exact, access","collapsed, compilation, sampling, xp, inference, importance, variables, variable, distribution, proposal","{'collapsed': 0.46026758153322855, 'compilation': 0.46026758153322855, 'importance': 0.3804556196827676, 'approximate': 0.36444469178431876, 'knowledge': 0.33629607975156356, 'online': 0.3046687066346574, 'sampling': 0.3046687066346574}","compilation, collapsed, inference, sampling, benchmarks, exact, compila, compiled, incrementally, approximate"
Layer-Wise Coordination between Encoder and Decoder for Neural Machine Translation,"Tianyu He, Xu Tan, Yingce Xia, Di He, Tao Qin, Zhibo Chen, Tie-Yan Liu","Neural Machine Translation (NMT) has achieved remarkable progress with the quick evolvement of model structures. In this paper, we propose the concept of layer-wise coordination for NMT, which explicitly coordinates the learning of hidden representations of the encoder and decoder together layer by layer, gradually from low level to high level. Specifically, we design a layer-wise attention and mixed attention mechanism, and further share the parameters of each layer between the encoder and decoder to regularize and coordinate the learning. Experiments show that combined with the state-of-the-art Transformer model, layer-wise coordination achieves  improvements on three IWSLT and two WMT translation tasks. More specifically, our method achieves 34.43 and 29.01 BLEU score on WMT16 English-Romanian and WMT14 English-German tasks, outperforming the Transformer baseline.",https://proceedings.neurips.cc/paper_files/paper/2018/file/4fb8a7a22a82c80f2c26fe6c1e0dcbb3-Paper.pdf,2018,"layer, wise, achieves, attention, coordination, decoder, encoder, english, learning, level","model, layer, en, target, attention, source, decoder, encoder, transformer, de","{'coordination': 0.4024445899635337, 'decoder': 0.4024445899635337, 'encoder': 0.37229703236064365, 'layer': 0.37229703236064365, 'wise': 0.37229703236064365, 'translation': 0.3376074406214773, 'machine': 0.3313893457039843, 'neural': 0.19095491796610206}","layer, coordination, wise, nmt, transformer, english, decoder, encoder, translation, attention"
A Lyapunov-based Approach to Safe Reinforcement Learning,"Yinlam Chow, Ofir Nachum, Edgar Duenez-Guzman, Mohammad Ghavamzadeh","In many real-world reinforcement learning (RL) problems, besides optimizing the main objective function, an agent must concurrently avoid violating a number of constraints. In particular, besides optimizing performance, it is crucial to guarantee the safety of an agent during training as well as deployment (e.g., a robot should avoid taking actions - exploratory or not - which irrevocably harm its hard- ware). To incorporate safety in RL, we derive algorithms under the framework of constrained Markov decision processes (CMDPs), an extension of the standard Markov decision processes (MDPs) augmented with constraints on expected cumulative costs. Our approach hinges on a novel Lyapunov method. We define and present a method for constructing Lyapunov functions, which provide an effective way to guarantee the global safety of a behavior policy during training via a set of local linear constraints. Leveraging these theoretical underpinnings, we show how to use the Lyapunov approach to systematically transform dynamic programming (DP) and RL algorithms into their safe counterparts. To illustrate their effectiveness, we evaluate these algorithms in several CMDP planning and decision-making tasks on a safety benchmark domain. Our results show that our proposed method significantly outperforms existing baselines in balancing constraint satisfaction and performance.",https://proceedings.neurips.cc/paper_files/paper/2018/file/4fe5149039b52765bde64beb9f674940-Paper.pdf,2018,"safety, algorithms, constraints, decision, lyapunov, method, rl, agent, approach, avoid","policy, function, lyapunov, x0, safe, constraint, algorithms, cost, d0, rl","{'lyapunov': 0.563683889126915, 'safe': 0.4921918625387132, 'approach': 0.39224917693477473, 'based': 0.37659218707729863, 'reinforcement': 0.33192235038895235, 'learning': 0.18480829182704583}","safety, lyapunov, rl, constraints, decision, besides, avoid, optimizing, markov, guarantee"
Reversible Recurrent Neural Networks,"Matthew MacKay, Paul Vicol, Jimmy Ba, Roger B. Grosse","Recurrent neural networks (RNNs) provide state-of-the-art performance in processing sequential data but are memory intensive to train, limiting the flexibility of RNN models which can be trained. Reversible RNNs---RNNs for which the hidden-to-hidden transition can be reversed---offer a path to reduce the memory requirements of training, as hidden states need not be stored and instead can be recomputed during backpropagation. We first show that perfectly reversible RNNs, which require no storage of the hidden activations, are fundamentally limited because they cannot forget information from their hidden state. We then provide a scheme for storing a small number of bits in order to allow perfect reversal with forgetting. Our method achieves comparable performance to traditional models while reducing the activation memory cost by a factor of 10--15. We extend our technique to attention-based sequence-to-sequence models, where it maintains performance while reducing activation memory cost by a factor of 5--10 in the encoder, and a factor of 10--15 in the decoder.",https://proceedings.neurips.cc/paper_files/paper/2018/file/4ff6fa96179cdc2838e8d8ce64cd10a7-Paper.pdf,2018,"hidden, memory, rnns, 10, factor, models, performance, 15, activation, cost","hidden, reversible, al, memory, et, state, models, bits, attention, forgetting","{'reversible': 0.739138467455732, 'recurrent': 0.4849097680999038, 'neural': 0.33102888072509845, 'networks': 0.3300859325302695}","rnns, hidden, memory, reversible, factor, 10, 15, activation, reducing, sequence"
Deep Poisson gamma dynamical systems,"Dandan Guo, Bo Chen, Hao Zhang, Mingyuan Zhou","We develop deep Poisson-gamma dynamical systems (DPGDS) to model sequentially observed multivariate count data, improving previously proposed models by not only mining deep hierarchical latent structure from the data, but also capturing both first-order and long-range temporal dependencies. Using sophisticated but simple-to-implement data augmentation techniques, we derived closed-form Gibbs sampling update equations by first backward and upward propagating auxiliary latent counts, and then forward and downward sampling latent variables. Moreover, we develop stochastic gradient MCMC inference that is scalable to very long multivariate count time series. Experiments on both synthetic and a variety of real-world data demonstrate that the proposed model not only has excellent predictive performance, but also provides highly interpretable multilayer latent structure to represent hierarchical and temporal information propagation.",https://proceedings.neurips.cc/paper_files/paper/2018/file/4ffb0d2ba92f664c2281970110a2e071-Paper.pdf,2018,"data, latent, also, count, deep, develop, first, hierarchical, long, model","layer, usa, 2003, dpgds, 001, gamma, temporal, time, latent, poisson","{'gamma': 0.5125355878731879, 'poisson': 0.5125355878731879, 'dynamical': 0.45814265744168176, 'systems': 0.44063204497252617, 'deep': 0.26563743844303056}","latent, count, multivariate, hierarchical, temporal, data, long, downward, dpgds, upward"
Regularization Learning Networks: Deep Learning for Tabular Datasets,"Ira Shavitt, Eran Segal","Despite their impressive performance, Deep Neural Networks (DNNs) typically underperform Gradient Boosting Trees (GBTs) on many tabular-dataset learning tasks. We propose that applying a different regularization coefficient to each weight might boost the performance of DNNs by allowing them to make more use of the more relevant inputs. However, this will lead to an intractable number of hyperparameters. Here, we introduce Regularization Learning Networks (RLNs), which overcome this challenge by introducing an efficient hyperparameter tuning scheme which minimizes a new Counterfactual Loss. Our results show that RLNs significantly improve DNNs on tabular datasets, and achieve comparable results to GBTs, with the best performance achieved with an ensemble that combines GBTs and RLNs. RLNs produce extremely sparse networks, eliminating up to 99.8% of the network edges and 82% of the input features, thus providing more interpretable models and reveal the importance that the network assigns to different inputs. RLNs could efficiently learn a single network in datasets that comprise both tabular and unstructured data, such as in the setting of medical imaging accompanied by electronic health records. An open source implementation of RLN can be found at https://github.com/irashavitt/regularizationlearningnetworks.",https://proceedings.neurips.cc/paper_files/paper/2018/file/500e75a036dc2d7d2fec5da1b71d36cc-Paper.pdf,2018,"rlns, dnns, gbts, network, networks, performance, tabular, datasets, different, inputs","regularization, λt, wt, rlns, network, models, loss, results, different, features","{'tabular': 0.5549535319810882, 'datasets': 0.523807781546305, 'regularization': 0.3952214958434592, 'learning': 0.3638919481898447, 'deep': 0.2600264935184301, 'networks': 0.24783225630983044}","rlns, gbts, tabular, dnns, inputs, regularization, network, irashavitt, regularizationlearningnetworks, rln"
Online Learning with an Unknown Fairness Metric,"Stephen Gillen, Christopher Jung, Michael Kearns, Aaron Roth","We consider the problem of online learning in the linear contextual bandits setting, but in which there are also strong individual fairness constraints governed by an unknown similarity metric. These constraints demand that we select similar actions or individuals with approximately equal probability DHPRZ12, which may be at odds with optimizing reward, thus modeling settings where profit and social policy are in tension. We assume we learn about an unknown Mahalanobis similarity metric from only weak feedback that identifies fairness violations, but does not quantify their extent. This is intended to represent the interventions of a regulator who ""knows unfairness when he sees it"" but nevertheless cannot enunciate a quantitative fairness metric over individuals. Our main result is an algorithm in the adversarial context setting that has a number of fairness violations that depends only logarithmically on T, while obtaining an optimal O(sqrt(T)) regret bound to the best fair policy.",https://proceedings.neurips.cc/paper_files/paper/2018/file/50905d7b2216bfeccb5b41016357176b-Paper.pdf,2018,"fairness, metric, constraints, individuals, policy, setting, similarity, unknown, violations, actions","fairness, algorithm, xt, distance, metric, learning, regret, round, linear, function","{'unknown': 0.5257696577228821, 'fairness': 0.5125394156818553, 'metric': 0.5010788587353825, 'online': 0.4104413571424683, 'learning': 0.20329162320656846}","fairness, metric, violations, individuals, similarity, constraints, unknown, dhprz12, enunciate, policy"
Completing State Representations using Spectral Learning,"Nan Jiang, Alex Kulesza, Satinder Singh","A central problem in dynamical system modeling is state discovery—that is, finding a compact summary of the past that captures the information needed to predict the future. Predictive State Representations (PSRs) enable clever spectral methods for state discovery; however, while consistent in the limit of infinite data, these methods often suffer from poor performance in the low data regime. In this paper we develop a novel algorithm for incorporating domain knowledge, in the form of an imperfect state representation, as side information to speed spectral learning for PSRs. We prove theoretical results characterizing the relevance of a user-provided state representation, and design spectral algorithms that can take advantage of a relevant representation. Our algorithm utilizes principal angles to extract the relevant components of the representation, and is robust to misspecification. Empirical evaluation on synthetic HMMs, an aircraft identification domain, and a gene splice dataset shows that, even with weak domain knowledge, the algorithm can significantly outperform standard PSR learning.",https://proceedings.neurips.cc/paper_files/paper/2018/file/51174add1c52758f33d414ceaf3fe6ba-Paper.pdf,2018,"state, representation, algorithm, domain, spectral, data, discovery, information, knowledge, learning","psr, rank, pt, state, algorithm, model, pf, learning, bo, relevant","{'completing': 0.5562608656645643, 'state': 0.47167230171193714, 'spectral': 0.43234085931841054, 'representations': 0.3829207872340557, 'using': 0.318263489389835, 'learning': 0.1823745939464945}","spectral, psrs, representation, state, discovery, domain, relevant, hmms, psr, splice"
From Stochastic Planning to Marginal MAP,"Hao(Jackson) Cui, Radu Marinescu, Roni Khardon",It is well known that the problems of stochastic planning and probabilistic inference are closely related. This paper makes two contributions in this context. The first is to provide an analysis of the recently developed SOGBOFA heuristic planning algorithm that was shown to be effective for problems with large factored state and action spaces. It is shown that SOGBOFA can be seen as a specialized inference algorithm that computes its solutions through a combination of a symbolic variant of belief propagation and gradient ascent. The second contribution is a new solver for Marginal MAP (MMAP) inference. We introduce a new reduction from MMAP to maximum expected utility problems which are suitable for the symbolic computation in SOGBOFA. This yields a novel algebraic gradient-based solver (AGS) for MMAP. An experimental evaluation illustrates the potential of AGS in solving difficult MMAP problems.,https://proceedings.neurips.cc/paper_files/paper/2018/file/5129a5ddcd0dcd755232baa04c231698-Paper.pdf,2018,"mmap, problems, inference, sogbofa, ags, algorithm, gradient, new, planning, shown","wk, nodes, variables, map, ags, cij, problems, time, mpbp, aaobf","{'map': 0.5477204676415007, 'marginal': 0.5477204676415007, 'planning': 0.529007775143223, 'stochastic': 0.34663432099428426}","mmap, sogbofa, ags, problems, symbolic, solver, planning, inference, shown, illustrates"
Generalizing Graph Matching beyond Quadratic Assignment Model,"Tianshu Yu, Junchi Yan, Yilin Wang, Wei Liu, baoxin Li","Graph matching has received persistent attention over decades, which can be formulated as a quadratic assignment problem (QAP). We show that a large family of functions, which we define as Separable Functions, can approximate discrete graph matching in the continuous domain asymptotically by varying the approximation controlling parameters. We also study the properties of global optimality and devise convex/concave-preserving extensions to the widely used Lawler's QAP form. Our theoretical findings show the potential for deriving new algorithms and techniques for graph matching. We deliver solvers based on two specific instances of Separable Functions, and the state-of-the-art performance of our method is verified on popular benchmarks.",https://proceedings.neurips.cc/paper_files/paper/2018/file/51d92be1c60d1db1d2e5e7a07da55b26-Paper.pdf,2018,"functions, graph, matching, qap, separable, show, algorithms, also, approximate, approximation","matching, problem, graph, hθ, following, function, accuracy, 20, matrix, 10","{'assignment': 0.41655184288186037, 'generalizing': 0.41655184288186037, 'beyond': 0.4045126612677227, 'quadratic': 0.3943336782499301, 'matching': 0.38551624209336993, 'graph': 0.310276338957931, 'model': 0.29823715734379325}","qap, matching, separable, graph, functions, lawler, persistent, deliver, extensions, decades"
On Learning Intrinsic Rewards for Policy Gradient Methods,"Zeyu Zheng, Junhyuk Oh, Satinder Singh","In many sequential decision making tasks, it is challenging to design reward functions that help an RL agent efficiently learn behavior that is considered good by the agent designer. A number of different formulations of the reward-design problem, or close variants thereof, have been proposed in the literature. In this paper we build on the Optimal Rewards Framework of Singh et al. that defines the optimal intrinsic reward function as one that when used by an RL agent achieves behavior that optimizes the task-specifying or extrinsic reward function. Previous work in this framework has shown how good intrinsic reward functions can be learned for lookahead search based planning agents. Whether it is possible to learn intrinsic reward functions for learning agents remains an open problem. In this paper we derive a novel algorithm for learning intrinsic rewards for policy-gradient based learning agents. We compare the performance of an augmented agent that uses our algorithm to provide additive intrinsic rewards to an A2C-based policy learner (for Atari games) and a PPO-based policy learner (for Mujoco domains) with a baseline agent that uses the same policy learners but with only extrinsic rewards. Our results show improved performance on most but not all of the domains.",https://proceedings.neurips.cc/paper_files/paper/2018/file/51de85ddd068f0bc787691d356176df9-Paper.pdf,2018,"reward, agent, intrinsic, based, policy, rewards, agents, functions, learning, algorithm","reward, intrinsic, policy, agent, rewards, learning, extrinsic, al, et, baseline","{'intrinsic': 0.5225609940374987, 'rewards': 0.5225609940374987, 'methods': 0.4302981086668396, 'policy': 0.3664705880970859, 'gradient': 0.3185214720553578, 'learning': 0.1815128992614224}","intrinsic, reward, agent, rewards, policy, agents, extrinsic, learner, functions, rl"
Regularizing by the Variance of the Activations' Sample-Variances,"Etai Littwin, Lior Wolf","Normalization techniques play an important role in supporting efficient and often more effective training of deep neural networks. While conventional methods explicitly normalize the activations, we suggest to add a loss term instead. This new loss term encourages the variance of the activations to be stable and not vary from one random mini-batch to the next. As we prove, this encourages the activations to be distributed around a few distinct modes. We also show that if the inputs are from a mixture of two Gaussians, the new loss would either join the two together, or separate between them optimally in the LDA sense, depending on the prior probabilities. Finally, we are able to link the new regularization term to the batchnorm method, which provides it with a regularization perspective. Our experiments demonstrate an improvement in accuracy over the batchnorm technique for both CNNs and fully connected networks.",https://proceedings.neurips.cc/paper_files/paper/2018/file/522a9ae9a99880d39e5daec35375e999-Paper.pdf,2018,"activations, loss, new, term, batchnorm, encourages, networks, regularization, two, able","σ2, vcl, variance, batchnorm, elu, relu, distribution, sample, two, experiments","{'activations': 0.4937675182923673, 'variances': 0.4937675182923673, 'regularizing': 0.46605572079711255, 'variance': 0.3909702925800747, 'sample': 0.3772553130055669}","activations, term, batchnorm, encourages, loss, regularization, join, new, add, normalize"
Single-Agent Policy Tree Search With Guarantees,"Laurent Orseau, Levi Lelis, Tor Lattimore, Theophane Weber","We introduce two novel tree search algorithms that use a policy to guide
search. The first algorithm is a best-first enumeration that uses a cost
function that allows us to provide an upper bound on the number of nodes
to be expanded before reaching a goal state. We show that this best-first
algorithm is particularly well suited for ``needle-in-a-haystack'' problems.
The second algorithm, which is based on sampling, provides an
upper bound on the expected number of nodes to be expanded before
reaching a set of goal states. We show that this algorithm is better
suited for problems where many paths lead to a goal. We validate these tree
search algorithms on 1,000 computer-generated levels of Sokoban, where the
policy used to guide search comes from a neural network trained using A3C. Our
results show that the policy tree search algorithms we introduce are
competitive with a state-of-the-art domain-independent planner that uses
heuristic search.",https://proceedings.neurips.cc/paper_files/paper/2018/file/52c5189391854c93e8a0e1326e56c14f-Paper.pdf,2018,"search, algorithm, algorithms, first, goal, policy, show, tree, best, bound","nodes, levints, policy, search, node, tree, cost, n1, lubyts, probability","{'single': 0.47868497258292253, 'agent': 0.41920657362773783, 'guarantees': 0.41920657362773783, 'tree': 0.40156486753201504, 'search': 0.38137023483257454, 'policy': 0.3357004549082604}","search, expanded, tree, reaching, goal, suited, policy, guide, nodes, upper"
Bias and Generalization in Deep Generative Models: An Empirical Study,"Shengjia Zhao, Hongyu Ren, Arianna Yuan, Jiaming Song, Noah Goodman, Stefano Ermon","In high dimensional settings, density estimation algorithms rely crucially on their inductive bias. Despite recent empirical success, the inductive bias of deep generative models is not well understood. In this paper we propose a framework to systematically investigate bias and generalization in deep generative models of images by probing the learning algorithm with carefully designed training datasets. By measuring properties of the learned distribution, we are able to find interesting patterns of generalization. We verify that these patterns are consistent across datasets, common models and architectures.",https://proceedings.neurips.cc/paper_files/paper/2018/file/5317b6799188715d5e00a638a4278901-Paper.pdf,2018,"bias, models, datasets, deep, generalization, generative, inductive, patterns, able, across","distribution, training, red, combinations, features, feature, figure, color, number, generalization","{'study': 0.46095730017324366, 'bias': 0.44763471200569543, 'empirical': 0.43637062420829326, 'generalization': 0.3911362340561649, 'generative': 0.32531402201805915, 'models': 0.2769843579424361, 'deep': 0.24735623965173809}","bias, inductive, patterns, generalization, probing, generative, models, systematically, crucially, measuring"
Joint Autoregressive and Hierarchical Priors for Learned Image Compression,"David Minnen, Johannes Ballé, George D. Toderici","Recent models for learned image compression are based on autoencoders that learn approximately invertible mappings from pixels to a quantized latent representation. The transforms are combined with an entropy model, which is a prior on the latent representation that can be used with standard arithmetic coding algorithms to generate a compressed bitstream. Recently, hierarchical entropy models were introduced as a way to exploit more structure in the latents than previous fully factorized priors, improving compression performance while maintaining end-to-end optimization. Inspired by the success of autoregressive priors in probabilistic generative models, we examine autoregressive, hierarchical, and combined priors as alternatives, weighing their costs and benefits in the context of image compression. While it is well known that autoregressive models can incur a significant computational penalty, we find that in terms of compression performance, autoregressive and hierarchical priors are complementary and can be combined to exploit the probabilistic structure in the latents better than all previous learned models. The combined model yields state-of-the-art rate-distortion performance and generates smaller files than existing methods: 15.8% rate reductions over the baseline hierarchical model and 59.8%, 35%, and 8.4% savings over JPEG, JPEG2000, and BPG, respectively. To the best of our knowledge, our model is the first learning-based method to outperform the top standard image codec (BPG) on both the PSNR and MS-SSIM distortion metrics.",https://proceedings.neurips.cc/paper_files/paper/2018/file/53edebc543333dfbf7c5933af792c9c4-Paper.pdf,2018,"models, autoregressive, combined, compression, hierarchical, model, priors, image, performance, based","model, context, entropy, latents, models, compression, image, hyperprior, rate, decoder","{'autoregressive': 0.4308045534324945, 'compression': 0.3870141332812994, 'joint': 0.3870141332812994, 'learned': 0.3870141332812994, 'hierarchical': 0.3688394668093775, 'priors': 0.3688394668093775, 'image': 0.30493034304731154}","autoregressive, priors, compression, hierarchical, combined, bpg, latents, models, distortion, image"
Link Prediction Based on Graph Neural Networks,"Muhan Zhang, Yixin Chen","Link prediction is a key problem for network-structured data. Link prediction heuristics use some score functions, such as common neighbors and Katz index, to measure the likelihood of links. They have obtained wide practical uses due to their simplicity, interpretability, and for some of them, scalability. However, every heuristic has a strong assumption on when two nodes are likely to link, which limits their effectiveness on networks where these assumptions fail. In this regard, a more reasonable way should be learning a suitable heuristic from a given network instead of using predefined ones. By extracting a local subgraph around each target link, we aim to learn a function mapping the subgraph patterns to link existence, thus automatically learning a ``heuristic'' that suits the current network. In this paper, we study this heuristic learning paradigm for link prediction. First, we develop a novel $\gamma$-decaying heuristic theory. The theory unifies a wide range of heuristics in a single framework, and proves that all these heuristics can be well approximated from local subgraphs. Our results show that local subgraphs reserve rich information related to link existence. Second, based on the $\gamma$-decaying theory, we propose a new method to learn heuristics from local subgraphs using a graph neural network (GNN). Its experimental results show unprecedented performance, working consistently well on a wide range of problems.",https://proceedings.neurips.cc/paper_files/paper/2018/file/53f0d7c537d99b3824f0f99d62ea2428-Paper.pdf,2018,"link, heuristic, heuristics, local, network, learning, prediction, subgraphs, theory, wide","heuristics, link, node, features, heuristic, seal, graph, enclosing, subgraphs, network","{'link': 0.5660491278507948, 'prediction': 0.4265661138478863, 'based': 0.41830599331663804, 'graph': 0.40722722860592886, 'neural': 0.2804132196989125, 'networks': 0.27961445211482105}","link, heuristic, subgraphs, heuristics, local, decaying, subgraph, wide, theory, gamma"
A flexible model for training action localization with varying levels of supervision,"Guilhem Chéron, Jean-Baptiste Alayrac, Ivan Laptev, Cordelia Schmid","Spatio-temporal action detection in videos is typically addressed in a fully-supervised setup with manual annotation of training videos required at every frame.  Since such annotation is extremely tedious and prohibits scalability, there is a clear need to minimize the amount of manual supervision. In this work we propose a unifying framework that can handle and combine varying types of less demanding weak supervision. Our model is based on discriminative clustering and integrates different types of supervision as constraints on the optimization. We investigate applications of such a model to training setups with alternative supervisory signals ranging from video-level class labels over temporal points or sparse action bounding boxes to the full per-frame annotation of action bounding boxes. Experiments on the challenging UCF101-24 and DALY datasets demonstrate competitive performance of our method at a fraction of supervision used by previous methods. The flexibility of our model enables joint learning from data with different levels of annotation. Experimental results demonstrate a significant gain by adding a few fully supervised examples to otherwise weakly labeled videos.",https://proceedings.neurips.cc/paper_files/paper/2018/file/53fde96fcc4b4ce72d7739202324cd49-Paper.pdf,2018,"annotation, supervision, action, model, videos, bounding, boxes, demonstrate, different, frame","action, supervision, temporal, bounding, video, spatial, annotation, one, time, videos","{'levels': 0.41339178191641235, 'localization': 0.390190925395334, 'varying': 0.390190925395334, 'supervision': 0.3737296557246038, 'flexible': 0.3609613030004928, 'action': 0.34170824292437746, 'training': 0.26662447485032015, 'model': 0.2584361940957975}","annotation, supervision, videos, manual, action, frame, boxes, bounding, types, temporal"
A probabilistic population code based on neural samples,"Sabyasachi Shivkumar, Richard Lange, Ankani Chattoraj, Ralf Haefner","Sensory processing is often characterized as implementing probabilistic inference: networks of neurons compute posterior beliefs over unobserved causes given the sensory inputs. How these beliefs are computed and represented by neural responses is much-debated (Fiser et al. 2010, Pouget et al. 2013). A central debate concerns the question of whether neural responses represent samples of latent variables (Hoyer & Hyvarinnen 2003) or parameters of their distributions (Ma et al. 2006) with efforts being made to distinguish between them (Grabska-Barwinska et al. 2013).
A separate debate addresses the question of whether neural responses are proportionally related to the encoded probabilities (Barlow 1969), or proportional to the logarithm of those probabilities (Jazayeri & Movshon 2006, Ma et al. 2006, Beck et al. 2012). 
Here, we show that these alternatives -- contrary to common assumptions -- are not mutually exclusive and that the very same system can be compatible with all of them.
As a central analytical result, we show that modeling neural responses in area V1 as samples from a posterior distribution over latents in a linear Gaussian model of the image implies that those neural responses form a linear Probabilistic Population Code (PPC, Ma et al. 2006). In particular, the posterior distribution over some experimenter-defined variable like ""orientation"" is part of the exponential family with sufficient statistics that are linear in the neural sampling-based firing rates.",https://proceedings.neurips.cc/paper_files/paper/2018/file/5401acfe633e6817b508b84d23686743-Paper.pdf,2018,"al, et, neural, responses, 2006, linear, posterior, 2013, beliefs, central","model, brain, neural, image, samples, posterior, responses, ppc, linear, variables","{'code': 0.4892505474246666, 'population': 0.4473312255248642, 'samples': 0.4473312255248642, 'probabilistic': 0.4134742288057266, 'based': 0.36155242743370075, 'neural': 0.24236822299100658}","al, et, 2006, responses, neural, 2013, debate, beliefs, posterior, sensory"
Generative Probabilistic Novelty Detection with Adversarial Autoencoders,"Stanislav Pidhorskyi, Ranya Almohsen, Gianfranco Doretto","Novelty detection is the problem of identifying whether a new data point is considered to be an inlier or an outlier. We assume that training data is available to describe only the inlier distribution. Recent approaches primarily leverage deep encoder-decoder network architectures to compute a reconstruction error that is used to either compute a novelty score or to train a one-class classifier. While we too leverage a novel network of that kind, we take a probabilistic approach and effectively compute how likely it is that a sample was generated by the inlier distribution. We achieve this with two main contributions. First, we make the computation of the novelty probability feasible because we linearize the parameterized manifold capturing the underlying structure of the inlier distribution, and show how the probability factorizes and can be computed with respect to local coordinates of the manifold tangent space. Second, we improve the training of the autoencoder network. An extensive set of results show that the approach achieves state-of-the-art performance on several benchmark datasets.",https://proceedings.neurips.cc/paper_files/paper/2018/file/5421e013565f7f1afa0cfe8ad87a99ab-Paper.pdf,2018,"inlier, compute, distribution, network, novelty, approach, data, leverage, manifold, probability","distribution, data, 100, 99, novelty, training, detection, 95, images, network","{'novelty': 0.5249934757626498, 'probabilistic': 0.4249633019973993, 'autoencoders': 0.4182644047855247, 'detection': 0.38288558483642327, 'generative': 0.342751585937612, 'adversarial': 0.32367098929816357}","inlier, novelty, compute, manifold, distribution, leverage, probability, network, linearize, outlier"
Monte-Carlo Tree Search for Constrained POMDPs,"Jongmin Lee, Geon-hyeong Kim, Pascal Poupart, Kee-Eung Kim","Monte-Carlo Tree Search (MCTS) has been successfully applied to very large POMDPs, a standard model for stochastic sequential decision-making problems. However, many real-world problems inherently have multiple goals, where multi-objective formulations are more natural. The constrained POMDP (CPOMDP) is such a model that maximizes the reward while constraining the cost, extending the standard POMDP model. To date, solution methods for CPOMDPs assume an explicit model of the environment, and thus are hardly applicable to large-scale real-world problems. In this paper, we present CC-POMCP (Cost-Constrained POMCP), an online MCTS algorithm for large CPOMDPs that leverages the optimization of LP-induced parameters and only requires a black-box simulator of the environment. In the experiments, we demonstrate that CC-POMCP converges to the optimal stochastic action selection in CPOMDP and pushes the state-of-the-art by being able to scale to very large problems.",https://proceedings.neurips.cc/paper_files/paper/2018/file/54c3d58c5efcf59ddeb7486b7061ea5a-Paper.pdf,2018,"large, model, problems, pomcp, cc, constrained, cost, cpomdp, cpomdps, environment","cost, pomcp, b0, optimal, policy, cc, function, reward, search, action","{'pomdps': 0.46243365100247147, 'tree': 0.4050185830618492, 'carlo': 0.39755890152108964, 'constrained': 0.39755890152108964, 'monte': 0.39755890152108964, 'search': 0.3846502635630579}","pomcp, cpomdp, cpomdps, pomdp, cc, mcts, problems, large, constrained, environment"
Learning Overparameterized Neural Networks via Stochastic Gradient Descent on Structured Data,"Yuanzhi Li, Yingyu Liang","Neural networks have many successful applications, while much less theoretical understanding has been gained. Towards bridging this gap, we study the problem of learning a two-layer overparameterized ReLU neural network for multi-class classification via stochastic gradient descent (SGD) from random initialization. In the overparameterized setting, when the data comes from mixtures of well-separated distributions, we prove that SGD learns a network with a small generalization error, albeit the network has enough capacity to fit arbitrary labels. Furthermore, the analysis provides interesting insights into several aspects of learning neural networks and can be verified based on empirical studies on synthetic data and on the MNIST dataset.",https://proceedings.neurips.cc/paper_files/paper/2018/file/54fe976ba170c19ebae453679b362263-Paper.pdf,2018,"network, neural, data, learning, networks, overparameterized, sgd, albeit, analysis, applications","data, initialization, network, number, learning, sgd, generalization, also, 10, networks","{'overparameterized': 0.5207767108213284, 'structured': 0.36651169539153505, 'descent': 0.34165413544969553, 'gradient': 0.2996184861594417, 'stochastic': 0.29796130450583214, 'data': 0.2947570335557844, 'via': 0.2732403971682215, 'neural': 0.23323387874032875, 'networks': 0.2325695032802498, 'learning': 0.1707408286926}","overparameterized, sgd, network, neural, bridging, separated, albeit, verified, aspects, gained"
Informative Features for Model Comparison,"Wittawat Jitkrittum, Heishiro Kanagawa, Patsorn Sangkloy, James Hays, Bernhard Schölkopf, Arthur Gretton","Given two candidate models, and a set of target observations, we address the problem of measuring the relative goodness of fit of the two models. We propose two new statistical tests which are nonparametric, computationally efficient (runtime complexity is linear in the sample size), and interpretable. As a unique advantage, our tests can produce a set of examples (informative features) indicating the regions in the data domain where one model fits significantly better than the other. In a real-world problem of comparing GAN models, the test power of our new test matches that of the state-of-the-art test of relative goodness of fit, while being one order of magnitude faster.",https://proceedings.neurips.cc/paper_files/paper/2018/file/550a141f12de6341fba65b0ad0433500-Paper.pdf,2018,"models, test, two, fit, goodness, new, one, problem, relative, set","rel, test, al, et, ume, model, two, power, ﬁt, locations","{'comparison': 0.5737363382391554, 'informative': 0.5737363382391554, 'features': 0.444128301594717, 'model': 0.38000431588396844}","goodness, tests, test, fit, relative, two, models, fits, set, indicating"
Discrimination-aware Channel Pruning for Deep Neural Networks,"Zhuangwei Zhuang, Mingkui Tan, Bohan Zhuang, Jing Liu, Yong Guo, Qingyao Wu, Junzhou Huang, Jinhui Zhu","Channel pruning is one of the predominant approaches for deep model compression. Existing pruning methods either train from scratch with sparsity constraints on channels, or  minimize the reconstruction error between the pre-trained feature maps and the compressed ones. Both strategies suffer from some limitations: the former kind is computationally expensive and difficult to converge, whilst the latter kind optimizes the reconstruction error but ignores the discriminative power of channels. To overcome these drawbacks, we investigate a simple-yet-effective method, called discrimination-aware channel pruning, to choose those channels that really contribute to discriminative power. To this end, we introduce additional losses into the network to increase the discriminative power of intermediate layers and then select the most discriminative channels for each layer by considering the additional loss and the reconstruction error. Last, we propose a greedy algorithm to conduct channel selection and parameter optimization in an iterative way. Extensive experiments demonstrate the effectiveness of our method. For example, on ILSVRC-12, our pruned ResNet-50 with 30% reduction of channels even outperforms the original model by 0.39% in top-1 accuracy.",https://proceedings.neurips.cc/paper_files/paper/2018/file/55a7cf9c71f1c9c495413f934dd1a158-Paper.pdf,2018,"channels, discriminative, channel, error, power, pruning, reconstruction, additional, kind, method","model, channel, channels, pruning, loss, lp, dcp, error, network, resnet","{'channel': 0.4827548170905887, 'discrimination': 0.4827548170905887, 'pruning': 0.46238848688718714, 'aware': 0.3975200734729541, 'deep': 0.23964715061435182, 'neural': 0.2290611149813866, 'networks': 0.22840862579553645}","channels, discriminative, channel, pruning, reconstruction, power, kind, error, additional, predominant"
Reinforcement Learning of Theorem Proving,"Cezary Kaliszyk, Josef Urban, Henryk Michalewski, Miroslav Olšák","We introduce a theorem proving algorithm that uses practically no domain heuristics for guiding its connection-style proof search. Instead, it runs many Monte-Carlo simulations guided by reinforcement learning from previous proof attempts. We produce several versions of the prover, parameterized by different learning and guiding algorithms. The strongest version of the system is trained on a large corpus of mathematical problems and evaluated on previously unseen problems. The trained system solves within the same number of inferences over 40% more problems than a baseline prover, which is an unusually high improvement in this hard AI domain. To our knowledge this is the first time reinforcement learning has been convincingly applied to solving general mathematical problems on a large scale.",https://proceedings.neurips.cc/paper_files/paper/2018/file/55acf8539596d25624059980986aaa78-Paper.pdf,2018,"problems, learning, domain, guiding, large, mathematical, proof, prover, reinforcement, system","learning, proof, value, problems, policy, search, rlcop, mlcop, tableau, prover","{'proving': 0.6530046847415628, 'theorem': 0.616355992965333, 'reinforcement': 0.3845184401316377, 'learning': 0.21409283229483114}","prover, guiding, mathematical, problems, proof, domain, system, strongest, unusually, reinforcement"
On Fast Leverage Score Sampling and Optimal Learning,"Alessandro Rudi, Daniele Calandriello, Luigi Carratino, Lorenzo Rosasco","Leverage score sampling provides an appealing way to perform approximate com- putations for large matrices. Indeed, it allows to derive faithful approximations with a complexity adapted to the problem at hand. Yet, performing leverage scores sampling is a challenge in its own right requiring further approximations. In this paper, we study the problem of leverage score sampling for positive definite ma- trices defined by a kernel. Our contribution is twofold. First we provide a novel algorithm for leverage score sampling and second, we exploit the proposed method in statistical learning by deriving a novel solver for kernel ridge regression. Our main technical contribution is showing that the proposed algorithms are currently the most efficient and accurate for these problems.",https://proceedings.neurips.cc/paper_files/paper/2018/file/56584778d5a8ab88d6393cc4cd11e090-Paper.pdf,2018,"leverage, sampling, score, approximations, contribution, kernel, novel, problem, proposed, accurate","bless, leverage, sampling, deff, falkon, scores, 10, jh, λh, uh","{'score': 0.5354568512382566, 'leverage': 0.5128671418124351, 'sampling': 0.37551441728889623, 'fast': 0.37217250128140245, 'optimal': 0.36896702870351145, 'learning': 0.18599230827908603}","leverage, score, sampling, contribution, approximations, kernel, putations, trices, twofold, faithful"
Robustness of conditional GANs to noisy labels,"Kiran K. Thekumparampil, Ashish Khetan, Zinan Lin, Sewoong Oh","We study the problem of learning conditional generators from noisy labeled samples, where the labels are corrupted by random noise. A standard training of conditional GANs will not only produce samples with wrong labels, but also generate poor quality samples. We consider two scenarios, depending on whether the noise model is known or not. When the distribution of the noise is known, we introduce a novel architecture which we call Robust Conditional GAN (RCGAN). The main idea is to corrupt the label of the generated sample before feeding to the adversarial discriminator, forcing the generator to produce samples with clean labels. This approach of passing through a matching noisy channel is justified by accompanying multiplicative approximation bounds between the loss of the RCGAN and the distance between the clean real distribution and the generator distribution. This shows that the proposed approach is robust, when used with a carefully chosen discriminator architecture, known as projection discriminator. When the distribution of the noise is not known, we provide an extension of our architecture, which we call RCGAN-U, that learns the noise model simultaneously while training the generator. We show experimentally on MNIST and CIFAR-10 datasets that both the approaches consistently improve upon baseline approaches, and RCGAN-U closely matches the performance of RCGAN.",https://proceedings.neurips.cc/paper_files/paper/2018/file/565e8a413d0562de9ee4378402d2b481-Paper.pdf,2018,"noise, rcgan, distribution, known, samples, architecture, conditional, discriminator, generator, labels","rcgan, noisy, label, conditional, distribution, samples, gan, data, generator, training","{'labels': 0.4713799469944986, 'conditional': 0.44924335667015963, 'gans': 0.44924335667015963, 'noisy': 0.44018017527250963, 'robustness': 0.42473894552299024}","rcgan, noise, discriminator, generator, samples, known, conditional, labels, clean, distribution"
Removing Hidden Confounding by Experimental Grounding,"Nathan Kallus, Aahlad Manas Puli, Uri Shalit","Observational data is increasingly used as a means for making individual-level causal predictions and intervention recommendations. The foremost challenge of causal inference from observational data is hidden confounding, whose presence cannot be tested in data and can invalidate any causal conclusion. Experimental data does not suffer from confounding but is usually limited in both scope and scale. We introduce a novel method of using limited experimental data to correct the hidden confounding in causal effect models trained on larger observational data, even if the observational data does not fully overlap with the experimental data. Our method makes strictly weaker assumptions than existing approaches, and we prove conditions under which it yields a consistent estimator. We demonstrate our method's efficacy using real-world data from a large educational experiment.",https://proceedings.neurips.cc/paper_files/paper/2018/file/566f0ea4f6c2e947f36795c8f58ba901-Paper.pdf,2018,"data, causal, observational, confounding, experimental, method, hidden, limited, using, approaches","data, observational, unconfounded, cate, study, unc, conf, sample, treatment, outcomes","{'grounding': 0.47882795420855845, 'confounding': 0.45195461238165074, 'removing': 0.45195461238165074, 'experimental': 0.43288767291911234, 'hidden': 0.41809820568501144}","observational, causal, confounding, data, experimental, hidden, limited, educational, foremost, method"
Legendre Decomposition for Tensors,"Mahito Sugiyama, Hiroyuki Nakahara, Koji Tsuda","We present a novel nonnegative tensor decomposition method, called Legendre decomposition, which factorizes an input tensor into a multiplicative combination of parameters. Thanks to the well-developed theory of information geometry, the reconstructed tensor is unique and always minimizes the KL divergence from an input tensor. We empirically show that Legendre decomposition can more accurately reconstruct tensors than other nonnegative tensor decomposition methods.",https://proceedings.neurips.cc/paper_files/paper/2018/file/56a3107cad6611c8337ee36d178ca129-Paper.pdf,2018,"tensor, decomposition, input, legendre, nonnegative, accurately, always, called, combination, developed","decomposition, tensor, legendre, θv, parameters, gradient, number, i2, tensors, ηv","{'legendre': 0.6323820963346627, 'tensors': 0.5717093408435115, 'decomposition': 0.522724893062933}","tensor, decomposition, legendre, nonnegative, reconstructed, input, factorizes, reconstruct, kl, thanks"
Bilevel learning of the Group Lasso structure,"Jordan Frecon, Saverio Salzo, Massimiliano Pontil","Regression with group-sparsity penalty plays a central role in high-dimensional prediction problems. Most of existing methods require the group structure to be known a priori. In practice, this may be a too strong assumption, potentially hampering the effectiveness of the regularization method. To circumvent this issue, we present a method to estimate the group structure by means of a continuous bilevel optimization problem where the data is split into training and validation sets. Our approach relies on an approximation scheme where the lower level problem is replaced by a smooth dual forward-backward algorithm with Bregman distances. We provide guarantees regarding the convergence of the approximate procedure to the exact problem and demonstrate the well behaviour of the proposed method on synthetic experiments. Finally, a preliminary application to genes expression data is tackled with the purpose of unveiling functional groups.",https://proceedings.neurips.cc/paper_files/paper/2018/file/56bd37d3a2fda0f2f41925019c81011d-Paper.pdf,2018,"group, method, problem, data, structure, algorithm, application, approach, approximate, approximation","problem, groups, group, algorithm, structure, 10, number, rp, bilevel, lasso","{'bilevel': 0.5265088572481276, 'lasso': 0.5265088572481276, 'group': 0.5042966433828047, 'structure': 0.39725950141891925, 'learning': 0.1828841996558742}","group, method, bilevel, bregman, hampering, unveiling, genes, replaced, problem, structure"
SING: Symbol-to-Instrument Neural Generator,"Alexandre Defossez, Neil Zeghidour, Nicolas Usunier, Leon Bottou, Francis Bach","Recent progress in deep learning for audio synthesis opens
the way to models that directly produce the waveform, shifting away
from the traditional paradigm of relying on vocoders or MIDI synthesizers for speech or music generation. Despite
their successes, current state-of-the-art neural audio synthesizers such
as WaveNet and SampleRNN suffer from prohibitive training and inference times because they are based on
autoregressive models that generate audio samples one at a time at a rate of 16kHz. In
this work, we study the more computationally efficient alternative of generating the waveform frame-by-frame with large strides.
We present a lightweight neural audio synthesizer for the original task of generating musical notes given desired instrument, pitch and velocity. Our model is trained end-to-end to generate notes from nearly 1000 instruments with a single decoder, thanks to a new loss function that minimizes the distances between the log spectrograms of the generated and target waveforms.
On the generalization task of synthesizing notes for pairs of pitch and instrument not seen during training, SING produces audio with significantly improved perceptual quality compared to a state-of-the-art autoencoder based on WaveNet  as measured by a Mean Opinion Score (MOS), and is about 32 times faster for training and 2, 500 times faster for inference.",https://proceedings.neurips.cc/paper_files/paper/2018/file/56dc0997d871e9177069bb472574eb29-Paper.pdf,2018,"audio, notes, times, training, art, based, end, faster, frame, generate","loss, waveform, sing, audio, model, time, training, wavenet, instrument, spectral","{'generator': 0.4879165484363349, 'instrument': 0.4879165484363349, 'sing': 0.4879165484363349, 'symbol': 0.4879165484363349, 'neural': 0.21851720080555345}","audio, notes, instrument, pitch, synthesizers, waveform, wavenet, times, frame, generating"
Forecasting Treatment Responses Over Time Using Recurrent Marginal Structural Networks,Bryan Lim,"Electronic health records provide a rich source of data for machine learning methods to learn dynamic treatment responses over time. However, any direct estimation is hampered by the presence of time-dependent confounding, where actions taken are dependent on time-varying variables related to the outcome of interest. Drawing inspiration from marginal structural models, a class of methods in epidemiology which use propensity weighting to adjust for time-dependent confounders, we introduce the Recurrent Marginal Structural Network - a sequence-to-sequence architecture for forecasting a patient's expected response to a series of planned treatments. Using simulations of a state-of-the-art pharmacokinetic-pharmacodynamic (PK-PD) model of tumor growth, we demonstrate the ability of our network to accurately learn unbiased treatment responses from observational data – even under changes in the policy of treatment assignments – and performance gains over benchmarks.",https://proceedings.neurips.cc/paper_files/paper/2018/file/56e6a93212e4482d99c84a639d254b67-Paper.pdf,2018,"time, dependent, treatment, data, learn, marginal, methods, network, responses, sequence","treatment, time, models, model, step, training, using, also, sequence, dependent","{'forecasting': 0.40183300193323557, 'marginal': 0.3848805794730767, 'responses': 0.3848805794730767, 'treatment': 0.3848805794730767, 'structural': 0.37173125904827803, 'time': 0.2871651304348284, 'recurrent': 0.27929644205890025, 'using': 0.24357829625030417, 'networks': 0.19012161147144513}","treatment, dependent, marginal, time, responses, structural, sequence, epidemiology, hampered, pharmacodynamic"
Optimal Subsampling with Influence Functions,"Daniel Ting, Eric Brochu","Subsampling is a common and often effective method to deal with the computational challenges of large datasets. However, for most statistical models, there is no well-motivated approach for drawing a non-uniform subsample. We show that the concept of an asymptotically linear estimator and the associated influence function leads to asymptotically optimal sampling probabilities for a wide class of popular models. This is the only tight optimality result for subsampling we are aware of as other methods only provide probabilistic error bounds or optimal rates. 
Furthermore, for linear regression models, which have well-studied procedures for non-uniform subsampling, we empirically show our optimal influence function based method outperforms previous approaches even when using approximations to the optimal probabilities.",https://proceedings.neurips.cc/paper_files/paper/2018/file/57c0531e13f40b91b3b0f1a30b529a1d-Paper.pdf,2018,"optimal, models, subsampling, asymptotically, function, influence, linear, method, non, probabilities","sampling, inﬂuence, regression, yi, leverage, function, xi, linear, based, data","{'influence': 0.5796845590680871, 'subsampling': 0.5552289830161583, 'functions': 0.44286779140180565, 'optimal': 0.39944299685407775}","subsampling, optimal, asymptotically, influence, probabilities, uniform, models, subsample, drawing, linear"
LinkNet: Relational Embedding for Scene Graph,"Sanghyun Woo, Dahun Kim, Donghyeon Cho, In So Kweon","Objects and their relationships are critical contents for image understanding. A scene graph provides a structured description that captures these properties of an image. However, reasoning about the relationships between objects is very challenging and only a few recent works have attempted to solve the problem of generating a scene graph from an image. In this paper, we present a novel method that improves scene graph generation by explicitly modeling inter-dependency among the entire object instances. We design a simple and effective relational embedding module that enables our model to jointly represent connections among all related objects, rather than focus on an object in isolation. Our novel method significantly benefits two main parts of the scene graph generation task: object classification and relationship classification. Using it on top of a basic Faster R-CNN, our model achieves state-of-the-art results on the Visual Genome benchmark. We further push the performance by introducing global context encoding module and geometrical layout encoding module. We validate our final model, LinkNet, through extensive ablation studies, demonstrating its efficacy in scene graph generation.",https://proceedings.neurips.cc/paper_files/paper/2018/file/58238e9ae2dd305d79c2ebc8c1883422-Paper.pdf,2018,"graph, scene, generation, image, model, module, object, objects, among, classification","object, graph, embedding, scene, relational, module, classiﬁcation, encoding, model, rn","{'linknet': 0.5440977195638447, 'relational': 0.47508980682415636, 'scene': 0.4308222730926368, 'embedding': 0.40915629016610455, 'graph': 0.35387942330971056}","scene, graph, module, objects, generation, object, encoding, relationships, image, among"
Meta-Learning MCMC Proposals,"Tongzhou Wang, YI WU, Dave Moore, Stuart J. Russell","Effective implementations of sampling-based probabilistic inference often require manually constructed, model-specific proposals. Inspired by recent progresses in meta-learning for training learning agents that can generalize to unseen environments, we propose a meta-learning approach to building effective and generalizable MCMC proposals. We parametrize the proposal as a neural network to provide fast approximations to block Gibbs conditionals. The learned neural proposals generalize to occurrences of common structural motifs across different models, allowing for the construction of a library of learned inference primitives that can accelerate inference on unseen models with no model-specific training required. We explore several applications including open-universe Gaussian mixture models, in which our learned proposals outperform a hand-tuned sampler, and a real-world named entity recognition task, in which our sampler yields higher final F1 scores than classical single-site Gibbs sampling.",https://proceedings.neurips.cc/paper_files/paper/2018/file/584b98aac2dddf59ee2cf19ca4ccb75e-Paper.pdf,2018,"proposals, inference, learned, learning, models, effective, generalize, gibbs, meta, model","proposal, models, model, block, proposals, variables, gibbs, ci, motif, neural","{'proposals': 0.6141280518071821, 'mcmc': 0.5796612398413149, 'meta': 0.496285243342107, 'learning': 0.20134681584274072}","proposals, sampler, gibbs, meta, unseen, learned, inference, generalize, effective, specific"
Bayesian Adversarial Learning,"Nanyang Ye, Zhanxing Zhu","Deep neural networks have been known to be vulnerable to adversarial attacks, raising lots of security concerns in the practical deployment. Popular defensive approaches can be formulated as a (distributionally) robust optimization problem, which minimizes a ``point estimate'' of worst-case loss derived from either per-datum perturbation or adversary data-generating distribution within certain pre-defined constraints. This point estimate ignores potential test adversaries that are beyond the pre-defined constraints. The model robustness might deteriorate sharply in the scenario of stronger test adversarial data. In this work, a novel robust training framework is proposed to alleviate this issue, Bayesian Robust Learning, in which a  distribution is put on the adversarial data-generating distribution to account for the uncertainty of the adversarial data-generating process. The uncertainty directly helps to consider the potential adversaries that are stronger than the point estimate in the cases of distributionally robust optimization. The uncertainty of model parameters is also incorporated to accommodate the full Bayesian framework. We design a scalable Markov Chain Monte Carlo sampling strategy to obtain the posterior distribution over model parameters. Various experiments are conducted to verify the superiority of BAL over existing adversarial training methods. The code for BAL is available at \url{https://tinyurl.com/ycxsaewr
}.",https://proceedings.neurips.cc/paper_files/paper/2018/file/586f9b4035e5997f77635b13cc04984c-Paper.pdf,2018,"adversarial, data, distribution, robust, estimate, generating, model, point, uncertainty, adversaries","05, 10, 12, 13, 15, 78, 82, 84, 86, 88","{'adversarial': 0.6616696534061457, 'bayesian': 0.650555472387994, 'learning': 0.37278793852192244}","adversarial, robust, bal, generating, uncertainty, distributionally, estimate, adversaries, distribution, stronger"
Bayesian Pose Graph Optimization via Bingham Distributions and Tempered Geodesic MCMC,"Tolga Birdal, Umut Simsekli, Mustafa Onur Eken, Slobodan Ilic","We introduce Tempered Geodesic Markov Chain Monte Carlo (TG-MCMC) algorithm for initializing pose graph optimization problems, arising in various scenarios such as SFM (structure from motion) or SLAM (simultaneous localization and mapping). TG-MCMC is first of its kind as it unites global non-convex optimization on the spherical manifold of quaternions  with posterior sampling, in order to provide both reliable initial poses and uncertainty estimates that are informative about the quality of solutions. We devise theoretical convergence guarantees and extensively evaluate our method on synthetic and real benchmarks. Besides its elegance in formulation and theory, we show that our method is robust to missing data, noise and the estimated uncertainties capture intuitive properties of the data.",https://proceedings.neurips.cc/paper_files/paper/2018/file/58a2fc6ed39fd083f55d4182bf88826d-Paper.pdf,2018,"data, mcmc, method, optimization, tg, algorithm, arising, benchmarks, besides, capture","algorithm, mcmc, optimization, posterior, problem, qi, poses, log, quaternions, graph","{'bingham': 0.39550176055483277, 'geodesic': 0.39550176055483277, 'mcmc': 0.373304948712255, 'tempered': 0.373304948712255, 'pose': 0.32692041202700123, 'distributions': 0.3131624363927867, 'graph': 0.2572330849967778, 'bayesian': 0.22628550405684714, 'optimization': 0.20751131125717662, 'via': 0.20751131125717662}","tg, mcmc, elegance, quaternions, sfm, slam, unites, geodesic, initializing, tempered"
Quadratic Decomposable Submodular Function Minimization,"Pan Li, Niao He, Olgica Milenkovic","We introduce a new convex optimization problem, termed quadratic decomposable submodular function minimization. The problem is closely related to decomposable submodular function minimization and arises in many learning on graphs and hypergraphs settings, such as graph-based semi-supervised learning and PageRank. We approach the problem via a new dual strategy and describe an objective that may be optimized via random coordinate descent (RCD) methods and projections onto cones. We also establish the linear convergence rate of the RCD algorithm and develop efficient projection algorithms with provable performance guarantees. Numerical experiments in semi-supervised learning on hypergraphs confirm the efficiency of the proposed algorithm and demonstrate the significant improvements in prediction accuracy with respect to state-of-the-art methods.",https://proceedings.neurips.cc/paper_files/paper/2018/file/58ae749f25eded36f486bc85feb3f0ab-Paper.pdf,2018,"learning, problem, algorithm, decomposable, function, hypergraphs, methods, minimization, new, rcd","problem, qdsfm, fr, methods, set, submodular, algorithm, dsfm, convergence, yr","{'decomposable': 0.5084471948027635, 'function': 0.45676455563911217, 'quadratic': 0.44527072837444975, 'minimization': 0.41867622875979427, 'submodular': 0.3991137491159238}","hypergraphs, rcd, decomposable, submodular, semi, minimization, supervised, cones, via, pagerank"
An Improved Analysis of Alternating Minimization for Structured Multi-Response Regression,"Sheng Chen, Arindam Banerjee","Multi-response linear models aggregate a set of vanilla linear models by assuming correlated noise across them, which has an unknown covariance structure. To find the coefficient vector, estimators with a joint approximation of the noise covariance are often preferred than the simple linear regression in view of their superior empirical performance, which can be generally solved by alternating-minimization type procedures. Due to the non-convex nature of such joint estimators, the theoretical justification of their efficiency is typically challenging. The existing analyses fail to fully explain the empirical observations due to the assumption of resampling on the alternating procedures, which requires access to fresh samples in each iteration. In this work, we present a resampling-free analysis for the alternating minimization algorithm applied to the multi-response regression. In particular, we focus on the high-dimensional setting of multi-response linear models with structured coefficient parameter, and the statistical error of the parameter can be expressed by the complexity measure, Gaussian width, which is related to the assumed structure. More importantly, to the best of our knowledge, our result reveals for the first time that the alternating minimization with random initialization can achieve the same performance as the well-initialized one when solving this multi-response regression problem. Experimental results support our theoretical developments.",https://proceedings.neurips.cc/paper_files/paper/2018/file/59a3adea76fadcb6dd9e54c96fc155d1-Paper.pdf,2018,"alternating, linear, multi, response, minimization, models, regression, coefficient, covariance, due","error, altmin, bound, 10, gaussian, given, d2, parameter, response, statistical","{'response': 0.45855925066239567, 'alternating': 0.41456361344416337, 'minimization': 0.3564045445529101, 'improved': 0.3350478819687306, 'structured': 0.322724355573973, 'analysis': 0.31909638028993526, 'regression': 0.30635935218754706, 'multi': 0.2806235542195491}","alternating, response, resampling, multi, minimization, coefficient, linear, regression, procedures, covariance"
Uniform Convergence of Gradients for Non-Convex Learning and Optimization,"Dylan J. Foster, Ayush Sekhari, Karthik Sridharan","We investigate 1) the rate at which refined properties of the empirical risk---in particular, gradients---converge to their population counterparts in standard non-convex learning tasks, and 2) the consequences of this convergence for optimization. Our analysis follows the tradition of norm-based capacity control. We propose vector-valued Rademacher complexities as a simple, composable, and user-friendly tool to derive dimension-free uniform convergence bounds for gradients in non-convex learning problems. As an application of our techniques, we give a new analysis of batch gradient descent methods for non-convex generalized linear models and non-convex robust regression, showing how to use any algorithm that finds approximate stationary points to obtain optimal sample complexity, even when dimension is high or possibly infinite and multiple passes over the dataset are allowed.Moving to non-smooth models we show----in contrast to the smooth case---that even for a single ReLU it is not possible to obtain dimension-independent convergence rates for gradients in the worst case. On the positive side, it is still possible to obtain dimension-independent rates under a new type of distributional assumption.",https://proceedings.neurips.cc/paper_files/paper/2018/file/59ab3ba90ae4b4ab84fe69de7b8e3f5f-Paper.pdf,2018,"non, convex, dimension, convergence, gradients, obtain, analysis, case, even, independent","non, dimension, convergence, norm, convex, complexity, gradient, risk, ld, theorem","{'uniform': 0.5153911873456584, 'gradients': 0.4513519038613181, 'convergence': 0.410614223145691, 'convex': 0.3648021134147528, 'non': 0.33888214221732604, 'optimization': 0.28649366011734245, 'learning': 0.17902244854920082}","dimension, non, convex, gradients, obtain, convergence, smooth, rates, independent, composable"
Posterior Concentration for Sparse Deep Learning,"Nicholas G. Polson, Veronika Ročková","We introduce Spike-and-Slab Deep Learning (SS-DL), a fully Bayesian  alternative to dropout for improving generalizability of deep ReLU networks. This new type of regularization enables  provable recovery of smooth input-output maps with {\sl unknown} levels of smoothness. Indeed, we  show that  the posterior distribution concentrates at the near minimax rate for alpha-Holder smooth maps, performing as well as if we knew the smoothness level alpha ahead of time. Our result sheds light on architecture design for deep neural networks, namely the choice of depth, width and sparsity level. These network attributes typically depend on  unknown smoothness  in order to be optimal. We obviate this constraint with the fully Bayes construction. As an aside, we show that SS-DL does not overfit in the sense that the posterior concentrates on smaller networks with fewer (up to the  optimal number of) nodes and links. Our results provide new theoretical justifications for deep ReLU networks from a Bayesian point of view.",https://proceedings.neurips.cc/paper_files/paper/2018/file/59b90e1005a220e2ebc542eb9d950b1e-Paper.pdf,2018,"deep, networks, smoothness, alpha, bayesian, concentrates, dl, fully, level, maps","deep, networks, f0, posterior, relu, 2017, learning, bayesian, network, functions","{'concentration': 0.6116545895661752, 'posterior': 0.5773265961377686, 'sparse': 0.41257952056148806, 'deep': 0.28659408221365656, 'learning': 0.2005358713746148}","smoothness, concentrates, dl, ss, alpha, deep, relu, networks, maps, smooth"
Sequence-to-Segment Networks for Segment Detection,"Zijun Wei, Boyu Wang, Minh Hoai Nguyen, Jianming Zhang, Zhe Lin, Xiaohui Shen, Radomir Mech, Dimitris Samaras","Detecting segments of interest from an input sequence is a challenging problem which often requires not only good knowledge of individual target segments, but also contextual understanding of the entire input sequence and the relationships between the target segments.  To address this problem, we propose the Sequence-to-Segment Network (S$^2$N), a novel end-to-end sequential encoder-decoder architecture. S$^2$N first encodes the input into a sequence of hidden states that progressively capture both local and holistic information. It then employs a novel decoding architecture, called Segment Detection Unit (SDU), that integrates the decoder state and encoder hidden states to detect segments sequentially.  During training, we formulate the assignment of predicted segments to ground truth as bipartite matching and use the Earth Mover's Distance to calculate the localization errors. We experiment with S$^2$N on temporal action proposal generation and video summarization and show that S$^2$N achieves state-of-the-art performance on both tasks.",https://proceedings.neurips.cc/paper_files/paper/2018/file/59e0b2658e9f2e77f8d4d83f8d07ca84-Paper.pdf,2018,"segments, sequence, input, architecture, decoder, encoder, end, hidden, novel, problem","segments, s2n, video, loss, segment, sequence, state, based, function, interest","{'segment': 0.853530538567496, 'sequence': 0.3858200480552436, 'detection': 0.29377815871165425, 'networks': 0.19058568601896558}","segments, sequence, segment, input, decoder, encoder, states, hidden, target, end"
Multi-Agent Reinforcement Learning via Double Averaging Primal-Dual Optimization,"Hoi-To Wai, Zhuoran Yang, Zhaoran Wang, Mingyi Hong","Despite the success of single-agent reinforcement learning, multi-agent reinforcement learning (MARL) remains challenging due to complex interactions between agents. Motivated by decentralized applications such as sensor networks, swarm robotics, and power grids, we study policy evaluation in MARL, where agents with jointly observed state-action pairs and private local rewards collaborate to learn the value of a given policy.In this paper, we propose a double averaging scheme, where each agent iteratively performs averaging over both space and time to incorporate neighboring gradient information and local reward information, respectively. We prove that the proposed algorithm converges to the optimal solution at a global geometric rate. In particular, such an algorithm is built upon a primal-dual reformulation of the mean squared Bellman error minimization problem, which gives rise to a decentralized convex-concave saddle-point problem. To the best of our knowledge, the proposed double averaging primal-dual optimization algorithm is the first to achieve fast finite-time convergence on decentralized convex-concave saddle-point problems.",https://proceedings.neurips.cc/paper_files/paper/2018/file/5a378f8490c8d6af8647a753812f6e31-Paper.pdf,2018,"agent, algorithm, averaging, decentralized, agents, concave, convex, double, dual, information","agent, agents, gradient, primal, local, dual, algorithm, method, pt, multi","{'averaging': 0.4323273514449417, 'double': 0.3908484863119945, 'primal': 0.3908484863119945, 'agent': 0.35736032038554594, 'dual': 0.34936962117904724, 'multi': 0.26457047322358745, 'reinforcement': 0.25457373076832157, 'optimization': 0.22683291084425963, 'via': 0.22683291084425963, 'learning': 0.14174199559686443}","averaging, decentralized, marl, double, agent, primal, saddle, concave, agents, dual"
Neural Tangent Kernel: Convergence and Generalization in Neural Networks,"Arthur Jacot, Franck Gabriel, Clement Hongler","At initialization, artificial neural networks (ANNs) are equivalent to Gaussian processes in the infinite-width limit, thus connecting them to kernel methods. We prove that the evolution of an ANN during training can also be described by a kernel: during gradient descent on the parameters of an ANN, the network function (which maps input vectors to output vectors) follows the so-called kernel gradient associated with a new object, which we call the Neural Tangent Kernel (NTK). This kernel is central to describe the generalization features of ANNs. While the NTK is random at initialization and varies during training, in the infinite-width limit it converges to an explicit limiting kernel and stays constant during training. This makes it possible to study the training of ANNs in function space instead of parameter space. Convergence of the training can then be related to the positive-definiteness of the limiting NTK.We then focus on the setting of least-squares regression and show that in the infinite-width limit, the network function follows a linear differential equation during training. The convergence is fastest along the largest kernel principal components of the input data with respect to the NTK, hence suggesting a theoretical motivation for early stopping.Finally we study the NTK numerically, observe its behavior for wide networks, and compare it to the infinite-width limit.",https://proceedings.neurips.cc/paper_files/paper/2018/file/5a4be1fa34e62bb8a6ec6b91d2462f5a-Paper.pdf,2018,"kernel, training, ntk, infinite, limit, width, anns, function, ann, convergence","kernel, function, network, ntk, pin, training, limit, fθ, gradient, width","{'tangent': 0.49513085891892883, 'neural': 0.469866783677807, 'kernel': 0.41536118566652863, 'convergence': 0.3944727383437038, 'generalization': 0.38866087850633013, 'networks': 0.23426417525195709}","ntk, kernel, width, infinite, anns, limit, training, ann, limiting, follows"
Randomized Prior Functions for Deep Reinforcement Learning,"Ian Osband, John Aslanides, Albin Cassirer","Dealing with uncertainty is essential for efficient reinforcement learning.
There is a growing literature on uncertainty estimation for deep learning from fixed datasets, but many of the most popular approaches are poorly-suited to sequential decision problems.
Other methods, such as bootstrap sampling, have no mechanism for uncertainty that does not come from the observed data.
We highlight why this can be a crucial shortcoming and propose a simple remedy through addition of a randomized untrainable `prior' network to each ensemble member.
We prove that this approach is efficient with linear representations, provide simple illustrations of its efficacy with nonlinear representations and show that this approach scales to large-scale problems far better than previous attempts.",https://proceedings.neurips.cc/paper_files/paper/2018/file/5a7b238ba0f6502e5d6be14424b20ded-Paper.pdf,2018,"uncertainty, approach, efficient, learning, problems, representations, simple, addition, approaches, attempts","prior, deep, learning, uncertainty, function, data, rl, simple, agent, even","{'randomized': 0.573191871633881, 'prior': 0.4808459241181093, 'functions': 0.43790750377766796, 'reinforcement': 0.3575901701164937, 'deep': 0.2845415393681937, 'learning': 0.1990996642315037}","uncertainty, illustrations, untrainable, representations, bootstrap, member, come, dealing, shortcoming, simple"
On the Convergence and Robustness of Training GANs with Regularized Optimal Transport,"Maziar Sanjabi, Jimmy Ba, Meisam Razaviyayn, Jason D. Lee","Generative Adversarial Networks (GANs) are one of the most practical methods for learning data distributions. A popular GAN formulation is based on the use of Wasserstein distance as a metric between probability distributions. Unfortunately, minimizing the Wasserstein distance between the data distribution and the generative model distribution is a computationally challenging problem as its objective is non-convex, non-smooth, and even hard to compute. In this work, we show that obtaining gradient information of the smoothed Wasserstein GAN formulation, which is based on regularized Optimal Transport (OT), is computationally effortless and hence one can apply first order optimization methods to minimize this objective. Consequently, we establish theoretical convergence guarantee to stationarity for a proposed class of GAN optimization algorithms. Unlike the original non-smooth formulation, our algorithm only requires solving the discriminator to approximate optimality. We apply our method to learning MNIST digits as well as CIFAR-10 images.  Our experiments show that our method is computationally efficient and generates images comparable to the state of the art algorithms given the same architecture and computational power.",https://proceedings.neurips.cc/paper_files/paper/2018/file/5a9d8bf5b7a4b35f3110dde8673bdda2-Paper.pdf,2018,"computationally, formulation, gan, non, wasserstein, algorithms, apply, based, data, distance","discriminator, problem, optimal, convergence, methods, regularized, transport, also, distance, dc","{'transport': 0.43352007116930524, 'gans': 0.4012202360046715, 'regularized': 0.4012202360046715, 'robustness': 0.3793352921815816, 'convergence': 0.37335565083557093, 'optimal': 0.32291543740773426, 'training': 0.3202200910506787}","wasserstein, gan, formulation, computationally, smooth, non, distance, apply, images, objective"
Uncertainty Sampling is Preconditioned Stochastic Gradient Descent on Zero-One Loss,"Stephen Mussmann, Percy S. Liang","Uncertainty sampling, a popular active learning algorithm, is used to reduce the amount of data required to learn a classifier, but it has been observed in practice to converge to different parameters depending on the initialization and sometimes to even better parameters than standard training on all the data. In this work, we give a theoretical explanation of this phenomenon, showing that uncertainty sampling on a convex (e.g., logistic) loss can be interpreted as performing a preconditioned stochastic gradient step on the population zero-one loss. Experiments on synthetic and real datasets support this connection.",https://proceedings.neurips.cc/paper_files/paper/2018/file/5abdf8b8520b71f3a528c7547ee92428-Paper.pdf,2018,"data, loss, parameters, sampling, uncertainty, active, algorithm, amount, better, classifier","sampling, loss, uncertainty, one, zero, θt, points, parameters, data, convex","{'preconditioned': 0.4468149201389065, 'uncertainty': 0.3610772227672861, 'zero': 0.3610772227672861, 'one': 0.3537927335925195, 'loss': 0.33105017847676405, 'sampling': 0.29576387580962094, 'descent': 0.2931317051511874, 'gradient': 0.2570660461262474, 'stochastic': 0.25564422085482646}","uncertainty, preconditioned, sampling, loss, parameters, explanation, interpreted, depending, phenomenon, logistic"
Negotiable Reinforcement Learning for Pareto Optimal Sequential Decision-Making,"Nishant Desai, Andrew Critch, Stuart J. Russell","It is commonly believed that an agent making decisions on behalf of two or more principals who have different utility functions should adopt a Pareto optimal policy, i.e. a policy that cannot be improved upon for one principal without making sacrifices for another. Harsanyi's theorem shows that when the principals have a common prior on the outcome distributions of all policies, a Pareto optimal policy for the agent is one that maximizes a fixed, weighted linear combination of the principals’ utilities. In this paper, we derive a more precise generalization for the sequential decision setting in the case of principals with different priors on the dynamics of the environment. We refer to this generalization as the Negotiable Reinforcement Learning (NRL) framework. In this more general case, the relative weight given to each principal’s utility should evolve over time according to how well the agent’s observations conform with that principal’s prior. To gain insight into the dynamics of this new framework, we implement a simple NRL agent and empirically examine its behavior in a simple environment.",https://proceedings.neurips.cc/paper_files/paper/2018/file/5b8e4fd39d9786228649a8a8bec4e008-Paper.pdf,2018,"agent, principals, policy, principal, case, different, dynamics, environment, framework, generalization","agent, principal, policy, principals, pareto, optimal, utility, environment, beliefs, belief","{'negotiable': 0.45341636281307535, 'pareto': 0.45341636281307535, 'making': 0.4099141506598612, 'sequential': 0.352407354224777, 'decision': 0.3409647756986006, 'optimal': 0.2949005577896927, 'reinforcement': 0.26699188632627513, 'learning': 0.14865619740827793}","principals, agent, principal, nrl, pareto, policy, utility, environment, dynamics, making"
Distributed Stochastic Optimization via Adaptive SGD,"Ashok Cutkosky, Róbert Busa-Fekete","Stochastic convex optimization algorithms are the most popular way to train machine learning models on large-scale data. Scaling up the training process of these models is crucial, but the most popular algorithm, Stochastic Gradient Descent (SGD), is a serial method that is surprisingly hard to parallelize. In this paper, we propose an efficient distributed stochastic optimization method by combining adaptivity with variance reduction techniques. Our analysis yields a linear speedup in the number of machines, constant memory footprint, and only a logarithmic number of communication rounds. Critically, our approach is a black-box reduction that parallelizes any serial online learning algorithm, streamlining prior analysis and allowing us to leverage the significant progress that has been made in designing adaptive algorithms. In particular, we achieve optimal convergence rates without any prior knowledge of smoothness parameters, yielding a more robust algorithm that reduces the need for hyperparameter tuning. We implement our algorithm in the Spark distributed framework and exhibit dramatic performance gains on large-scale logistic regression problems.",https://proceedings.neurips.cc/paper_files/paper/2018/file/5c936263f3428a40227908d5a3847c0b-Paper.pdf,2018,"algorithm, stochastic, algorithms, analysis, distributed, large, learning, method, models, number","algorithm, svrg, rf, wt, learning, sgd, ol, online, algorithms, vk","{'sgd': 0.5055972441683799, 'adaptive': 0.4395556752475159, 'distributed': 0.4395556752475159, 'stochastic': 0.36533541047974155, 'optimization': 0.33502468659364426, 'via': 0.33502468659364426}","serial, stochastic, reduction, algorithm, distributed, popular, spark, scale, parallelizes, streamlining"
Smoothed Analysis of Discrete Tensor Decomposition and Assemblies of Neurons,"Nima Anari, Constantinos Daskalakis, Wolfgang Maass, Christos Papadimitriou, Amin Saberi, Santosh Vempala","We analyze linear independence of rank one tensors produced by tensor powers of randomly perturbed vectors. This enables efficient decomposition of sums of high-order tensors. Our analysis builds upon [BCMV14] but allows for a wider range of perturbation models, including discrete ones. We give an application to recovering assemblies of neurons.Assemblies are large sets of neurons representing specific memories or concepts. The size of the intersection of two assemblies has been shown in experiments to represent the extent to which these memories co-occur or these concepts are related; the phenomenon is called association of assemblies.  This suggests that an animal's memory is a complex web of associations, and poses the problem of recovering this representation from cognitive data.  Motivated by this problem, we study the following more general question: Can we reconstruct the Venn diagram of a family of sets, given the sizes of their l-wise intersections? We show that as long as the family of sets is randomly perturbed, it is enough for the number of measurements to be polynomially larger than the number of nonempty regions of the Venn diagram to fully reconstruct the diagram.",https://proceedings.neurips.cc/paper_files/paper/2018/file/5cc3749a6e56ef6d656735dff9176074-Paper.pdf,2018,"assemblies, diagram, sets, concepts, family, memories, neurons, number, perturbed, problem","tree, echelon, n1, nodes, tensor, al, assemblies, et, probability, assume","{'assemblies': 0.43761808369710065, 'neurons': 0.4130575703656125, 'smoothed': 0.39563160883553367, 'discrete': 0.37107109550404554, 'decomposition': 0.3617336216961997, 'tensor': 0.34651058217255737, 'analysis': 0.30452410731099044}","assemblies, diagram, venn, memories, reconstruct, sets, perturbed, tensors, randomly, concepts"
Deep State Space Models for Time Series Forecasting,"Syama Sundar Rangapuram, Matthias W. Seeger, Jan Gasthaus, Lorenzo Stella, Yuyang Wang, Tim Januschowski","We present a novel approach to probabilistic time series forecasting that combines state space models with deep learning. By parametrizing a per-time-series linear state space model with a jointly-learned recurrent neural network, our method retains desired properties of state space models such as data efficiency and interpretability, while making use of the ability to learn complex patterns from raw data offered by deep learning approaches. Our method scales gracefully from regimes where little training data is available to regimes where data from millions of time series can be leveraged to learn accurate models. We provide qualitative as well as quantitative results with the proposed method, showing that it compares favorably to the state-of-the-art.",https://proceedings.neurips.cc/paper_files/paper/2018/file/5cf68969fb67aa6082363a6d4e6468e2-Paper.pdf,2018,"data, state, method, models, series, space, time, deep, learn, learning","time, ti, series, state, parameters, model, space, data, models, range","{'forecasting': 0.4816871091820685, 'state': 0.43272457913350293, 'series': 0.41240329310378104, 'space': 0.40408333511465927, 'time': 0.3442319094538347, 'models': 0.26775836748710224, 'deep': 0.23911712346825859}","series, regimes, state, space, data, time, gracefully, method, offered, parametrizing"
Learning Temporal Point Processes via Reinforcement Learning,"Shuang Li, Shuai Xiao, Shixiang Zhu, Nan Du, Yao Xie, Le Song","Social goods, such as healthcare, smart city, and information networks, often produce ordered event data in continuous time. The generative processes of these event data can be very complex, requiring flexible models to capture their dynamics. Temporal point processes offer an elegant framework for modeling event data without discretizing the time. However, the existing maximum-likelihood-estimation (MLE) learning paradigm requires hand-crafting the intensity function beforehand and cannot directly monitor the goodness-of-fit of the estimated model in the process of training. To alleviate the risk of model-misspecification in MLE, we propose to generate samples from the generative model and monitor the quality of the samples in the process of training until the samples and the real data are indistinguishable. We take inspiration from reinforcement learning (RL) and treat the generation of each event as the action taken by a stochastic policy. We parameterize the policy as a flexible recurrent neural network and gradually improve the policy to mimic the observed event distribution. Since the reward function is unknown in this setting, we uncover an analytic and nonparametric form of the reward function using an inverse reinforcement learning formulation. This new RL framework allows us to derive an efficient policy gradient algorithm for learning flexible point process models, and we show that it performs well in both synthetic and real data.",https://proceedings.neurips.cc/paper_files/paper/2018/file/5d50d22735a7469266aab23fd8aeb536-Paper.pdf,2018,"data, event, learning, policy, flexible, function, model, process, samples, framework","policy, function, πθ, process, reward, intensity, ti, event, learning, point","{'temporal': 0.4757292341560265, 'point': 0.4535009756118551, 'processes': 0.41700380228396217, 'learning': 0.4013542742384403, 'reinforcement': 0.3604233682559782, 'via': 0.3211481463977096}","event, monitor, policy, mle, flexible, data, samples, process, rl, processes"
GLoMo: Unsupervised Learning of Transferable Relational Graphs,"Zhilin Yang, Jake Zhao, Bhuwan Dhingra, Kaiming He, William W. Cohen, Russ R. Salakhutdinov, Yann LeCun","Modern deep transfer learning approaches have mainly focused on learning generic feature vectors from one task that are transferable to other tasks, such as word embeddings in language and pretrained convolutional features in vision. However, these approaches usually transfer unary features and largely ignore more structured graphical representations. This work explores the possibility of learning generic latent relational graphs that capture dependencies between pairs of data units (e.g., words or pixels) from large-scale unlabeled data and transferring the graphs to downstream tasks. Our proposed transfer learning framework improves performance on various tasks including question answering, natural language inference, sentiment analysis, and image classification. We also show that the learned graphs are generic enough to be transferred to different embeddings on which the graphs have not been trained (including GloVe embeddings, ELMo embeddings, and task-specific RNN hidden units), or embedding-free units such as image pixels.",https://proceedings.neurips.cc/paper_files/paper/2018/file/5dbc8390f17e019d300d5a162c3ce3bc-Paper.pdf,2018,"embeddings, graphs, learning, generic, tasks, transfer, units, approaches, data, features","graphs, graph, transfer, learning, features, feature, predictor, task, input, attention","{'glomo': 0.5071503576623558, 'transferable': 0.5071503576623558, 'relational': 0.4428284787625142, 'graphs': 0.3705511401830952, 'unsupervised': 0.35290934194226403, 'learning': 0.16627331933191503}","embeddings, graphs, units, generic, transfer, pixels, tasks, language, elmo, glove"
Neural-Symbolic VQA: Disentangling Reasoning from Vision and Language Understanding,"Kexin Yi, Jiajun Wu, Chuang Gan, Antonio Torralba, Pushmeet Kohli, Josh Tenenbaum","We marry two powerful ideas: deep representation learning for visual recognition and language understanding, and symbolic program execution for reasoning. Our neural-symbolic visual question answering (NS-VQA) system first recovers a structural scene representation from the image and a program trace from the question. It then executes the program on the scene representation to obtain an answer. Incorporating symbolic structure as prior knowledge offers three unique advantages. First, executing programs on a symbolic space is more robust to long program traces; our model can solve complex reasoning tasks better, achieving an accuracy of 99.8% on the CLEVR dataset. Second, the model is more data- and memory-efficient: it performs well after learning on a small number of training data; it can also encode an image into a compact representation, requiring less storage than existing methods for offline question answering. Third, symbolic program execution offers full transparency to the reasoning process; we are thus able to interpret and diagnose each execution step.",https://proceedings.neurips.cc/paper_files/paper/2018/file/5e388103a391daabe3de1d76a6739ccd-Paper.pdf,2018,"program, symbolic, representation, execution, question, reasoning, answering, data, first, image","al, et, program, question, scene, programs, model, representation, vqa, clevr","{'vqa': 0.4262286047670355, 'symbolic': 0.38533487284411394, 'vision': 0.38533487284411394, 'disentangling': 0.3721700316333333, 'reasoning': 0.36141357315466116, 'understanding': 0.3312762997104117, 'language': 0.3256532674717248, 'neural': 0.19088977800699086}","symbolic, program, execution, reasoning, representation, question, offers, answering, scene, visual"
Deep Anomaly Detection Using Geometric Transformations,"Izhak Golan, Ran El-Yaniv","We consider the problem of anomaly detection in images, and 
present a new detection technique. Given a sample
of images, all known to belong to a ``normal'' class (e.g., dogs), 
we show how to train a deep neural model that can detect 
out-of-distribution images (i.e., non-dog objects). The main 
idea behind our scheme is to train a multi-class model to discriminate between
dozens of geometric transformations applied on all the given images. The auxiliary expertise learned by the model generates feature detectors that effectively identify, at test time, anomalous images based on the softmax activation statistics of the model when applied on transformed images.
We present extensive experiments using the proposed detector, which indicate that our algorithm improves state-of-the-art methods by a wide margin.",https://proceedings.neurips.cc/paper_files/paper/2018/file/5e62d03aec0d17facfc5355dd90d441c-Paper.pdf,2018,"images, model, applied, class, detection, given, present, train, activation, algorithm","images, class, model, method, normal, set, anomaly, transformations, methods, detection","{'transformations': 0.5012183581060075, 'anomaly': 0.48007309300702, 'geometric': 0.48007309300702, 'detection': 0.36554603634909344, 'using': 0.3038225161434617, 'deep': 0.24881274531781394}","images, detection, model, applied, dog, dogs, dozens, train, anomalous, discriminate"
On Oracle-Efficient PAC RL with Rich Observations,"Christoph Dann, Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, Robert E. Schapire","We study the computational tractability of PAC reinforcement learning with rich observations. We present new provably sample-efficient algorithms for environments with deterministic hidden state dynamics and stochastic rich observations. These methods operate in an oracle model of computation -- accessing policy and value function classes exclusively through standard optimization primitives -- and therefore represent computationally efficient alternatives to prior algorithms that require enumeration. With stochastic hidden state dynamics, we prove that the only known sample-efficient algorithm, OLIVE, cannot be implemented in the oracle model. We also present several examples that illustrate fundamental challenges of tractable PAC reinforcement learning in such general settings.",https://proceedings.neurips.cc/paper_files/paper/2018/file/5f0f5e5f33945135b874349cfbed4fb9-Paper.pdf,2018,"efficient, algorithms, dynamics, hidden, learning, model, observations, oracle, pac, present","state, oracle, function, algorithm, efﬁcient, hidden, policy, valor, value, algorithms","{'rich': 0.4726503799867816, 'rl': 0.4461237431971713, 'observations': 0.42730279487336376, 'oracle': 0.42730279487336376, 'pac': 0.3742495212941431, 'efficient': 0.2689556893840023}","pac, oracle, rich, hidden, observations, efficient, dynamics, olive, reinforcement, accessing"
Joint Sub-bands Learning with Clique Structures for Wavelet Domain Super-Resolution,"Zhisheng Zhong, Tiancheng Shen, Yibo Yang, Zhouchen Lin, Chao Zhang","Convolutional neural networks (CNNs) have recently achieved great success in single-image super-resolution (SISR).  However, these methods tend to produce over-smoothed outputs and miss some textural details. To solve these problems, we propose the Super-Resolution CliqueNet (SRCliqueNet) to reconstruct the high resolution (HR) image with better textural details in the wavelet domain. The proposed SRCliqueNet firstly extracts a set of feature maps from the low resolution (LR) image by the clique blocks group. Then we send the set of feature maps to the clique up-sampling module to reconstruct the HR image. The clique up-sampling module consists of four sub-nets which predict the high resolution wavelet coefficients of four sub-bands. Since we consider the edge feature properties of four sub-bands, the four sub-nets are connected to the others so that they can learn the coefficients of four sub-bands jointly.  Finally we apply inverse discrete wavelet transform (IDWT) to the output of four sub-nets at the end of the clique up-sampling module to increase the resolution and reconstruct the HR image. Extensive quantitative and qualitative experiments on benchmark datasets show that our method achieves superior performance over the state-of-the-art methods.",https://proceedings.neurips.cc/paper_files/paper/2018/file/5f93f983524def3dca464469d2cf9f3e-Paper.pdf,2018,"four, resolution, sub, image, clique, bands, feature, hr, module, nets","clique, block, sub, sampling, image, srcliquenet, feature, blocks, four, 32","{'bands': 0.361920451824999, 'clique': 0.361920451824999, 'super': 0.361920451824999, 'sub': 0.34160832942162356, 'wavelet': 0.34160832942162356, 'resolution': 0.3271966492251158, 'structures': 0.3160180816068906, 'joint': 0.3068845268217404, 'domain': 0.24179583119164322, 'learning': 0.11865852788989531}","resolution, four, clique, sub, hr, bands, reconstruct, wavelet, image, nets"
Towards Understanding Learning Representations: To What Extent Do Different Neural Networks Learn the Same Representation,"Liwei Wang, Lunjia Hu, Jiayuan Gu, Zhiqiang Hu, Yue Wu, Kun He, John Hopcroft","It is widely believed that learning good representations is one of the main reasons for the success of deep neural networks. Although highly intuitive, there is a lack of theory and systematic approach quantitatively characterizing what representations do deep neural networks learn. In this work, we move a tiny step towards a theory and better understanding of the representations. Specifically, we study a simpler problem: How similar are the representations learned by two networks with identical architecture but trained from different initializations.  We develop a rigorous theory based on the neuron activation subspace match model. The theory gives a complete characterization of the structure of neuron activation subspace matches, where the core concepts are maximum match and simple match which describe the overall and the finest similarity between sets of neurons in two networks respectively. We also propose efficient algorithms to find the maximum match and simple matches. Finally, we conduct extensive experiments using our algorithms. Experimental results suggest that, surprisingly, representations learned by the same convolutional layers of networks trained from different initializations are not as similar as prevalently expected, at least in terms of subspace match.",https://proceedings.neurips.cc/paper_files/paper/2018/file/5fc34ed307aac159a30d81181c99847e-Paper.pdf,2018,"match, networks, representations, theory, subspace, activation, algorithms, deep, different, initializations","match, matches, simple, neuron, representations, similarity, algorithm, networks, maximum, zx","{'different': 0.43283844953006206, 'extent': 0.43283844953006206, 'learn': 0.3670182828974734, 'understanding': 0.336413648284078, 'towards': 0.32069485487187016, 'representation': 0.30119811626488463, 'representations': 0.29795883562868775, 'neural': 0.19385004811876447, 'networks': 0.1932978589788299, 'learning': 0.14190956321036047}","match, representations, subspace, theory, initializations, networks, neuron, activation, matches, maximum"
Non-delusional Q-learning and value-iteration,"Tyler Lu, Dale Schuurmans, Craig Boutilier","We identify a fundamental source of error in Q-learning and other forms of dynamic programming with function approximation. Delusional bias arises when the approximation architecture limits the class of expressible greedy policies. Since standard Q-updates make globally uncoordinated action choices with respect to the expressible policy class, inconsistent or even conflicting Q-value estimates can result, leading to pathological behaviour such as over/under-estimation, instability and even divergence. To solve this problem, we introduce a new notion of policy consistency and define a local backup process that ensures global consistency through the use of information sets---sets that record constraints on policies consistent with backed-up Q-values. We prove that both the model-based and model-free algorithms using this backup remove delusional bias, yielding the first known algorithms that guarantee optimal results under general conditions. These algorithms furthermore only require polynomially many information sets (from a potentially exponential support). Finally, we suggest other practical heuristics for value-iteration and Q-learning that attempt to reduce delusional bias.",https://proceedings.neurips.cc/paper_files/paper/2018/file/5fd0245f6c9ddbdf3eff0f505975b6a7-Paper.pdf,2018,"algorithms, bias, delusional, sets, approximation, backup, class, consistency, even, expressible","policy, value, learning, function, values, class, consistent, a2, delusional, sa","{'delusional': 0.5809410633941837, 'iteration': 0.5252037246896555, 'value': 0.46946638598512735, 'non': 0.3605446843379547, 'learning': 0.19046619505900694}","delusional, backup, expressible, bias, sets, consistency, policies, value, algorithms, backed"
An intriguing failing of convolutional neural networks and the CoordConv solution,"Rosanne Liu, Joel Lehman, Piero Molino, Felipe Petroski Such, Eric Frank, Alex Sergeev, Jason Yosinski","Few ideas have enjoyed as large an impact on deep learning as convolution. For any problem involving pixels or spatial representations, common intuition holds that convolutional neural networks may be appropriate. In this paper we show a striking counterexample to this intuition via the seemingly trivial coordinate transform problem, which simply requires learning a mapping between coordinates in (x,y) Cartesian space and coordinates in one-hot pixel space. Although convolutional networks would seem appropriate for this task, we show that they fail spectacularly. We demonstrate and carefully analyze the failure first on a toy problem, at which point a simple fix becomes obvious. We call this solution CoordConv, which works by giving convolution access to its own input coordinates through the use of extra coordinate channels. Without sacrificing the computational and parametric efficiency of ordinary convolution, CoordConv allows networks to learn either complete translation invariance or varying degrees of translation dependence, as required by the end task. CoordConv solves the coordinate transform problem with perfect generalization and 150 times faster with 10--100 times fewer parameters than convolution. This stark contrast raises the question: to what extent has this inability of convolution persisted insidiously inside other tasks, subtly hampering performance from within? A complete answer to this question will require further investigation, but we show preliminary evidence that swapping convolution for CoordConv can improve models on a diverse set of tasks. Using CoordConv in a GAN produced less mode collapse as the transform between high-level spatial latents and pixels becomes easier to learn. A Faster R-CNN detection model trained on MNIST detection showed 24% better IOU when using CoordConv, and in the Reinforcement Learning (RL) domain agents playing Atari games benefit significantly from the use of CoordConv layers.",https://proceedings.neurips.cc/paper_files/paper/2018/file/60106888f8977b71e1f15db7bc9a88d1-Paper.pdf,2018,"coordconv, convolution, problem, coordinate, coordinates, learning, networks, show, transform, appropriate","coordconv, coordinate, models, test, convolution, convolutional, layer, coordinates, layers, train","{'coordconv': 0.463058428006479, 'failing': 0.463058428006479, 'intriguing': 0.463058428006479, 'solution': 0.4186311253624051, 'convolutional': 0.3093652125088015, 'neural': 0.20738429926526428, 'networks': 0.20679355730281163}","coordconv, convolution, coordinates, coordinate, transform, intuition, pixels, becomes, appropriate, complete"
Adversarially Robust Optimization with Gaussian Processes,"Ilija Bogunovic, Jonathan Scarlett, Stefanie Jegelka, Volkan Cevher","In this paper, we consider the problem of Gaussian process (GP) optimization with an added robustness requirement: The returned point may be perturbed by an adversary, and we require the function value to remain as high as possible even after this perturbation. This problem is motivated by settings in which the underlying functions during optimization and implementation stages are different, or when one is interested in finding an entire region of good inputs rather than only a single point.  We show that standard GP optimization algorithms do not exhibit the desired robustness properties, and provide a novel confidence-bound based algorithm StableOpt for this purpose.  We rigorously establish the required number of samples for StableOpt to find a near-optimal point, and we complement this guarantee with an algorithm-independent lower bound.  We experimentally demonstrate several potential applications of interest using real-world data sets, and we show that StableOpt consistently succeeds in finding a stable maximizer where several baseline methods fail.",https://proceedings.neurips.cc/paper_files/paper/2018/file/60243f9b1ac2dba11ff8131c8f4431e0-Paper.pdf,2018,"optimization, point, stableopt, algorithm, bound, finding, gp, problem, robustness, several","gp, xt, point, optimization, stableopt, function, robust, algorithm, min, ucb","{'adversarially': 0.5715240109329784, 'processes': 0.44592684993624204, 'gaussian': 0.4257102225547718, 'robust': 0.4187376137458878, 'optimization': 0.3434227230102681}","stableopt, point, gp, robustness, finding, optimization, succeeds, maximizer, bound, returned"
Learning Hierarchical Semantic Image Manipulation through Structured Representations,"Seunghoon Hong, Xinchen Yan, Thomas S. Huang, Honglak Lee","Understanding, reasoning, and manipulating semantic concepts of images have been a fundamental research problem for decades. Previous work mainly focused on direct manipulation of natural image manifold through color strokes, key-points, textures, and holes-to-fill. In this work, we present a novel hierarchical framework for semantic image manipulation. Key to our hierarchical framework is that we employ structured semantic layout as our intermediate representations for manipulation. Initialized with coarse-level bounding boxes, our layout generator first creates pixel-wise semantic layout capturing the object shape, object-object interactions, and object-scene relations. Then our image generator fills in the pixel-level textures guided by the semantic layout. Such framework allows a user to manipulate images at object-level by adding, removing, and moving one bounding box at a time. Experimental evaluations demonstrate the advantages of the hierarchical manipulation framework over existing image generation and context hole-filing models, both qualitatively and quantitatively. Benefits of the hierarchical framework are further demonstrated in applications such as semantic object manipulation, interactive image editing, and data-driven image manipulation.",https://proceedings.neurips.cc/paper_files/paper/2018/file/602d1305678a8d5fdb372271e980da6a-Paper.pdf,2018,"image, manipulation, object, semantic, framework, hierarchical, layout, level, bounding, generator","image, manipulation, object, layout, semantic, model, structure, bounding, generation, box","{'manipulation': 0.475139173795394, 'hierarchical': 0.42471494445986824, 'semantic': 0.4161466072173015, 'structured': 0.36988046875879854, 'representations': 0.36178916132787636, 'image': 0.3511242298221835, 'learning': 0.172310184223778}","manipulation, semantic, layout, object, image, hierarchical, framework, textures, pixel, level"
Neural Proximal Gradient Descent for Compressive Imaging,"Morteza Mardani, Qingyun Sun, David Donoho, Vardan Papyan, Hatef Monajemi, Shreyas Vasanawala, John Pauly","Recovering high-resolution images from limited sensory data typically leads to a serious ill-posed inverse problem, demanding inversion algorithms that effectively capture the prior information. Learning a good inverse mapping from training data faces severe challenges, including: (i) scarcity of training data; (ii) need for  plausible reconstructions that are physically feasible; (iii) need for fast reconstruction, especially in real-time applications. We develop a successful system solving all these challenges, using as basic architecture the repetitive application of alternating proximal and data fidelity constraints. We learn a proximal map that works well with real images based on residual networks with recurrent blocks. Extensive experiments are carried out under different settings: (a) reconstructing abdominal MRI of pediatric patients from highly undersampled k-space data and (b) super-resolving natural face images. Our key findings include: 1. a recurrent ResNet with a single residual block (10-fold repetition) yields an effective proximal which accurately reveals MR image details. 2. Our architecture significantly outperforms conventional non-recurrent deep ResNets by 2dB SNR; it is also trained much more rapidly. 3. It outperforms state-of-the-art compressed-sensing Wavelet-based methods by 4dB SNR, with 100x speedups in reconstruction time.",https://proceedings.neurips.cc/paper_files/paper/2018/file/61d009da208a34ae155420e55f97abc7-Paper.pdf,2018,"data, images, proximal, recurrent, architecture, based, challenges, inverse, need, outperforms","proximal, image, iterations, training, xt, network, images, deep, one, neural","{'compressive': 0.5237654742922829, 'imaging': 0.5237654742922829, 'proximal': 0.4329427621787069, 'descent': 0.34361490554274016, 'gradient': 0.30133801156840967, 'neural': 0.23457241958227373}","proximal, snr, recurrent, images, data, reconstruction, residual, inverse, challenges, need"
Power-law efficient neural codes provide general link between perceptual bias and discriminability,"Michael Morais, Jonathan W. Pillow","Recent work in theoretical neuroscience has shown that information-theoretic ""efficient"" neural codes, which allocate neural resources to maximize the mutual information between stimuli and neural responses, give rise to a lawful relationship between perceptual bias and discriminability that is observed across a wide variety of psychophysical tasks in human observers (Wei & Stocker 2017). Here we generalize these results to show that the same law arises under a much larger family of optimal neural codes, introducing a unifying framework that we call power-law efficient coding. Specifically, we show that the same lawful relationship between bias and discriminability arises whenever Fisher information is allocated proportional to any power of the prior distribution. This family includes neural codes that are optimal for minimizing Lp error for any p, indicating that the lawful relationship observed in human psychophysical data does not require information-theoretically optimal neural codes. Furthermore, we derive the exact constant of proportionality governing the relationship between bias and discriminability for different power laws (which includes information-theoretically optimal codes, where the power is 2, and so-called discrimax codes, where power is 1/2), and different choices of optimal decoder. As a bonus, our framework provides new insights into ""anti-Bayesian"" perceptual biases, in which percepts are biased away from the center of mass of the prior. We derive an explicit formula that clarifies precisely which combinations of neural encoder and decoder can give rise to such biases.",https://proceedings.neurips.cc/paper_files/paper/2018/file/61d77652c97ef636343742fc3dcf3ba9-Paper.pdf,2018,"neural, codes, information, optimal, power, relationship, bias, discriminability, lawful, arises","bias, law, power, information, discriminability, 10, prior, relationship, snr, efﬁcient","{'discriminability': 0.34449501824411194, 'law': 0.34449501824411194, 'perceptual': 0.34449501824411194, 'provide': 0.34449501824411194, 'codes': 0.32516086638106406, 'power': 0.32516086638106406, 'link': 0.31144306732552773, 'bias': 0.2921089154624798, 'general': 0.26320600509729786, 'efficient': 0.19603051017073297}","codes, lawful, power, discriminability, relationship, neural, optimal, information, bias, psychophysical"
Stochastic Nonparametric Event-Tensor Decomposition,"Shandian Zhe, Yishuai Du","Tensor decompositions are fundamental tools for multiway data analysis. Existing approaches, however, ignore the valuable temporal information along with data, or simply discretize them into time steps so that important temporal patterns are easily missed. Moreover, most methods are limited to multilinear decomposition forms, and hence are unable to capture intricate, nonlinear relationships in data. To address these issues, we formulate event-tensors, to preserve the complete temporal information for multiway data, and propose a novel Bayesian nonparametric decomposition model. Our model can (1) fully exploit the time stamps to capture the critical, causal/triggering effects between the interaction events,  (2) flexibly estimate the complex relationships between the entities in tensor modes, and (3) uncover hidden structures from their temporal interactions. For scalable inference, we develop a doubly stochastic variational Expectation-Maximization algorithm to conduct an online decomposition. Evaluations on both synthetic and real-world datasets show that our model not only improves upon the predictive performance of existing methods, but also discovers interesting clusters underlying the data.",https://proceedings.neurips.cc/paper_files/paper/2018/file/61f2585b0ebcf1f532c4d1ec9a7d51aa-Paper.pdf,2018,"data, temporal, decomposition, model, capture, existing, information, methods, multiway, relationships","time, tensor, latent, event, events, factors, model, al, et, decomposition","{'event': 0.5018741690111842, 'decomposition': 0.4751050570695376, 'nonparametric': 0.45511094364502946, 'tensor': 0.45511094364502946, 'stochastic': 0.3288549242072348}","temporal, decomposition, multiway, data, tensor, relationships, capture, discretize, missed, stamps"
A Smoother Way to Train Structured Prediction Models,"Venkata Krishna Pillutla, Vincent Roulet, Sham M. Kakade, Zaid Harchaoui","We present a framework to train a structured prediction model by performing smoothing on the inference algorithm it builds upon. Smoothing overcomes the non-smoothness inherent to the maximum margin structured prediction objective, and paves the way for the use of fast primal gradient-based optimization algorithms. We illustrate the proposed framework by developing a novel primal incremental optimization algorithm for the structural support vector machine. The proposed algorithm blends an extrapolation scheme for acceleration and an adaptive smoothing scheme and builds upon the stochastic variance-reduced gradient algorithm. We establish its worst-case global complexity bound and study several practical variants. We present experimental results on two real-world problems, namely named entity recognition and visual object localization. The experimental results show that the proposed framework allows us to build upon efficient inference algorithms to develop large-scale optimization algorithms for structured prediction which can achieve competitive performance on the two real-world problems.",https://proceedings.neurips.cc/paper_files/paper/2018/file/6211080fa89981f66b1a0c9d55c61d0f-Paper.pdf,2018,"algorithm, algorithms, framework, optimization, prediction, proposed, smoothing, structured, upon, builds","oracle, algorithms, smoothing, optimization, smooth, svrg, inference, 100, loss, algorithm","{'smoother': 0.49232757472245453, 'way': 0.49232757472245453, 'train': 0.4646965914224146, 'structured': 0.3464897917860949, 'prediction': 0.33541471346962615, 'models': 0.25831374418005426}","smoothing, upon, structured, primal, builds, prediction, scheme, optimization, algorithm, framework"
Evolutionary Stochastic Gradient Descent for Optimization of Deep Neural Networks,"Xiaodong Cui, Wei Zhang, Zoltán Tüske, Michael Picheny","We propose a population-based Evolutionary Stochastic Gradient Descent (ESGD) framework for optimizing deep neural networks. ESGD combines SGD and gradient-free evolutionary algorithms as complementary algorithms in one framework in which the optimization alternates between the SGD step and evolution step to improve the average fitness of the population. With a back-off strategy in the SGD step and an elitist strategy in the evolution step, it guarantees that the best fitness in the population will never degrade. In addition, individuals in the population optimized with various SGD-based optimizers using distinct hyper-parameters in the SGD step are considered as competing species in a coevolution setting such that the complementarity of the optimizers is also taken into account. The effectiveness of ESGD is demonstrated across multiple applications including speech recognition, image recognition and language modeling, using networks with a variety of deep architectures.",https://proceedings.neurips.cc/paper_files/paper/2018/file/62da8c91ce7b10846231921795d6059e-Paper.pdf,2018,"sgd, step, population, esgd, algorithms, based, deep, evolution, evolutionary, fitness","population, esgd, sgd, ﬁtness, baseline, step, evolution, parameters, set, ea","{'evolutionary': 0.5789348834059331, 'descent': 0.37980864535930714, 'gradient': 0.33307862995142706, 'stochastic': 0.33123638115750187, 'optimization': 0.30375474591959534, 'deep': 0.2712630860644311, 'neural': 0.2592804662518248, 'networks': 0.2585418961093314}","sgd, esgd, population, step, fitness, optimizers, evolutionary, evolution, recognition, strategy"
On Coresets for Logistic Regression,"Alexander Munteanu, Chris Schwiegelshohn, Christian Sohler, David Woodruff","Coresets are one of the central methods to facilitate the analysis of large data. We continue a recent line of research applying the theory of coresets to logistic regression. First, we show the negative result that no strongly sublinear sized coresets exist for logistic regression. To deal with intractable worst-case instances   we introduce a complexity measure $\mu(X)$, which quantifies the hardness of compressing a data set for logistic regression. $\mu(X)$ has an intuitive statistical interpretation that may be of independent interest. For data sets with bounded $\mu(X)$-complexity, we show that a novel sensitivity sampling scheme produces the first provably sublinear $(1\pm\eps)$-coreset. We illustrate the performance of our method by comparing to uniform sampling as well as to state of the art methods in the area. The experiments are conducted on real world benchmark data for logistic regression.",https://proceedings.neurips.cc/paper_files/paper/2018/file/63bfd6e8f26d1d3537f4c5038264ef36-Paper.pdf,2018,"data, logistic, regression, coresets, mu, complexity, first, methods, sampling, show","data, regression, sampling, logistic, sensitivity, log, coreset, let, rn, set","{'coresets': 0.6541050339327402, 'logistic': 0.6173945870737432, 'regression': 0.4370017487789849}","logistic, coresets, mu, regression, sublinear, data, coreset, quantifies, sampling, continue"
Assessing the Scalability of Biologically-Motivated Deep Learning Algorithms and Architectures,"Sergey Bartunov, Adam Santoro, Blake Richards, Luke Marris, Geoffrey E. Hinton, Timothy Lillicrap","The backpropagation of error algorithm (BP) is impossible to implement in a real brain. The recent success of deep networks in machine learning and AI, however, has inspired proposals for understanding how the brain might learn across multiple layers, and hence how it might approximate BP. As of yet, none of these proposals have been rigorously evaluated on tasks where BP-guided deep learning has proved critical, or in architectures more structured than simple fully-connected networks. Here we present results on scaling up biologically motivated models of deep learning on datasets which need deep networks with appropriate architectures to achieve good performance. We present results on the MNIST, CIFAR-10, and ImageNet datasets and explore variants of target-propagation (TP) and feedback alignment (FA) algorithms, and explore performance in both fully- and locally-connected architectures. We also introduce weight-transport-free variants of difference target propagation (DTP) modified to remove backpropagation from the penultimate layer. Many of these algorithms perform well for MNIST, but for CIFAR and ImageNet we find that TP and FA variants perform significantly worse than BP, especially for networks composed of locally connected units, opening questions about whether new architectures and algorithms are required to scale these approaches. Our results and implementation details help establish baselines for biologically motivated deep learning schemes going forward.",https://proceedings.neurips.cc/paper_files/paper/2018/file/63c3ddcc7b23daa1e42dc41f9a44a873-Paper.pdf,2018,"deep, architectures, bp, learning, networks, algorithms, connected, results, variants, backpropagation","hl, learning, layer, bp, propagation, target, tp, algorithms, performance, networks","{'biologically': 0.4323709681385232, 'scalability': 0.4323709681385232, 'assessing': 0.4081049395562473, 'motivated': 0.4081049395562473, 'architectures': 0.39088791828086544, 'algorithms': 0.28121270014396355, 'deep': 0.2025897670078445, 'learning': 0.14175629568028247}","bp, architectures, tp, variants, connected, biologically, fa, deep, proposals, locally"
3D-Aware Scene Manipulation via Inverse Graphics,"Shunyu Yao, Tzu Ming Hsu, Jun-Yan Zhu, Jiajun Wu, Antonio Torralba, Bill Freeman, Josh Tenenbaum","We aim to obtain an interpretable, expressive, and disentangled scene representation that contains comprehensive structural and textural information for each object. Previous scene representations learned by neural networks are often uninterpretable, limited to a single object, or lacking 3D knowledge. In this work, we propose 3D scene de-rendering networks (3D-SDN) to address the above issues by integrating disentangled representations for semantics, geometry, and appearance into a deep generative model. Our scene encoder performs inverse graphics, translating a scene into a structured object-wise representation. Our decoder has two components: a differentiable shape renderer and a neural texture generator. The disentanglement of semantics, geometry, and appearance supports 3D-aware scene manipulation, e.g., rotating and moving objects freely while keeping the consistent shape and texture, and changing the object appearance without affecting its shape. Experiments demonstrate that our editing scheme based on 3D-SDN is superior to its 2D counterpart.",https://proceedings.neurips.cc/paper_files/paper/2018/file/64223ccf70bbb65a3a4aceac37e21016-Paper.pdf,2018,"scene, 3d, object, appearance, shape, disentangled, geometry, networks, neural, representation","3d, al, et, image, object, renderer, textural, geometric, map, semantic","{'graphics': 0.44829297440839316, 'manipulation': 0.42938051114248416, 'inverse': 0.38381242802042737, 'scene': 0.3760692714301884, 'aware': 0.36914278183327465, '3d': 0.35715680816427947, 'via': 0.24919536498984457}","scene, 3d, appearance, object, shape, sdn, texture, semantics, disentangled, geometry"
Learn What Not to Learn: Action Elimination with Deep Reinforcement Learning,"Tom Zahavy, Matan Haroush, Nadav Merlis, Daniel J. Mankowitz, Shie Mannor","Learning how to act when there are many available actions in each state is a challenging task for Reinforcement Learning (RL) agents, especially when many of the actions are redundant or irrelevant. In such cases, it is easier to learn which actions not to take. In this work, we propose the Action-Elimination Deep Q-Network (AE-DQN) architecture that combines a Deep RL algorithm with an Action Elimination Network (AEN) that eliminates sub-optimal actions. The AEN is trained to predict invalid actions, supervised by an external elimination signal provided by the environment. Simulations demonstrate a considerable speedup and added robustness over vanilla DQN in text-based games with over a thousand discrete actions.",https://proceedings.neurips.cc/paper_files/paper/2018/file/645098b086d2f9e1e0e939c27f9f2d6f-Paper.pdf,2018,"actions, elimination, action, aen, deep, dqn, learning, many, network, rl","action, actions, elimination, et, state, learning, al, st, agent, dqn","{'learn': 0.7413383174421332, 'elimination': 0.4371440645485714, 'action': 0.36134179907794267, 'reinforcement': 0.2574100274327204, 'deep': 0.20482622727200323, 'learning': 0.1433211936865624}","actions, elimination, aen, dqn, rl, action, ae, redundant, thousand, invalid"
Connecting Optimization and Regularization Paths,"Arun Suggala, Adarsh Prasad, Pradeep K. Ravikumar","We study the implicit regularization properties of optimization techniques by explicitly connecting their optimization paths to the regularization paths of ``corresponding'' regularized problems. This surprising connection shows that iterates of optimization techniques such as gradient descent and mirror descent are \emph{pointwise} close to solutions of appropriately regularized objectives. While such a tight connection between optimization and regularization is of independent intellectual interest, it also has important implications for machine learning: we can port results from regularized estimators to optimization, and vice versa. We investigate one key consequence, that borrows from the well-studied analysis of regularized estimators, to then obtain tight excess risk bounds of the iterates generated by optimization techniques.",https://proceedings.neurips.cc/paper_files/paper/2018/file/6459257ddab7b85bf4b57845e875e4d4-Paper.pdf,2018,"optimization, regularized, regularization, techniques, connection, descent, estimators, iterates, paths, tight","gd, optimization, regularization, k2, risk, path, regularized, bounds, convex, iterates","{'connecting': 0.599493411551034, 'paths': 0.599493411551034, 'regularization': 0.4269414810564218, 'optimization': 0.31454136574885394}","regularized, optimization, regularization, iterates, paths, techniques, tight, connection, estimators, borrows"
A Theory-Based Evaluation of Nearest Neighbor Models Put Into Practice,"Hendrik Fichtenberger, Dennis Rohde","In the $k$-nearest neighborhood model ($k$-NN), we are given a set of points $P$, and we shall answer queries $q$ by returning the $k$ nearest neighbors of $q$ in $P$ according to some metric. This concept is crucial in many areas of data analysis and data processing, e.g., computer vision, document retrieval and machine learning. Many $k$-NN algorithms have been published and implemented, but often the relation between parameters and accuracy of the computed $k$-NN is not explicit. We study property testing of $k$-NN graphs in theory and evaluate it empirically: given a point set $P \subset \mathbb{R}^\delta$ and a directed graph $G=(P,E)$, is $G$ a $k$-NN graph, i.e., every point $p \in P$ has outgoing edges to its $k$ nearest neighbors, or is it $\epsilon$-far from being a $k$-NN graph? Here, $\epsilon$-far means that one has to change more than an $\epsilon$-fraction of the edges in order to make $G$ a $k$-NN graph. We develop a randomized algorithm with one-sided error that decides this question, i.e., a property tester for the $k$-NN property, with complexity $O(\sqrt{n} k^2 / \epsilon^2)$ measured in terms of the number of vertices and edges it inspects, and we prove a lower bound of $\Omega(\sqrt{n / \epsilon k})$. We evaluate our tester empirically on the $k$-NN models computed by various algorithms and show that it can be used to detect $k$-NN models with bad accuracy in significantly less time than the building time of the $k$-NN model.",https://proceedings.neurips.cc/paper_files/paper/2018/file/6463c88460bd63bbe256e495c63aa40b-Paper.pdf,2018,"nn, epsilon, graph, edges, nearest, property, accuracy, algorithms, computed, data","graph, tester, property, ann, nearest, nn, algorithm, one, 10, vertices","{'practice': 0.41713759699139863, 'put': 0.41713759699139863, 'neighbor': 0.39372651346071125, 'nearest': 0.36423203635662743, 'evaluation': 0.353705001822229, 'theory': 0.33709457371443424, 'based': 0.2786859142035727, 'models': 0.21886317169592462}","nn, epsilon, nearest, edges, graph, tester, property, neighbors, far, computed"
MetaReg: Towards Domain Generalization using Meta-Regularization,"Yogesh Balaji, Swami Sankaranarayanan, Rama Chellappa","Training models that generalize to new domains at test time is a problem of fundamental importance in machine learning. In this work, we encode this notion of domain generalization using a novel regularization function. We pose the problem of finding such a regularization function in a Learning to Learn (or) meta-learning framework. The objective of domain generalization is explicitly modeled by learning a regularizer that makes the model trained on one domain to perform well on another domain. Experimental validations on computer vision and natural language datasets indicate that our method can learn regularizers that achieve good cross-domain generalization.",https://proceedings.neurips.cc/paper_files/paper/2018/file/647bba344396e7c8170902bcf2e15551-Paper.pdf,2018,"domain, learning, generalization, function, learn, problem, regularization, achieve, another, computer","domain, network, learning, using, regularizer, task, generalization, meta, data, training","{'metareg': 0.49802229024103767, 'meta': 0.4024585960122662, 'generalization': 0.3689902924872051, 'towards': 0.3689902924872051, 'regularization': 0.3546767488978805, 'domain': 0.33272425753663837, 'using': 0.2849424104222526}","domain, generalization, regularization, validations, learning, regularizers, encode, regularizer, learn, modeled"
Mirrored Langevin Dynamics,"Ya-Ping Hsieh, Ali Kavis, Paul Rolland, Volkan Cevher","We consider the problem of sampling from constrained distributions, which has posed significant challenges to both non-asymptotic analysis and algorithmic design. We propose a unified framework, which is inspired by the classical mirror descent, to derive novel first-order sampling schemes. We prove that, for a general target distribution with strongly convex potential, our framework implies the existence of a first-order algorithm achieving O~(\epsilon^{-2}d) convergence, suggesting that the state-of-the-art O~(\epsilon^{-6}d^5) can be vastly improved. With the important Latent Dirichlet Allocation (LDA) application in mind, we specialize our algorithm to sample from Dirichlet posteriors, and derive the first non-asymptotic O~(\epsilon^{-2}d^2) rate for first-order sampling. We further extend our framework to the mini-batch setting and prove convergence rates when only stochastic gradients are available. Finally, we report promising experimental results for LDA on real datasets.",https://proceedings.neurips.cc/paper_files/paper/2018/file/6490791e7abf6b29a381288cc23a8223-Paper.pdf,2018,"first, epsilon, framework, order, sampling, algorithm, asymptotic, convergence, derive, dirichlet","dynamics, distribution, xt, mirror, sampling, see, convergence, dx, langevin, map","{'mirrored': 0.6528597115536139, 'langevin': 0.5700575155762727, 'dynamics': 0.49880720320093597}","epsilon, lda, dirichlet, first, sampling, order, asymptotic, framework, derive, mind"
Multivariate Convolutional Sparse Coding for Electromagnetic Brain Signals,"Tom Dupré la Tour, Thomas Moreau, Mainak Jas, Alexandre Gramfort","Frequency-specific patterns of neural activity are traditionally interpreted as sustained rhythmic oscillations, and related to cognitive mechanisms such as attention, high level visual processing or motor control. While alpha waves (8--12\,Hz) are known to closely resemble short sinusoids, and thus are revealed by Fourier analysis or wavelet transforms, there is an evolving debate that electromagnetic neural signals are composed of more complex waveforms that cannot be analyzed by linear filters and traditional signal representations. In this paper, we propose to learn dedicated representations of such recordings using a multivariate convolutional sparse coding (CSC) algorithm. Applied to electroencephalography (EEG) or magnetoencephalography (MEG) data, this method is able to learn not only prototypical temporal waveforms, but also associated spatial patterns so their origin can be localized in the brain. Our algorithm is based on alternated minimization and a greedy coordinate descent solver that leads to state-of-the-art running time on long time series. To demonstrate the implications of this method, we apply it to MEG data and show that it is able to recover biological artifacts. More remarkably, our approach also reveals the presence of non-sinusoidal mu-shaped patterns, along with their topographic maps related to the somatosensory cortex.",https://proceedings.neurips.cc/paper_files/paper/2018/file/64f1f27bf1b4ec22924fd0acb550c235-Paper.pdf,2018,"patterns, able, algorithm, also, data, learn, meg, method, neural, related","et, al, multivariate, signals, zn, atoms, step, coordinate, 10, channels","{'electromagnetic': 0.43443782123471664, 'signals': 0.43443782123471664, 'brain': 0.3927564708981739, 'multivariate': 0.3927564708981739, 'coding': 0.37933807319199797, 'sparse': 0.2930414502831173, 'convolutional': 0.2902440399729823}","patterns, meg, waveforms, related, alternated, csc, eeg, electroencephalography, electromagnetic, magnetoencephalography"
Complex Gated Recurrent Neural Networks,"Moritz Wolter, Angela Yao","Complex numbers have long been favoured for digital signal processing, yet
complex representations rarely appear in deep learning architectures. RNNs, widely
used to process time series and sequence information, could greatly benefit from
complex representations. We present a novel complex gated recurrent cell, which
is a hybrid cell combining complex-valued and norm-preserving state transitions
with a gating mechanism. The resulting RNN exhibits excellent stability and
convergence properties and performs competitively on the synthetic memory and
adding task, as well as on the real-world tasks of human motion prediction.",https://proceedings.neurips.cc/paper_files/paper/2018/file/652cf38361a209088302ba2b8b7f51e0-Paper.pdf,2018,"complex, cell, representations, adding, appear, architectures, benefit, combining, competitively, convergence","complex, state, real, matrices, non, problem, cgrnn, transition, norm, unitary","{'complex': 0.5921414343325934, 'gated': 0.54140635079174, 'recurrent': 0.42969950843237126, 'neural': 0.29333900177318595, 'networks': 0.29250341461357293}","complex, cell, favoured, representations, competitively, digital, rarely, gating, appear, transitions"
Active Matting,"Xin Yang, Ke Xu, Shaozhe Chen, Shengfeng He, Baocai Yin Yin, Rynson Lau","Image matting is an ill-posed problem. It requires a user input trimap or some  strokes to obtain an alpha matte of the foreground object. A fine user input is essential to obtain a good result, which is either time consuming or suitable for experienced users who know where to place the strokes. In this paper, we explore the intrinsic relationship between the user input and the matting algorithm to address the problem of where and when the user should provide the input. Our aim is to discover the most informative sequence of regions for user input in order to produce a good alpha matte with minimum labeling efforts. To this end, we propose an active matting method with recurrent reinforcement learning. The proposed framework involves human in the loop by sequentially detecting informative regions for trivial human judgement. Comparing to traditional matting algorithms, the proposed framework requires much less efforts, and can produce satisfactory results with just 10 regions. Through extensive experiments, we show that the proposed model reduces user efforts significantly and achieves comparable performance to dense trimaps in a user-friendly manner. We further show that the learned informative knowledge can be generalized across different matting algorithms.",https://proceedings.neurips.cc/paper_files/paper/2018/file/653ac11ca60b3e021a8c609c7198acfc-Paper.pdf,2018,"user, input, matting, efforts, informative, proposed, regions, algorithms, alpha, framework","regions, matting, informative, model, matte, user, region, trimap, proposed, input","{'matting': 0.7839909537215898, 'active': 0.6207722484798368}","matting, user, efforts, input, informative, regions, matte, strokes, alpha, produce"
Scalable End-to-End Autonomous Vehicle Testing via Rare-event Simulation,"Matthew O'Kelly, Aman Sinha, Hongseok Namkoong, Russ Tedrake, John C. Duchi","While recent developments in autonomous vehicle (AV) technology highlight substantial progress, we lack tools for rigorous and scalable testing. Real-world testing, the de facto evaluation environment, places the public in danger, and, due to the rare nature of accidents, will require billions of miles in order to statistically validate performance claims. We implement a simulation framework that can test an entire modern autonomous driving system, including, in particular, systems that employ deep-learning perception and control algorithms. Using adaptive importance-sampling methods to accelerate rare-event probability evaluation, we estimate the probability of an accident under a base distribution governing standard traffic behavior. We demonstrate our framework on a highway scenario, accelerating system evaluation by 2-20 times over naive Monte Carlo sampling methods and 10-300P times (where P is the number of processors) over real-world testing.",https://proceedings.neurips.cc/paper_files/paper/2018/file/653c579e3f9ba5c03f2f2f8cf4512b39-Paper.pdf,2018,"evaluation, testing, autonomous, framework, methods, probability, rare, real, sampling, system","distribution, cross, entropy, method, rare, based, importance, sampling, model, p0","{'end': 0.5683394973833837, 'rare': 0.3351320606047826, 'simulation': 0.3351320606047826, 'autonomous': 0.31632338758844997, 'vehicle': 0.31632338758844997, 'event': 0.292627261994295, 'scalable': 0.2604736230975369, 'testing': 0.2604736230975369, 'via': 0.17583662141695175}","testing, evaluation, rare, autonomous, times, probability, system, 300p, accident, accidents"
Improving Explorability in Variational Inference with Annealed Variational Objectives,"Chin-Wei Huang, Shawn Tan, Alexandre Lacoste, Aaron C. Courville","Despite the advances in the representational capacity of approximate distributions for variational inference, the optimization process can still limit the density that is ultimately learned.
We demonstrate the drawbacks of biasing the true posterior to be unimodal, and introduce Annealed Variational Objectives (AVO) into the training of hierarchical variational methods.
Inspired by Annealed Importance Sampling, the proposed method facilitates learning by incorporating energy tempering into the optimization objective.
In our experiments, we demonstrate our method's robustness to deterministic warm up, and the benefits of encouraging exploration in the latent space.",https://proceedings.neurips.cc/paper_files/paper/2018/file/65b0df23fd2d449ae1e4b2d27151d73b-Paper.pdf,2018,"variational, annealed, demonstrate, method, optimization, advances, approximate, avo, benefits, biasing","zt, posterior, al, et, variational, distribution, log, qt, vi, model","{'variational': 0.5293659202314197, 'annealed': 0.43545143309867645, 'explorability': 0.43545143309867645, 'objectives': 0.39367283360669203, 'improving': 0.338444529173191, 'inference': 0.26648231099826264}","annealed, variational, avo, biasing, warm, tempering, encouraging, ultimately, unimodal, drawbacks"
Learning Loop Invariants for Program Verification,"Xujie Si, Hanjun Dai, Mukund Raghothaman, Mayur Naik, Le Song","A fundamental problem in program verification concerns inferring loop invariants. The problem is undecidable and even practical instances are challenging. Inspired by how human experts construct loop invariants, we propose a reasoning framework Code2Inv that constructs the solution by multi-step decision making and querying an external program graph memory block. By training with reinforcement learning, Code2Inv captures rich program features and avoids the need for ground truth solutions as supervision. Compared to previous learning tasks in domains with graph-structured data, it addresses unique challenges, such as a binary objective function and an extremely sparse reward that is given by an automated theorem prover only after the complete loop invariant is proposed. We evaluate Code2Inv on a suite of 133 benchmark problems and compare it to three state-of-the-art systems. It solves 106 problems compared to 73 by a stochastic search-based system, 77 by a heuristic search-based system, and 100 by a decision tree learning-based system. Moreover, the strategy learned can be generalized to new programs: compared to solving new instances from scratch, the pre-trained agent is more sample efficient in finding solutions.",https://proceedings.neurips.cc/paper_files/paper/2018/file/65b1e92c585fd4c2159d5f33b5030ff2-Paper.pdf,2018,"based, code2inv, compared, learning, loop, program, system, decision, graph, instances","loop, program, invariant, learning, code2inv, step, invariants, memory, graph, model","{'invariants': 0.5407543093017728, 'loop': 0.5104054642301453, 'verification': 0.4888726159723495, 'program': 0.4202887479498546, 'learning': 0.1772906448594166}","code2inv, loop, invariants, program, system, compared, instances, solutions, search, decision"
Faster Online Learning of Optimal Threshold for Consistent F-measure Optimization,"Xiaoxuan Zhang, Mingrui Liu, Xun Zhou, Tianbao Yang","In this paper, we consider online F-measure optimization (OFO). Unlike traditional performance metrics (e.g., classification error rate), F-measure is non-decomposable over training examples and is a non-convex function of model parameters, making it much more difficult to be optimized in an online fashion. Most existing results of OFO usually suffer from high memory/computational costs and/or lack  statistical consistency  guarantee for optimizing F-measure at the population level. To advance OFO, we propose an efficient online algorithm based on simultaneously learning a posterior probability of class and learning an optimal threshold by minimizing  a stochastic strongly convex function with unknown strong convexity parameter. A key component of the proposed method is  a novel stochastic algorithm with low memory and computational costs, which can enjoy a  convergence rate of $\widetilde O(1/\sqrt{n})$ for learning the optimal threshold under a mild condition on the convergence of the posterior probability,  where $n$ is the number of processed examples. It is provably  faster than its predecessor based on a heuristic for updating the threshold.   The experiments verify  the efficiency of the proposed algorithm in comparison with state-of-the-art OFO algorithms.",https://proceedings.neurips.cc/paper_files/paper/2018/file/65fc52ed8f88c81323a418ca94cec2ed-Paper.pdf,2018,"ofo, algorithm, learning, measure, online, threshold, based, computational, convergence, convex","measure, fofo, online, algorithm, learning, probability, ofo, convergence, optimal, threshold","{'threshold': 0.46474718045295776, 'consistent': 0.4386640962603335, 'measure': 0.4386640962603335, 'faster': 0.361213969497549, 'online': 0.3076339243990056, 'optimal': 0.30227008548270223, 'optimization': 0.24384290144139162, 'learning': 0.15237109700612497}","ofo, threshold, measure, online, costs, posterior, memory, examples, probability, computational"
Learning Others' Intentional Models in Multi-Agent Settings Using Interactive POMDPs,"Yanlin Han, Piotr Gmytrasiewicz","Interactive partially observable Markov decision processes (I-POMDPs) provide a principled framework for planning and acting in a partially observable, stochastic and multi-agent environment. It extends POMDPs to multi-agent settings by including models of other agents in the state space and forming a hierarchical belief structure. In order to predict other agents' actions using I-POMDPs, we propose an approach that effectively uses Bayesian inference and sequential Monte Carlo sampling to learn others' intentional models which ascribe to them beliefs, preferences and rationality in action selection. Empirical results show that our algorithm accurately learns models of the other agent and has superior performance than methods that use subintentional models. Our approach serves as a generalized Bayesian learning algorithm that learns other agents' beliefs, strategy levels, and transition, observation and reward functions. It also effectively mitigates the belief space complexity due to the nested belief hierarchy.",https://proceedings.neurips.cc/paper_files/paper/2018/file/65fc9fb4897a89789352e211ca2d398f-Paper.pdf,2018,"models, agent, agents, belief, pomdps, algorithm, approach, bayesian, beliefs, effectively","agent, belief, models, agents, level, algorithm, pomdps, model, pomdp, intentional","{'intentional': 0.39827652734010666, 'others': 0.39827652734010666, 'settings': 0.39827652734010666, 'pomdps': 0.3600646068869018, 'interactive': 0.3377120663412372, 'agent': 0.32921402482773315, 'multi': 0.24373246096977183, 'using': 0.22787308106223156, 'models': 0.20896717201804224, 'learning': 0.13057815934129086}","pomdps, belief, agents, beliefs, observable, agent, partially, models, effectively, learns"
Online Structure Learning for Feed-Forward and Recurrent Sum-Product Networks,"Agastya Kalra, Abdullah Rashwan, Wei-Shou Hsu, Pascal Poupart, Prashant Doshi, Georgios Trimponias","Sum-product networks have recently emerged as an attractive representation due to their dual view as a special type of deep neural network with clear semantics and a special type of probabilistic graphical model for which inference is always tractable. Those properties follow from some conditions (i.e., completeness and decomposability) that must be respected by the structure of the network.  As a result, it is not easy to specify a valid sum-product network by hand and therefore structure learning techniques are typically used in practice.  This paper describes a new online structure learning technique for feed-forward and recurrent SPNs. The algorithm is demonstrated on real-world datasets with continuous features for which it is not clear what network architecture might be best, including sequence datasets of varying length.",https://proceedings.neurips.cc/paper_files/paper/2018/file/66121d1f782d29b62a286909165517bc-Paper.pdf,2018,"network, structure, clear, datasets, learning, product, special, sum, type, algorithm","network, node, data, structure, spns, variables, nodes, product, learning, leaf","{'feed': 0.440510624114083, 'forward': 0.41578777225016267, 'product': 0.3982466296983018, 'sum': 0.38464066249369927, 'structure': 0.3137186408667395, 'online': 0.29159082127963043, 'recurrent': 0.2889957890028192, 'networks': 0.19672411402251458, 'learning': 0.14442494728789357}","clear, network, product, structure, type, special, sum, completeness, decomposability, respected"
Balanced Policy Evaluation and Learning,Nathan Kallus,"We present a new approach to the problems of evaluating and learning personalized decision policies from observational data of past contexts, decisions, and outcomes. Only the outcome of the enacted decision is available and the historical policy is unknown. These problems arise in personalized medicine using electronic health records and in internet advertising. Existing approaches use inverse propensity weighting (or, doubly robust versions) to make historical outcome (or, residual) data look like it were generated by a new policy being evaluated or learned. But this relies on a plug-in approach that rejects data points with a decision that disagrees with the new policy, leading to high variance estimates and ineffective learning. We propose a new, balance-based approach that too makes the data look like the new policy but does so directly by finding weights that optimize for balance between the weighted data and the target policy in the given, finite sample, which is equivalent to minimizing worst-case or posterior conditional mean square error. Our policy learner proceeds as a two-level optimization problem over policies and weights. We demonstrate that this approach markedly outperforms existing ones both in evaluation and learning, which is unsurprising given the wider support of balance-based weights. We establish extensive theoretical consistency guarantees and regret bounds that support this empirical success.",https://proceedings.neurips.cc/paper_files/paper/2018/file/6616758da438b02b8d360ad83a5b3d77-Paper.pdf,2018,"policy, data, new, approach, balance, decision, learning, weights, based, existing","policy, xi, approach, pn, weights, data, ipw, evaluation, dr, estimator","{'balanced': 0.642889707480598, 'evaluation': 0.577541256130337, 'policy': 0.4508567839356792, 'learning': 0.22330938596951094}","policy, balance, weights, historical, look, new, outcome, personalized, decision, data"
Visual Memory for Robust Path Following,"Ashish Kumar, Saurabh Gupta, David Fouhey, Sergey Levine, Jitendra Malik","Humans routinely retrace a path in a novel environment both forwards and backwards despite uncertainty in their motion. In this paper, we present an approach for doing so. Given a demonstration of a path, a first network generates an abstraction of the path. Equipped with this abstraction, a second network then observes the world and decides how to act in order to retrace the path under noisy actuation and a changing environment. The two networks are optimized end-to-end at training time. We evaluate the method in two realistic simulators, performing path following both forwards and backwards. Our experiments show that our approach outperforms both a classical approach to solving this task as well as a number of other baselines.",https://proceedings.neurips.cc/paper_files/paper/2018/file/66368270ffd51418ec58bd793f2d9b1b-Paper.pdf,2018,"path, approach, abstraction, backwards, end, environment, forwards, network, retrace, two","path, agent, environment, rpf, noise, actuation, actions, visual, following, task","{'following': 0.5499946918465706, 'path': 0.4802388661234554, 'memory': 0.4354915575094963, 'visual': 0.3916899565294984, 'robust': 0.35185586778811384}","path, forwards, retrace, backwards, abstraction, environment, end, actuation, approach, simulators"
Representation Learning of Compositional Data,"Marta Avalos, Richard Nock, Cheng Soon Ong, Julien Rouar, Ke Sun","We consider the problem of learning a low dimensional representation for compositional data. Compositional data consists of a collection of nonnegative data that sum to a constant value. Since the parts of the collection are statistically dependent, many standard tools cannot be directly applied. Instead, compositional data must be first transformed before analysis. Focusing on principal component analysis (PCA), we propose an approach that allows low dimensional representation learning directly from the original data. Our approach combines the benefits of the log-ratio transformation from compositional data analysis and exponential family PCA. A key tool in its derivation is a generalization of the scaled Bregman theorem, that relates the perspective transform of a Bregman divergence to the Bregman divergence of a perspective transform and a remainder conformal divergence. Our proposed approach includes a convenient surrogate (upper bound) loss of the exponential family PCA which has an easy to optimize form. We also derive the corresponding form for nonlinear autoencoders. Experiments on simulated data and microbiome data show the promise of our method.",https://proceedings.neurips.cc/paper_files/paper/2018/file/664dd858db942cad06f24ff25df56716-Paper.pdf,2018,"data, compositional, analysis, approach, bregman, divergence, pca, collection, dimensional, directly","pca, coda, data, clr, bregman, log, exp, kl, xi, convex","{'compositional': 0.7029433884499102, 'representation': 0.5182406115166863, 'data': 0.42151947749365254, 'learning': 0.24416918581763766}","compositional, bregman, pca, data, divergence, transform, collection, perspective, exponential, analysis"
How SGD Selects the Global Minima in Over-parameterized Learning: A Dynamical Stability Perspective,"Lei Wu, Chao Ma, Weinan E","The question of which global minima are accessible by a stochastic gradient decent (SGD)  algorithm with specific learning rate and batch size is studied from the perspective of dynamical stability.  The concept of non-uniformity is introduced, which, together with sharpness, characterizes the stability property of a global minimum and hence the accessibility of a particular SGD algorithm to that global minimum. In particular, this analysis shows that  learning rate and batch size play different roles in minima selection.  Extensive empirical results seem to correlate well with the theoretical findings and provide further support to these  claims.",https://proceedings.neurips.cc/paper_files/paper/2018/file/6651526b6fb8f29a00507de6a49ce30f-Paper.pdf,2018,"global, algorithm, batch, learning, minima, minimum, particular, rate, sgd, size","sgd, sharpness, non, minima, learning, uniformity, gd, global, rate, batch","{'selects': 0.3985090567346993, 'perspective': 0.3761434659027037, 'stability': 0.3761434659027037, 'parameterized': 0.3602748266696562, 'minima': 0.32940623283556086, 'dynamical': 0.3220405966046132, 'global': 0.31554364500566506, 'sgd': 0.31554364500566506, 'learning': 0.1306543959715071}","global, stability, minima, sgd, minimum, batch, decent, roles, size, rate"
TADAM: Task dependent adaptive metric for improved few-shot learning,"Boris Oreshkin, Pau Rodríguez López, Alexandre Lacoste","Few-shot learning has become essential for producing models that generalize from few examples. In this work, we identify that metric scaling and metric task conditioning are important to improve the performance of few-shot algorithms. Our analysis reveals that simple metric scaling completely changes the nature of few-shot algorithm parameter updates. Metric scaling provides improvements up to 14% in accuracy for certain metrics on the mini-Imagenet 5-way 5-shot classification task. We further propose a simple and effective way of conditioning a learner on the task sample set, resulting in learning a task-dependent metric space. Moreover, we propose and empirically test a practical end-to-end optimization procedure based on auxiliary task co-training to learn a task-dependent metric space. The resulting few-shot learning model based on the task-dependent scaled metric achieves state of the art on mini-Imagenet. We confirm these results on another few-shot dataset that we introduce in this paper based on CIFAR100.",https://proceedings.neurips.cc/paper_files/paper/2018/file/66808e327dc79d135ba18e051673d906-Paper.pdf,2018,"metric, task, shot, based, dependent, learning, scaling, conditioning, end, imagenet","task, shot, learning, metric, feature, training, set, extractor, classiﬁcation, scaling","{'tadam': 0.4622645028069014, 'dependent': 0.3919696234214974, 'metric': 0.3735622409509648, 'task': 0.3660258749640616, 'shot': 0.34761849249352905, 'improved': 0.33775513709744837, 'adaptive': 0.3182152444136144, 'learning': 0.1515571311934147}","metric, shot, task, scaling, dependent, conditioning, mini, imagenet, resulting, end"
A Bayes-Sard Cubature Method,"Toni Karvonen, Chris J. Oates, Simo Sarkka","This paper focusses on the formulation of numerical integration as an inferential task. To date, research effort has largely focussed on the development of Bayesian cubature, whose distributional output provides uncertainty quantification for the integral. However, the point estimators associated to Bayesian cubature can be inaccurate and acutely sensitive to the prior when the domain is high-dimensional. To address these drawbacks we introduce Bayes-Sard cubature, a probabilistic framework that combines the flexibility of Bayesian cubature with the robustness of classical cubatures which are well-established. This is achieved by considering a Gaussian process model for the integrand whose mean is a parametric regression model, with an improper prior on each regression coefficient. The features in the regression model consist of test functions which are guaranteed to be exactly integrated, with remaining degrees of freedom afforded to the non-parametric part. The asymptotic convergence of the Bayes-Sard cubature method is established and the theoretical results are numerically verified. In particular, we report two orders of magnitude reduction in error compared to Bayesian cubature in the context of a high-dimensional financial integral.",https://proceedings.neurips.cc/paper_files/paper/2018/file/6775a0635c302542da2c32aa19d86be0-Paper.pdf,2018,"cubature, bayesian, model, regression, bayes, dimensional, established, high, integral, parametric","10, bsc, bc, cubature, kx, posterior, sec, kernel, gaussian, method","{'cubature': 0.561387007551175, 'sard': 0.561387007551175, 'bayes': 0.4536644871241068, 'method': 0.4048182167688294}","cubature, sard, bayesian, regression, established, integral, bayes, parametric, whose, acutely"
Learning to Infer Graphics Programs from Hand-Drawn Images,"Kevin Ellis, Daniel Ritchie, Armando Solar-Lezama, Josh Tenenbaum","We introduce a model that learns to convert simple hand drawings
  into graphics programs written in a subset of \LaTeX.~The model
  combines techniques from deep learning and program synthesis.  We
  learn a convolutional neural network that proposes plausible drawing
  primitives that explain an image. These drawing primitives are a
  specification (spec) of what the graphics program needs to draw.  We
  learn a model that uses program synthesis techniques to recover a
  graphics program from that spec. These programs have constructs like
  variable bindings, iterative loops, or simple kinds of
  conditionals. With a graphics program in hand, we can correct errors
  made by the deep network and extrapolate drawings.",https://proceedings.neurips.cc/paper_files/paper/2018/file/6788076842014c83cedadbe6b0ba0314-Paper.pdf,2018,"program, graphics, model, deep, drawing, drawings, hand, learn, network, primitives","program, programs, model, search, line, network, spec, drawing, drawings, rectangle","{'drawn': 0.44530615405064533, 'graphics': 0.420314161853347, 'hand': 0.420314161853347, 'infer': 0.4025820611959964, 'programs': 0.3680885545393494, 'images': 0.3598579683413474, 'learning': 0.14599720030607768}","graphics, program, drawings, spec, primitives, drawing, synthesis, programs, hand, bindings"
Neural Guided Constraint Logic Programming for Program Synthesis,"Lisa Zhang, Gregory Rosenblatt, Ethan Fetaya, Renjie Liao, William Byrd, Matthew Might, Raquel Urtasun, Richard Zemel","Synthesizing programs using example input/outputs is a classic problem in artificial intelligence. We present a method for solving Programming By Example (PBE) problems by using a neural model to guide the search of a constraint logic programming system called miniKanren. Crucially, the neural model uses miniKanren's internal representation as input; miniKanren represents a PBE problem as recursive constraints imposed by the provided examples. We explore Recurrent Neural Network and Graph Neural Network models. We contribute a modified miniKanren, drivable by an external agent, available at https://github.com/xuexue/neuralkanren. We show that our neural-guided approach using constraints can synthesize programs faster in many cases, and importantly, can generalize to larger problems.",https://proceedings.neurips.cc/paper_files/paper/2018/file/67d16d00201083a2b118dd5128dd6f59-Paper.pdf,2018,"neural, minikanren, using, constraints, example, input, model, network, pbe, problem","constraints, constraint, neural, minikanren, program, search, model, evalo, synthesis, use","{'logic': 0.43964631444503843, 'constraint': 0.42462594041074925, 'programming': 0.4019771171916188, 'guided': 0.38506048800488785, 'program': 0.37796839708715513, 'synthesis': 0.3553195738680247, 'neural': 0.21779494481403008}","minikanren, pbe, neural, programs, programming, constraints, example, drivable, neuralkanren, xuexue"
Overcoming Language Priors in Visual Question Answering with Adversarial Regularization,"Sainandan Ramakrishnan, Aishwarya Agrawal, Stefan Lee","Modern Visual Question Answering (VQA) models have been shown to rely heavily on superficial correlations between question and answer words learned during training -- \eg overwhelmingly reporting the type of room as kitchen or the sport being played as tennis, irrespective of the image. Most alarmingly, this shortcoming is often not well reflected during evaluation because the same strong priors exist in test distributions; however, a VQA system that fails to ground questions in image content would likely perform poorly in real-world settings.In this work, we present a novel regularization scheme for VQA that reduces this effect. We introduce a question-only model that takes as input the question encoding from the VQA model and must leverage language biases in order to succeed. We then pose training as an adversarial game between the VQA model and this question-only adversary -- discouraging the VQA model from capturing language biases in its question encoding.Further, we leverage this question-only model to estimate the mutual information between the image and answer given the question, which we maximize explicitly to encourage visual grounding. Our approach is a model agnostic training procedure and simple to implement. We show empirically that it can improve performance significantly on a bias-sensitive split of the VQA dataset for multiple base models -- achieving state-of-the-art on this task. Further, on standard VQA tasks, our approach shows significantly less drop in accuracy compared to existing bias-reducing VQA models.",https://proceedings.neurips.cc/paper_files/paper/2018/file/67d96d458abdef21792e6d8e590244e7-Paper.pdf,2018,"vqa, question, model, image, models, training, answer, approach, bias, biases","vqa, question, model, san, models, image, cp, performance, language, answer","{'overcoming': 0.42871480512189303, 'answering': 0.3754453864488074, 'question': 0.3754453864488074, 'priors': 0.3670502989686322, 'language': 0.34702918723915466, 'regularization': 0.3234722975483961, 'visual': 0.3234722975483961, 'adversarial': 0.26431289436310484}","vqa, question, model, biases, answer, encoding, bias, image, leverage, language"
New Insight into Hybrid Stochastic Gradient Descent: Beyond With-Replacement Sampling and Convexity,"Pan Zhou, Xiaotong Yuan, Jiashi Feng","As an incremental-gradient algorithm, the hybrid stochastic gradient descent (HSGD)  enjoys  merits of both stochastic and full gradient methods for finite-sum minimization problem. However, the existing rate-of-convergence analysis for HSGD is made under with-replacement sampling (WRS) and is restricted to convex problems. It is not clear whether HSGD still carries these advantages under the common practice of without-replacement sampling (WoRS) for non-convex problems. In this paper, we affirmatively answer this open question by showing that under WoRS and for both convex and non-convex problems, it is still possible for HSGD (with constant step-size) to match full gradient descent in rate of convergence, while maintaining comparable sample-size-independent incremental first-order oracle  complexity to stochastic gradient descent. For a special class of finite-sum problems with linear prediction models, our convergence results can be further improved in some cases. Extensive numerical results confirm our theoretical affirmation and demonstrate the favorable efficiency of WoRS-based HSGD.",https://proceedings.neurips.cc/paper_files/paper/2018/file/67e103b0761e60683e83c559be18d40c-Paper.pdf,2018,"gradient, hsgd, convex, problems, convergence, descent, stochastic, wors, finite, full","hsgd, convex, wors, problems, ifo, sgd, non, strongly, complexity, gradient","{'convexity': 0.38903793474061166, 'insight': 0.38903793474061166, 'replacement': 0.3672038932816099, 'new': 0.3517123943306123, 'beyond': 0.3298783528716105, 'hybrid': 0.31438685392061294, 'sampling': 0.2575190805626692, 'descent': 0.2552272720516118, 'gradient': 0.2238252107736226, 'stochastic': 0.22258723965353638}","hsgd, wors, gradient, convex, replacement, incremental, problems, descent, convergence, stochastic"
Variational PDEs for Acceleration on Manifolds and Application to Diffeomorphisms,"Ganesh Sundaramoorthi, Anthony Yezzi","We consider the optimization of cost functionals on manifolds and derive a variational approach to accelerated methods on manifolds. We demonstrate the methodology on the infinite-dimensional manifold of diffeomorphisms, motivated by registration problems in computer vision. We build on the variational approach to accelerated optimization by Wibisono, Wilson and Jordan, which applies in finite dimensions, and generalize that approach to infinite dimensional manifolds. We derive the continuum evolution equations, which are partial differential equations (PDE), and relate them to simple mechanical principles. Our approach can also be viewed as a generalization of the $L^2$ optimal mass transport problem. Our approach evolves an infinite number of particles endowed with mass, represented as a mass density. The density evolves with the optimization variable, and endows the particles with dynamics. This is different than current accelerated methods where only a single particle moves and hence the dynamics does not depend on the mass. We derive the theory, compute the PDEs for acceleration, and illustrate the behavior of this new accelerated optimization scheme.",https://proceedings.neurips.cc/paper_files/paper/2018/file/68148596109e38cf9367d27875e185be-Paper.pdf,2018,"approach, accelerated, mass, optimization, derive, infinite, manifolds, density, dimensional, dynamics","rn, gradient, descent, accelerated, image, energy, action, mass, optimization, potential","{'diffeomorphisms': 0.46613808645325444, 'pdes': 0.46613808645325444, 'manifolds': 0.42141531155432, 'application': 0.40701779383302966, 'acceleration': 0.3766925366553856, 'variational': 0.28333540589625644}","mass, accelerated, manifolds, infinite, evolves, particles, equations, approach, derive, optimization"
Transfer Learning from Speaker Verification to Multispeaker Text-To-Speech Synthesis,"Ye Jia, Yu Zhang, Ron Weiss, Quan Wang, Jonathan Shen, Fei Ren, zhifeng Chen, Patrick Nguyen, Ruoming Pang, Ignacio Lopez Moreno, Yonghui Wu","We describe a neural network-based system for text-to-speech (TTS) synthesis that is able to generate speech audio in the voice of many different speakers, including those unseen during training. Our system consists of three independently trained components: (1) a speaker encoder network, trained on a speaker verification task using an independent dataset of noisy speech from thousands of speakers without transcripts, to generate a fixed-dimensional embedding vector from seconds of reference speech from a target speaker; (2) a sequence-to-sequence synthesis network based on Tacotron 2, which generates a mel spectrogram from text, conditioned on the speaker embedding; (3) an auto-regressive WaveNet-based vocoder that converts the mel spectrogram into a sequence of time domain waveform samples. We demonstrate that the proposed model is able to transfer the knowledge of speaker variability learned by the discriminatively-trained speaker encoder to the new task, and is able to synthesize natural speech from speakers that were not seen during training. We quantify the importance of training the speaker encoder on a large and diverse speaker set in order to obtain the best generalization performance. Finally, we show that randomly sampled speaker embeddings can be used to synthesize speech in the voice of novel speakers dissimilar from those used in training, indicating that the model has learned a high quality speaker representation.",https://proceedings.neurips.cc/paper_files/paper/2018/file/6832a7b24bc06775d02b7406880b93fc-Paper.pdf,2018,"speaker, speech, speakers, training, able, based, encoder, network, sequence, trained","speaker, speakers, speech, training, encoder, model, network, trained, embedding, librispeech","{'multispeaker': 0.4315767304093897, 'speaker': 0.4073552769648, 'speech': 0.39016988225292, 'verification': 0.39016988225292, 'text': 0.3487630340964503, 'transfer': 0.31976003304462164, 'synthesis': 0.3153330114304327, 'learning': 0.1414958984596823}","speaker, speech, speakers, mel, spectrogram, encoder, voice, sequence, synthesize, synthesis"
Learning to Solve SMT Formulas,"Mislav Balunovic, Pavol Bielik, Martin Vechev","We present a new approach for learning to solve SMT formulas. We phrase the challenge of solving SMT formulas as a tree search problem where at each step a transformation is applied to the input formula until the formula is solved. Our approach works in two phases: first, given a dataset of unsolved formulas we learn a policy that for each formula selects a suitable transformation to apply at each step in order to solve the formula, and second, we synthesize a strategy in the form of a loop-free program with branches. This strategy is an interpretable representation of the policy decisions and is used to guide the SMT solver to decide formulas more efficiently, without requiring any modification to the solver itself and without needing to evaluate the learned policy at inference time. We show that our approach is effective in practice - it solves 17% more formulas over a range of benchmarks and achieves up to 100x runtime improvement over a state-of-the-art SMT solver.",https://proceedings.neurips.cc/paper_files/paper/2018/file/68331ff0427b551b68e911eebe35233b-Paper.pdf,2018,"formulas, formula, smt, approach, policy, solver, solve, step, strategy, transformation","strategy, formulas, formula, strategies, smt, state, tactics, learning, dataset, model","{'formulas': 0.5775048336152061, 'smt': 0.5775048336152061, 'solve': 0.5450934326850231, 'learning': 0.1893395995184431}","formulas, smt, formula, solver, policy, transformation, strategy, solve, step, branches"
Learning to Repair Software Vulnerabilities with Generative Adversarial Networks,"Jacob Harer, Onur Ozdemir, Tomo Lazovich, Christopher Reale, Rebecca Russell, Louis Kim, peter chin","Motivated by the problem of automated repair of software vulnerabilities, we propose an adversarial learning approach that maps from one discrete source domain to another target domain without requiring paired labeled examples or source and target domains to be bijections. We demonstrate that the proposed adversarial learning approach is an effective technique for repairing software vulnerabilities, performing close to seq2seq approaches that require labeled pairs. The proposed Generative Adversarial Network approach is application-agnostic in that it can be applied to other problems similar to code repair, such as grammar correction or sentiment translation.",https://proceedings.neurips.cc/paper_files/paper/2018/file/68abef8ee1ac9b664a90b0bbaff4f770-Paper.pdf,2018,"adversarial, approach, domain, labeled, learning, proposed, repair, software, source, target","data, one, loss, approach, discriminator, gan, training, sequence, generator, code","{'repair': 0.49842644393011704, 'software': 0.49842644393011704, 'vulnerabilities': 0.49842644393011704, 'generative': 0.30714395229618446, 'adversarial': 0.2900455927131708, 'networks': 0.22258827646833618, 'learning': 0.16341311412470438}","repair, vulnerabilities, software, adversarial, labeled, source, target, domain, bijections, repairing"
Algorithmic Linearly Constrained Gaussian Processes,Markus Lange-Hegermann,"We algorithmically construct multi-output Gaussian process priors which satisfy linear differential equations. Our approach attempts to parametrize all solutions of the equations using Gröbner bases. If successful, a push forward Gaussian process along the paramerization is the desired prior. We consider several examples from physics, geomathmatics and control, among them the full inhomogeneous system of Maxwell's equations. By bringing together stochastic learning and computeralgebra in a novel way, we combine noisy observations with precise algebraic computations.",https://proceedings.neurips.cc/paper_files/paper/2018/file/68b1fbe7f16e4ae3024973f12f3cb313-Paper.pdf,2018,"equations, gaussian, process, algebraic, algorithmically, along, among, approach, attempts, bases","gaussian, process, equations, solf, set, functions, function, differential, example, covariance","{'linearly': 0.5497690091931877, 'algorithmic': 0.49702241680114373, 'constrained': 0.42729521440852886, 'processes': 0.37454862201648487, 'gaussian': 0.357568012015914}","equations, gaussian, algorithmically, bringing, computeralgebra, geomathmatics, gröbner, inhomogeneous, maxwell, paramerization"
RenderNet: A deep convolutional network for differentiable rendering from 3D shapes,"Thu H. Nguyen-Phuoc, Chuan Li, Stephen Balaban, Yongliang Yang","Traditional computer graphics rendering pipelines are designed for procedurally
generating 2D images from 3D shapes with high performance. The nondifferentiability due to discrete operations (such as visibility computation) makes it hard to explicitly correlate rendering parameters and the resulting image, posing a significant challenge for inverse rendering tasks. Recent work on differentiable rendering achieves differentiability either by designing surrogate gradients for non-differentiable operations or via an approximate but differentiable renderer. These methods, however, are still limited when it comes to handling occlusion, and restricted to particular rendering effects. We present RenderNet, a differentiable rendering convolutional network with a novel projection unit that can render 2D images from 3D shapes. Spatial occlusion and shading calculation are automatically encoded in the network. Our experiments show that RenderNet can successfully learn to implement different shaders, and can be used in inverse rendering tasks to estimate shape, pose, lighting and texture from a single image.",https://proceedings.neurips.cc/paper_files/paper/2018/file/68d3743587f71fbaa5062152985aff40-Paper.pdf,2018,"rendering, differentiable, 2d, 3d, image, images, inverse, network, occlusion, operations","rendernet, rendering, 3d, shape, image, network, input, images, texture, based","{'rendering': 0.4410839506241177, 'rendernet': 0.4410839506241177, 'shapes': 0.41632892186002446, 'differentiable': 0.3564459482442227, '3d': 0.3316909194801294, 'convolutional': 0.29468425983846386, 'network': 0.26633681771639356, 'deep': 0.20667228230552864}","rendering, differentiable, rendernet, occlusion, shapes, 2d, 3d, inverse, operations, nondifferentiability"
Universal Growth in Production Economies,"Simina Branzei, Ruta Mehta, Noam Nisan","We study a simple variant of the von Neumann model of an expanding economy, in which multiple producers make goods according to their production function. The players trade their goods at the market and then use the bundles received as inputs for the production in the next round.  The decision that players have to make is how to invest their money (i.e. bids) in each round.We show that a simple decentralized dynamic, where players update their  bids on the goods in the market proportionally to how useful the investments were, leads to growth of the economy in the long term (whenever growth is possible) but also creates unbounded inequality, i.e. very rich and very poor players emerge. We analyze several other phenomena, such as how the relation of a player with others influences its development and the Gini index of the system.",https://proceedings.neurips.cc/paper_files/paper/2018/file/692f93be8c7a41525c0baf2076aecfb4-Paper.pdf,2018,"players, goods, bids, economy, growth, make, market, production, round, simple","grant, players, growth, production, research, round, university, bids, economy, edu","{'economies': 0.506961257733394, 'growth': 0.506961257733394, 'production': 0.506961257733394, 'universal': 0.4785089857799178}","players, goods, bids, economy, market, production, growth, round, make, bundles"
Neural Ordinary Differential Equations,"Ricky T. Q. Chen, Yulia Rubanova, Jesse Bettencourt, David K. Duvenaud","We introduce a new family of deep neural network models. Instead of specifying a discrete sequence of hidden layers, we parameterize the derivative of the hidden state using a neural network. The output of the network is computed using a blackbox differential equation solver. These continuous-depth models have constant memory cost, adapt their evaluation strategy to each input, and can explicitly trade numerical precision for speed. We demonstrate these properties in continuous-depth residual networks and continuous-time latent variable models. We also construct continuous normalizing flows, a generative model that can train by maximum likelihood, without partitioning or ordering the data dimensions. For training, we show how to scalably backpropagate through any ODE solver, without access to its internal operations. This allows end-to-end training of ODEs within larger models.",https://proceedings.neurips.cc/paper_files/paper/2018/file/69386f6bb1dfed68692a24c8686939b9-Paper.pdf,2018,"continuous, models, network, depth, end, hidden, neural, solver, training, using","ode, time, al, et, continuous, latent, hidden, neural, model, state","{'ordinary': 0.6175691297379842, 'equations': 0.5583175775124346, 'differential': 0.479991285959795, 'neural': 0.27658311235138333}","continuous, solver, depth, models, hidden, network, end, backpropagate, odes, scalably"
MetaAnchor: Learning to Detect Objects with Customized Anchors,"Tong Yang, Xiangyu Zhang, Zeming Li, Wenqiang Zhang, Jian Sun","We propose a novel and flexible anchor mechanism named MetaAnchor for object detection frameworks. Unlike many previous detectors model anchors via a predefined manner, in MetaAnchor anchor functions could be dynamically generated from the arbitrary customized prior boxes. Taking advantage of weight prediction, MetaAnchor is able to work with most of the anchor-based object detection systems such as RetinaNet. Compared with the predefined anchor scheme, we empirically find that MetaAnchor is more robust to anchor settings and bounding box distributions; in addition, it also shows the potential on the transfer task. Our experiment on COCO detection task shows MetaAnchor consistently outperforms the counterparts in various scenarios.",https://proceedings.neurips.cc/paper_files/paper/2018/file/69adc1e107f7f7d035d7baf04342e1ca-Paper.pdf,2018,"anchor, metaanchor, detection, object, predefined, shows, task, able, addition, advantage","anchor, metaanchor, box, 27, boxes, bi, anchors, detection, training, results","{'anchors': 0.4472854190158049, 'customized': 0.4472854190158049, 'detect': 0.4472854190158049, 'metaanchor': 0.4472854190158049, 'objects': 0.4221823441978967, 'learning': 0.14664611822681303}","metaanchor, anchor, predefined, detection, shows, object, anchors, retinanet, customized, detectors"
ChannelNets: Compact and Efficient Convolutional Neural Networks via Channel-Wise Convolutions,"Hongyang Gao, Zhengyang Wang, Shuiwang Ji","Convolutional neural networks (CNNs) have shown great capability of solving various artificial intelligence tasks. However, the increasing model size has raised challenges in employing them in resource-limited applications. In this work, we propose to compress deep models by using channel-wise convolutions, which replace dense connections among feature maps with sparse ones in CNNs. Based on this novel operation, we build light-weight CNNs known as ChannelNets. ChannelNets use three instances of channel-wise convolutions; namely group channel-wise convolutions, depth-wise separable channel-wise convolutions, and the convolutional classification layer. Compared to prior CNNs designed for mobile devices, ChannelNets achieve a significant reduction in terms of the number of parameters and computational cost without loss in accuracy. Notably, our work represents the first attempt to compress the fully-connected classification layer, which usually accounts for about 25% of total parameters in compact CNNs. Experimental results on the ImageNet dataset demonstrate that ChannelNets achieve consistently better performance compared to prior methods.",https://proceedings.neurips.cc/paper_files/paper/2018/file/6a4cbdaedcbda0fa8ddc7ea32073c475-Paper.pdf,2018,"cnns, wise, channel, channelnets, convolutions, achieve, classification, compared, compress, convolutional","wise, convolution, channel, convolutions, layer, group, depth, channelnet, parameters, df","{'channelnets': 0.4249835392968462, 'channel': 0.40113211662623327, 'wise': 0.37108287781436516, 'compact': 0.3603578402560244, 'convolutions': 0.3603578402560244, 'convolutional': 0.28392771839469, 'efficient': 0.24183147973272154, 'via': 0.2229797697448626, 'neural': 0.1903321658041719, 'networks': 0.18978999748408512}","channelnets, cnns, wise, channel, convolutions, compress, layer, convolutional, compared, prior"
On Controllable Sparse Alternatives to Softmax,"Anirban Laha, Saneem Ahmed Chemmengath, Priyanka Agrawal, Mitesh Khapra, Karthik Sankaranarayanan, Harish G. Ramaswamy","Converting an n-dimensional vector to a probability distribution over n objects is a commonly used component in many machine learning tasks like multiclass classification, multilabel classification, attention mechanisms etc. For this, several probability mapping functions have been proposed and employed in literature such as softmax, sum-normalization, spherical softmax, and sparsemax, but there is very little understanding in terms how they relate with each other. Further, none of the above formulations offer an explicit control over the degree of sparsity. To address this, we develop a unified framework that encompasses all these formulations as special cases. This framework ensures simple closed-form solutions and existence of sub-gradients suitable for learning via backpropagation. Within this framework, we propose two novel sparse formulations, sparsegen-lin and sparsehourglass, that seek to provide a control over the degree of desired sparsity. We further develop novel convex loss functions that help induce the behavior of aforementioned formulations in the multilabel classification setting, showing improved performance. We also demonstrate empirically that the proposed formulations, when used to compute attention weights, achieve better or comparable performance on standard seq2seq tasks like neural machine translation and abstractive summarization.",https://proceedings.neurips.cc/paper_files/paper/2018/file/6a4d5952d4c018a1c1af9fa590a10dda-Paper.pdf,2018,"formulations, classification, framework, attention, control, degree, develop, functions, learning, like","softmax, sparsemax, sparsegen, sparse, loss, functions, function, probability, mapping, formulations","{'alternatives': 0.5575018724335506, 'controllable': 0.5575018724335506, 'softmax': 0.48679391101083597, 'sparse': 0.3760518749706543}","formulations, multilabel, degree, softmax, classification, sparsity, attention, like, control, framework"
Gaussian Process Conditional Density Estimation,"Vincent Dutordoir, Hugh Salimbeni, James Hensman, Marc Deisenroth","Conditional Density Estimation (CDE) models deal with estimating conditional distributions. The conditions imposed on the distribution are the inputs of the model. CDE is a challenging task as there is a fundamental trade-off between model complexity, representational capacity and overfitting. In this work, we propose to extend the model's input with latent variables and use Gaussian processes (GP) to map this augmented input onto samples from the conditional distribution. Our Bayesian approach allows for the modeling of small datasets, but we also provide the machinery for it to be applied to big data using stochastic variational inference. Our approach can be used to model densities even in sparse data regions, and allows for sharing learned structure between conditions. We illustrate the effectiveness and wide-reaching applicability of our model on a variety of real-world problems, such as spatio-temporal density estimation of taxi drop-offs, non-Gaussian noise modeling, and few-shot learning on omniglot images.",https://proceedings.neurips.cc/paper_files/paper/2018/file/6a61d423d02a1c56250dc23ae7ff12f3-Paper.pdf,2018,"model, conditional, allows, approach, cde, conditions, data, density, distribution, estimation","gp, model, wn, gaussian, latent, data, use, cde, variational, conditional","{'density': 0.5302883880066128, 'conditional': 0.47401143850312627, 'process': 0.4345928278550636, 'estimation': 0.39961735785990976, 'gaussian': 0.38150022672018014}","cde, conditional, model, density, conditions, modeling, estimation, gaussian, machinery, allows"
Deep Network for the Integrated 3D Sensing of Multiple People in Natural Images,"Andrei Zanfir, Elisabeta Marinoiu, Mihai Zanfir, Alin-Ionut Popa, Cristian Sminchisescu","We present MubyNet -- a feed-forward, multitask, bottom up system for the integrated localization, as well as 3d pose and shape estimation, of multiple people in monocular images. The challenge is the formal modeling of the problem that intrinsically requires discrete and continuous computation, e.g. grouping people vs. predicting 3d pose. The model identifies human body structures (joints and limbs) in images, groups them based on 2d and 3d information fused using learned scoring functions, and optimally aggregates such responses into partial or complete 3d human skeleton hypotheses under kinematic tree constraints, but without knowing in advance the number of people in the scene and their visibility relations. We design a multi-task deep neural network with differentiable stages where the person grouping problem is formulated as an integer program based on learned body part scores parameterized by both 2d and 3d information. This avoids suboptimality resulting from separate 2d and 3d reasoning, with grouping performed based on the combined representation. The final stage of 3d pose and shape prediction is based on a learned attention process where information from different human body parts is optimally integrated. State-of-the-art results are obtained in large scale datasets like Human3.6M and Panoptic, and qualitatively by reconstructing the 3d shape and pose of multiple people, under occlusion, in difficult monocular images.",https://proceedings.neurips.cc/paper_files/paper/2018/file/6a6610feab86a1f294dbbf5855c74af9-Paper.pdf,2018,"3d, based, people, pose, 2d, body, grouping, human, images, information","3d, 2d, person, people, pose, multiple, joint, volume, shape, limb","{'people': 0.41385467382553687, 'sensing': 0.41385467382553687, 'integrated': 0.374148136307437, 'images': 0.33444159878933716, 'natural': 0.33444159878933716, '3d': 0.31121476330777104, 'multiple': 0.31121476330777104, 'network': 0.24989514279035893, 'deep': 0.19391385667356237}","3d, people, grouping, pose, body, 2d, shape, monocular, optimally, human"
Learning Attentional Communication for Multi-Agent Cooperation,"Jiechuan Jiang, Zongqing Lu","Communication could potentially be an effective way for multi-agent cooperation. However, information sharing among all agents or in predefined communication architectures that existing methods adopt can be problematic. When there is a large number of agents, agents cannot differentiate valuable information that helps cooperative decision making from globally shared information. Therefore, communication barely helps, and could even impair the learning of multi-agent cooperation. Predefined communication architectures, on the other hand, restrict communication among agents and thus restrain potential cooperation. To tackle these difficulties, in this paper, we propose an attentional communication model that learns when communication is needed and how to integrate shared information for cooperative decision making. Our model leads to efficient and effective communication for large-scale multi-agent cooperation. Empirically, we show the strength of our model in a variety of cooperative scenarios, where agents are able to develop more coordinated and sophisticated strategies than existing methods.",https://proceedings.neurips.cc/paper_files/paper/2018/file/6a8018b3a00b69c008601b8becae392b-Paper.pdf,2018,"communication, agents, cooperation, information, agent, cooperative, model, multi, among, architectures","agents, communication, atoc, agent, commnet, information, reward, attention, bicnet, ddpg","{'attentional': 0.5226910527783637, 'cooperation': 0.49335597489136723, 'agent': 0.4320546490043726, 'communication': 0.4062495006659573, 'multi': 0.31987016024110676, 'learning': 0.17136846108351417}","communication, cooperation, agents, cooperative, predefined, agent, information, helps, multi, shared"
Speaker-Follower Models for Vision-and-Language Navigation,"Daniel Fried, Ronghang Hu, Volkan Cirik, Anna Rohrbach, Jacob Andreas, Louis-Philippe Morency, Taylor Berg-Kirkpatrick, Kate Saenko, Dan Klein, Trevor Darrell","Navigation guided by natural language instructions presents a challenging reasoning problem for instruction followers. Natural language instructions typically identify only a few high-level decisions and landmarks rather than complete low-level motor behaviors; much of the missing information must be inferred based on perceptual context. In machine learning settings, this is doubly challenging: it is difficult to collect enough annotated data to enable learning of this reasoning process from scratch, and also difficult to implement the reasoning process using generic sequence models. Here we describe an approach to vision-and-language navigation that addresses both these issues with an embedded speaker model. We use this speaker model to (1) synthesize new instructions for data augmentation and to (2) implement pragmatic reasoning, which evaluates how well candidate action sequences explain an instruction. Both steps are supported by a panoramic action space that reflects the granularity of human-generated instructions. Experiments show that all three components of this approach---speaker-driven data augmentation, pragmatic reasoning and panoramic action space---dramatically improve the performance of a baseline instruction follower, more than doubling the success rate over the best existing approach on a standard benchmark.",https://proceedings.neurips.cc/paper_files/paper/2018/file/6a81681a7af700c6385d36577ebec359-Paper.pdf,2018,"reasoning, instructions, action, approach, data, instruction, language, speaker, augmentation, challenging","model, follower, speaker, instruction, training, routes, navigation, using, language, data","{'follower': 0.4679206559483738, 'navigation': 0.4679206559483738, 'speaker': 0.44165946625664887, 'vision': 0.42302685564604753, 'language': 0.35750742400404406, 'models': 0.2455079561313441}","instructions, reasoning, instruction, speaker, panoramic, pragmatic, navigation, action, language, augmentation"
Training DNNs with Hybrid Block Floating Point,"Mario Drumond, Tao LIN, Martin Jaggi, Babak Falsafi","The wide adoption of DNNs has given birth to unrelenting computing requirements, forcing datacenter operators to adopt domain-specific accelerators to train them. These accelerators typically employ densely packed full-precision floating-point arithmetic to maximize performance per area. Ongoing research efforts seek to further increase that performance density by replacing floating-point with fixed-point arithmetic. However, a significant roadblock for these attempts has been fixed point's narrow dynamic range, which is insufficient for DNN training convergence. We identify block floating point (BFP) as a promising alternative representation since it exhibits wide dynamic range and enables the majority of DNN operations to be performed with fixed-point logic. Unfortunately, BFP alone introduces several limitations that preclude its direct applicability. In this work, we introduce HBFP, a hybrid BFP-FP approach, which performs all dot products in BFP and other operations in floating point. HBFP delivers the best of both worlds: the high accuracy of floating point at the superior hardware density of fixed point. For a wide variety of models, we show that HBFP matches floating point's accuracy while enabling hardware implementations that deliver up to 8.5x higher throughput.",https://proceedings.neurips.cc/paper_files/paper/2018/file/6a9aeddfc689c1d0e3b9ccc3ab651bc5-Paper.pdf,2018,"point, floating, bfp, fixed, hbfp, wide, accelerators, accuracy, arithmetic, density","bfp, point, training, bit, ﬂoating, arithmetic, hbfp, operations, fp32, ﬁxed","{'floating': 0.4746413610920462, 'dnns': 0.43908546561949635, 'block': 0.4156653959947208, 'hybrid': 0.4063709753204821, 'point': 0.37257731982259706, 'training': 0.32433097595795934}","floating, point, bfp, hbfp, fixed, accelerators, wide, arithmetic, hardware, dnn"
Coupled Variational Bayes via Optimization Embedding,"Bo Dai, Hanjun Dai, Niao He, Weiyang Liu, Zhen Liu, Jianshu Chen, Lin Xiao, Le Song","Variational inference plays a vital role in learning graphical models, especially on large-scale datasets. Much of its success depends on a proper choice of auxiliary distribution class for posterior approximation. However, how to pursue an auxiliary distribution class that achieves both good approximation ability and computation efficiency remains a core challenge.  In this paper, we proposed coupled variational Bayes which exploits the primal-dual view of the ELBO with the variational distribution class generated by an optimization procedure, which is termed optimization embedding. This flexible function class couples the variational distribution with the original parameters in the graphical models, allowing end-to-end learning of the graphical models by back-propagation through the variational distribution. Theoretically,  we establish an interesting connection to gradient flow and demonstrate the extreme flexibility of this implicit distribution family in the limit sense. Empirically, we demonstrate the effectiveness of the proposed method on multiple graphical models with either continuous or discrete latent variables comparing to state-of-the-art methods.",https://proceedings.neurips.cc/paper_files/paper/2018/file/6aaba9a124857622930ca4e50f5afed2-Paper.pdf,2018,"distribution, variational, class, graphical, models, approximation, auxiliary, demonstrate, end, learning","variational, optimization, distribution, al, cvb, embedding, et, log, zt, gradient","{'coupled': 0.5258370437363828, 'bayes': 0.4700324939353677, 'embedding': 0.43738892493214754, 'variational': 0.35354256985304433, 'optimization': 0.3051748988104078, 'via': 0.3051748988104078}","graphical, variational, distribution, class, auxiliary, models, end, pursue, approximation, couples"
Multi-domain Causal Structure Learning in Linear Systems,"AmirEmad Ghassami, Negar Kiyavash, Biwei Huang, Kun Zhang","We study the problem of causal structure learning in linear systems from observational data given in multiple domains, across which the causal coefficients and/or the distribution of the exogenous noises may vary. The main tool used in our approach is the principle that in a causally sufficient system, the causal modules, as well as their included parameters, change independently across domains. We first introduce our approach for finding causal direction in a system comprising two variables and propose efficient methods for identifying causal direction. Then we generalize our methods to causal structure learning in networks of variables. Most of previous work in structure learning from multi-domain data assume that certain types of invariance are held in causal modules across domains. Our approach unifies the idea in those works and generalizes to the case that there is no such invariance across the domains. Our proposed methods are generally capable of identifying causal direction from fewer than ten domains. When the invariance property holds, two domains are generally sufficient.",https://proceedings.neurips.cc/paper_files/paper/2018/file/6ad4174eba19ecb5fed17411a34ff5e6-Paper.pdf,2018,"causal, domains, across, approach, direction, invariance, learning, methods, structure, data","causal, σ2, variables, domains, order, variable, direction, ib, mc, system","{'systems': 0.45296545491510015, 'structure': 0.41505079091198355, 'causal': 0.410160270874564, 'domain': 0.3893614866926213, 'linear': 0.3728409518243053, 'multi': 0.3566530725819064, 'learning': 0.1910746789978746}","causal, domains, invariance, direction, across, identifying, modules, sufficient, structure, generally"
Policy Optimization via Importance Sampling,"Alberto Maria Metelli, Matteo Papini, Francesco Faccio, Marcello Restelli","Policy optimization is an effective reinforcement learning approach to solve continuous control tasks. Recent achievements have shown that alternating online and offline optimization is a successful choice for efficient trajectory reuse. However, deciding when to stop optimizing and collect new trajectories is non-trivial, as it requires to account for the variance of the objective function estimate. In this paper, we propose a novel, model-free, policy search algorithm, POIS, applicable in both action-based and parameter-based settings. We first derive a high-confidence bound for importance sampling estimation; then we define a surrogate objective function, which is optimized offline whenever a new batch of trajectories is collected. Finally, the algorithm is tested on a selection of continuous control tasks, with both linear and deep policies, and compared with state-of-the-art policy optimization methods.",https://proceedings.neurips.cc/paper_files/paper/2018/file/6aed000af86a084f9cb0264161e29dd3-Paper.pdf,2018,"optimization, policy, algorithm, based, continuous, control, function, new, objective, offline","policy, pois, based, importance, variance, optimization, trajectories, νρ, function, action","{'importance': 0.5690307730702676, 'policy': 0.4556796133309689, 'sampling': 0.4556796133309689, 'optimization': 0.36118981110223114, 'via': 0.36118981110223114}","offline, policy, trajectories, optimization, control, continuous, achievements, deciding, pois, objective"
Task-Driven Convolutional Recurrent Models of the Visual System,"Aran Nayebi, Daniel Bear, Jonas Kubilius, Kohitij Kar, Surya Ganguli, David Sussillo, James J. DiCarlo, Daniel L. Yamins","Feed-forward convolutional neural networks (CNNs) are currently state-of-the-art for object classification tasks such as ImageNet. Further, they are quantitatively accurate models of temporally-averaged responses of neurons in the primate brain's visual system.  However, biological visual systems have two ubiquitous architectural features not shared with typical CNNs: local recurrence within cortical areas, and long-range feedback from downstream areas to upstream areas.  Here we explored the role of recurrence in improving classification performance. We found that standard forms of recurrence (vanilla RNNs and LSTMs) do not perform well within deep CNNs on the ImageNet task. In contrast, novel cells that incorporated two structural features, bypassing and gating, were able to boost task accuracy substantially. We extended these design principles in an automated search over thousands of model architectures, which identified novel local recurrent cells and long-range feedback connections useful for object recognition. Moreover, these task-optimized ConvRNNs matched the dynamics of neural activity in the primate visual system better than feedforward networks, suggesting a role for the brain's recurrent connections in performing difficult visual behaviors.",https://proceedings.neurips.cc/paper_files/paper/2018/file/6be93f7a96fed60c477d30ae1de032fd-Paper.pdf,2018,"visual, areas, cnns, recurrence, task, brain, cells, classification, connections, features","model, recurrent, feedforward, al, et, models, time, visual, neural, performance","{'system': 0.46240489689682396, 'driven': 0.4336991479598098, 'task': 0.4049933990227955, 'visual': 0.36425929302777227, 'convolutional': 0.3417136962039256, 'recurrent': 0.33555354409075805, 'models': 0.26836140454697593}","recurrence, areas, visual, cnns, primate, cells, role, brain, connections, imagenet"
Contrastive Learning from Pairwise Measurements,"Yi Chen, Zhuoran Yang, Yuchen Xie, Zhaoran Wang","Learning from pairwise measurements naturally arises from many applications, such as rank aggregation, ordinal embedding, and crowdsourcing. However, most existing models and algorithms are susceptible to potential model misspecification. In this paper, we study a semiparametric model where the pairwise measurements follow a natural exponential family distribution with an unknown base measure. Such a semiparametric model includes various popular parametric models, such as the Bradley-Terry-Luce model and the paired cardinal model, as special cases. To estimate this semiparametric model without specifying the base measure, we propose a data augmentation technique to create virtual examples, which enables us to define a contrastive estimator. In particular, we prove that such a contrastive estimator is invariant to model misspecification within the natural exponential family, and moreover, attains the optimal statistical rate of convergence up to a logarithmic factor. We provide numerical experiments to corroborate our theory.",https://proceedings.neurips.cc/paper_files/paper/2018/file/6bf733bb7f81e866306e9b5f012419cb-Paper.pdf,2018,"model, semiparametric, base, contrastive, estimator, exponential, family, measure, measurements, misspecification","model, contrastive, function, pairwise, distribution, matrix, measurements, estimator, d2, al","{'contrastive': 0.5660792087547638, 'measurements': 0.5660792087547638, 'pairwise': 0.5660792087547638, 'learning': 0.19662906256894444}","semiparametric, model, contrastive, misspecification, base, pairwise, measurements, estimator, exponential, family"
Regret Bounds for Online Portfolio Selection with a Cardinality Constraint,"Shinji Ito, Daisuke Hatano, Hanna Sumita, Akihiro Yabe, Takuro Fukunaga, Naonori Kakimura, Ken-Ichi Kawarabayashi","Online portfolio selection is a sequential decision-making problem in which a learner repetitively selects a portfolio over a set of assets, aiming to maximize long-term return. In this paper, we study the problem with the cardinality constraint that the number of assets in a portfolio is restricted to be at most k, and consider two scenarios: (i) in the full-feedback setting, the learner can observe price relatives (rates of return to cost) for all assets, and (ii) in the bandit-feedback setting, the learner can observe price relatives only for invested assets. We propose efficient algorithms for these scenarios that achieve sublinear regrets. We also provide regret (statistical) lower bounds for both scenarios which nearly match the upper bounds when k is a constant. In addition, we give a computational lower bound which implies that no algorithm maintains both computational efficiency, as well as a small regret upper bound.",https://proceedings.neurips.cc/paper_files/paper/2018/file/6c1e55ec7c43dc51a37472ddcbd756fb-Paper.pdf,2018,"assets, learner, portfolio, scenarios, bound, bounds, computational, feedback, lower, observe","algorithm, log, setting, xs, regret, rt, feedback, portfolio, problem, time","{'portfolio': 0.4411401503083917, 'cardinality': 0.42252944845914847, 'constraint': 0.40809386661111335, 'selection': 0.3700688264053567, 'regret': 0.3632528537349743, 'bounds': 0.30937037429386127, 'online': 0.30937037429386127}","assets, portfolio, scenarios, learner, relatives, price, return, observe, feedback, upper"
Hunting for Discriminatory Proxies in Linear Regression Models,"Samuel Yeom, Anupam Datta, Matt Fredrikson","A machine learning model may exhibit discrimination when used to make decisions involving people. One potential cause for such outcomes is that the model uses a statistical proxy for a protected demographic attribute. In this paper we formulate a definition of proxy use for the setting of linear regression and present algorithms for detecting proxies. Our definition follows recent work on proxies in classification models, and characterizes a model's constituent behavior that: 1) correlates closely with a protected random variable, and 2) is causally influential in the overall behavior of the model. We show that proxies in linear regression models can be efficiently identified by solving a second-order cone program, and further extend this result to account for situations where the use of a certain input variable is justified as a ``business necessity''. Finally, we present empirical results on two law enforcement datasets that exhibit varying degrees of racial disparity in prediction outcomes, demonstrating that proxies shed useful light on the causes of discriminatory behavior in models.",https://proceedings.neurips.cc/paper_files/paper/2018/file/6cd9313ed34ef58bad3fdd504355e72c-Paper.pdf,2018,"model, proxies, behavior, models, definition, exhibit, linear, outcomes, present, protected","proxy, model, use, problem, inﬂuence, association, linear, proxies, variables, x1","{'hunting': 0.498642464057467, 'proxies': 0.498642464057467, 'discriminatory': 0.47065706916092914, 'regression': 0.33313859014113933, 'linear': 0.31900358223073166, 'models': 0.2616270314951703}","proxies, protected, proxy, definition, behavior, outcomes, exhibit, variable, model, regression"
Entropy and mutual information in models of deep neural networks,"Marylou Gabrié, Andre Manoel, Clément Luneau, jean barbier, Nicolas Macris, Florent Krzakala, Lenka Zdeborová","We examine a class of stochastic deep learning models with a tractable method to compute information-theoretic quantities. Our contributions are three-fold: (i) We show how entropies and mutual informations can be derived from heuristic statistical physics methods, under the assumption that weight matrices are independent and orthogonally-invariant. (ii) We extend particular cases in which this result is known to be rigorously exact by providing a proof for two-layers networks with Gaussian random weights, using the recently introduced adaptive interpolation method. (iii) We propose an experiment framework with generative models of synthetic datasets, on which we train deep neural networks with a weight constraint designed so that the assumption in (i) is verified during learning. We study the behavior of entropies and mutual information throughout learning and conclude that, in the proposed setting, the relationship between compression and generalization remains elusive.",https://proceedings.neurips.cc/paper_files/paper/2018/file/6d0f846348a856321729a2f36734d1a7-Paper.pdf,2018,"learning, assumption, deep, entropies, information, method, models, mutual, networks, weight","layers, linear, layer, learning, tℓ, replica, information, networks, mutual, network","{'mutual': 0.5587391640836535, 'entropy': 0.45227922076426846, 'information': 0.4119269500657208, 'models': 0.31058976579254616, 'deep': 0.2773669860329246, 'neural': 0.265114736047788, 'networks': 0.26435954676874013}","entropies, mutual, weight, assumption, informations, conclude, orthogonally, elusive, fold, quantities"
Paraphrasing Complex Network: Network Compression via Factor Transfer,"Jangho Kim, Seonguk Park, Nojun Kwak","Many researchers have sought ways of model compression to reduce the size of a deep neural network (DNN) with minimal performance degradation in order to use DNNs in embedded systems. Among the model compression methods, a method called knowledge transfer is to train a student network with a stronger teacher network. In this paper, we propose a novel knowledge transfer method which uses convolutional operations to paraphrase teacher's knowledge and to translate it for the student. This is done by two convolutional modules, which are called a paraphraser and a translator. The paraphraser is trained in an unsupervised manner to extract the teacher factors which are defined as paraphrased information of the teacher network. The translator located at the student network extracts the student factors and helps to translate the teacher factors by mimicking them. We observed that our student network trained with the proposed factor transfer method outperforms the ones trained with conventional knowledge transfer methods.",https://proceedings.neurips.cc/paper_files/paper/2018/file/6d9cb7de5e8ac30bd5e8734bc96a35c1-Paper.pdf,2018,"network, student, teacher, knowledge, transfer, factors, method, trained, called, compression","network, student, teacher, ft, transfer, factors, paraphraser, resnet, knowledge, kd","{'network': 0.5053905963542545, 'paraphrasing': 0.4184920484511974, 'factor': 0.39500494881337345, 'complex': 0.37834058641924057, 'compression': 0.35485348678141654, 'transfer': 0.3100654456386671, 'via': 0.21957382339583803}","student, teacher, transfer, network, paraphraser, translator, factors, knowledge, translate, compression"
A Simple Cache Model for Image Recognition,Emin Orhan,"Training large-scale image recognition models is computationally expensive. This raises the question of whether there might be simple ways to improve the test performance of an already trained model without having to re-train or fine-tune it with new data. Here, we show that, surprisingly, this is indeed possible. The key observation we make is that the layers of a deep network close to the output layer contain independent, easily extractable class-relevant information that is not contained in the output layer itself. We propose to extract this extra class-relevant information using a simple key-value cache memory to improve the classification performance of the model at test time. Our cache memory is directly inspired by a similar cache model previously proposed for language modeling (Grave et al., 2017). This cache component does not require any training or fine-tuning; it can be applied to any pre-trained model and, by properly setting only two hyper-parameters, leads to significant improvements in its classification performance. Improvements are observed across several architectures and datasets. In the cache component, using features extracted from layers close to the output (but not from the output layer itself) as keys leads to the largest improvements. Concatenating features from multiple layers to form keys can further improve performance over using single-layer features as keys. The cache component also has a regularizing effect, a simple consequence of which is that it substantially increases the robustness of models against adversarial attacks.",https://proceedings.neurips.cc/paper_files/paper/2018/file/6e0917469214d8fbd8c517dcdc6b8dcf-Paper.pdf,2018,"cache, layer, model, output, performance, component, features, improve, improvements, keys","cache, model, models, training, test, adversarial, component, layers, resnet32, image","{'cache': 0.5727971086431685, 'simple': 0.45354674995579236, 'recognition': 0.437636629596829, 'image': 0.3826806478886537, 'model': 0.35809010053506435}","cache, keys, layer, output, component, improvements, layers, features, improve, fine"
Learning Attractor Dynamics for Generative Memory,"Yan Wu, Gregory Wayne, Karol Gregor, Timothy Lillicrap","A central challenge faced by memory systems is the robust retrieval of a stored pattern in the presence of interference due to other stored patterns and noise. A theoretically well-founded solution to robust retrieval is given by attractor dynamics, which iteratively cleans up patterns during recall. However, incorporating attractor dynamics into modern deep learning systems poses difficulties: attractor basins are characterised by vanishing gradients, which are known to make training neural networks difficult.  In this work, we exploit recent advances in variational inference and avoid the vanishing gradient problem by training a generative distributed memory with a variational lower-bound-based Lyapunov function. The model is minimalistic with surprisingly few parameters. Experiments shows it converges to correct patterns upon iterative retrieval and achieves competitive performance as both a memory model and a generative model.",https://proceedings.neurips.cc/paper_files/paper/2018/file/6e4243f5511fd6ef0f03e9f386d54403-Paper.pdf,2018,"attractor, memory, model, patterns, retrieval, dynamics, generative, robust, stored, systems","latexit, memory, sha1_base64, model, dynamics, attractor, patterns, wt, using, distribution","{'attractor': 0.6088130140673792, 'memory': 0.4820645210921722, 'dynamics': 0.4651540161616219, 'generative': 0.37516716383576393, 'learning': 0.1996042380174091}","attractor, retrieval, patterns, stored, vanishing, memory, dynamics, systems, robust, basins"
Quadrature-based features for kernel approximation,"Marina Munkhoeva, Yermek Kapushev, Evgeny Burnaev, Ivan Oseledets","We consider the problem of improving kernel approximation via randomized feature maps. These maps arise as Monte Carlo approximation to integral representations of kernel functions and scale up kernel methods for larger datasets. Based on an efficient numerical integration technique, we propose a unifying approach that reinterprets the previous random features methods and extends to better estimates of the kernel approximation. We derive the convergence behavior and conduct an extensive empirical study that supports our hypothesis.",https://proceedings.neurips.cc/paper_files/paper/2018/file/6e923226e43cd6fac7cfe1e13ad000ac-Paper.pdf,2018,"kernel, approximation, maps, methods, approach, arise, based, behavior, better, carlo","kernel, approximation, random, rules, features, matrix, 10, quadrature, radial, methods","{'quadrature': 0.5444709632109583, 'kernel': 0.4567521913986177, 'features': 0.4214740256834225, 'approximation': 0.41081220269069507, 'based': 0.3853852431883204}","kernel, approximation, maps, reinterprets, unifying, supports, integration, integral, arise, extends"
Efficient Algorithms for Non-convex Isotonic Regression through Submodular Optimization,Francis Bach,"We consider the minimization of submodular functions subject to ordering constraints. We show that this potentially non-convex optimization problem can be cast as a convex optimization problem on a space of uni-dimensional measures, with ordering constraints corresponding to first-order stochastic dominance.  We propose new discretization schemes that lead to simple and efficient algorithms based on zero-th, first, or higher order oracles;  these algorithms also lead to improvements without isotonic constraints. Finally,   our experiments  show that non-convex loss functions can be much more robust to outliers for isotonic regression, while still being solvable in polynomial time.",https://proceedings.neurips.cc/paper_files/paper/2018/file/6ea9ab1baa0efb9e19094440c317e21b-Paper.pdf,2018,"constraints, convex, algorithms, first, functions, isotonic, lead, non, optimization, order","function, convex, constraints, isotonic, order, problem, submodular, algorithms, non, functions","{'isotonic': 0.4880786533450578, 'submodular': 0.3831251370667186, 'convex': 0.3454698656566067, 'regression': 0.3454698656566067, 'algorithms': 0.33632015368163937, 'non': 0.3209234920526318, 'efficient': 0.29424914653336, 'optimization': 0.271311274339249}","isotonic, ordering, constraints, convex, lead, dominance, oracles, solvable, th, uni"
Deep Neural Nets with Interpolating Function as Output Activation,"Bao Wang, Xiyang Luo, Zhen Li, Wei Zhu, Zuoqiang Shi, Stanley Osher","We replace the output layer of deep neural nets, typically the softmax function, by a novel interpolating function. And we propose end-to-end training and testing algorithms for this new architecture. Compared to classical neural nets with softmax function as output activation, the surrogate with interpolating function as output activation combines advantages of both deep and manifold learning. The new framework demonstrates the following major advantages: First, it is better applicable to the case with insufficient training data. Second, it significantly improves the generalization accuracy on a wide variety of networks. The algorithm is implemented in PyTorch, and the code is available at https://github.com/
BaoWangMath/DNN-DataDependentActivation.",https://proceedings.neurips.cc/paper_files/paper/2018/file/6ecbdd6ec859d284dc13885a37ce8d81-Paper.pdf,2018,"function, output, activation, advantages, deep, end, interpolating, nets, neural, new","wnll, dnns, training, data, activation, xte, linear, function, output, softmax","{'interpolating': 0.4789854171263409, 'activation': 0.43303002834228843, 'function': 0.40614784919755614, 'output': 0.40614784919755614, 'nets': 0.39592771901241286, 'deep': 0.2244312204252615, 'neural': 0.21451732455593417}","interpolating, output, function, nets, softmax, activation, advantages, end, baowangmath, datadependentactivation"
Where Do You Think You're Going?: Inferring Beliefs about Dynamics from Behavior,"Sid Reddy, Anca Dragan, Sergey Levine","Inferring intent from observed behavior has been studied extensively within the frameworks of Bayesian inverse planning and inverse reinforcement learning. These methods infer a goal or reward function that best explains the actions of the observed agent, typically a human demonstrator. Another agent can use this inferred intent to predict, imitate, or assist the human user. However, a central assumption in inverse reinforcement learning is that the demonstrator is close to optimal. While models of suboptimal behavior exist, they typically assume that suboptimal actions are the result of some type of random noise or a known cognitive bias, like temporal inconsistency. In this paper, we take an alternative approach, and model suboptimal behavior as the result of internal model misspecification: the reason that user actions might deviate from near-optimal actions is that the user has an incorrect set of beliefs about the rules -- the dynamics -- governing how actions affect the environment. Our insight is that while demonstrated actions may be suboptimal in the real world, they may actually be near-optimal with respect to the user's internal model of the dynamics. By estimating these internal beliefs from observed behavior, we arrive at a new method for inferring intent. We demonstrate in simulation and in a user study with 12 participants that this approach enables us to more accurately model human intent, and can be used in a variety of applications, including offering assistance in a shared autonomy framework and inferring human preferences.",https://proceedings.neurips.cc/paper_files/paper/2018/file/6f2268bd1d3d3ebaabb04d6b5d099425-Paper.pdf,2018,"actions, user, behavior, human, intent, model, suboptimal, inferring, internal, inverse","dynamics, internal, user, real, model, action, algorithm, demonstrations, state, actions","{'beliefs': 0.44720497487652666, 'going': 0.42210641483858025, 'think': 0.42210641483858025, 'behavior': 0.40429870309499477, 'inferring': 0.40429870309499477, 'dynamics': 0.3416799334191819}","actions, intent, suboptimal, user, inferring, behavior, internal, human, inverse, demonstrator"
Image Inpainting via Generative Multi-column Convolutional Neural Networks,"Yi Wang, Xin Tao, Xiaojuan Qi, Xiaoyong Shen, Jiaya Jia","In this paper, we propose a generative multi-column network for image inpainting. This network synthesizes different image components in a parallel manner within one stage. To better characterize global structures, we design a confidence-driven reconstruction loss while an implicit diversified MRF regularization is adopted to enhance local details. The multi-column network combined with the reconstruction and MRF loss propagates local and global information derived from context to the target inpainting regions. Extensive experiments on challenging street view, face, natural objects and scenes manifest that our method produces visual compelling results even without previously common post-processing.",https://proceedings.neurips.cc/paper_files/paper/2018/file/6f3ef77ac0e3619e98159e9b6febf557-Paper.pdf,2018,"network, column, global, image, inpainting, local, loss, mrf, multi, reconstruction","loss, image, mrf, results, inpainting, yl, figure, id, 24, 26","{'inpainting': 0.48718880001850334, 'column': 0.4598462210356654, 'convolutional': 0.32548649918433936, 'image': 0.32548649918433936, 'generative': 0.3002190099951814, 'multi': 0.2981439202015014, 'via': 0.2556175390466658, 'neural': 0.21819127304666752, 'networks': 0.21756974701366363}","mrf, inpainting, column, reconstruction, global, network, manifest, local, diversified, synthesizes"
Clustering Redemption–Beyond the Impossibility of Kleinberg’s Axioms,"Vincent Cohen-Addad, Varun Kanade, Frederik Mallmann-Trenn","Kleinberg (2002) stated three axioms that any clustering procedure should satisfy and showed there is no clustering procedure that simultaneously satisfies all three. One of these, called the consistency axiom, requires that when the data is modified in a helpful way, i.e. if points in the same cluster are made more similar and those in different ones made less similar, the algorithm should output the same clustering. To circumvent this impossibility result, research has focused on considering clustering procedures that have a clustering quality measure (or a cost) and showing that a modification of Kleinberg’s axioms that takes cost into account lead to feasible clustering procedures. In this work, we take a different approach, based on the observation that the consistency axiom fails to be  satisfied when the “correct” number of clusters changes. We modify this axiom by making use of cost functions to determine the correct number of clusters, and require that consistency holds only if the number of clusters remains unchanged. We show that single linkage satisfies the modified axioms, and if the input is well-clusterable, some popular procedures such as k-means also satisfy the axioms, taking a step towards explaining the success of these objective functions for guiding the design of algorithms.",https://proceedings.neurips.cc/paper_files/paper/2018/file/6fbd841e2e4b2938351a4f9b68f12e6b-Paper.pdf,2018,"clustering, axioms, axiom, clusters, consistency, cost, number, procedures, correct, different","clustering, instance, axioms, means, clusters, function, center, optimal, cost, partition","{'axioms': 0.4363175389646383, 'impossibility': 0.4363175389646383, 'kleinberg': 0.4363175389646383, 'redemption': 0.4363175389646383, 'beyond': 0.36996831987249545, 'clustering': 0.3187968993859386}","axioms, clustering, axiom, clusters, procedures, consistency, kleinberg, cost, modified, satisfy"
A Statistical Recurrent Model on the Manifold of Symmetric Positive Definite Matrices,"Rudrasis Chakraborty, Chun-Hao Yang, Xingjian Zhen, Monami Banerjee, Derek Archer, David Vaillancourt, Vikas Singh, Baba Vemuri","In a number of disciplines, the data (e.g., graphs, manifolds) to be
analyzed are non-Euclidean in nature.  Geometric deep learning
corresponds to techniques that generalize deep neural network models
to such non-Euclidean spaces. Several recent papers have shown how
convolutional neural networks (CNNs) can be extended to learn with
graph-based data.  In this work, we study the setting where the data
(or measurements) are ordered, longitudinal or temporal in nature and
live on a Riemannian manifold -- this setting is common in a variety
of problems in statistical machine learning, vision and medical
imaging. We show how recurrent statistical recurrent network models
can be defined in such spaces. We give an efficient algorithm and
conduct a rigorous analysis of its statistical properties. We perform
extensive numerical experiments demonstrating competitive performance
with state of the art methods but with significantly less number of
parameters. We also show applications to a statistical analysis task
in brain imaging, a regime where deep neural network models have only
been utilized in limited ways.",https://proceedings.neurips.cc/paper_files/paper/2018/file/7070f9088e456682f0f84f815ebda761-Paper.pdf,2018,"statistical, data, deep, models, network, neural, analysis, euclidean, imaging, learning","spd, sru, data, model, number, parameters, fm, tt, recurrent, time","{'definite': 0.4215704762043888, 'matrices': 0.3979106054478909, 'positive': 0.38112365999423947, 'symmetric': 0.38112365999423947, 'manifold': 0.3406768437840902, 'statistical': 0.3338039184812436, 'recurrent': 0.2765701568174429, 'model': 0.2635491903306502}","statistical, imaging, euclidean, nature, spaces, recurrent, deep, network, live, longitudinal"
Nearly tight sample complexity bounds for learning mixtures of Gaussians via sample compression schemes,"Hassan Ashtiani, Shai Ben-David, Nicholas Harvey, Christopher Liaw, Abbas Mehrabian, Yaniv Plan","We prove that ϴ(k d^2 / ε^2) samples are necessary and sufficient for learning a mixture of k Gaussians in R^d, up to error ε in total variation distance. This improves both the known upper bounds and lower bounds for this problem. For mixtures of axis-aligned Gaussians, we show that O(k d / ε^2) samples suffice, matching a known lower bound.The upper bound is based on a novel technique for distribution learning based on a notion of sample compression. Any class of distributions that allows such a sample compression scheme can also be learned with few samples. Moreover, if a class of distributions has such a compression scheme, then so do the classes of products and mixtures of those distributions. The core of our main result is showing that the class of Gaussians in R^d has an efficient sample compression.",https://proceedings.neurips.cc/paper_files/paper/2018/file/70ece1e1e0931919438fcfc6bd5f199c-Paper.pdf,2018,"compression, class, distributions, gaussians, sample, samples, based, bound, bounds, known","class, learning, sample, distribution, gaussians, compression, distributions, bound, ε2, complexity","{'sample': 0.5070617394102918, 'gaussians': 0.331831796741238, 'mixtures': 0.331831796741238, 'nearly': 0.331831796741238, 'schemes': 0.2999947956867112, 'tight': 0.2897455705903005, 'compression': 0.2813713439343932, 'complexity': 0.2627478921820751, 'bounds': 0.21965215103056046, 'via': 0.1741050435831184}","compression, gaussians, mixtures, distributions, sample, samples, class, upper, scheme, lower"
Object-Oriented Dynamics Predictor,"Guangxiang Zhu, Zhiao Huang, Chongjie Zhang","Generalization has been one of the major challenges for learning dynamics models in model-based reinforcement learning. However, previous work on action-conditioned dynamics prediction focuses on learning the pixel-level motion and thus does not generalize well to novel environments with different object layouts. In this paper, we present a novel object-oriented framework, called object-oriented dynamics predictor (OODP), which decomposes the environment into objects and predicts the dynamics of objects conditioned on both actions and object-to-object relations. It is an end-to-end neural network and can be trained in an unsupervised manner. To enable the generalization ability of dynamics learning, we design a novel CNN-based relation mechanism that is class-specific (rather than object-specific) and exploits the locality principle. Empirical results show that OODP significantly outperforms previous methods in terms of generalization over novel environments with various object layouts. OODP is able to learn from very few environments and accurately predict dynamics in a large number of unseen environments. In addition, OODP learns semantically and visually interpretable dynamics models.",https://proceedings.neurips.cc/paper_files/paper/2018/file/713fd63d76c8a57b16fc433fb4ae718a-Paper.pdf,2018,"dynamics, object, environments, learning, novel, oodp, generalization, based, conditioned, end","object, dynamics, objects, oodp, environments, learning, dynamic, model, 99, dj","{'predictor': 0.5875878870503509, 'oriented': 0.5312128308822499, 'dynamics': 0.4489372914080696, 'object': 0.41353199595793816}","oodp, dynamics, object, environments, layouts, oriented, generalization, conditioned, novel, objects"
Fast Rates of ERM and Stochastic Approximation: Adaptive to Error Bound Conditions,"Mingrui Liu, Xiaoxuan Zhang, Lijun Zhang, Rong Jin, Tianbao Yang","Error bound conditions (EBC) are properties that characterize the growth of an objective function when a point is moved away from the optimal set. They have  recently received increasing attention in the field  of optimization for developing optimization algorithms with fast convergence.  However,  the studies of EBC in statistical learning are hitherto still limited.  The main contributions of this paper are two-fold. First,  we develop fast and intermediate rates of  empirical risk minimization (ERM) under EBC for risk minimization with Lipschitz continuous, and  smooth  convex random functions. Second, we establish fast and intermediate rates of an efficient stochastic approximation (SA) algorithm for risk minimization  with Lipschitz continuous random functions, which requires only one pass of $n$ samples and adapts to EBC. For both approaches, the convergence rates span a full spectrum between $\widetilde O(1/\sqrt{n})$ and $\widetilde O(1/n)$ depending on the power constant in EBC, and could be even faster than $O(1/n)$ in special cases for ERM. Moreover, these  convergence rates are automatically adaptive without using any knowledge of EBC. Overall, this work not only strengthens the understanding of ERM for statistical learning but also brings new fast stochastic algorithms for solving a broad range of statistical learning problems.",https://proceedings.neurips.cc/paper_files/paper/2018/file/716e1b8c6cd17b771da77391355749f3-Paper.pdf,2018,"ebc, fast, rates, convergence, erm, learning, minimization, risk, statistical, algorithms","condition, ebc, rates, fast, erm, problem, risk, log, rate, convex","{'conditions': 0.401303696141106, 'erm': 0.401303696141106, 'bound': 0.37878126129156464, 'rates': 0.3628013394570874, 'error': 0.35040634912112767, 'approximation': 0.2857966260890504, 'adaptive': 0.27625083253466365, 'fast': 0.26327419123950896, 'stochastic': 0.22960506935234265}","ebc, erm, rates, fast, risk, minimization, widetilde, statistical, intermediate, lipschitz"
Adversarial Multiple Source Domain Adaptation,"Han Zhao, Shanghang Zhang, Guanhang Wu, José M. F. Moura, Joao P. Costeira, Geoffrey J. Gordon","While domain adaptation has been actively researched, most algorithms focus on the single-source-single-target adaptation setting. In this paper we propose new generalization bounds and algorithms under both classification and regression settings for unsupervised multiple source domain adaptation. Our theoretical analysis naturally leads to an efficient learning strategy using adversarial neural networks: we show how to interpret it as learning feature representations that are invariant to the multiple domain shifts while still being discriminative for the learning task. To this end, we propose multisource domain adversarial networks (MDAN) that approach domain adaptation by optimizing task-adaptive generalization bounds. To demonstrate the effectiveness of MDAN, we conduct extensive experiments showing superior adaptation performance on both classification and regression problems: sentiment analysis, digit classification, and vehicle counting.",https://proceedings.neurips.cc/paper_files/paper/2018/file/717d8b3d60d9eea997b35b02b6a4e867-Paper.pdf,2018,"adaptation, domain, classification, learning, adversarial, algorithms, analysis, bounds, generalization, mdan","source, domain, target, domains, bound, one, adaptation, mdan, multiple, max","{'source': 0.5468319141630589, 'adaptation': 0.45485236520209016, 'multiple': 0.45485236520209016, 'domain': 0.40410461879823156, 'adversarial': 0.35198432700736765}","adaptation, domain, mdan, classification, source, generalization, regression, multisource, researched, bounds"
To Trust Or Not To Trust A Classifier,"Heinrich Jiang, Been Kim, Melody Guan, Maya Gupta","Knowing when a classifier's prediction can be trusted is useful in many applications and critical for safely using AI. While the bulk of the effort in machine learning research has been towards improving classifier performance, understanding when a classifier's predictions should and should not be trusted has received far less attention. The standard approach is to use the classifier's discriminant or confidence score; however, we show there exists an alternative that is more effective in many situations. We propose a new score, called the {\it trust score}, which measures the agreement between the classifier and a modified nearest-neighbor classifier on the testing example. We show empirically that high (low) trust scores produce surprisingly high precision at identifying correctly (incorrectly) classified examples, consistently outperforming the classifier's confidence score as well as many other baselines. Further, under some mild distributional assumptions, we show that if the trust score for an example is high (low), the classifier will likely agree (disagree) with the Bayes-optimal classifier. Our guarantees consist of non-asymptotic rates of statistical consistency under various nonparametric settings and build on recent developments in topological data analysis.",https://proceedings.neurips.cc/paper_files/paper/2018/file/7180cffd6a8e829dacfc2a31b3f72ece-Paper.pdf,2018,"classifier, score, high, many, show, trust, confidence, example, low, trusted","trust, classiﬁer, score, data, high, density, level, hα, set, algorithm","{'trust': 0.9019075768741045, 'classifier': 0.43192907146553733}","classifier, score, trust, trusted, confidence, many, high, example, bulk, disagree"
Deep Reinforcement Learning of Marked Temporal Point Processes,"Utkarsh Upadhyay, Abir De, Manuel Gomez Rodriguez","In a wide variety of applications, humans interact with a complex environment by means of asynchronous stochastic discrete events in continuous time. Can we design online interventions that will help humans achieve certain goals in such asynchronous setting? In this paper, we address the above problem from the perspective of deep reinforcement learning of marked temporal point processes, where both the actions taken by an agent and the feedback it receives from the environment are asynchronous stochastic discrete events characterized using marked temporal point processes. In doing so, we define the agent's policy using the intensity and mark distribution of the corresponding process and then derive 
a flexible policy gradient method, which embeds the agent's actions and the feedback it receives into real-valued vectors using deep recurrent neural networks. Our method does not make any assumptions on the functional form of the intensity and mark distribution of the feedback and it allows for arbitrarily complex reward functions. We apply our methodology to two different applications in viral marketing and personalized teaching and, using data gathered from Twitter and Duolingo, we show that it may be able to find interventions to help marketers and learners achieve their goals more effectively than alternatives.",https://proceedings.neurips.cc/paper_files/paper/2018/file/71a58e8cb75904f24cde464161c3e766-Paper.pdf,2018,"using, agent, asynchronous, feedback, achieve, actions, applications, complex, deep, discrete","latexit, sha1_base64, time, events, policy, feedback, method, 4dthoakjwawfo5hlpdt9as2v, ab63icbva9swnbej2lxzf, folgch0","{'marked': 0.5512381295128224, 'temporal': 0.42843705411115507, 'point': 0.4084185037994824, 'processes': 0.37554950962945366, 'reinforcement': 0.32459372904076367, 'deep': 0.2582856214991386, 'learning': 0.1807278495452324}","asynchronous, mark, feedback, intensity, marked, receives, agent, interventions, goals, events"
"Learning to Play With Intrinsically-Motivated, Self-Aware Agents","Nick Haber, Damian Mrowca, Stephanie Wang, Li F. Fei-Fei, Daniel L. Yamins","Infants are experts at playing, with an amazing ability to generate novel structured behaviors in unstructured environments that lack clear extrinsic reward signals. We seek to mathematically formalize these abilities using a neural network that implements curiosity-driven intrinsic motivation.  Using a simple but ecologically naturalistic simulated environment in which an agent can move and interact with objects it sees, we propose a ""world-model"" network that learns to predict the dynamic consequences of the agent's actions.  Simultaneously, we train a separate explicit ""self-model"" that allows the agent to track the error map of its world-model. It then uses the self-model to adversarially challenge the developing world-model. We demonstrate that this policy causes the agent to explore novel and informative interactions with its environment, leading to the generation of a spectrum of complex behaviors, including ego-motion prediction, object attention, and object gathering.  Moreover, the world-model that the agent learns supports improved performance on object dynamics prediction, detection, localization and recognition tasks.  Taken together, our results are initial steps toward creating flexible autonomous agents that self-supervise in realistic physical environments.",https://proceedings.neurips.cc/paper_files/paper/2018/file/71e63ef5b7249cfc60852f0e0f5bf4c8-Paper.pdf,2018,"model, agent, world, object, self, behaviors, environment, environments, learns, network","agent, model, object, world, self, loss, prediction, learning, sp, id","{'intrinsically': 0.44507508770889537, 'play': 0.44507508770889537, 'motivated': 0.4200960636867672, 'agents': 0.38862621517944407, 'self': 0.3678975558860324, 'aware': 0.34592429156667676, 'learning': 0.1459214433494904}","agent, self, world, model, object, behaviors, environments, environment, learns, amazing"
Distributed Learning without Distress: Privacy-Preserving Empirical Risk Minimization,"Bargav Jayaraman, Lingxiao Wang, David Evans, Quanquan Gu","Distributed learning allows a group of independent data owners to collaboratively learn a model over their data sets without exposing their private data. We present a distributed learning approach that combines differential privacy with secure multi-party computation. We explore two popular methods of differential privacy, output perturbation and gradient perturbation, and advance the state-of-the-art for both methods in the distributed learning setting. In our output perturbation method, the parties combine local models within a secure computation and then add the required differential privacy noise before revealing the model. In our gradient perturbation method, the data owners collaboratively train a global model via an iterative learning algorithm.  At each iteration, the parties aggregate their local gradients within a secure computation, adding sufficient noise to ensure privacy before the gradient updates are revealed. For both methods, we show that the noise can be reduced in the multi-party setting by adding the noise inside the secure computation after aggregation, asymptotically improving upon the best previous results. Experiments on real world data sets demonstrate that our methods provide substantial utility gains for typical privacy requirements.",https://proceedings.neurips.cc/paper_files/paper/2018/file/7221e5c8ec6b08ef6d3f9ff3ce6eb1d1-Paper.pdf,2018,"data, privacy, computation, learning, methods, noise, perturbation, secure, differential, distributed","data, privacy, noise, model, method, differential, learning, perturbation, local, mpc","{'distress': 0.4271438849781907, 'empirical': 0.3530756846088022, 'preserving': 0.34518079132631996, 'privacy': 0.34518079132631996, 'risk': 0.34518079132631996, 'without': 0.34518079132631996, 'minimization': 0.33198768003110934, 'distributed': 0.2940387914987585, 'learning': 0.14004255447047925}","secure, privacy, perturbation, computation, noise, owners, differential, parties, party, collaboratively"
Continuous-time Value Function Approximation in Reproducing Kernel Hilbert Spaces,"Motoya Ohnishi, Masahiro Yukawa, Mikael Johansson, Masashi Sugiyama","Motivated by the success of reinforcement learning (RL) for discrete-time tasks such as AlphaGo and Atari games, there has been a recent surge of interest in using RL for continuous-time control of physical systems (cf. many challenging tasks in OpenAI Gym and DeepMind Control Suite).
Since discretization of time is susceptible to error, it is methodologically more desirable to handle the system dynamics directly in continuous time.
However, very few techniques exist for continuous-time RL and they lack flexibility in value function approximation.
In this paper, we propose a novel framework for model-based continuous-time value function approximation in reproducing kernel Hilbert spaces.
The resulting framework is so flexible that it can accommodate any kind of kernel-based approach, such as Gaussian processes and kernel adaptive filters, and it allows us to handle uncertainties and nonstationarity without prior knowledge about the environment or what basis functions to employ.
We demonstrate the validity of the presented framework through experiments.",https://proceedings.neurips.cc/paper_files/paper/2018/file/729c68884bd359ade15d5f163166738a-Paper.pdf,2018,"time, continuous, framework, kernel, rl, approximation, based, control, function, handle","ct, time, vf, based, kernel, policy, function, framework, rkhs, dt","{'reproducing': 0.400484317703186, 'hilbert': 0.3780078689675594, 'spaces': 0.36206057480015014, 'function': 0.33958412606452354, 'value': 0.32363683189711434, 'kernel': 0.31710767732889694, 'continuous': 0.30116038316148774, 'approximation': 0.28521308899407855, 'time': 0.2701387852048458}","time, continuous, rl, kernel, handle, value, control, framework, cf, deepmind"
Hybrid Knowledge Routed Modules for Large-scale Object Detection,"ChenHan Jiang, Hang Xu, Xiaodan Liang, Liang Lin","Abstract The dominant object detection approaches treat the recognition of each region separately and overlook crucial semantic correlations between objects in one scene. This paradigm leads to substantial performance drop when facing heavy long-tail problems, where very few samples are available for rare classes and plenty of confusing categories exists. We exploit diverse human commonsense knowledge for reasoning over large-scale object categories and reaching semantic coherency within one image. Particularly, we present Hybrid Knowledge Routed Modules (HKRM) that incorporates the reasoning routed by two kinds of knowledge forms: an explicit knowledge module for structured constraints that are summarized with linguistic knowledge (e.g. shared attributes, relationships) about concepts; and an implicit knowledge module that depicts some implicit constraints (e.g. common spatial layouts). By functioning over a region-to-region graph, both modules can be individualized and adapted to coordinate with visual patterns in each image, guided by specific knowledge forms. HKRM are light-weight, general-purpose and extensible by easily incorporating multiple knowledge to endow any detection networks the ability of global semantic reasoning. Experiments on large-scale object detection benchmarks show HKRM obtains around 34.5% improvement on VisualGenome (1000 categories) and 30.4% on ADE in terms of mAP.",https://proceedings.neurips.cc/paper_files/paper/2018/file/72da7fd6d1302c0a159f6436d01e9eb0-Paper.pdf,2018,"knowledge, categories, detection, hkrm, object, reasoning, region, semantic, constraints, forms","knowledge, region, module, graph, hkrm, relationship, detection, attribute, explicit, object","{'modules': 0.4387214090863121, 'routed': 0.4387214090863121, 'hybrid': 0.35453674624873427, 'scale': 0.3299143108008158, 'knowledge': 0.3205532952968951, 'object': 0.3087628999292071, 'detection': 0.30200856776624163, 'large': 0.30200856776624163}","knowledge, hkrm, categories, region, routed, reasoning, semantic, detection, object, forms"
Supervising Unsupervised Learning,"Vikas Garg, Adam T. Kalai","We introduce a framework to transfer knowledge acquired from a repository of (heterogeneous) supervised datasets to new unsupervised datasets. Our perspective avoids the subjectivity inherent in unsupervised learning by reducing it to supervised learning, and  provides a principled way to evaluate unsupervised algorithms. We demonstrate the versatility of our framework via rigorous agnostic bounds on a variety of unsupervised problems. In the context of clustering, our approach helps choose the number of clusters and the clustering algorithm,  remove the outliers, and provably circumvent Kleinberg's  impossibility result.  Experiments across hundreds of problems demonstrate improvements in performance on unsupervised data with simple algorithms despite the fact our problems come from heterogeneous domains. Additionally, our framework lets us leverage deep networks to learn common features across many small datasets, and perform zero shot learning.",https://proceedings.neurips.cc/paper_files/paper/2018/file/72e6d3238361fe70f22fb0ac624a7072-Paper.pdf,2018,"unsupervised, datasets, framework, learning, problems, across, algorithms, clustering, demonstrate, heterogeneous","meta, clustering, algorithm, data, problems, training, problem, learning, clusters, different","{'supervising': 0.7926224476070144, 'unsupervised': 0.5515600298164222, 'learning': 0.25986763757415704}","unsupervised, heterogeneous, clustering, datasets, problems, framework, supervised, subjectivity, versatility, across"
"Overlapping Clustering Models, and One (class) SVM to Bind Them All","Xueyu Mao, Purnamrita Sarkar, Deepayan Chakrabarti","People belong to multiple communities, words belong to multiple topics, and books cover multiple genres; overlapping clusters are commonplace. Many existing overlapping clustering methods model each person (or word, or book) as a non-negative weighted combination of ""exemplars"" who belong solely to one community, with some small noise. Geometrically, each person is a point on a cone whose corners are these exemplars. This basic form encompasses the widely used Mixed Membership Stochastic Blockmodel of networks and its degree-corrected variants, as well as topic models such as LDA. We show that a simple one-class SVM yields provably consistent parameter inference for all such models, and scales to large datasets. Experimental results on several simulated and real datasets show our algorithm (called SVM-cone) is both accurate and scalable.",https://proceedings.neurips.cc/paper_files/paper/2018/file/731c83db8d2ff01bdc000083fd3c3740-Paper.pdf,2018,"belong, multiple, cone, datasets, exemplars, models, one, overlapping, person, show","matrix, cone, svm, corners, models, one, error, rows, yp, corner","{'bind': 0.4408955055610641, 'overlapping': 0.4408955055610641, 'svm': 0.41615105293094656, 'class': 0.3985945843915669, 'one': 0.34910567913133184, 'clustering': 0.3221418063083056, 'models': 0.23132843797717517}","belong, exemplars, cone, svm, overlapping, person, multiple, books, corners, corrected"
BRITS: Bidirectional Recurrent Imputation for Time Series,"Wei Cao, Dong Wang, Jian Li, Hao Zhou, Lei Li, Yitan Li","Time series are widely used as signals in many classification/regression tasks. It is ubiquitous that time series contains many missing values. Given multiple correlated time series data, how to fill in missing values and to predict their class labels? Existing imputation methods often impose strong assumptions of the underlying data generating process, such as linear dynamics in the state space. 
In this paper, we propose BRITS, a novel method based on recurrent neural networks for missing value imputation in time series data.  Our proposed method directly learns the missing values in a bidirectional recurrent dynamical system, without any specific assumption. The imputed values are treated as variables of RNN graph and can be effectively updated during the backpropagation. BRITS has three advantages: (a) it can handle multiple correlated missing values in time series; (b) it generalizes to time series with nonlinear dynamics underlying; (c) it provides a data-driven imputation procedure and applies to general settings with missing data.
We evaluate our model on three real-world datasets, including an air quality dataset, a health-care data, and a localization data for human activity.
Experiments show that our model outperforms the state-of-the-art methods in both imputation and classification/regression accuracies.",https://proceedings.neurips.cc/paper_files/paper/2018/file/734e6bfcd358e25ac1db0a4241b95651-Paper.pdf,2018,"data, missing, series, time, values, imputation, brits, classification, correlated, dynamics","missing, values, imputation, data, time, xt, series, rnn, recurrent, model","{'bidirectional': 0.47514972579138903, 'brits': 0.47514972579138903, 'imputation': 0.44848281779672367, 'series': 0.3839749651467674, 'time': 0.32050286126517236, 'recurrent': 0.31172067682979115}","missing, series, imputation, values, time, brits, data, correlated, recurrent, dynamics"
Improving Online Algorithms via ML Predictions,"Manish Purohit, Zoya Svitkina, Ravi Kumar","In this work we study the problem of using machine-learned predictions to improve performance of online algorithms.  We consider two classical problems, ski rental and non-clairvoyant job scheduling, and obtain new online algorithms that use predictions to make their decisions.  These algorithms are oblivious to the performance of the predictor, improve with better predictions, but do not degrade much if the predictions are poor.",https://proceedings.neurips.cc/paper_files/paper/2018/file/73a427badebe0e32caa2e1fc7530b7f3-Paper.pdf,2018,"predictions, algorithms, improve, online, performance, better, clairvoyant, classical, consider, decisions","algorithm, opt, job, algorithms, predictions, ratio, competitive, jobs, alg, online","{'predictions': 0.5342955908953959, 'ml': 0.46653088211612426, 'improving': 0.41526885883259196, 'online': 0.35367067586303236, 'algorithms': 0.34750415005332036, 'via': 0.28033346428115363}","predictions, online, algorithms, rental, scheduling, ski, improve, clairvoyant, job, oblivious"
Learning Latent Subspaces in Variational Autoencoders,"Jack Klys, Jake Snell, Richard Zemel","Variational autoencoders (VAEs) are widely used deep generative models capable of learning unsupervised latent representations of data. Such representations are often difficult to interpret or control. We consider the problem of unsupervised learning of features correlated to specific labels in a dataset. We propose a VAE-based generative model which we show is capable of extracting features correlated to binary labels in the data and structuring it in a latent subspace which is easy to interpret. Our model, the Conditional Subspace VAE (CSVAE), uses mutual information minimization to learn a low-dimensional latent subspace associated with each label that can easily be inspected and independently manipulated. We demonstrate the utility of the learned representations for attribute manipulation tasks on both the Toronto Face and CelebA datasets.",https://proceedings.neurips.cc/paper_files/paper/2018/file/73e5080f0f3804cb9cf470a8ce895dac-Paper.pdf,2018,"latent, representations, subspace, capable, correlated, data, features, generative, interpret, labels","model, data, csvae, latent, attribute, condvae, log, vae, dataset, images","{'subspaces': 0.5990849230855456, 'autoencoders': 0.477293357610492, 'latent': 0.47026128206735346, 'variational': 0.38579742331056677, 'learning': 0.20809368195066696}","subspace, vae, representations, interpret, latent, correlated, capable, labels, unsupervised, csvae"
VideoCapsuleNet: A Simplified Network for Action Detection,"Kevin Duarte, Yogesh Rawat, Mubarak Shah","The recent advances in Deep Convolutional Neural Networks (DCNNs) have shown extremely good results for video human action classification, however, action detection is still a challenging problem. The current action detection approaches follow a complex pipeline which involves multiple tasks such as tube proposals, optical flow, and tube classification. In this work, we present a more elegant solution for action detection based on the recently developed capsule network. We propose a 3D capsule network for videos, called VideoCapsuleNet: a unified network for action detection which can jointly perform pixel-wise action segmentation along with action classification. The proposed network is a generalization of capsule network from 2D to 3D, which takes a sequence of video frames as input. The 3D generalization drastically increases the number of capsules in the network, making capsule routing computationally expensive. We introduce capsule-pooling in the convolutional capsule layer to address this issue and make the voting algorithm tractable. The routing-by-agreement in the network inherently models the action representations and various action characteristics are captured by the predicted capsules. This inspired us to utilize the capsules for action localization and the class-specific capsules predicted by the network are used to determine a pixel-wise localization of actions. The localization is further improved by parameterized skip connections with the convolutional capsule layers and the network is trained end-to-end with a classification as well as localization loss. The proposed network achieves state-of-the-art performance on multiple action detection datasets including UCF-Sports, J-HMDB, and UCF-101 (24 classes) with an impressive ~20% improvement on UCF-101 and ~15% improvement on J-HMDB in terms of v-mAP scores.",https://proceedings.neurips.cc/paper_files/paper/2018/file/73f104c9fba50050eea11d9d075247cc-Paper.pdf,2018,"action, network, capsule, detection, capsules, classification, localization, 3d, convolutional, ucf","capsule, capsules, action, network, localization, class, video, map, videocapsulenet, layer","{'simplified': 0.5328703705190977, 'videocapsulenet': 0.5328703705190977, 'action': 0.44046883847671797, 'detection': 0.3668191569239747, 'network': 0.32175960730059694}","action, capsule, capsules, network, localization, ucf, detection, hmdb, tube, 3d"
Causal Inference via Kernel Deviance Measures,"Jovana Mitrovic, Dino Sejdinovic, Yee Whye Teh","Discovering the causal structure among a set of variables is a fundamental problem in many areas of science. In this paper, we propose Kernel Conditional Deviance for Causal Inference (KCDC) a fully nonparametric causal discovery method based on purely observational data. From a novel interpretation of the notion of asymmetry between cause and effect, we derive a corresponding asymmetry measure using the framework of reproducing kernel Hilbert spaces. Based on this, we propose three decision rules for causal discovery. We demonstrate the wide applicability and robustness of our method across a range of diverse synthetic datasets. Furthermore, we test our method on real-world time series data and the real-world benchmark dataset Tübingen Cause-Effect Pairs where we outperform state-of-the-art approaches.",https://proceedings.neurips.cc/paper_files/paper/2018/file/73fed7fd472e502d8908794430511f4d-Paper.pdf,2018,"causal, method, asymmetry, based, cause, data, discovery, effect, kernel, propose","causal, conditional, data, noise, direction, cause, variables, kcdc, particular, distributions","{'deviance': 0.5278254120388978, 'measures': 0.4771841583533974, 'kernel': 0.41793768956234795, 'causal': 0.37147242305058364, 'inference': 0.3230122234363206, 'via': 0.27693869987682623}","causal, asymmetry, discovery, cause, effect, kernel, deviance, kcdc, tübingen, method"
"Sequential Attend, Infer, Repeat: Generative Modelling of Moving Objects","Adam Kosiorek, Hyunjik Kim, Yee Whye Teh, Ingmar Posner","We present Sequential Attend, Infer, Repeat (SQAIR), an interpretable deep generative model for image sequences.
It can reliably discover and track objects through the sequence; it can also conditionally generate future frames, thereby simulating expected motion of objects. 
This is achieved by explicitly encoding object numbers, locations and appearances in the latent variables of the model.
SQAIR retains all strengths of its predecessor, Attend, Infer, Repeat (AIR, Eslami et. al. 2016), including unsupervised learning, made possible by inductive biases present in the model structure.
We use a moving multi-\textsc{mnist} dataset to show limitations of AIR in detecting overlapping or partially occluded objects, and show how \textsc{sqair} overcomes them by leveraging temporal consistency of objects.
Finally, we also apply SQAIR to real-world pedestrian CCTV data, where it learns to reliably detect, track and generate walking pedestrians with no supervision.",https://proceedings.neurips.cc/paper_files/paper/2018/file/7417744a2bac776fabe5a09b21c707a2-Paper.pdf,2018,"objects, sqair, model, air, also, attend, generate, infer, present, reliably","objects, sqair, model, air, al, et, object, time, variables, latent","{'attend': 0.3936133825621184, 'moving': 0.3936133825621184, 'repeat': 0.3936133825621184, 'objects': 0.37152255247530613, 'infer': 0.3558488590933863, 'modelling': 0.34369139799876836, 'sequential': 0.3059268745300363, 'generative': 0.24255528868719012}","sqair, objects, repeat, textsc, air, attend, reliably, track, infer, generate"
Learning with SGD and Random Features,"Luigi Carratino, Alessandro Rudi, Lorenzo Rosasco","Sketching and stochastic gradient methods are arguably the most common  techniques to derive efficient large scale learning algorithms. In this paper, we investigate their application in the context of nonparametric statistical learning. More precisely, we study the estimator defined by stochastic gradient with mini batches and   random features. The latter can be seen as form of nonlinear sketching and  used to define approximate kernel methods. The considered estimator is not explicitly penalized/constrained and regularization is implicit. Indeed, our study highlights how different parameters, such as number of features, iterations, step-size and mini-batch size control the learning properties of the solutions. We do this by deriving optimal finite sample bounds, under standard  assumptions. The obtained results are corroborated and illustrated by numerical experiments.",https://proceedings.neurips.cc/paper_files/paper/2018/file/741a0099c9ac04c7bfc822caf7c7459f-Paper.pdf,2018,"learning, estimator, features, gradient, methods, mini, size, sketching, stochastic, study","random, features, number, kernel, φm, size, 2r, mini, pn, gradient","{'sgd': 0.5873715181836784, 'random': 0.5496140182201469, 'features': 0.5420047084669724, 'learning': 0.24320778483045608}","sketching, mini, estimator, corroborated, size, features, arguably, penalized, highlights, batches"
Boolean Decision Rules via Column Generation,"Sanjeeb Dash, Oktay Gunluk, Dennis Wei","This paper considers the learning of Boolean rules in either disjunctive normal form (DNF, OR-of-ANDs, equivalent to decision rule sets) or conjunctive normal form (CNF, AND-of-ORs) as an interpretable model for classification.  An integer program is formulated to optimally trade classification accuracy for rule simplicity.  Column generation (CG) is used to efficiently search over an exponential number of candidate clauses (conjunctions or disjunctions) without the need for heuristic rule mining.  This approach also bounds the gap between the selected rule set and the best possible rule set on the training data. To handle large datasets, we propose an approximate CG algorithm using randomization.  Compared to three recently proposed alternatives, the CG algorithm dominates the accuracy-simplicity trade-off in 8 out of 16 datasets. When maximized for accuracy, CG is competitive with rule learners designed for this purpose, sometimes finding significantly simpler solutions that are no less accurate.",https://proceedings.neurips.cc/paper_files/paper/2018/file/743394beff4b1282ba735e5e3723ed74-Paper.pdf,2018,"rule, cg, accuracy, algorithm, classification, datasets, form, normal, set, simplicity","rule, clauses, set, clause, mlp, problem, features, restricted, datasets, accuracy","{'boolean': 0.4965249302054157, 'column': 0.46865837801748766, 'rules': 0.44888674458911804, 'decision': 0.3733820067848923, 'generation': 0.35361037335652273, 'via': 0.2605160067916287}","rule, cg, simplicity, normal, accuracy, trade, form, ands, clauses, cnf"
Neural Interaction Transparency (NIT): Disentangling Learned Interactions for Improved Interpretability,"Michael Tsang, Hanpeng Liu, Sanjay Purushotham, Pavankumar Murali, Yan Liu","Neural networks are known to model statistical interactions, but they entangle the interactions at intermediate hidden layers for shared representation learning. We propose a framework, Neural Interaction Transparency (NIT), that disentangles the shared learning across different interactions to obtain their intrinsic lower-order and interpretable structure. This is done through a novel regularizer that directly penalizes interaction order. We show that disentangling interactions reduces a feedforward neural network to a generalized additive model with interactions, which can lead to transparent models that perform comparably to the state-of-the-art models. NIT is also flexible and efficient; it can learn generalized additive models with maximum $K$-order interactions by training only $O(1)$ models.",https://proceedings.neurips.cc/paper_files/paper/2018/file/74378afe5e8b20910cf1f939e57f0480-Paper.pdf,2018,"interactions, models, neural, order, additive, generalized, interaction, learning, model, nit","interactions, neural, interaction, nit, network, hidden, layer, model, 10, feedforward","{'nit': 0.39015268149528853, 'transparency': 0.39015268149528853, 'interaction': 0.35272018872583266, 'interactions': 0.35272018872583266, 'disentangling': 0.34066961764167675, 'interpretability': 0.33082358414718516, 'learned': 0.33082358414718516, 'improved': 0.28506638867407086, 'neural': 0.17473289668152236}","interactions, nit, additive, interaction, shared, order, generalized, models, entangle, transparent"
Boosting Black Box Variational Inference,"Francesco Locatello, Gideon Dresdner, Rajiv Khanna, Isabel Valera, Gunnar Raetsch","Approximating a probability density in a tractable manner is a central task in Bayesian statistics. Variational Inference (VI) is a popular technique that achieves tractability by choosing a relatively simple variational approximation. Borrowing ideas from the classic boosting framework, recent approaches attempt to \emph{boost} VI by replacing the selection of a single density with an iteratively constructed mixture of densities. In order to guarantee convergence, previous works impose stringent assumptions that require significant effort for practitioners. Specifically, they require a custom implementation of the greedy step (called the LMO) for every probabilistic model with respect to an unnatural variational family of truncated distributions. Our work fixes these issues with novel theoretical and algorithmic insights. On the theoretical side, we show that boosting VI satisfies a relaxed smoothness assumption which is sufficient for the convergence of the functional Frank-Wolfe (FW) algorithm. Furthermore, we rephrase the LMO problem and propose to maximize the Residual ELBO (RELBO) which replaces the standard ELBO optimization in VI. These theoretical enhancements allow for black box implementation of the boosting subroutine. Finally, we present a stopping criterion drawn from the duality gap in the classic FW analyses and exhaustive experiments to illustrate the usefulness of our theoretical and algorithmic contributions.",https://proceedings.neurips.cc/paper_files/paper/2018/file/74563ba21a90da13dacf2a73e3ddefa7-Paper.pdf,2018,"theoretical, vi, boosting, variational, algorithmic, classic, convergence, density, elbo, fw","vi, boosting, algorithm, problem, bounded, variational, bbvi, qt, family, lmo","{'black': 0.5485146103814426, 'box': 0.4958883690126716, 'boosting': 0.4789465033877407, 'inference': 0.3356733492656468, 'variational': 0.3334068043119199}","vi, boosting, lmo, fw, elbo, theoretical, variational, classic, algorithmic, density"
Transfer of Deep Reactive Policies for MDP Planning,"Aniket (Nick) Bajpai, Sankalp Garg, Mausam","Domain-independent probabilistic planners input an MDP description in a factored representation language such as PPDDL or RDDL, and exploit the specifics of the representation for faster planning. Traditional algorithms operate on each problem instance independently, and good methods for transferring experience from policies of other instances of a domain to a new instance do not exist.  Recently, researchers have begun exploring the use of deep reactive policies, trained via deep reinforcement learning (RL), for MDP planning domains. One advantage of deep reactive policies is that they are more amenable to transfer learning.In this paper, we present the first domain-independent transfer algorithm for MDP planning domains expressed in an RDDL representation. Our architecture exploits the symbolic state configuration and transition function of the domain (available via RDDL) to learn a shared embedding space for states and state-action pairs for all problem instances of a domain. We then learn an RL agent in the embedding space, making a near zero-shot transfer possible, i.e., without much training on the new instance, and without using the domain simulator at all. Experiments on three different benchmark domains underscore the value of our transfer algorithm. Compared against planning from scratch, and a state-of-the-art RL transfer algorithm, our transfer solution has significantly superior learning curves.",https://proceedings.neurips.cc/paper_files/paper/2018/file/74627b65e6e6a4c21e06809b8e02114a-Paper.pdf,2018,"domain, transfer, planning, algorithm, deep, domains, instance, learning, mdp, policies","state, transfer, action, instance, rl, learning, domain, torpido, rddl, problem","{'mdp': 0.49041512194172143, 'reactive': 0.49041512194172143, 'planning': 0.4282157729565676, 'policies': 0.3883158300374813, 'transfer': 0.3633540562970427, 'deep': 0.22978667073564063}","transfer, domain, rddl, planning, mdp, reactive, rl, instance, policies, domains"
Variational Bayesian Monte Carlo,Luigi Acerbi,"Many probabilistic models of interest in scientific computing and machine learning have expensive, black-box likelihoods that prevent the application of standard techniques for Bayesian inference, such as MCMC, which would require access to the gradient or a large number of likelihood evaluations.
We introduce here a novel sample-efficient inference framework, Variational Bayesian Monte Carlo (VBMC). VBMC combines variational inference with Gaussian-process based, active-sampling Bayesian quadrature, using the latter to efficiently approximate the intractable integral in the variational objective.
Our method produces both a nonparametric approximation of the posterior distribution and an approximate lower bound of the model evidence, useful for model selection.
We demonstrate VBMC both on several synthetic likelihoods and on a neuronal model with data from real neurons. Across all tested problems and dimensions (up to D = 10), VBMC performs consistently well in reconstructing the posterior and the model evidence with a limited budget of likelihood evaluations, unlike other methods that work only in very low dimensions. Our framework shows great promise as a novel tool for posterior and model inference with expensive, black-box likelihoods.",https://proceedings.neurips.cc/paper_files/paper/2018/file/747c1bcceb6109a4ef936bc70cfe67de-Paper.pdf,2018,"model, inference, vbmc, bayesian, likelihoods, posterior, variational, approximate, black, box","posterior, 10, vbmc, variational, function, gp, model, bayesian, mean, log","{'carlo': 0.5631220021561354, 'monte': 0.5631220021561354, 'variational': 0.4403935817816067, 'bayesian': 0.414536746864374}","vbmc, likelihoods, inference, posterior, variational, bayesian, black, model, dimensions, box"
Tangent: Automatic differentiation using source-code transformation for dynamically typed array programming,"Bart van Merrienboer, Dan Moldovan, Alexander Wiltschko","The need to efficiently calculate first- and higher-order derivatives of increasingly complex models expressed in Python has stressed or exceeded the capabilities of available tools. In this work, we explore techniques from the field of automatic differentiation (AD) that can give researchers expressive power, performance and strong usability. These include source-code transformation (SCT), flexible gradient surgery, efficient in-place array operations, and higher-order derivatives. We implement and demonstrate these ideas in the Tangent software library for Python, the first AD framework for a dynamic language that uses SCT.",https://proceedings.neurips.cc/paper_files/paper/2018/file/748d6b6ed8e13f857ceaa6cfbdca14b8-Paper.pdf,2018,"ad, derivatives, first, higher, order, python, sct, array, automatic, available","tangent, python, code, ad, function, sct, using, array, numpy, adjoint","{'array': 0.33199941561154805, 'dynamically': 0.33199941561154805, 'transformation': 0.33199941561154805, 'typed': 0.33199941561154805, 'tangent': 0.3133665565571732, 'code': 0.300146332667934, 'differentiation': 0.300146332667934, 'source': 0.300146332667934, 'automatic': 0.28151347361355916, 'programming': 0.27442958937069806}","sct, python, ad, derivatives, higher, exceeded, stressed, surgery, usability, calculate"
Learning Task Specifications from Demonstrations,"Marcell Vazquez-Chanlatte, Susmit Jha, Ashish Tiwari, Mark K. Ho, Sanjit Seshia","Real-world applications often naturally decompose into several
  sub-tasks. In many settings (e.g., robotics) demonstrations provide
  a natural way to specify the sub-tasks. However, most methods for
  learning from demonstrations either do not provide guarantees that
  the artifacts learned for the sub-tasks can be safely recombined or
  limit the types of composition available.  Motivated by this
  deficit, we consider the problem of inferring Boolean non-Markovian
  rewards (also known as logical trace properties or
  specifications) from demonstrations provided by an agent
  operating in an uncertain, stochastic environment. Crucially,
  specifications admit well-defined composition rules that are
  typically easy to interpret.  In this paper, we formulate the
  specification inference task as a maximum a posteriori (MAP)
  probability inference problem, apply the principle of maximum
  entropy to derive an analytic demonstration likelihood model and
  give an efficient approach to search for the most likely
  specification in a large candidate pool of specifications. In our
  experiments, we demonstrate how learning specifications can help
  avoid common problems that often arise due to ad-hoc reward composition.",https://proceedings.neurips.cc/paper_files/paper/2018/file/74934548253bcab8490ebd74afed7031-Paper.pdf,2018,"specifications, composition, demonstrations, sub, tasks, inference, learning, maximum, often, problem","speciﬁcations, demonstrations, speciﬁcation, task, learning, reward, agent, inference, rewards, pr","{'specifications': 0.6108264060250707, 'demonstrations': 0.5650687421869862, 'task': 0.51241722399803, 'learning': 0.21217211611299322}","specifications, composition, demonstrations, sub, specification, maximum, tasks, deficit, recombined, operating"
Sparse PCA from Sparse Linear Regression,"Guy Bresler, Sung Min Park, Madalina Persu","Sparse Principal Component Analysis (SPCA) and Sparse Linear Regression (SLR) have a wide range of applications and have attracted a tremendous amount of attention in the last two decades as canonical examples of statistical problems in high dimension. A variety of algorithms have been proposed for both SPCA and SLR, but an explicit connection between the two had not been made. We show how to efficiently transform a black-box solver for SLR into an algorithm for SPCA: assuming the SLR solver satisfies prediction error guarantees achieved by existing efficient algorithms such as those based on the Lasso, the SPCA algorithm derived from it achieves near state of the art guarantees for testing and for support recovery for the single spiked covariance model as obtained by the current best polynomial-time algorithms. Our reduction not only highlights the inherent similarity between the two problems, but also, from a practical standpoint, allows one to obtain a collection of algorithms for SPCA directly from known algorithms for SLR. We provide experimental results on simulated data comparing our proposed framework to other algorithms for SPCA.",https://proceedings.neurips.cc/paper_files/paper/2018/file/74f23f9e28cbc5ddaae8582f48642a59-Paper.pdf,2018,"algorithms, spca, slr, two, algorithm, guarantees, problems, proposed, solver, sparse","support, algorithms, slr, algorithm, spca, condition, covariance, thresholding, matrix, recovery","{'sparse': 0.7275753264477526, 'pca': 0.47091805983433277, 'regression': 0.36031490072280625, 'linear': 0.34502680705044947}","spca, slr, algorithms, solver, two, sparse, spiked, guarantees, standpoint, highlights"
GILBO: One Metric to Measure Them All,"Alexander A. Alemi, Ian Fischer","We propose a simple, tractable lower bound on the mutual information contained in the joint generative density of any latent variable generative model: the GILBO (Generative Information Lower BOund). It offers a data-independent measure of the complexity of the learned latent variable description, giving the log of the effective description length. It is well-defined for both VAEs and GANs. We compute the GILBO for 800 GANs and VAEs each trained on four datasets (MNIST, FashionMNIST, CIFAR-10 and CelebA) and discuss the results.",https://proceedings.neurips.cc/paper_files/paper/2018/file/7535bbb91c8fde347ad861f293126633-Paper.pdf,2018,"generative, bound, description, gans, gilbo, information, latent, lower, vaes, variable","gilbo, generative, gans, information, model, log, models, figure, encoder, fid","{'gilbo': 0.5615749214500331, 'measure': 0.530057557660173, 'metric': 0.45381634290523853, 'one': 0.44466090459791563}","gilbo, vaes, description, gans, generative, variable, 800, fashionmnist, latent, lower"
Maximizing Induced Cardinality Under a Determinantal Point Process,"Jennifer A. Gillenwater, Alex Kulesza, Sergei Vassilvitskii, Zelda E. Mariet","Determinantal point processes (DPPs) are well-suited to recommender systems where the goal is to generate collections of diverse, high-quality items. In the existing literature this is usually formulated as finding the mode of the DPP (the so-called MAP set). However, the MAP objective inherently assumes that the DPP models ""optimal"" recommendation sets, and yet obtaining such a DPP is nontrivial when there is no ready source of example optimal sets. In this paper we advocate an alternative framework for applying DPPs to recommender systems. Our approach assumes that the DPP simply models user engagements with recommended items, which is more consistent with how DPPs for recommender systems are typically trained.  With this assumption, we are able to formulate a metric that measures the expected number of items that a user will engage with.  We formalize this optimization of this metric as the Maximum Induced Cardinality (MIC) problem. Although the MIC objective is not submodular, we show that it can be approximated by a submodular function, and that empirically it is well-optimized by a greedy algorithm.",https://proceedings.neurips.cc/paper_files/paper/2018/file/758be1f9f7a7efac938ed8bd97c0e1cb-Paper.pdf,2018,"dpp, dpps, items, recommender, systems, assumes, map, metric, mic, models","set, matrix, mic, map, items, matrices, objective, sets, bs, gic","{'induced': 0.4650690090770651, 'maximizing': 0.438967862842429, 'cardinality': 0.42044880487173913, 'determinantal': 0.42044880487173913, 'point': 0.3445748374094368, 'process': 0.3445748374094368}","dpp, dpps, recommender, items, mic, assumes, systems, submodular, user, map"
"FishNet: A Versatile Backbone for Image, Region, and Pixel Level Prediction","Shuyang Sun, Jiangmiao Pang, Jianping Shi, Shuai Yi, Wanli Ouyang","The basic principles in designing convolutional neural network (CNN) structures for predicting objects on different levels, e.g., image-level, region-level, and pixel-level, are diverging. Generally, network structures designed specifically for image classification are directly used as default backbone structure for other tasks including detection and segmentation, but there is seldom backbone structure designed under the consideration of unifying the advantages of networks designed for pixel-level or region-level predicting tasks, which may require very deep features with high resolution. Towards this goal, we design a fish-like network, called FishNet. In FishNet, the information of all resolutions is preserved and refined for the final task. Besides, we observe that existing works still cannot \emph{directly} propagate the gradient information from deep layers to shallow layers. Our design can better handle this problem. Extensive experiments have been conducted to demonstrate the remarkable performance of the FishNet. In particular, on ImageNet-1k, the accuracy of FishNet is able to surpass the performance of DenseNet and ResNet with fewer parameters. FishNet was applied as one of the modules in the winning entry of the COCO Detection 2018 challenge. The code is available at https://github.com/kevin-ssy/FishNet.",https://proceedings.neurips.cc/paper_files/paper/2018/file/75fc093c0ee742f6dddaa13fff98f104-Paper.pdf,2018,"fishnet, level, designed, network, backbone, deep, design, detection, directly, image","features, fishnet, block, resnet, network, 50, level, body, head, resolution","{'backbone': 0.38974572448493283, 'fishnet': 0.38974572448493283, 'pixel': 0.38974572448493283, 'level': 0.3678719596230396, 'region': 0.3678719596230396, 'versatile': 0.3678719596230396, 'prediction': 0.26552737895662565, 'image': 0.2603856480892969}","fishnet, level, backbone, designed, pixel, predicting, region, structures, detection, layers"
Simple random search of static linear policies is competitive for reinforcement learning,"Horia Mania, Aurelia Guy, Benjamin Recht","Model-free reinforcement learning aims to offer off-the-shelf solutions for controlling dynamical systems without requiring models of the system dynamics.  We introduce a model-free random search algorithm for training static, linear policies for continuous control problems. Common evaluation methodology shows that our method matches state-of-the-art sample efficiency on the benchmark MuJoCo locomotion tasks.  Nonetheless, more rigorous evaluation reveals that the assessment of performance on these benchmarks is optimistic. We evaluate the performance of our method over hundreds of random seeds and many different hyperparameter configurations for each benchmark task. This extensive evaluation is possible because of the small computational footprint of our method. Our simulations highlight a high variability in performance in these benchmark tasks, indicating that commonly used estimations of sample efficiency do not adequately evaluate the performance of RL algorithms. Our results stress the need for new baselines, benchmarks and evaluation methodology for RL algorithms.",https://proceedings.neurips.cc/paper_files/paper/2018/file/7634ea65a4e6d9041cfd3f7de18e334a-Paper.pdf,2018,"evaluation, performance, benchmark, method, algorithms, benchmarks, efficiency, evaluate, free, methodology","ars, v1, random, policies, methods, tasks, rl, v2, al, et","{'static': 0.4418221585263666, 'competitive': 0.41702569919598725, 'policies': 0.34983941265478724, 'simple': 0.34983941265478724, 'search': 0.332246044774346, 'random': 0.3273509854811285, 'linear': 0.2826531261135873, 'reinforcement': 0.26016469893992855, 'learning': 0.14485494438215493}","evaluation, benchmark, methodology, performance, benchmarks, rl, free, evaluate, efficiency, adequately"
Automatic differentiation in ML: Where we are and where we should be going,"Bart van Merrienboer, Olivier Breuleux, Arnaud Bergeron, Pascal Lamblin","We review the current state of automatic differentiation (AD) for array programming in machine learning (ML), including the different approaches such as operator overloading (OO) and source transformation (ST) used for AD, graph-based intermediate representations for programs, and source languages. Based on these insights, we introduce a new graph-based intermediate representation (IR) which specifically aims to efficiently support fully-general AD for array programming. Unlike existing dataflow programming representations in ML frameworks, our IR naturally supports function calls, higher-order functions and recursion, making ML models easier to implement. The ability to represent closures allows us to perform AD using ST without a tape, making the resulting derivative (adjoint) program amenable to ahead-of-time optimization using tools from functional language compilers, and enabling higher-order derivatives. Lastly, we introduce a proof of concept compiler toolchain called Myia which uses a subset of Python as a front end.",https://proceedings.neurips.cc/paper_files/paper/2018/file/770f8e448d07586afbf77bb59f698587-Paper.pdf,2018,"ad, based, ml, programming, array, graph, higher, intermediate, introduce, ir","ad, ml, frameworks, program, python, graph, intermediate, function, using, order","{'going': 0.5284994326679255, 'differentiation': 0.5062032409429126, 'ml': 0.48890897105706205, 'automatic': 0.4747785236807759}","ad, ml, programming, st, ir, array, intermediate, source, higher, making"
Improving Neural Program Synthesis with Inferred Execution Traces,"Eui Chul Shin, Illia Polosukhin, Dawn Song","The task of program synthesis, or automatically generating programs that are consistent with a provided specification, remains a challenging task in artificial intelligence. As in other fields of AI, deep learning-based end-to-end approaches have made great advances in program synthesis. However, more so than other fields such as computer vision, program synthesis provides greater opportunities to explicitly exploit structured information such as execution traces, which contain a superset of the information input/output pairs. While they are highly useful for program synthesis, as execution traces are more difficult to obtain than input/output pairs, we use the insight that we can split the process into two parts: infer the trace from the input/output example, then infer the program from the trace. This simple modification leads to state-of-the-art results in program synthesis in the Karel domain, improving accuracy to 81.3% from the 77.12% of prior work.",https://proceedings.neurips.cc/paper_files/paper/2018/file/7776e88b0c189539098176589250bcba-Paper.pdf,2018,"program, synthesis, input, output, end, execution, fields, infer, information, pairs","program, trace, input, output, traces, al, et, model, execution, synthesis","{'inferred': 0.45485110292704944, 'traces': 0.45485110292704944, 'execution': 0.42932341795822127, 'improving': 0.35352247270975806, 'program': 0.35352247270975806, 'synthesis': 0.33233851116668, 'neural': 0.20370858523546606}","program, synthesis, traces, execution, trace, output, fields, input, infer, pairs"
Discretely Relaxing Continuous Variables for tractable Variational Inference,"Trefor Evans, Prasanth Nair","We explore a new research direction in Bayesian variational inference with discrete latent variable priors where we exploit Kronecker matrix algebra for efficient and exact computations of the evidence lower bound (ELBO). The proposed ""DIRECT"" approach has several advantages over its predecessors; (i) it can exactly compute ELBO gradients (i.e. unbiased, zero-variance gradient estimates), eliminating the need for high-variance stochastic gradient estimators and enabling the use of quasi-Newton optimization methods; (ii) its training complexity is independent of the number of training points, permitting inference on large datasets; and (iii) its posterior samples consist of sparse and low-precision quantized integers which permit fast inference on hardware limited devices. In addition, our DIRECT models can exactly compute statistical moments of the parameterized predictive posterior without relying on Monte Carlo sampling. The DIRECT approach is not practical for all likelihoods, however, we identify a popular model structure which is practical, and demonstrate accurate inference using latent variables discretized as extremely low-precision 4-bit quantized integers. While the ELBO computations considered in the numerical studies require over 10^2352 log-likelihood evaluations, we train on datasets with over two-million points in just seconds.",https://proceedings.neurips.cc/paper_files/paper/2018/file/7790583c0d8d74e930a4441ad75ebc64-Paper.pdf,2018,"inference, direct, elbo, approach, computations, compute, datasets, exactly, gradient, integers","log, elbo, direct, variational, model, prior, inference, discrete, distribution, models","{'discretely': 0.43398515668065196, 'relaxing': 0.43398515668065196, 'tractable': 0.43398515668065196, 'variables': 0.43398515668065196, 'continuous': 0.3263526942126345, 'inference': 0.26558499685772363, 'variational': 0.26379170485009074}","elbo, direct, integers, inference, quantized, exactly, computations, precision, compute, variance"
The Limits of Post-Selection Generalization,"Jonathan Ullman, Adam Smith, Kobbi Nissim, Uri Stemmer, Thomas Steinke","While statistics and machine learning offers numerous methods for ensuring generalization, these methods often fail in the presence ofpost selection---the common practice in which the choice of analysis depends on previous interactions with the same dataset.  A recent line of work has introduced powerful, general purpose algorithms that ensure a property calledpost hoc generalization(Cummings et al., COLT'16), which says that no person when given the output of the algorithm should be able to find any statistic for which the data differs significantly from the population it came from.In this work we show several limitations on the power of algorithms satisfying post hoc generalization.  First, we show a tight lower bound on the error of any algorithm that satisfies post hoc generalization and answers adaptively chosen statistical queries, showing a strong barrier to progress in post selection data analysis.  Second, we show that post hoc generalization is not closed under composition, despite many examples of such algorithms exhibiting strong composition properties.",https://proceedings.neurips.cc/paper_files/paper/2018/file/77ee3bc58ce560b86c2b59363281e914-Paper.pdf,2018,"generalization, hoc, post, algorithms, show, algorithm, analysis, composition, data, methods","aj, qj, post, generalization, algorithms, hoc, sample, algorithm, pj, queries","{'post': 0.5557858707709865, 'limits': 0.5323385261443188, 'selection': 0.46624417384160655, 'generalization': 0.4362729991560199}","hoc, post, generalization, composition, selection, strong, algorithms, barrier, calledpost, came"
"Revisiting $(\epsilon, \gamma, \tau)$-similarity learning for domain adaptation","Sofiane Dhouib, Ievgen Redko","Similarity learning is an active research area in machine learning that tackles the problem of finding a similarity function tailored to an observable data sample in order to achieve efficient classification. This learning scenario has been generally formalized by the means of a $(\epsilon, \gamma, \tau)-$good similarity learning framework in the context of supervised classification and has been shown to have strong theoretical guarantees. In this paper, we propose to extend the theoretical analysis of similarity learning to the domain adaptation setting, a particular situation occurring when the similarity is learned and then deployed on samples following different probability distributions. We give a new definition of an $(\epsilon, \gamma)-$good similarity for domain adaptation and prove several results quantifying the performance of a similarity function on a target domain after it has been trained on a source domain. We particularly show that if the source distribution dominates the target one, then principally new domain adaptation learning bounds can be proved.",https://proceedings.neurips.cc/paper_files/paper/2018/file/781397bc0630d47ab531ea850bddcf63-Paper.pdf,2018,"similarity, learning, domain, adaptation, classification, epsilon, function, gamma, good, new","similarity, function, distribution, domain, source, term, target, learning, data, adaptation","{'tau': 0.43322412242796693, 'epsilon': 0.4089102121239791, 'gamma': 0.3916592182263932, 'revisiting': 0.3916592182263932, 'similarity': 0.35810158823439997, 'adaptation': 0.3257804037208318, 'domain': 0.28943317860741274, 'learning': 0.1420360091685307}","similarity, domain, adaptation, gamma, learning, epsilon, source, good, target, formalized"
Learning and Testing Causal Models with Interventions,"Jayadev Acharya, Arnab Bhattacharyya, Constantinos Daskalakis, Saravanan Kandasamy","We consider testing and learning problems on causal Bayesian networks as defined by Pearl (Pearl, 2009). Given a causal Bayesian network M on a graph with n discrete variables and bounded in-degree and bounded ``confounded components'', we show that O(log n) interventions on an unknown causal Bayesian network X on the same graph, and O(n/epsilon^2) samples per intervention, suffice to efficiently distinguish whether X=M or whether there exists some intervention under which X and M are farther than epsilon in total variation distance.  We also obtain sample/time/intervention efficient algorithms for: (i) testing the identity of two unknown causal Bayesian networks on the same graph; and (ii) learning a causal Bayesian network on a given graph.  Although our algorithms are non-adaptive, we show that adaptivity does not help in general: Omega(log n) interventions are necessary for testing the identity of two unknown causal Bayesian networks on the same graph, even adaptively.  Our algorithms are enabled by a new subadditivity inequality for the squared Hellinger distance between two causal Bayesian networks.",https://proceedings.neurips.cc/paper_files/paper/2018/file/78631a4bb5303be54fa1cfdcb958c00a-Paper.pdf,2018,"bayesian, causal, graph, networks, algorithms, intervention, network, testing, two, unknown","causal, algorithm, interventions, variables, pa, graph, intervention, set, two, distributions","{'interventions': 0.6347236592198259, 'testing': 0.4933242462221026, 'causal': 0.44670516098710644, 'models': 0.3330259228830883, 'learning': 0.20809925120321657}","causal, bayesian, graph, intervention, pearl, testing, unknown, interventions, identity, bounded"
Evolved Policy Gradients,"Rein Houthooft, Yuhua Chen, Phillip Isola, Bradly Stadie, Filip Wolski, OpenAI Jonathan Ho, Pieter Abbeel","We propose a metalearning approach for learning gradient-based reinforcement learning (RL) algorithms. The idea is to evolve a differentiable loss function, such that an agent, which optimizes its policy to minimize this loss, will achieve high rewards. The loss is parametrized via temporal convolutions over the agent's experience. Because this loss is highly flexible in its ability to take into account the agent's history, it enables fast task learning. Empirical results show that our evolved policy gradient algorithm (EPG) achieves faster learning on several randomized environments compared to an off-the-shelf policy gradient method. We also demonstrate that EPG's learned loss can generalize to out-of-distribution test time tasks, and exhibits qualitatively different behavior from other popular metalearning algorithms.",https://proceedings.neurips.cc/paper_files/paper/2018/file/7876acb66640bad41f1e1371ef30c180-Paper.pdf,2018,"loss, learning, agent, gradient, policy, algorithms, epg, metalearning, ability, account","loss, policy, function, learning, epg, agent, distribution, loop, task, time","{'evolved': 0.6865722477598337, 'gradients': 0.5675182881842838, 'policy': 0.45446841604493227}","loss, epg, metalearning, agent, policy, gradient, learning, evolve, evolved, parametrized"
Demystifying excessively volatile human learning: A Bayesian persistent prior and a neural approximation,"Chaitanya Ryali, Gautam Reddy, Angela J. Yu","Understanding how humans and animals learn about statistical regularities in stable and volatile environments, and utilize these regularities to make predictions and decisions, is an important problem in neuroscience and psychology. Using a Bayesian modeling framework, specifically the Dynamic Belief Model (DBM), it has previously been shown that humans tend to make the {\it default} assumption that environmental statistics undergo abrupt, unsignaled changes, even when environmental statistics are actually stable. Because exact Bayesian inference in this setting, an example of switching state space models, is computationally intense, a number of approximately Bayesian and heuristic algorithms have been proposed to account for learning/prediction in the brain. Here, we examine a neurally plausible algorithm, a special case of leaky integration dynamics we denote as EXP (for exponential filtering), that is significantly simpler than all previously suggested algorithms except for the delta-learning rule, and which far outperforms the delta rule in approximating Bayesian prediction performance. We derive the theoretical relationship between DBM and EXP, and show that EXP gains computational efficiency by foregoing the representation of inferential uncertainty (as does the delta rule), but that it nevertheless achieves near-Bayesian performance due to its ability to incorporate a ""persistent prior"" influence unique to DBM and absent from the other algorithms. Furthermore, we show that EXP is comparable to DBM but better than all other models in reproducing human behavior in a visual search task, suggesting that human learning and prediction also incorporates an element of persistent prior. More broadly, our work demonstrates that when observations are information-poor, detecting changes or modulating the learning rate is both {\it difficult} and (thus) {\it unnecessary} for making Bayes-optimal predictions.",https://proceedings.neurips.cc/paper_files/paper/2018/file/78b9cab19959e4af8ff46156ee460c74-Paper.pdf,2018,"bayesian, dbm, exp, learning, algorithms, delta, prediction, rule, changes, environmental","exp, dbm, learning, pt, p0, gt, model, data, prior, predictive","{'demystifying': 0.39592035594578345, 'excessively': 0.39592035594578345, 'persistent': 0.39592035594578345, 'volatile': 0.3737000512036594, 'human': 0.33571418934972796, 'prior': 0.3134938846076038, 'approximation': 0.28196277038398915, 'bayesian': 0.2265250025331732, 'neural': 0.17731599430368533, 'learning': 0.12980566961959406}","dbm, exp, bayesian, delta, rule, regularities, environmental, persistent, prediction, humans"
Distributionally Robust Graphical Models,"Rizal Fathony, Ashkan Rezaei, Mohammad Ali Bashiri, Xinhua Zhang, Brian Ziebart","In many structured prediction problems, complex relationships between variables are compactly defined using graphical structures. The most prevalent graphical prediction methods---probabilistic graphical models and large margin methods---have their own distinct strengths but also possess significant drawbacks. Conditional random fields (CRFs)  are Fisher consistent, but they do not permit integration of customized loss metrics into their learning process. Large-margin models, such as structured support vector machines (SSVMs), have the flexibility to incorporate customized loss metrics, but lack Fisher consistency guarantees. We present adversarial graphical models (AGM), a distributionally robust approach for constructing a predictor that performs robustly for a class of data distributions defined using a graphical structure. Our approach enjoys both the flexibility of incorporating customized loss metrics into its design as well as the statistical guarantee of Fisher consistency. We present exact learning and prediction algorithms for AGM with time complexity similar to existing graphical models and show the practical benefits of our approach with experiments.",https://proceedings.neurips.cc/paper_files/paper/2018/file/79a3308b13cd31f096d8a4a34f96b66b-Paper.pdf,2018,"graphical, models, approach, customized, fisher, loss, metrics, prediction, agm, consistency","loss, prediction, optimization, graphical, yi, metric, pt, learning, metrics, eq","{'distributionally': 0.6280346279804777, 'graphical': 0.5499989755154884, 'robust': 0.4256714903865904, 'models': 0.3491094602863048}","graphical, customized, fisher, metrics, agm, prediction, consistency, margin, flexibility, loss"
Natasha 2: Faster Non-Convex Optimization Than SGD,Zeyuan Allen-Zhu,"We design a stochastic algorithm to find $\varepsilon$-approximate local minima of any smooth nonconvex function in rate $O(\varepsilon^{-3.25})$, with only oracle access to stochastic gradients. The best result before this work was $O(\varepsilon^{-4})$ by stochastic gradient descent (SGD).",https://proceedings.neurips.cc/paper_files/paper/2018/file/79a49b3e3762632813f9e35f4ba53d6c-Paper.pdf,2018,"stochastic, varepsilon, 25, access, algorithm, approximate, best, descent, design, find","approximate, needed, algorithm, points, natasha1, sgd, saddle, gradient, point, local","{'natasha': 0.5473518437755004, 'sgd': 0.4333989227263003, 'faster': 0.42541652863661295, 'convex': 0.36568089300454926, 'non': 0.3396984826358052, 'optimization': 0.28718380080419464}","varepsilon, stochastic, 25, oracle, minima, access, sgd, nonconvex, smooth, gradients"
Iterative Value-Aware Model Learning,Amir-massoud Farahmand,"This paper introduces a model-based reinforcement learning (MBRL) framework that incorporates the underlying decision problem in learning the transition model of the environment. This is in contrast with conventional approaches to MBRL that learn the model of the environment, for example by finding the maximum likelihood estimate, without taking into account the decision problem. Value-Aware Model Learning (VAML) framework argues that this might not be a good idea, especially if the true model of the environment does not belong to the model class from which we are estimating the model. The original VAML framework, however, may result in an optimization problem that is difficult to solve. This paper introduces a new MBRL class of algorithms, called Iterative VAML, that benefits from the structure of how the planning is performed (i.e., through approximate value iteration) to devise a simpler optimization problem. The paper theoretically analyzes Iterative VAML and provides finite sample error upper bound guarantee for it.",https://proceedings.neurips.cc/paper_files/paper/2018/file/7a2347d96752880e3d58d72e9813cc14-Paper.pdf,2018,"model, problem, vaml, environment, framework, learning, mbrl, paper, class, decision","model, function, value, al, et, qk, error, space, ai, vk","{'iterative': 0.5802187497112803, 'value': 0.49676256337398833, 'aware': 0.47777586437285124, 'model': 0.3842978287269992, 'learning': 0.20154046832354}","vaml, mbrl, model, environment, introduces, iterative, problem, value, framework, decision"
PCA of high dimensional random walks with comparison to neural network training,"Joseph Antognini, Jascha Sohl-Dickstein","One technique to visualize the training of neural networks is to perform PCA on the parameters over the course of training and to project to the subspace spanned by the first few PCA components.  In this paper we compare this technique to the PCA of a high dimensional random walk.  We compute the eigenvalues and eigenvectors of the covariance of the trajectory and prove that in the long trajectory and high dimensional limit most of the variance is in the first few PCA components, and that the projection of the trajectory onto any subspace spanned by PCA components is a Lissajous curve.  We generalize these results to a random walk with momentum and to an Ornstein-Uhlenbeck processes (i.e., a random walk in a quadratic potential) and show that in high dimensions the walk is not mean reverting, but will instead be trapped at a fixed distance from the minimum.  We finally analyze PCA projected training trajectories for: a linear model trained on CIFAR-10; a fully connected model trained on MNIST; and ResNet-50-v2 trained on Imagenet. In all cases, both the distribution of PCA eigenvalues and the projected trajectories resemble those of a random walk with drift.",https://proceedings.neurips.cc/paper_files/paper/2018/file/7a576629fef88f3e636afd33b09e8289-Paper.pdf,2018,"pca, walk, random, components, high, trained, training, trajectory, dimensional, eigenvalues","10, pca, random, walk, matrix, distribution, training, eigenvalues, dimensional, components","{'comparison': 0.40878190581294216, 'walks': 0.40878190581294216, 'pca': 0.3781596130554932, 'dimensional': 0.34998446274545386, 'high': 0.33660775093349743, 'random': 0.3208798881524856, 'training': 0.27932802603048973, 'network': 0.26150878840664926, 'neural': 0.19396189497195207}","pca, walk, trajectory, random, spanned, components, eigenvalues, projected, subspace, trajectories"
Learning Libraries of Subroutines for Neurally–Guided Bayesian Program Induction,"Kevin Ellis, Lucas Morales, Mathias Sablé-Meyer, Armando Solar-Lezama, Josh Tenenbaum","Successful approaches to program induction require a hand-engineered
  domain-specific language (DSL), constraining the space of allowed
  programs and imparting prior knowledge of the domain.  We contribute
  a program induction algorithm that learns a DSL while
  jointly training a neural network to efficiently search for programs
  in the learned DSL.  We use our model to synthesize functions on lists,
  edit text, and solve symbolic regression problems, showing how the
  model learns a domain-specific library of program components for
  expressing solutions to problems in the domain.",https://proceedings.neurips.cc/paper_files/paper/2018/file/7aa685b3b1dc1d6780bf36f7340078c9-Paper.pdf,2018,"domain, dsl, program, induction, learns, model, problems, programs, specific, algorithm","programs, dsl, program, model, tasks, recognition, learning, algorithm, neural, search","{'induction': 0.4201128362780538, 'libraries': 0.4201128362780538, 'neurally': 0.4201128362780538, 'subroutines': 0.4201128362780538, 'guided': 0.33264974392061936, 'program': 0.32652296046415735, 'bayesian': 0.24036667949231832, 'learning': 0.13773736866494254}","dsl, domain, program, induction, programs, learns, specific, imparting, expressing, lists"
Streaming Kernel PCA with $\tilde{O}(\sqrt{n})$ Random Features,"Enayat Ullah, Poorya Mianjy, Teodor Vanislavov Marinov, Raman Arora","We study the statistical and computational aspects of kernel principal component analysis using random Fourier features and show that under mild assumptions, $O(\sqrt{n} \log n)$ features suffices to achieve $O(1/\epsilon^2)$ sample complexity. Furthermore, we give a memory efficient streaming algorithm based on classical Oja's algorithm that achieves this rate",https://proceedings.neurips.cc/paper_files/paper/2018/file/7ae11af20803185120e83d3ce4fb4ed7-Paper.pdf,2018,"algorithm, features, achieve, achieves, analysis, aspects, assumptions, based, classical, complexity","hs, kernel, random, operator, space, features, algorithm, log, erm, l2","{'sqrt': 0.4404194408720499, 'tilde': 0.4404194408720499, 'pca': 0.38456104402208025, 'streaming': 0.3640492140227281, 'kernel': 0.3487287261243892, 'random': 0.3263116962611766, 'features': 0.32179396801804344}","features, oja, suffices, aspects, fourier, streaming, mild, principal, component, classical"
Faster Neural Networks Straight from JPEG,"Lionel Gueguen, Alex Sergeev, Ben Kadlec, Rosanne Liu, Jason Yosinski","The simple, elegant approach of training convolutional neural
  networks (CNNs) directly from RGB pixels has enjoyed overwhelming
  empirical success. But can more performance be squeezed out of
  networks by using different input representations?  In this paper we
  propose and explore a simple idea: train CNNs directly on the
  blockwise discrete cosine transform (DCT) coefficients computed and
  available in the middle of the JPEG codec. Intuitively, when
  processing JPEG images using CNNs, it seems unnecessary to
  decompress a blockwise frequency representation to an expanded pixel
  representation, shuffle it from CPU to GPU, and then process it with
  a CNN that will learn something similar to a transform back to
  frequency representation in its first layers. Why not skip both
  steps and feed the frequency domain into the network directly?  In
  this paper we modify \libjpeg to produce DCT coefficients directly,
  modify a ResNet-50 network to accommodate the differently sized and
  strided input, and evaluate performance on ImageNet. We find
  networks that are both faster and more accurate, as well as networks
  with about the same accuracy but 1.77x faster than ResNet-50.",https://proceedings.neurips.cc/paper_files/paper/2018/file/7af6266cc52234b5aa339b16695f7fc4-Paper.pdf,2018,"directly, networks, cnns, frequency, representation, 50, blockwise, coefficients, dct, faster","dct, resnet, rfa, 50, block4, late, channels, network, concat, jpeg","{'jpeg': 0.5769567699175431, 'straight': 0.5769567699175431, 'faster': 0.4484262710045528, 'neural': 0.2583945528230961, 'networks': 0.25765850623826503}","frequency, directly, dct, cnns, blockwise, jpeg, modify, 50, coefficients, transform"
Bayesian Model Selection Approach to Boundary Detection with Non-Local Priors,"Fei Jiang, Guosheng Yin, Francesca Dominici","Based on non-local prior distributions, we propose a Bayesian model selection (BMS) procedure for boundary detection in a sequence of data with multiple systematic mean changes. The BMS method can effectively suppress the non-boundary spike points with large instantaneous changes. We speed up the algorithm by reducing the multiple change points to a series of single change point detection problems. We establish the consistency of the estimated number and locations of the change points under various prior distributions. Extensive simulation studies are conducted to compare the BMS with existing methods, and our approach is illustrated with application to the magnetic resonance imaging guided radiation therapy data.",https://proceedings.neurips.cc/paper_files/paper/2018/file/7b13b2203029ed80337f27127a9f1d28-Paper.pdf,2018,"bms, change, points, boundary, changes, data, detection, distributions, multiple, non","points, change, ni, bms, p0, pr, yn, data, prior, local","{'boundary': 0.4541397490315192, 'priors': 0.36699651676258943, 'selection': 0.3595926098280966, 'local': 0.3234249006281245, 'approach': 0.3160209936936317, 'detection': 0.3126222981831882, 'model': 0.2839102117203139, 'non': 0.28184902527502487, 'bayesian': 0.2598351063663594}","bms, change, points, boundary, changes, detection, radiation, therapy, instantaneous, magnetic"
Densely Connected Attention Propagation for Reading Comprehension,"Yi Tay, Anh Tuan Luu, Siu Cheung Hui, Jian Su","We propose DecaProp (Densely Connected Attention Propagation), a new densely connected neural architecture for reading comprehension (RC). There are two distinct characteristics of our model. Firstly, our model densely connects all pairwise layers of the network, modeling relationships between passage and query across all hierarchical levels. Secondly, the dense connectors in our network are learned via attention instead of standard residual skip-connectors. To this end, we propose novel Bidirectional Attention Connectors (BAC) for efficiently forging connections throughout the network. We conduct extensive experiments on four challenging RC benchmarks. Our proposed approach achieves state-of-the-art results on all four, outperforming existing baselines by up to 2.6% to 14.2% in absolute F1 score.",https://proceedings.neurips.cc/paper_files/paper/2018/file/7b66b4fd401a271a1c7224027ce111bc-Paper.pdf,2018,"attention, connectors, densely, network, connected, four, model, propose, rc, 14","al, et, attention, layers, layer, model, 2017, bac, f1, 2016","{'connected': 0.44193921062486174, 'densely': 0.44193921062486174, 'reading': 0.44193921062486174, 'comprehension': 0.39953815306233936, 'propagation': 0.37473512439734463, 'attention': 0.3376567089919341}","connectors, densely, rc, attention, four, connected, network, bac, decaprop, forging"
Query Complexity of Bayesian Private Learning,Kuang Xu,"We study the query complexity of Bayesian Private Learning: a learner wishes to locate a random target within an interval by submitting queries, in the presence of an adversary who observes all of her queries but not the responses. How many queries are necessary and sufficient in order for the learner to accurately estimate the target, while simultaneously concealing the target from the adversary? 

Our main result is a query complexity lower bound that is tight up to the first order. We show that if the learner wants to estimate the target within an error of $\epsilon$, while ensuring that no adversary estimator can achieve a constant additive error with probability greater than $1/L$, then the query complexity is on the order of $L\log(1/\epsilon)$ as $\epsilon \to 0$. Our result demonstrates that increased privacy, as captured by $L$, comes at the expense of a \emph{multiplicative} increase in query complexity. The proof  builds on Fano's inequality and properties of certain proportional-sampling estimators.",https://proceedings.neurips.cc/paper_files/paper/2018/file/7bccfde7714a1ebadf06c5f4cea752c1-Paper.pdf,2018,"complexity, query, target, adversary, epsilon, learner, order, queries, error, estimate","learner, query, queries, log, strategy, adversary, target, complexity, bound, private","{'query': 0.5782163534994245, 'complexity': 0.5064258790203713, 'private': 0.48095780289318785, 'bayesian': 0.3659341670142792, 'learning': 0.20969133232443402}","query, adversary, queries, target, learner, complexity, epsilon, order, estimate, within"
NAIS-Net: Stable Deep Networks from Non-Autonomous  Differential Equations,"Marco Ciccone, Marco Gallieri, Jonathan Masci, Christian Osendorfer, Faustino Gomez","This paper introduces Non-Autonomous Input-Output Stable Network (NAIS-Net), a very deep architecture where each stacked processing block is derived from a time-invariant non-autonomous dynamical system. Non-autonomy is implemented by skip connections from the block input to each of the unrolled processing stages and allows stability to be enforced so that blocks can be unrolled adaptively to a  pattern-dependent processing depth. NAIS-Net induces non-trivial, Lipschitz input-output maps, even for an infinite unroll length. We prove that the network is globally asymptotically stable so that for every initial condition there is exactly one input-dependent equilibrium assuming tanh units, and multiple stable equilibria for ReL units. An efficient implementation that enforces the stability under derived conditions for both fully-connected and convolutional layers is also presented. Experimental results show how NAIS-Net exhibits stability in practice, yielding a significant reduction in generalization gap compared to ResNets.",https://proceedings.neurips.cc/paper_files/paper/2018/file/7bd28f15a49d5e5848d6ec70e584e625-Paper.pdf,2018,"input, non, nais, net, processing, stability, stable, autonomous, block, dependent","nais, input, net, stability, resnet, network, non, layer, block, stable","{'nais': 0.42684815442094665, 'autonomous': 0.4028920836420411, 'equations': 0.38589498092799285, 'stable': 0.3619389101490873, 'net': 0.34494180743503916, 'differential': 0.33175783031286094, 'non': 0.2649112668964753, 'deep': 0.20000202262461594, 'networks': 0.19062270102902085}","nais, net, stability, unrolled, stable, input, processing, autonomous, non, units"
Sequential Test for the Lowest Mean: From Thompson to Murphy Sampling,"Emilie Kaufmann, Wouter M. Koolen, Aurélien Garivier","Learning the minimum/maximum mean among a finite set of distributions is a fundamental sub-problem in planning, game tree search and reinforcement learning. We formalize this learning task as the problem of sequentially testing how the minimum mean among a finite set of distributions compares to a given threshold. We develop refined non-asymptotic lower bounds, which show that optimality mandates very different sampling behavior for a low vs high true minimum. We show that Thompson Sampling and the intuitive Lower Confidence Bounds policy each nail only one of these cases. We develop a novel approach that we call Murphy Sampling. Even though it entertains exclusively low true minima, we prove that MS is optimal for both possibilities. We then design advanced self-normalized deviation inequalities, fueling more aggressive stopping rules. We complement our theoretical guarantees by experiments showing that MS works best in practice.",https://proceedings.neurips.cc/paper_files/paper/2018/file/7c78335a8924215ea5c22fda1aac7b75-Paper.pdf,2018,"learning, minimum, sampling, among, bounds, develop, distributions, finite, low, lower","sampling, stopping, rule, µa, ln, arms, lower, ms, algorithm, bound","{'lowest': 0.42338862203866734, 'murphy': 0.42338862203866734, 'test': 0.42338862203866734, 'thompson': 0.38276736711766607, 'mean': 0.35900545625664565, 'sequential': 0.32906898898801307, 'sampling': 0.28025711359173366}","minimum, ms, sampling, true, mean, among, finite, entertains, fueling, mandates"
Content preserving text generation with attribute controls,"Lajanugen Logeswaran, Honglak Lee, Samy Bengio","In this work, we address the problem of modifying textual attributes of sentences. Given an input sentence and a set of attribute labels, we attempt to generate sentences that are compatible with the conditioning information. To ensure that the model generates content compatible sentences, we introduce a reconstruction loss which interpolates between auto-encoding and back-translation loss components. We propose an adversarial loss to enforce generated samples to be attribute compatible and realistic. Through quantitative, qualitative and human evaluations we demonstrate that our model is capable of generating fluent sentences that better reflect the conditioning information compared to prior methods. We further demonstrate that the model is capable of simultaneously controlling multiple attributes.",https://proceedings.neurips.cc/paper_files/paper/2018/file/7cf64379eb6f29a4d25c4b6a2df713e4-Paper.pdf,2018,"sentences, compatible, loss, model, attribute, attributes, capable, conditioning, demonstrate, information","attribute, model, sentence, sentences, content, attributes, models, generated, loss, data","{'controls': 0.47028219079721884, 'content': 0.44388846428789297, 'attribute': 0.425161817308684, 'preserving': 0.38004144382014926, 'text': 0.38004144382014926, 'generation': 0.3349210703316145}","sentences, compatible, attribute, conditioning, attributes, loss, capable, fluent, modifying, interpolates"
Latent Gaussian Activity Propagation: Using Smoothness and Structure to Separate and Localize Sounds in Large Noisy Environments,"Daniel Johnson, Daniel Gorelik, Ross E. Mawhorter, Kyle Suver, Weiqing Gu, Steven Xing, Cody Gabriel, Peter Sankhagowit","We present an approach for simultaneously separating and localizing
multiple sound sources using recorded microphone data. Inspired by topic
models, our approach is based on a probabilistic model of inter-microphone
phase differences, and poses separation and localization as a Bayesian
inference problem. We assume sound activity is locally smooth across time,
frequency, and location, and use the known position of the microphones to
obtain a consistent separation. We compare the performance of our method
against existing algorithms on simulated anechoic voice data and find that it
obtains high performance across a variety of input conditions.",https://proceedings.neurips.cc/paper_files/paper/2018/file/7dd0240cd412efde8bc165e864d3644f-Paper.pdf,2018,"across, approach, data, microphone, performance, separation, sound, activity, algorithms, anechoic","source, microphone, time, location, sources, microphones, frequency, activity, phase, across","{'activity': 0.33100370964849435, 'localize': 0.33100370964849435, 'sounds': 0.33100370964849435, 'separate': 0.31242673276739086, 'smoothness': 0.31242673276739086, 'environments': 0.28066918102996935, 'propagation': 0.28066918102996935, 'noisy': 0.2620922041488658, 'latent': 0.2452443555858408, 'structure': 0.23573105443622996}","microphone, separation, sound, anechoic, separating, across, localizing, microphones, voice, position"
Differentially Private Testing of Identity and Closeness of Discrete Distributions,"Jayadev Acharya, Ziteng Sun, Huanyu Zhang","We study the fundamental problems of identity testing (goodness of fit), and closeness testing (two sample test) of distributions over $k$ elements, under differential privacy. While the problems have a long history in statistics,  finite sample bounds for these problems have only been established recently. 

In this work, we derive upper and lower bounds on the sample complexity of both the problems under $(\varepsilon, \delta)$-differential privacy. We provide optimal sample complexity algorithms for identity testing problem for all parameter ranges, and the first results for closeness testing. Our closeness testing bounds are optimal in the sparse regime where the number of samples is at most $k$. 

Our upper bounds are obtained by privatizing non-private estimators for these problems. The non-private estimators are chosen to have small sensitivity. We propose a general framework to establish lower bounds on the sample complexity of statistical tasks under differential privacy. We show a bound on differentially private algorithms in terms of a coupling between the two hypothesis classes we aim to test. By constructing carefully chosen priors over the hypothesis classes, and using Le Cam's two point theorem we provide a general mechanism for proving lower bounds.  We believe that the framework can be used to obtain strong lower bounds for other statistical tasks under privacy.",https://proceedings.neurips.cc/paper_files/paper/2018/file/7de32147a4f1055bed9e4faf3485a84d-Paper.pdf,2018,"bounds, problems, sample, testing, lower, privacy, closeness, complexity, differential, private","testing, xm, bounds, privacy, identity, sample, algorithm, lower, distribution, complexity","{'closeness': 0.4409742808170052, 'identity': 0.4409742808170052, 'discrete': 0.37391692795111714, 'differentially': 0.3491680542040581, 'distributions': 0.3491680542040581, 'testing': 0.3427370344990408, 'private': 0.3316084488322881}","bounds, testing, closeness, privacy, sample, private, lower, differential, problems, identity"
Non-Ergodic Alternating Proximal  Augmented Lagrangian Algorithms with Optimal Rates,Quoc Tran Dinh,"We develop two new non-ergodic alternating proximal augmented Lagrangian algorithms (NEAPAL) to solve a class of nonsmooth constrained convex optimization problems. Our approach relies on a novel combination of the augmented Lagrangian framework,  alternating/linearization scheme, Nesterov's acceleration techniques, and adaptive strategy for parameters. Our algorithms have several new features compared to existing methods. Firstly, they have a Nesterov's acceleration step on the primal variables compared to the dual one in  several methods in the literature.
Secondly, they achieve non-ergodic optimal convergence rates under standard assumptions, i.e. an $\mathcal{O}\left(\frac{1}{k}\right)$ rate without any smoothness or strong convexity-type assumption, or an $\mathcal{O}\left(\frac{1}{k^2}\right)$ rate under only semi-strong convexity, where $k$ is the iteration counter. 
Thirdly, they preserve or have better per-iteration complexity compared to existing algorithms. Fourthly, they can be implemented in a parallel fashion.
Finally, all the parameters are adaptively updated without heuristic tuning.
We verify our algorithms on different numerical examples and compare them with some state-of-the-art methods.",https://proceedings.neurips.cc/paper_files/paper/2018/file/7e3b7a5bafcb0fa8e8dfe3ea6aca9186-Paper.pdf,2018,"algorithms, compared, methods, acceleration, alternating, augmented, convexity, ergodic, existing, frac","algorithm, zk, 10, neapal, yk, convex, rate, admm, ergodic, algorithms","{'ergodic': 0.3940550106170236, 'lagrangian': 0.3940550106170236, 'augmented': 0.37193939496459116, 'alternating': 0.35624811594400124, 'rates': 0.35624811594400124, 'proximal': 0.3257245334420233, 'algorithms': 0.25629212344657015, 'optimal': 0.25629212344657015, 'non': 0.24455912719377346}","ergodic, lagrangian, nesterov, left, mathcal, augmented, alternating, frac, compared, acceleration"
Multi-objective Maximization of Monotone Submodular Functions with Cardinality Constraint,Rajan Udwani,"We consider the problem of multi-objective maximization of monotone submodular functions subject to cardinality constraint, often formulated as $\max_{|A|=k}\min_{i\in\{1,\dots,m\}}f_i(A)$. While it is widely known that greedy methods work well for a single objective, the problem becomes much harder with multiple objectives. In fact, Krause et al.\ (2008) showed that when the number of objectives $m$ grows as the cardinality $k$ i.e., $m=\Omega(k)$, the problem is inapproximable (unless $P=NP$). On the other hand, when $m$ is constant Chekuri et al.\ (2010) showed a randomized $(1-1/e)-\epsilon$ approximation with runtime (number of queries to function oracle) $n^{m/\epsilon^3}$. %In fact, the result of Chekuri et al.\ (2010) is for the far more general case of matroid constant. 
	
	We focus on finding a fast and practical algorithm that has (asymptotic) approximation guarantees even when $m$ is super constant. We first modify the algorithm of Chekuri et al.\ (2010) to achieve a $(1-1/e)$ approximation for $m=o(\frac{k}{\log^3 k})$. This demonstrates a steep transition from constant factor approximability to inapproximability around $m=\Omega(k)$. Then using Multiplicative-Weight-Updates (MWU), we find a much faster $\tilde{O}(n/\delta^3)$ time asymptotic $(1-1/e)^2-\delta$ approximation. While the above results are all randomized, we also give a simple deterministic $(1-1/e)-\epsilon$ approximation with runtime $kn^{m/\epsilon^4}$. Finally, we run synthetic experiments using Kronecker graphs and find that our MWU inspired heuristic outperforms existing heuristics.",https://proceedings.neurips.cc/paper_files/paper/2018/file/7e448ed9dd44e6e22442dac8e21856ae-Paper.pdf,2018,"approximation, al, constant, epsilon, et, 2010, chekuri, problem, algorithm, asymptotic","fi, set, algorithm, stage, approximation, s1, mwu, greedy, problem, function","{'cardinality': 0.3943381668793674, 'monotone': 0.3943381668793674, 'constraint': 0.3808657310419358, 'objective': 0.3808657310419358, 'maximization': 0.35248895877547265, 'submodular': 0.32317611124675266, 'functions': 0.31453630551238665, 'multi': 0.266932683828062}","chekuri, 2010, al, et, epsilon, approximation, constant, mwu, cardinality, runtime"
Fully Understanding The Hashing Trick,"Casper B. Freksen, Lior Kamma, Kasper Green Larsen","Feature hashing, also known as {\em the hashing trick}, introduced by Weinberger et al. (2009), is one of the key techniques used in scaling-up machine learning algorithms. Loosely speaking, feature hashing uses a random sparse projection matrix $A : \mathbb{R}^n \to \mathbb{R}^m$ (where $m \ll n$) in order to reduce the dimension of the data from $n$ to $m$ while approximately preserving the Euclidean norm. Every column of $A$ contains exactly one non-zero entry, equals to either $-1$ or $1$.

Weinberger et al. showed tail bounds on $\|Ax\|_2^2$. Specifically they showed that for every $\varepsilon, \delta$, if $\|x\|_{\infty} / \|x\|_2$ is sufficiently small, and $m$ is sufficiently large, then 
\begin{equation*}\Pr[ \; | \;\|Ax\|_2^2 - \|x\|_2^2\; | < \varepsilon \|x\|_2^2 \;] \ge 1 - \delta \;.\end{equation*}
These bounds were later extended by Dasgupta et al. (2010) and most recently refined by Dahlgaard et al. (2017), however, the true nature of the performance of this key technique, and specifically the correct tradeoff between the pivotal parameters $\|x\|_{\infty} / \|x\|_2, m, \varepsilon, \delta$ remained an open question.

We settle this question by giving tight asymptotic bounds on the exact tradeoff between the central parameters, thus providing a complete understanding of the performance of feature hashing. We complement the asymptotic bound with empirical data, which shows that the constants ""hiding"" in the asymptotic notation are, in fact, very close to $1$, thus further illustrating the tightness of the presented bounds in practice.",https://proceedings.neurips.cc/paper_files/paper/2018/file/7e83722522e8aeb7512b7075311316b7-Paper.pdf,2018,"_2, al, bounds, et, hashing, asymptotic, delta, feature, varepsilon, ax","lg, ln, hashing, data, every, figure, vectors, min, theorem, bound","{'hashing': 0.5540688455699085, 'trick': 0.5229727465394102, 'fully': 0.4837962949377944, 'understanding': 0.43063716252817436}","_2, hashing, al, et, varepsilon, delta, weinberger, bounds, asymptotic, ax"
Policy-Conditioned Uncertainty Sets for Robust Markov Decision Processes,"Andrea Tirinzoni, Marek Petrik, Xiangli Chen, Brian Ziebart","What policy should be employed in a Markov decision process with uncertain parameters? Robust optimization answer to this question is to use rectangular uncertainty sets, which independently reflect available knowledge about each state, and then obtains a decision policy that maximizes expected reward for the worst-case decision process parameters from these uncertainty sets. While this rectangularity is convenient computationally and leads to tractable solutions, it often produces policies that are too conservative in practice, and does not facilitate knowledge transfer between portions of the state space or across related decision processes. In this work, we propose non-rectangular uncertainty sets that bound marginal moments of state-action features defined over entire trajectories through a decision process. This enables generalization to different portions of the state space while retaining appropriate uncertainty of the decision process. We develop algorithms for solving the resulting robust decision problems, which reduce to finding an optimal policy for a mixture of decision processes, and demonstrate the benefits of our approach experimentally.",https://proceedings.neurips.cc/paper_files/paper/2018/file/7ec0dbeee45813422897e04ad8424a5e-Paper.pdf,2018,"decision, process, state, uncertainty, policy, sets, knowledge, parameters, portions, processes","st, state, bt, policy, control, s1, a1, robust, dynamics, action","{'conditioned': 0.41730370765643293, 'sets': 0.3913978066963408, 'uncertainty': 0.3730172774525229, 'markov': 0.3654919057362486, 'decision': 0.34711137649243073, 'processes': 0.31447380114370593, 'policy': 0.3055441572242713, 'robust': 0.2952995745722464}","decision, uncertainty, portions, rectangular, process, sets, policy, state, processes, robust"
Visual Reinforcement Learning with Imagined Goals,"Ashvin V. Nair, Vitchyr Pong, Murtaza Dalal, Shikhar Bahl, Steven Lin, Sergey Levine","For an autonomous agent to fulfill a wide range of user-specified goals at test time, it must be able to learn broadly applicable and general-purpose skill repertoires. Furthermore, to provide the requisite level of generality, these skills must handle raw sensory input such as images. In this paper, we propose an algorithm that acquires such general-purpose skills by combining unsupervised representation learning and reinforcement learning of goal-conditioned policies. Since the particular goals that might be required at test-time are not known in advance, the agent performs a self-supervised ""practice"" phase where it imagines goals and attempts to achieve them. We learn a visual representation with three distinct purposes: sampling goals for self-supervised practice, providing a structured transformation of raw sensory inputs, and computing a reward signal for goal reaching. We also propose a retroactive goal relabeling scheme to further improve the sample-efficiency of our method. Our off-policy algorithm is efficient enough to learn policies that operate on raw image observations and goals in a real-world physical system, and substantially outperforms prior techniques.",https://proceedings.neurips.cc/paper_files/paper/2018/file/7ec69dd44416c46745f6edd947b470cd-Paper.pdf,2018,"goals, goal, learn, raw, agent, algorithm, general, learning, must, policies","goal, goals, learning, latent, method, policy, state, reward, use, vae","{'goals': 0.5810992313922434, 'imagined': 0.5810992313922434, 'visual': 0.4138416898518872, 'reinforcement': 0.34217728484608095, 'learning': 0.19051805170793193}","goals, raw, skills, goal, sensory, purpose, must, self, learn, policies"
DropBlock: A regularization method for convolutional networks,"Golnaz Ghiasi, Tsung-Yi Lin, Quoc V. Le","Deep neural networks often work well when they are over-parameterized and trained with a massive amount of noise and regularization, such as weight decay and dropout. Although dropout is widely used as a regularization technique for fully connected layers, it is often less effective for convolutional layers. This lack of success of dropout for convolutional layers is perhaps due to the fact that activation units in  convolutional layers are spatially correlated so information can still flow through convolutional networks despite dropout. Thus a structured form of dropout is needed to regularize convolutional networks. In this paper, we introduce DropBlock, a form of structured dropout, where units in a contiguous region of a feature map are dropped together. We found that applying DropbBlock in skip connections in addition to the convolution layers increases the accuracy. Also, gradually increasing number of dropped units during training leads to better accuracy and more robust to hyperparameter choices. Extensive experiments show that DropBlock works better than dropout in regularizing convolutional networks.
  On ImageNet classification, ResNet-50 architecture with DropBlock achieves $78.13\%$ accuracy, which is more than $1.6\%$ improvement on the baseline. On COCO detection, DropBlock improves Average Precision of RetinaNet from $36.8\%$ to $38.4\%$.",https://proceedings.neurips.cc/paper_files/paper/2018/file/7edcfb2d8f6a659ef4cd1e6c9b6d7079-Paper.pdf,2018,"dropout, convolutional, layers, dropblock, networks, accuracy, units, better, dropped, form","dropblock, block_size, model, dropout, trained, keep_prob, resnet, accuracy, 50, training","{'dropblock': 0.611651298159334, 'method': 0.44106397988312845, 'regularization': 0.43559996839764886, 'convolutional': 0.408638785932429, 'networks': 0.273152457930172}","dropout, dropblock, convolutional, layers, units, dropped, accuracy, networks, structured, regularization"
Exact natural gradient in deep linear networks and its application to the nonlinear case,"Alberto Bernacchia, Mate Lengyel, Guillaume Hennequin","Stochastic gradient descent (SGD) remains the method of choice for deep learning, despite the limitations arising for ill-behaved objective functions. In cases where it could be estimated, the natural gradient has proven very effective at mitigating the catastrophic effects of pathological curvature in the objective function, but little is known theoretically about its convergence properties, and it has yet to find a practical implementation that would scale to very deep and large networks. Here, we derive an exact expression for the natural gradient in deep linear networks, which exhibit pathological curvature similar to the nonlinear case. We provide for the first time an analytical solution for its convergence rate, showing that the loss decreases exponentially to the global minimum in parameter space. Our expression for the natural gradient is surprisingly simple, computationally tractable, and explains why some approximations proposed previously work well in practice. This opens new avenues for approximating the natural gradient in the nonlinear case, and we show in preliminary experiments that our online natural gradient descent outperforms SGD on MNIST autoencoding while sharing its computational simplicity.",https://proceedings.neurips.cc/paper_files/paper/2018/file/7f018eb7b301a66658931cb8a93fd6e8-Paper.pdf,2018,"gradient, natural, deep, case, convergence, curvature, descent, expression, networks, nonlinear","gradient, natural, matrix, eq, linear, deep, loss, network, networks, et","{'exact': 0.4054517465023194, 'nonlinear': 0.4054517465023194, 'application': 0.3915996188141616, 'case': 0.3915996188141616, 'natural': 0.362423107784085, 'linear': 0.28691268746234816, 'gradient': 0.25802423821922255, 'deep': 0.21013792193446112, 'networks': 0.2002832658495442}","natural, gradient, pathological, curvature, expression, sgd, nonlinear, deep, avenues, descent"
RetGK: Graph Kernels based on Return Probabilities of Random Walks,"Zhen Zhang, Mianzhi Wang, Yijian Xiang, Yan Huang, Arye Nehorai","Graph-structured data arise in wide applications, such as computer vision, bioinformatics, and social networks. Quantifying similarities among graphs is a fundamental problem. In this paper, we develop a framework for computing graph kernels, based on return probabilities of random walks. The advantages of our proposed kernels are that they can effectively exploit various node attributes, while being scalable to large datasets. We conduct extensive graph classification experiments to evaluate our graph kernels. The experimental results show that our graph kernels significantly outperform other state-of-the-art approaches in both accuracy and computational efficiency.",https://proceedings.neurips.cc/paper_files/paper/2018/file/7f16109f1619fd7a733daf5a84c708c1-Paper.pdf,2018,"graph, kernels, accuracy, advantages, among, applications, approaches, arise, art, attributes","graph, graphs, kernels, rpf, datasets, nodes, set, vi, let, return","{'retgk': 0.40765636843802006, 'return': 0.40765636843802006, 'probabilities': 0.38477740149245376, 'walks': 0.38477740149245376, 'kernels': 0.36854451611011774, 'random': 0.30203716896168353, 'based': 0.272351589831402, 'graph': 0.2651384032394382}","kernels, graph, bioinformatics, quantifying, walks, similarities, return, attributes, arise, probabilities"
Revisiting Multi-Task Learning with ROCK: a Deep Residual Auxiliary Block for Visual Detection,"Taylor Mordan, Nicolas THOME, Gilles Henaff, Matthieu Cord","Multi-Task Learning (MTL) is appealing for deep learning regularization. In this paper, we tackle a specific MTL context denoted as primary MTL, where the ultimate goal is to improve the performance of a given primary task by leveraging several other auxiliary tasks. Our main methodological contribution is to introduce ROCK, a new generic multi-modal fusion block for deep learning tailored to the primary MTL context. ROCK architecture is based on a residual connection, which makes forward prediction explicitly impacted by the intermediate auxiliary representations. The auxiliary predictor's architecture is also specifically designed to our primary MTL context, by incorporating intensive pooling operators for maximizing complementarity of intermediate representations. Extensive experiments on NYUv2 dataset (object detection with scene classification, depth prediction, and surface normal estimation as auxiliary tasks) validate the relevance of the approach and its superiority to flat MTL approaches. Our method outperforms state-of-the-art object detection models on NYUv2 dataset by a large margin, and is also able to handle large-scale heterogeneous inputs (real and synthetic images) with missing annotation modalities.",https://proceedings.neurips.cc/paper_files/paper/2018/file/7f5d04d189dfb634e6a85bb9d9adf21e-Paper.pdf,2018,"mtl, auxiliary, primary, context, learning, also, architecture, dataset, deep, detection","rock, auxiliary, task, tasks, detection, map, primary, mtl, depth, block","{'auxiliary': 0.38319681364024055, 'residual': 0.38319681364024055, 'rock': 0.38319681364024055, 'revisiting': 0.3464316890205855, 'block': 0.3167491846988459, 'task': 0.3034192505469089, 'visual': 0.2729014397812756, 'detection': 0.26378635385288096, 'multi': 0.2345041598310272, 'deep': 0.17954894966179377}","mtl, primary, auxiliary, nyuv2, rock, context, intermediate, detection, object, architecture"
Gradient Descent Meets Shift-and-Invert Preconditioning for Eigenvector Computation,Zhiqiang Xu,"Shift-and-invert preconditioning, as a classic acceleration technique for the leading eigenvector computation, has received much attention again recently, owing to fast least-squares solvers for efficiently approximating matrix inversions in power iterations. In this work, we adopt an inexact Riemannian gradient descent perspective to investigate this technique on the effect of the step-size scheme. The shift-and-inverted power method is included as a special case with adaptive step-sizes. Particularly, two other step-size settings, i.e., constant step-sizes and Barzilai-Borwein (BB) step-sizes, are examined theoretically and/or empirically. We present a novel convergence analysis for the constant step-size setting that achieves a rate at $\tilde{O}(\sqrt{\frac{\lambda_{1}}{\lambda_{1}-\lambda_{p+1}}})$, where $\lambda_{i}$ represents the $i$-th largest eigenvalue of the given real symmetric matrix and $p$ is the multiplicity of $\lambda_{1}$. Our experimental studies show that the proposed algorithm can be significantly faster than the shift-and-inverted power method in practice.",https://proceedings.neurips.cc/paper_files/paper/2018/file/7fb8ceb3bd59c7956b1df66729296a4c-Paper.pdf,2018,"step, lambda_, power, shift, size, sizes, constant, inverted, matrix, method","10, xt, λ1, si, log, sin2, αt, al, et, rgeigs","{'invert': 0.40658846994358616, 'preconditioning': 0.40658846994358616, 'shift': 0.40658846994358616, 'eigenvector': 0.3675790752036164, 'meets': 0.3675790752036164, 'computation': 0.3285696804636466, 'descent': 0.2667412526249653, 'gradient': 0.2339225609037964}","lambda_, step, shift, sizes, inverted, power, size, constant, matrix, technique"
Inequity aversion improves cooperation in intertemporal social dilemmas,"Edward Hughes, Joel Z. Leibo, Matthew Phillips, Karl Tuyls, Edgar Dueñez-Guzman, Antonio García Castañeda, Iain Dunning, Tina Zhu, Kevin McKee, Raphael Koster, Heather Roff, Thore Graepel","Groups of humans are often able to find ways to cooperate with one another in complex, temporally extended social dilemmas. Models based on behavioral economics are only able to explain this phenomenon for unrealistic stateless matrix games. Recently, multi-agent reinforcement learning has been applied to generalize social dilemma problems to temporally and spatially extended Markov games. However, this has not yet generated an agent that learns to cooperate in social dilemmas as humans do. A key insight is that many, but not all, human individuals have inequity averse social preferences. This promotes a particular resolution of the matrix game social dilemma wherein inequity-averse individuals are personally pro-social and punish defectors. Here we extend this idea to Markov games and show that it promotes cooperation in several types of sequential social dilemma, via a profitable interaction with policy learnability. In particular, we find that inequity aversion improves temporal credit assignment for the important class of intertemporal social dilemmas. These results help explain how large-scale cooperation may emerge and persist.",https://proceedings.neurips.cc/paper_files/paper/2018/file/7fea637fd6d02b8f0adf6f7dc36aed93-Paper.pdf,2018,"social, dilemma, dilemmas, games, inequity, able, agent, averse, cooperate, cooperation","inequity, aversion, agents, game, social, cooperation, dilemmas, games, advantageous, reward","{'aversion': 0.3839965200103189, 'dilemmas': 0.3839965200103189, 'improves': 0.3839965200103189, 'inequity': 0.3839965200103189, 'intertemporal': 0.3839965200103189, 'cooperation': 0.36244541871833874, 'social': 0.36244541871833874}","social, dilemma, dilemmas, inequity, averse, promotes, games, cooperate, cooperation, temporally"
Towards Deep Conversational Recommendations,"Raymond Li, Samira Ebrahimi Kahou, Hannes Schulz, Vincent Michalski, Laurent Charlin, Chris Pal","There has been growing interest in using neural networks and deep learning techniques to create dialogue systems. Conversational recommendation is an interesting setting for the scientific exploration of dialogue with natural language as the associated discourse involves goal-driven dialogue that often transforms naturally into more free-form chat. This paper provides two contributions. First, until now there has been no publicly available large-scale data set consisting of real-world dialogues centered around recommendations.
To address this issue and to facilitate our exploration here, we have collected ReDial, a data set consisting of over 10,000 conversations centered around the theme of providing movie recommendations. We make this data available to the community for further research. Second, we use this dataset to explore multiple facets of  conversational recommendations. In particular we explore new neural architectures, mechanisms and methods suitable for composing conversational recommendation systems. Our dataset allows us to systematically probe model sub-components addressing different parts of the overall problem domain ranging from: sentiment analysis and cold-start recommendation generation to detailed aspects of how natural language is used in this setting in the real world. We combine such sub-components into a full-blown dialogue system and examine its behavior.",https://proceedings.neurips.cc/paper_files/paper/2018/file/800de15c79c8d840f4e78d3af937d4d4-Paper.pdf,2018,"dialogue, conversational, data, recommendation, recommendations, around, available, centered, components, consisting","movie, dialogue, model, recommendation, data, seeker, dataset, movies, recommendations, recommender","{'recommendations': 0.6218727117727115, 'conversational': 0.5622082601592089, 'towards': 0.4607524568745016, 'deep': 0.29138183890784947}","dialogue, conversational, recommendations, recommendation, centered, consisting, around, exploration, components, sub"
Deep Generative Models for Distribution-Preserving Lossy Compression,"Michael Tschannen, Eirikur Agustsson, Mario Lucic","We propose and study the problem of distribution-preserving lossy compression. Motivated by recent advances in extreme image compression which allow to maintain artifact-free reconstructions even at very low bitrates, we propose to optimize the rate-distortion tradeoff under the constraint that the reconstructed samples follow the distribution of the training data. The resulting compression system recovers both ends of the spectrum: On one hand, at zero bitrate it learns a generative model of the data, and at high enough bitrates it achieves perfect reconstruction. Furthermore, for intermediate bitrates it smoothly interpolates between learning a generative model of the training data and perfectly reconstructing the training samples. We study several methods to approximately solve the proposed optimization problem, including a novel combination of Wasserstein GAN and Wasserstein Autoencoder, and present an extensive theoretical and empirical characterization of the proposed compression systems.",https://proceedings.neurips.cc/paper_files/paper/2018/file/801fd8c2a4e79c1d24a40dc735c051ae-Paper.pdf,2018,"compression, bitrates, data, training, distribution, generative, model, problem, propose, proposed","distribution, wasserstein, px, dplc, wae, compression, data, 10, distortion, wgan","{'lossy': 0.47795451867998845, 'distribution': 0.44215043100478285, 'compression': 0.4293713989812814, 'preserving': 0.4092075824743653, 'generative': 0.31204134307695114, 'models': 0.26568350951336095, 'deep': 0.23726420632157}","bitrates, compression, wasserstein, training, bitrate, ends, lossy, artifact, perfectly, reconstructed"
"With Friends Like These, Who Needs Adversaries?","Saumya Jetley, Nicholas Lord, Philip Torr","The vulnerability of deep image classification networks to adversarial attack is now well known, but less well understood. Via a novel experimental analysis, we illustrate some facts about deep convolutional networks for image classification that shed new light on their behaviour and how it connects to the problem of adversaries. In short, the celebrated performance of these networks and their vulnerability to adversarial attack are simply two sides of the same coin: the input image-space directions along which the networks are most vulnerable to attack are the same directions which they use to achieve their classification performance in the first place. We develop this result in two main steps. The first uncovers the fact that classes tend to be associated with specific image-space directions. This is shown by an examination of the class-score outputs of nets as functions of 1D movements along these directions. This provides a novel perspective on the existence of universal adversarial perturbations. The second is a clear demonstration of the tight coupling between classification performance and vulnerability to adversarial attack within the spaces spanned by these directions. Thus, our analysis resolves the apparent contradiction between accuracy and vulnerability. It provides a new perspective on much of the prior art and reveals profound implications for efforts to construct neural nets that are both accurate and robust to adversarial attack.",https://proceedings.neurips.cc/paper_files/paper/2018/file/803a82dee7e3fbb3438a149508484250-Paper.pdf,2018,"adversarial, attack, directions, classification, image, networks, vulnerability, performance, along, analysis","directions, class, image, adversarial, target, classiﬁcation, images, space, perturbations, attack","{'friends': 0.5155443421812548, 'like': 0.5155443421812548, 'needs': 0.5155443421812548, 'adversaries': 0.45015785424081145}","directions, attack, vulnerability, adversarial, classification, image, nets, along, perspective, networks"
Learning to Teach with Dynamic Loss Functions,"Lijun Wu, Fei Tian, Yingce Xia, Yang Fan, Tao Qin, Lai Jian-Huang, Tie-Yan Liu","Teaching is critical to human society: it is with teaching that prospective students are educated and human civilization can be inherited and advanced. A good teacher not only provides his/her students with qualified teaching materials (e.g., textbooks), but also sets up appropriate learning objectives (e.g., course projects and exams) considering different situations of a student. When it comes to artificial intelligence, treating machine learning models as students, the loss functions that are optimized act as perfect counterparts of the learning objective set by the teacher. In this work, we explore the possibility of imitating human teaching behaviors by dynamically and automatically outputting appropriate loss functions to train machine learning models. Different from typical learning settings in which the loss function of a machine learning model is predefined and fixed, in our framework, the loss function of a machine learning model (we call it student) is defined by another machine learning model (we call it teacher). The ultimate goal of teacher model is cultivating the student to have better performance measured on development dataset. Towards that end, similar to human teaching, the teacher, a parametric model, dynamically outputs different loss functions that will be used and optimized by its student model at different training stages. We develop an efficient learning method for the teacher model that makes gradient based optimization possible, exempt of the ineffective solutions such as policy optimization. We name our method as ``learning to teach with dynamic loss functions'' (L2T-DLF for short). Extensive experiments on real world tasks including image classification and neural machine translation demonstrate that our method significantly improves the quality of various student models.",https://proceedings.neurips.cc/paper_files/paper/2018/file/8051a3c40561002834e59d566b7430cf-Paper.pdf,2018,"learning, model, loss, machine, teacher, student, teaching, different, functions, human","model, loss, student, teacher, training, function, learning, teaching, functions, fω","{'teach': 0.5944930881712247, 'dynamic': 0.4804179617916071, 'loss': 0.44046658710748265, 'functions': 0.4286911321383428, 'learning': 0.1949093352625029}","teacher, student, teaching, students, loss, machine, learning, human, model, functions"
Learning Bounds for Greedy Approximation with Explicit Feature Maps from Multiple Kernels,"Shahin Shahrampour, Vahid Tarokh","Nonlinear kernels can be approximated using finite-dimensional feature maps for efficient risk minimization. Due to the inherent trade-off between the dimension of the (mapped) feature space and the approximation accuracy, the key problem is to identify promising (explicit) features leading to a satisfactory out-of-sample performance. In this work, we tackle this problem by efficiently choosing such features from multiple kernels in a greedy fashion. Our method sequentially selects these explicit features from a set of candidate features using a correlation metric. We establish an out-of-sample error bound capturing the trade-off between the error in terms of explicit features (approximation error) and the error due to spectral properties of the best model in the Hilbert space associated to the combined kernel (spectral error). The result verifies that when the (best) underlying data model is sparse enough, i.e., the spectral error is negligible, one can control the test error with a small number of explicit features, that can scale poly-logarithmically with data. Our empirical results show that given a fixed number of explicit features, the method can achieve a lower test error with a smaller time cost, compared to the state-of-the-art in data-dependent random features.",https://proceedings.neurips.cc/paper_files/paper/2018/file/80537a945c7aaa788ccfcdf1b99b5d8f-Paper.pdf,2018,"error, features, explicit, data, spectral, approximation, best, due, feature, kernels","kernel, features, error, feature, explicit, kernels, approximation, data, maps, method","{'explicit': 0.42697714335659426, 'kernels': 0.38601159425370624, 'maps': 0.38601159425370624, 'greedy': 0.3529378565506972, 'multiple': 0.3210827351041777, 'feature': 0.31635214748369644, 'approximation': 0.30408049604793036, 'bounds': 0.2826324930286727, 'learning': 0.1399878868901951}","features, error, explicit, spectral, kernels, trade, test, feature, due, best"
Distributed Multitask Reinforcement Learning with Quadratic Convergence,"Rasul Tutunov, Dongho Kim, Haitham Bou Ammar","Multitask reinforcement learning (MTRL) suffers from scalability issues when the number of tasks or trajectories grows large. The main reason behind this drawback is the reliance on centeralised solutions. Recent methods exploited the connection between MTRL and general consensus to propose scalable solutions. These methods, however, suffer from two drawbacks. First, they rely on predefined objectives, and, second, exhibit linear convergence guarantees. In this paper, we improve over state-of-the-art by deriving multitask reinforcement learning from a variational inference perspective. We then propose a novel distributed solver for MTRL with quadratic convergence guarantees.",https://proceedings.neurips.cc/paper_files/paper/2018/file/8073bd4ed0fe0c330290c58056a2cd5e-Paper.pdf,2018,"mtrl, convergence, guarantees, learning, methods, multitask, propose, reinforcement, solutions, art","distributed, newton, λs, convergence, mtrl, τt, method, θsh, learning, nodes","{'multitask': 0.5224652429825473, 'quadratic': 0.477700063224795, 'convergence': 0.434584275993141, 'distributed': 0.3978250426537515, 'reinforcement': 0.3403004764272589, 'learning': 0.18947307911852154}","mtrl, multitask, solutions, centeralised, drawback, guarantees, reinforcement, exploited, reliance, convergence"
The Global Anchor Method for Quantifying Linguistic Shifts and Domain Adaptation,"Zi Yin, Vin Sachidananda, Balaji Prabhakar","Language is dynamic, constantly evolving and adapting with respect to time, domain or topic. The adaptability of language is an active research area, where researchers discover social, cultural and domain-specific changes in language using distributional tools such as word embeddings. In this paper, we introduce the global anchor method for detecting corpus-level language shifts. We show both theoretically and empirically that the global anchor method is equivalent to the alignment method, a widely-used method for comparing word embeddings, in terms of detecting corpus-level language shifts. Despite their equivalence in terms of detection abilities, we demonstrate that the global anchor method is superior in terms of applicability as it can compare embeddings of different dimensionalities. Furthermore, the global anchor method has implementation and parallelization advantages. We show that the global anchor method reveals fine structures in the evolution of language and domain adaptation. When combined with the graph Laplacian technique, the global anchor method recovers the evolution trajectory and domain clustering of disparate text corpora.",https://proceedings.neurips.cc/paper_files/paper/2018/file/80b618ebcac7aa97a6dac2ba65cb7e36-Paper.pdf,2018,"method, anchor, global, language, domain, embeddings, terms, corpus, detecting, evolution","anchor, method, word, global, embeddings, corpora, level, math, corpus, et","{'anchor': 0.40656812568526285, 'linguistic': 0.40656812568526285, 'shifts': 0.40656812568526285, 'quantifying': 0.3837502343707838, 'global': 0.3219249002093724, 'adaptation': 0.30573534867691926, 'method': 0.29317775691477865, 'domain': 0.27162454453839807}","anchor, language, global, method, domain, embeddings, shifts, corpus, detecting, terms"
Understanding Weight Normalized Deep Neural Networks with Rectified Linear Units,"Yixi Xu, Xiao Wang","This paper presents a general framework for norm-based capacity control for $L_{p,q}$ weight normalized deep neural networks. We establish the upper bound on the Rademacher complexities of this family. With an $L_{p,q}$ normalization where $q\le p^*$ and $1/p+1/p^{*}=1$, we discuss properties of a width-independent capacity control, which only depends on the depth by a square root term. We further analyze the approximation properties of $L_{p,q}$ weight normalized deep neural networks. In particular, for an $L_{1,\infty}$ weight normalized network, the approximation error can be controlled by the $L_1$ norm of the output layer, and the corresponding generalization error only depends on the architecture by the square root of the depth.",https://proceedings.neurips.cc/paper_files/paper/2018/file/812b4ba287f5ee0bc9d43bbf5bbe87fb-Paper.pdf,2018,"l_, normalized, weight, approximation, capacity, control, deep, depends, depth, error","co, dnns, layer, bias, wn, m1, hidden, norm, lp, neural","{'normalized': 0.4326964433090399, 'rectified': 0.4326964433090399, 'weight': 0.4084121480290147, 'units': 0.39118216632532393, 'understanding': 0.3363032772416141, 'linear': 0.2768150035015323, 'deep': 0.20274227016791027, 'neural': 0.19378644953410037, 'networks': 0.19323444155712194}","l_, normalized, weight, root, square, depends, capacity, depth, norm, control"
Low-shot Learning via Covariance-Preserving Adversarial Augmentation Networks,"Hang Gao, Zheng Shou, Alireza Zareian, Hanwang Zhang, Shih-Fu Chang","Deep neural networks suffer from over-fitting and catastrophic forgetting when trained with small data. One natural remedy for this problem is data augmentation, which has been recently shown to be effective. However, previous works either assume that intra-class variances can always be generalized to new classes, or employ naive generation methods to hallucinate finite examples without modeling their latent distributions. In this work, we propose Covariance-Preserving Adversarial Augmentation Networks to overcome existing limits of low-shot learning. Specifically, a novel Generative Adversarial Network is designed to model the latent distribution of each novel class given its related base counterparts. Since direct estimation on novel classes can be inductively biased, we explicitly preserve covariance information as the ``variability'' of base examples during the generation process. Empirical results show that our model can generate realistic yet diverse examples, leading to substantial improvements on the ImageNet benchmark over the state of the art.",https://proceedings.neurips.cc/paper_files/paper/2018/file/81448138f5f163ccdba4acc69819f280-Paper.pdf,2018,"examples, novel, adversarial, augmentation, base, class, classes, covariance, data, generation","novel, classes, base, examples, class, yn, gan, generation, data, set","{'augmentation': 0.4478844801248593, 'covariance': 0.4478844801248593, 'preserving': 0.3834626898096011, 'shot': 0.3568313591325322, 'low': 0.3421750802452281, 'adversarial': 0.2761314558485042, 'via': 0.24896829276722984, 'networks': 0.21191021818694913, 'learning': 0.15557382094964461}","examples, augmentation, base, covariance, classes, generation, novel, hallucinate, inductively, latent"
Bilevel Distance Metric Learning for Robust Image Recognition,"Jie Xu, Lei Luo, Cheng Deng, Heng Huang","Metric learning, aiming to learn a discriminative Mahalanobis distance matrix M that can effectively reflect the similarity between data samples, has been widely studied in various image recognition problems. Most of the existing metric learning methods input the features extracted directly from the original data in the preprocess phase. What's worse, these features usually take no consideration of the local geometrical structure of the data and the noise existed in the data, thus they may not be optimal for the subsequent metric learning task. In this paper, we integrate both feature extraction and metric learning into one joint optimization framework and propose a new bilevel distance metric learning model. Specifically,  the lower level characterizes the intrinsic data structure using graph regularized sparse coefficients, while the upper level forces the data samples from the same class to be close to each other and pushes those from different classes far away. 
 In addition, leveraging the KKT conditions and the alternating direction method (ADM), we derive an efficient algorithm to solve the proposed new model. Extensive experiments on various occluded datasets demonstrate the effectiveness and robustness of our method.",https://proceedings.neurips.cc/paper_files/paper/2018/file/814a9c18f5abff398787c9cfcbf3d80c-Paper.pdf,2018,"data, learning, metric, distance, features, level, method, model, new, samples","learning, model, distance, metric, sparse, data, matrix, 10, bilevel, methods","{'bilevel': 0.485859073784336, 'distance': 0.42548926591129854, 'metric': 0.41597518013976403, 'recognition': 0.3932854136917615, 'image': 0.34389881179591963, 'robust': 0.32930724969842834, 'learning': 0.16876439328867268}","metric, data, learning, distance, level, adm, existed, preprocess, features, various"
Integrated accounts of behavioral and neuroimaging data using flexible recurrent neural network models,"Amir Dezfouli, Richard Morris, Fabio T. Ramos, Peter Dayan, Bernard Balleine","Neuroscience studies of human decision-making abilities commonly involve
subjects completing a decision-making task while BOLD signals are
recorded using fMRI. Hypotheses are tested about which brain regions
mediate the effect of past experience, such as rewards, on future
actions. One standard approach to this is model-based fMRI data
analysis, in which a model is fitted to the behavioral data, i.e., a
subject's choices, and then the neural data are parsed to find brain
regions whose BOLD signals are related to the model's internal
signals. However, the internal mechanics of such purely behavioral
models are not constrained by the neural data, and therefore might miss
or mischaracterize aspects of the brain. To address this limitation, we
introduce a new method using recurrent neural network models that are
flexible enough to be jointly fitted to the behavioral and neural
data. We trained a model so that its internal states were suitably
related to neural activity during the task, while at the same time its
output predicted the next action a subject would execute. We then used
the fitted model to create a novel visualization of the relationship
between the activity in brain regions at different times following a
reward and the choices the subject subsequently made. Finally, we
validated our method using a previously published dataset. We found that
the model was able to recover the underlying neural substrates that were
discovered by explicit model engineering in the previous work, and also
derived new results regarding the temporal pattern of brain activity.",https://proceedings.neurips.cc/paper_files/paper/2018/file/819e3d6c1381eac87c17617e5165f38c-Paper.pdf,2018,"model, neural, brain, data, activity, behavioral, fitted, internal, regions, signals","action, choice, model, reward, time, neural, brain, activity, eye, hand","{'accounts': 0.39228317707893834, 'behavioral': 0.39228317707893834, 'neuroimaging': 0.39228317707893834, 'integrated': 0.3546462777673444, 'flexible': 0.3425299024744955, 'recurrent': 0.25735630440345625, 'network': 0.2368697679411058, 'using': 0.2244439982613481, 'data': 0.2220303312090637, 'models': 0.2058225893750374}","brain, fitted, behavioral, subject, activity, internal, signals, regions, bold, fmri"
Learning from Group Comparisons: Exploiting Higher Order Interactions,"Yao Li, Minhao Cheng, Kevin Fujii, Fushing Hsieh, Cho-Jui Hsieh","We study the problem of learning from group comparisons, with applications in predicting outcomes of sports and online games. Most of the previous works in this area focus on learning individual effects---they assume each player has an underlying score, and the ''ability'' of the team is modeled by the sum of team members' scores. Therefore, all the current approaches cannot model deeper interaction between team members: some players perform much better if they play together, and some players perform poorly together. In this paper, we propose a new model that takes the player-interaction effects into consideration. However, under certain circumstances, the total number of individuals can be very large, and number of player interactions grows quadratically, which makes learning intractable. In this case, we propose a latent factor model, and show that the sample complexity of our model is bounded under mild assumptions. Finally, we show that our proposed models have much better prediction power on several E-sports datasets, and furthermore can be used to reveal interesting patterns that cannot be discovered by previous methods.",https://proceedings.neurips.cc/paper_files/paper/2018/file/8208974663db80265e9bfe7b222dcb18-Paper.pdf,2018,"model, learning, player, team, better, cannot, effects, interaction, members, much","model, players, game, ot, player, score, team, comparisons, heroes, hoi","{'comparisons': 0.45259603781733965, 'higher': 0.42719491424703154, 'group': 0.40917253026088435, 'interactions': 0.40917253026088435, 'exploiting': 0.37411434770217455, 'order': 0.34034789913412095, 'learning': 0.1483872472677306}","team, player, sports, members, players, interaction, together, effects, cannot, model"
"Cooperative Holistic Scene Understanding: Unifying 3D Object, Layout, and Camera Pose Estimation","Siyuan Huang, Siyuan Qi, Yinxue Xiao, Yixin Zhu, Ying Nian Wu, Song-Chun Zhu","Holistic 3D indoor scene understanding refers to jointly recovering the i) object bounding boxes, ii) room layout, and iii) camera pose, all in 3D. The existing methods either are ineffective or only tackle the problem partially. In this paper, we propose an end-to-end model that simultaneously solves all three tasks in real-time given only a single RGB image. The essence of the proposed method is to improve the prediction by i) parametrizing the targets (e.g., 3D boxes) instead of directly estimating the targets, and ii) cooperative training across different modules in contrast to training these modules individually. Specifically, we parametrize the 3D object bounding boxes by the predictions from several modules, i.e., 3D camera pose and object attributes. The proposed method provides two major advantages: i) The parametrization helps maintain the consistency between the 2D image and the 3D world, thus largely reducing the prediction variances in 3D coordinates. ii) Constraints can be imposed on the parametrization to train different modules simultaneously. We call these constraints ""cooperative losses"" as they enable the joint training and inference. We employ three cooperative losses for 3D bounding boxes, 2D projections, and physical constraints to estimate a geometrically consistent and physically plausible 3D scene. Experiments on the SUN RGB-D dataset shows that the proposed method significantly outperforms prior approaches on 3D layout estimation, 3D object detection, 3D camera pose estimation, and holistic scene understanding.",https://proceedings.neurips.cc/paper_files/paper/2018/file/82161242827b703e6acf9c726942a1e4-Paper.pdf,2018,"3d, boxes, modules, object, bounding, camera, constraints, cooperative, ii, method","3d, 2d, bounding, al, et, object, box, estimation, boxes, model","{'camera': 0.34801681078534, 'holistic': 0.34801681078534, 'layout': 0.34801681078534, 'unifying': 0.34801681078534, 'cooperative': 0.31462696785655664, 'pose': 0.28766951382127304, 'scene': 0.2755633558199411, 'understanding': 0.2704879964051218, '3d': 0.26170531890946547, 'object': 0.24492691146753587}","3d, boxes, modules, camera, cooperative, object, bounding, pose, ii, scene"
A Bandit Approach to Sequential Experimental Design with False Discovery Control,"Kevin G. Jamieson, Lalit Jain","We propose a new adaptive sampling approach to multiple testing which aims to maximize statistical power while ensuring anytime false discovery control. We consider $n$ distributions whose means are partitioned by whether they are below or equal to a baseline (nulls), versus above the baseline (true positives). In addition, each distribution can be sequentially and repeatedly sampled. Using techniques from multi-armed bandits, we provide an algorithm that takes as few samples as possible to exceed a target true positive proportion (i.e. proportion of true positives discovered) while giving anytime control of the false discovery proportion (nulls predicted as true positives). Our sample complexity results match known information theoretic lower bounds and through simulations we show a substantial performance improvement over uniform sampling and an adaptive elimination style algorithm. Given the simplicity of the approach, and its sample efficiency, the method has promise for wide adoption in the biological sciences, clinical testing for drug discovery, and maximization of click through in A/B/n testing problems.",https://proceedings.neurips.cc/paper_files/paper/2018/file/82cadb0649a3af4968404c9f6031b233-Paper.pdf,2018,"true, discovery, positives, proportion, testing, adaptive, algorithm, anytime, approach, baseline","h1, log, st, h0, µ0, µi, algorithm, fdr, fwer, arm","{'false': 0.4216803222159934, 'experimental': 0.3812229670291045, 'discovery': 0.36819860774960805, 'design': 0.35755693136641803, 'bandit': 0.3407656118422157, 'sequential': 0.32774125256271924, 'control': 0.32217822366291926, 'approach': 0.2934335405168426}","positives, proportion, true, discovery, nulls, testing, anytime, false, baseline, adaptive"
HitNet: Hybrid Ternary Recurrent Neural Network,"Peiqi Wang, Xinfeng Xie, Lei Deng, Guoqi Li, Dongsheng Wang, Yuan Xie","Quantization is a promising technique to reduce the model size, memory footprint, and massive computation operations of recurrent neural networks (RNNs) for embedded devices with limited resources. Although extreme low-bit quantization has achieved impressive success on convolutional neural networks, it still suffers from huge accuracy degradation on RNNs with the same low-bit precision. In this paper, we first investigate the accuracy degradation on RNN models under different quantization schemes, and the distribution of tensor values in the full precision model. Our observation reveals that due to the difference between the distributions of weights and activations, different quantization methods are suitable for different parts of models. Based on our observation, we propose HitNet, a hybrid ternary recurrent neural network, which bridges the accuracy gap between the full precision model and the quantized model. In HitNet, we develop a hybrid quantization method to quantize weights and activations. Moreover, we introduce a sloping factor motivated by prior work on Boltzmann machine to activation functions, further closing the accuracy gap between the full precision model and the quantized model. Overall, our HitNet can quantize RNN models into ternary values, {-1, 0, 1}, outperforming the state-of-the-art quantization methods on RNN models significantly. We test it on typical RNN models, such as Long-Short-Term Memory (LSTM) and Gated Recurrent Units (GRU), on which the results outperform previous work significantly. For example, we improve the perplexity per word (PPW) of a ternary LSTM on Penn Tree Bank (PTB) corpus from 126 (the state-of-the-art result to the best of our knowledge) to 110.3 with a full precision model in 97.2, and a ternary GRU from 142 to 113.5 with a full precision model in 102.7.",https://proceedings.neurips.cc/paper_files/paper/2018/file/82cec96096d4281b7c95cd7e74623496-Paper.pdf,2018,"model, precision, quantization, full, models, accuracy, rnn, ternary, different, hitnet","quantization, accuracy, weights, ternary, activations, hitnet, quantized, distribution, precision, activation","{'hitnet': 0.5235225475172113, 'ternary': 0.5235225475172113, 'hybrid': 0.42306570124113724, 'recurrent': 0.34345553409699364, 'network': 0.3161151728853893, 'neural': 0.23446362294674353}","quantization, ternary, precision, hitnet, rnn, full, model, quantize, gru, degradation"
SLAYER: Spike Layer Error Reassignment in Time,"Sumit Bam Shrestha, Garrick Orchard","Configuring deep Spiking Neural Networks (SNNs) is an exciting research avenue for low power spike event based computation. However, the spike generation function is non-differentiable and therefore not directly compatible with the standard error backpropagation algorithm. In this paper, we introduce a new general backpropagation mechanism for learning synaptic weights and axonal delays which overcomes the problem of non-differentiability of the spike function and uses a temporal credit assignment policy for backpropagating error to preceding layers. We describe and release a GPU accelerated software implementation of our method which allows training both fully connected and convolutional neural network (CNN) architectures. Using our software, we compare our method against existing SNN based learning approaches and standard ANN to SNN conversion techniques and show that our method achieves state of the art performance for an SNN on the MNIST, NMNIST, DVS Gesture, and TIDIGITS datasets.",https://proceedings.neurips.cc/paper_files/paper/2018/file/82f2b308c3b01637c607ce05f52a2fed-Paper.pdf,2018,"method, snn, spike, backpropagation, based, error, function, learning, neural, non","spike, slayer, spiking, neuron, snn, learning, function, error, time, layer","{'reassignment': 0.4531085408391455, 'slayer': 0.4531085408391455, 'spike': 0.4276786539755266, 'error': 0.39564078546443054, 'layer': 0.39564078546443054, 'time': 0.30563541536461336}","snn, spike, software, backpropagation, avenue, conversion, delays, dvs, nmnist, tidigits"
A Convex Duality Framework for GANs,"Farzan Farnia, David Tse","Generative adversarial network (GAN) is a minimax game between a generator mimicking the true model and a discriminator distinguishing the samples produced by the generator from the real training samples. Given an unconstrained discriminator able to approximate any function, this game reduces to finding the generative model minimizing a divergence measure, e.g. the Jensen-Shannon (JS) divergence, to the data distribution. However, in practice the discriminator is constrained to be in a smaller class F such as neural nets. Then, a natural question is how the divergence minimization interpretation changes as we constrain F. In this work, we address this question by developing a convex duality framework for analyzing GANs. For a convex set F, this duality framework interprets the original GAN formulation as finding the generative model with minimum JS-divergence to the distributions penalized to match the moments of the data distribution, with the moments specified by the discriminators in F. We show that this interpretation more generally holds for f-GAN and Wasserstein GAN. As a byproduct, we apply the duality framework to a hybrid of f-divergence and Wasserstein distance. Unlike the f-divergence, we prove that the proposed hybrid divergence changes continuously with the generative model, which suggests regularizing the discriminator's Lipschitz constant in f-GAN and vanilla GAN. We numerically evaluate the power of the suggested regularization schemes for improving GAN's training performance.",https://proceedings.neurips.cc/paper_files/paper/2018/file/831caa1b600f852b7844499430ecac17-Paper.pdf,2018,"divergence, gan, discriminator, generative, model, duality, framework, changes, convex, data","divergence, gan, discriminator, problem, convex, lipschitz, w1, distance, theorem, vanilla","{'duality': 0.6125773415882403, 'gans': 0.4950320932928689, 'framework': 0.4606523120974941, 'convex': 0.40925746730145046}","divergence, gan, discriminator, duality, js, generative, moments, interpretation, generator, hybrid"
Empirical Risk Minimization Under Fairness Constraints,"Michele Donini, Luca Oneto, Shai Ben-David, John S. Shawe-Taylor, Massimiliano Pontil","We address the problem of algorithmic fairness: ensuring that sensitive information does not unfairly influence the outcome of a classifier. We present an approach based on empirical risk minimization, which incorporates a fairness constraint into the learning problem. It encourages the conditional risk of the learned classifier to be approximately constant with respect to the sensitive variable. We derive both risk and fairness bounds that support the statistical consistency of our methodology. We specify our approach to kernel methods and observe that the fairness requirement implies an orthogonality constraint which can be easily added to these methods. We further observe that for linear models the constraint translates into a simple data preprocessing step. Experiments indicate that the method is empirically effective and performs favorably against state-of-the-art approaches.",https://proceedings.neurips.cc/paper_files/paper/2018/file/83cdcec08fbf90370fcf53bdd56604ff-Paper.pdf,2018,"fairness, constraint, risk, approach, classifier, methods, observe, problem, sensitive, added","fairness, linear, method, 03, svm, 02, model, constraint, problem, 04","{'empirical': 0.4566413470373649, 'fairness': 0.4566413470373649, 'constraints': 0.44643069005818686, 'risk': 0.44643069005818686, 'minimization': 0.4293677192106366}","fairness, constraint, risk, sensitive, observe, classifier, translates, unfairly, orthogonality, preprocessing"
End-to-End Differentiable Physics for Learning and Control,"Filipe de Avila Belbute-Peres, Kevin Smith, Kelsey Allen, Josh Tenenbaum, J. Zico Kolter","We present a differentiable physics engine that can be integrated as a module in deep neural networks for end-to-end learning.  As a result, structured physics knowledge can be embedded into larger systems, allowing them, for example, to match observations by performing precise simulations, while achieves high sample efficiency.  Specifically, in this paper we demonstrate how to perform backpropagation analytically through a physical simulator defined via a linear complementarity problem.  Unlike traditional finite difference methods, such gradients can be computed analytically, which allows for greater flexibility of the engine. Through experiments in diverse domains, we highlight the system's ability to learn physical parameters from data, efficiently match and simulate observed visual behavior, and readily enable control via gradient-based planning methods. Code for the engine and experiments is included with the paper.",https://proceedings.neurips.cc/paper_files/paper/2018/file/842424a1d0595b76ec4fa03c46e8d755-Paper.pdf,2018,"engine, analytically, end, experiments, match, methods, paper, physical, physics, via","physics, model, engine, et, al, lcp, parameters, used, learning, physical","{'end': 0.7555813886383311, 'physics': 0.4027959655374323, 'differentiable': 0.3600491720464998, 'control': 0.34040994352140624, 'learning': 0.14607477314896639}","engine, physics, analytically, physical, match, end, via, simulate, complementarity, included"
A Unified Feature Disentangler for Multi-Domain Image Translation and Manipulation,"Alexander H. Liu, Yen-Cheng Liu, Yu-Ying Yeh, Yu-Chiang Frank Wang","We present a novel and unified deep learning framework which is capable of learning domain-invariant representation from data across multiple domains. Realized by adversarial training with additional ability to exploit domain-specific information, the proposed network is able to perform continuous cross-domain image translation and manipulation, and produces desirable output images accordingly. In addition, the resulting feature representation exhibits superior performance of unsupervised domain adaptation, which also verifies the effectiveness of the proposed model in learning disentangled features for describing cross-domain data.",https://proceedings.neurips.cc/paper_files/paper/2018/file/84438b7aae55a0638073ef798e50b4ef-Paper.pdf,2018,"domain, learning, cross, data, proposed, representation, ability, able, accordingly, across","domain, image, representation, translation, domains, feature, data, ufdn, images, model","{'disentangler': 0.4481368310363362, 'manipulation': 0.40514115400240275, 'unified': 0.3799902952444339, 'translation': 0.354839436486465, 'feature': 0.33202959706553553, 'domain': 0.29939622644042935, 'image': 0.29939622644042935, 'multi': 0.2742453676824604}","domain, cross, verifies, representation, accordingly, realized, describing, manipulation, exhibits, desirable"
Horizon-Independent Minimax Linear Regression,"Alan Malek, Peter L. Bartlett","We consider online linear regression: at each round, an adversary reveals a covariate vector, the learner predicts a real value, the adversary reveals a label, and the learner suffers the squared prediction error. The aim is to minimize the difference between the cumulative loss and that of the linear predictor that is best in hindsight. Previous work demonstrated that the minimax optimal strategy is easy to compute recursively from the end of the game; this requires the entire sequence of covariate vectors in advance. We show that, once provided with a measure of the scale of the problem, we can invert the recursion and play the minimax strategy without knowing the future covariates. Further, we show that this forward recursion remains optimal even against adaptively chosen labels and covariates, provided that the adversary adheres to a set of constraints that prevent misrepresentation of the scale of the problem. This strategy is horizon-independent in that the regret and minimax strategies depend on the size of the constraint set and not on the time-horizon, and hence it incurs no more regret than the optimal strategy that knows in advance the number of rounds of the game. We also provide an interpretation of the minimax algorithm as a follow-the-regularized-leader strategy with a data-dependent regularizer and obtain an explicit expression for the minimax regret.",https://proceedings.neurips.cc/paper_files/paper/2018/file/84b20b1f5a0d103f5710bb67a043cd78-Paper.pdf,2018,"minimax, strategy, adversary, optimal, regret, advance, covariate, covariates, game, horizon","xt, yt, pt, minimax, strategy, adversary, regret, γ0, bt, game","{'independent': 0.5429710322973398, 'horizon': 0.4908766594928314, 'minimax': 0.46040340490360604, 'regression': 0.3627541118643446, 'linear': 0.34736252292064684}","minimax, strategy, adversary, recursion, covariate, regret, covariates, horizon, advance, reveals"
Support Recovery for Orthogonal Matching Pursuit: Upper and Lower bounds,"Raghav Somani, Chirag Gupta, Prateek Jain, Praneeth Netrapalli","This paper studies the problem of sparse regression where the goal is to learn a sparse vector that best optimizes a given objective function. Under the assumption that the objective function satisfies restricted strong convexity (RSC), we analyze orthogonal matching pursuit (OMP), a greedy algorithm that is used heavily in applications, and obtain support recovery result as well as a tight generalization error bound for OMP. Furthermore, we obtain lower bounds for OMP, showing that both our results on support recovery and generalization error are tight up to logarithmic factors. To the best of our knowledge, these support recovery and generalization bounds are the first such matching upper and lower bounds (up to logarithmic factors) for {\em any} sparse regression algorithm under the RSC assumption.",https://proceedings.neurips.cc/paper_files/paper/2018/file/84b64e537f08e81b8dea8cce972a28b2-Paper.pdf,2018,"bounds, generalization, omp, recovery, sparse, support, algorithm, assumption, best, error","support, omp, recovery, log, sparse, theorem, error, κs, generalization, results","{'orthogonal': 0.37418191844240056, 'pursuit': 0.37418191844240056, 'recovery': 0.37418191844240056, 'support': 0.37418191844240056, 'upper': 0.37418191844240056, 'lower': 0.3583960324453061, 'matching': 0.3203611897515172, 'bounds': 0.26241275041864726}","omp, recovery, rsc, support, sparse, generalization, bounds, logarithmic, tight, factors"
Beauty-in-averageness and its contextual modulations: A Bayesian statistical account,"Chaitanya Ryali, Angela J. Yu","Understanding how humans perceive the likability of high-dimensionalobjects'' such as faces is an important problem in both cognitive science and AI/ML. Existing models generally assume these preferences to be fixed. However, psychologists have found human assessment of facial attractiveness to be context-dependent. Specifically, the classical Beauty-in-Averageness (BiA) effect, whereby a blended face is judged to be more attractive than the originals, is significantly diminished or reversed when the original faces are recognizable, or when the blend is mixed-race/mixed-gender and the attractiveness judgment is preceded by a race/gender categorization, respectively. This ""Ugliness-in-Averageness"" (UiA) effect has previously been explained via a qualitative disfluency account, which posits that the negative affect associated with the difficult race or gender categorization is inadvertently interpreted by the brain as a dislike for the face itself. In contrast, we hypothesize that human preference for an object is increased when it incurs lower encoding cost, in particular when its perceived {\it statistical typicality} is high, in consonance with Barlow's seminalefficient coding hypothesis.'' This statistical coding cost account explains both BiA, where facial blends generally have higher likelihood than ``parent faces'', and UiA, when the preceding context or task restricts face representation to a task-relevant subset of features, thus redefining statistical typicality and encoding cost within that subspace. We use simulations to show that our model provides a parsimonious, statistically grounded, and quantitative account of both BiA and UiA. We validate our model using experimental data from a gender categorization task. We also propose a novel experiment, based on model predictions, that will be able to arbitrate between the disfluency account and our statistical coding cost account of attractiveness.",https://proceedings.neurips.cc/paper_files/paper/2018/file/84ddfb34126fc3a48ee38d7044e87276-Paper.pdf,2018,"account, cost, gender, statistical, attractiveness, bia, categorization, coding, face, faces","faces, face, attractiveness, gender, subspace, race, features, typicality, task, figure","{'account': 0.4232851772845994, 'averageness': 0.4232851772845994, 'beauty': 0.4232851772845994, 'modulations': 0.4232851772845994, 'contextual': 0.33516163675590005, 'statistical': 0.33516163675590005, 'bayesian': 0.24218172775581867}","gender, account, attractiveness, bia, uia, categorization, race, faces, coding, face"
The committee machine: Computational to statistical gaps in learning a two-layers neural network,"Benjamin Aubin, Antoine Maillard, jean barbier, Florent Krzakala, Nicolas Macris, Lenka Zdeborová","Heuristic tools from statistical physics have been used in the past to compute the optimal learning and generalization errors in the teacher-student scenario in multi- layer neural networks. In this contribution, we provide a rigorous justification of these approaches for a two-layers neural network model called the committee machine. We also introduce a version of the approximate message passing (AMP) algorithm for the committee machine that allows to perform optimal learning in polynomial time for a large set of parameters. We find that there are regimes in which a low generalization error is information-theoretically achievable while the AMP algorithm fails to deliver it; strongly suggesting that no efficient algorithm exists for those cases, and unveiling a large computational gap.",https://proceedings.neurips.cc/paper_files/paper/2018/file/84f0f20482cde7e5eacaf7364a643d33-Paper.pdf,2018,"algorithm, amp, committee, generalization, large, learning, machine, neural, optimal, achievable","amp, generalization, case, optimal, algorithm, error, two, see, phase, teacher","{'committee': 0.38212454309677174, 'computational': 0.38212454309677174, 'gaps': 0.38212454309677174, 'layers': 0.3534991820861247, 'two': 0.3534991820861247, 'statistical': 0.32056112123470754, 'machine': 0.314656987501677, 'network': 0.24445535593595594, 'neural': 0.1813133102037478, 'learning': 0.132731938449015}","amp, committee, generalization, unveiling, algorithm, deliver, machine, regimes, student, optimal"
Adversarial vulnerability for any classifier,"Alhussein Fawzi, Hamza Fawzi, Omar Fawzi","Despite achieving impressive performance, state-of-the-art classifiers remain highly vulnerable to small, imperceptible, adversarial perturbations.  This vulnerability has proven empirically to be very intricate to address. In this paper, we study the phenomenon of adversarial perturbations under the assumption that the data is generated with a smooth generative model. We derive fundamental upper bounds on the robustness to perturbations of any classification function, and prove the existence of adversarial perturbations that transfer well across different classifiers with small risk. Our analysis of the robustness also provides insights onto key properties of generative models, such as their smoothness and dimensionality of latent space. We conclude with numerical experimental results showing that our bounds provide informative baselines to the maximal achievable robustness on several datasets.",https://proceedings.neurips.cc/paper_files/paper/2018/file/851ddf5058cf22df63d3344ad89919cf-Paper.pdf,2018,"perturbations, adversarial, robustness, bounds, classifiers, generative, small, achievable, achieving, across","robustness, distribution, perturbations, 10, space, classiﬁers, bound, data, generative, adversarial","{'vulnerability': 0.6810521958407546, 'classifier': 0.6157098757553832, 'adversarial': 0.3963196379663479}","perturbations, robustness, adversarial, classifiers, small, bounds, conclude, intricate, vulnerability, generative"
A Deep Bayesian Policy Reuse Approach Against Non-Stationary Agents,"YAN ZHENG, Zhaopeng Meng, Jianye Hao, Zongzhang Zhang, Tianpei Yang, Changjie Fan","In multiagent domains, coping with non-stationary agents that change behaviors from time to time is a challenging problem, where an agent is usually required to be able to quickly detect the other agent's policy during online interaction, and then adapt its own policy accordingly. This paper studies efficient policy detecting and reusing techniques when playing against non-stationary agents in Markov games. We propose a new deep BPR+ algorithm by extending the recent BPR+ algorithm with a neural network as the value-function approximator. To detect policy accurately, we propose the \textit{rectified belief model} taking advantage of the \textit{opponent model} to infer the other agent's policy from reward signals and its behaviors. Instead of directly storing individual policies as BPR+, we introduce \textit{distilled policy network} that serves as the policy library in BPR+, using policy distillation to achieve efficient online policy learning and reuse. Deep BPR+ inherits all the advantages of BPR+ and empirically shows better performance in terms of detection accuracy, cumulative rewards and speed of convergence compared to existing algorithms in complex Markov games with raw visual inputs.",https://proceedings.neurips.cc/paper_files/paper/2018/file/85422afb467e9456013a2a51d4dff702-Paper.pdf,2018,"policy, bpr, agent, textit, agents, algorithm, behaviors, deep, detect, efficient","policy, bpr, opponent, agent, policies, deep, response, model, using, learning","{'reuse': 0.45676238081198833, 'agents': 0.42254581900503163, 'stationary': 0.42254581900503163, 'approach': 0.33674520514633144, 'policy': 0.3203262015855371, 'non': 0.3003322871914772, 'bayesian': 0.27687472650119, 'deep': 0.2267440928484408}","bpr, policy, textit, agent, detect, behaviors, stationary, agents, games, markov"
Adversarial Examples that Fool both Computer Vision and Time-Limited Humans,"Gamaleldin Elsayed, Shreya Shankar, Brian Cheung, Nicolas Papernot, Alexey Kurakin, Ian Goodfellow, Jascha Sohl-Dickstein","Machine learning models are vulnerable to adversarial examples: small changes to images can cause computer vision models to make mistakes such as identifying a school bus as an ostrich. However, it is still an open question whether humans are prone to similar mistakes. Here, we address this question by leveraging recent techniques that transfer adversarial examples from computer vision models with known parameters and architecture to other models with unknown parameters and architecture, and by matching the initial processing of the human visual system. We find that adversarial examples that strongly transfer across computer vision models influence the classifications made by time-limited human observers.",https://proceedings.neurips.cc/paper_files/paper/2018/file/8562ae5e286544710b2e7ebe9858833b-Paper.pdf,2018,"models, adversarial, computer, examples, vision, architecture, human, mistakes, parameters, question","adversarial, image, examples, class, human, images, humans, models, transfer, subjects","{'fool': 0.4154063494277858, 'computer': 0.39209242899535757, 'humans': 0.39209242899535757, 'vision': 0.37555093920288024, 'examples': 0.35223701877045194, 'limited': 0.34337347758717773, 'time': 0.2802041469298438, 'adversarial': 0.2417343267073723}","computer, vision, mistakes, examples, models, adversarial, transfer, question, human, architecture"
Trajectory Convolution for Action Recognition,"Yue Zhao, Yuanjun Xiong, Dahua Lin","How to leverage the temporal dimension is a key question in video analysis. Recent works suggest an efficient approach to video feature learning, i.e.,
factorizing 3D convolutions into separate components respectively for spatial and temporal convolutions. The temporal convolution, however, comes with an implicit assumption – the feature maps across time steps are well aligned so that the features at the same locations can be aggregated. This assumption may be overly strong in practical applications, especially in action recognition where the motion serves as a crucial cue. In this work, we propose a new CNN architecture TrajectoryNet, which incorporates trajectory convolution, a new operation for integrating features along the temporal dimension, to replace the existing temporal convolution. This operation explicitly takes into account the changes in contents caused by deformation or motion, allowing the visual features to be aggregated along the the motion paths, trajectories. On two large-scale action recognition datasets, namely, Something-Something and Kinetics, the proposed network architecture achieves notable improvement over strong baselines.",https://proceedings.neurips.cc/paper_files/paper/2018/file/8597a6cfa74defcbde3047c891d78f90-Paper.pdf,2018,"temporal, convolution, features, motion, action, aggregated, along, architecture, assumption, convolutions","trajectory, convolution, motion, feature, motionnet, 3d, map, network, trajectorynet, something","{'trajectory': 0.5787019792454758, 'convolution': 0.49070087688060704, 'action': 0.4783530905914707, 'recognition': 0.4421481531880017}","temporal, motion, convolution, something, aggregated, operation, convolutions, features, along, video"
Adversarial Attacks on Stochastic Bandits,"Kwang-Sung Jun, Lihong Li, Yuzhe Ma, Jerry Zhu","We study adversarial attacks that manipulate the reward signals to control the actions chosen by a stochastic multi-armed bandit algorithm.  We propose the first attack against two popular bandit algorithms: $\epsilon$-greedy and UCB, \emph{without} knowledge of the mean rewards.  The attacker is able to spend only logarithmic effort, multiplied by a problem-specific parameter that becomes smaller as the bandit problem gets easier to attack.  The result means the attacker can easily hijack the behavior of the bandit algorithm to promote or obstruct certain actions, say, a particular medical treatment.  As bandits are seeing increasingly wide use in practice, our study exposes a significant security threat.",https://proceedings.neurips.cc/paper_files/paper/2018/file/85f007f8c50dd25f5a45fca73cad64bd-Paper.pdf,2018,"bandit, actions, algorithm, attack, attacker, problem, study, able, adversarial, algorithms","attack, arm, alice, log, cost, target, bob, reward, bandit, algorithm","{'attacks': 0.5779947518889431, 'bandits': 0.5577190778370431, 'adversarial': 0.4247837044410954, 'stochastic': 0.41764853818340786}","bandit, attacker, attack, actions, exposes, hijack, multiplied, obstruct, spend, gets"
Evolution-Guided Policy Gradient in Reinforcement Learning,"Shauharda Khadka, Kagan Tumer","Deep Reinforcement Learning (DRL) algorithms have been successfully applied to a range of challenging control tasks. However, these methods typically suffer from three core difficulties: temporal credit assignment with sparse rewards, lack of effective exploration, and brittle convergence properties that are extremely sensitive to hyperparameters. Collectively, these challenges severely limit the applicability of these approaches to real world problems. Evolutionary Algorithms (EAs), a class of black box optimization techniques inspired by natural evolution, are well suited to address each of these three challenges. However, EAs typically suffer from high sample complexity and struggle to solve problems that require optimization of a large number of parameters. In this paper, we introduce Evolutionary Reinforcement Learning (ERL), a hybrid algorithm that leverages the population of an EA to provide diversified data to train an RL agent, and reinserts the RL agent into the EA population periodically to inject gradient information into the EA. ERL inherits EA's ability of temporal credit assignment with a fitness metric, effective exploration with a diverse set of policies, and stability of a population-based approach and complements it with off-policy DRL's ability to leverage gradients for higher sample efficiency and faster learning. Experiments in a range of challenging continuous control benchmarks demonstrate that ERL significantly outperforms prior DRL and EA methods.",https://proceedings.neurips.cc/paper_files/paper/2018/file/85fc37b18c57097425b52fc7afbb6969-Paper.pdf,2018,"ea, drl, erl, learning, population, ability, agent, algorithms, assignment, challenges","erl, ea, population, learning, policy, gradient, evolutionary, ddpg, rl, actor","{'evolution': 0.5401904632565779, 'guided': 0.4898570332183567, 'policy': 0.40951099607413405, 'reinforcement': 0.3642914519085887, 'gradient': 0.3559304608036736, 'learning': 0.20283081532632308}","ea, erl, drl, population, eas, evolutionary, credit, assignment, suffer, exploration"
Policy Regret in Repeated Games,"Raman Arora, Michael Dinitz, Teodor Vanislavov Marinov, Mehryar Mohri","The notion ofpolicy regret'' in online learning is supposed to capture the reactions of the adversary to the actions taken by the learner, which more traditional notions such as external regret do not take into account.  We revisit this notion of policy regret, and first show that there are online learning settings in which policy regret and external regret are incompatible: any sequence of play which does well with respect to one must do poorly with respect to the other.  We then focus on the game theoretic setting, when the adversary is a self-interested agent.  In this setting we show that the external regret and policy regret are not in conflict, and in fact that a wide class of algorithms can ensure both as long as the adversary is also using such an algorithm.  We also define a new notion of equilibrium which we call apolicy equilibrium'', and show that no-policy regret algorithms will have play which converges to such an equilibrium.  Relating this back to external regret, we show that coarse correlated equilibria (which no-external regret players will converge to) are a strict subset of policy equilibria.  So in game-theoretic settings every sequence of play with no external regret also has no policy regret, but the converse is not true.",https://proceedings.neurips.cc/paper_files/paper/2018/file/8643c8e2107ba86c47371e037059c4b7-Paper.pdf,2018,"regret, external, policy, show, adversary, also, equilibrium, notion, play, algorithms","regret, policy, player, distribution, action, play, utility, external, a1, equilibrium","{'repeated': 0.6170967945622851, 'regret': 0.47962417439694205, 'games': 0.47148310847268127, 'policy': 0.4084799577701978}","regret, external, policy, equilibrium, adversary, play, notion, equilibria, theoretic, game"
Causal Inference with Noisy and Missing Covariates via Matrix Factorization,"Nathan Kallus, Xiaojie Mao, Madeleine Udell","Valid causal inference in observational studies often requires controlling for confounders. However, in practice measurements of confounders may be noisy, and can lead to biased estimates of causal effects. We show that we can reduce bias induced by measurement noise using a large number of noisy measurements of the underlying confounders. We propose the use of matrix factorization to infer the confounders from noisy covariates. This flexible and principled framework adapts to missing values, accommodates a wide variety of data types, and can enhance a wide variety of causal inference methods. We bound the error for the induced average treatment effect estimator and show it is consistent in a linear regression setting, using Exponential Family Matrix Completion preprocessing. We demonstrate the effectiveness of the proposed procedure in numerical experiments with both synthetic data and real clinical data.",https://proceedings.neurips.cc/paper_files/paper/2018/file/86a1793f65aeef4aeef4b479fc9b2bca-Paper.pdf,2018,"confounders, causal, data, noisy, induced, inference, matrix, measurements, show, using","matrix, confounders, covariates, factorization, causal, noise, data, ui, inference, missing","{'covariates': 0.45541235784106404, 'missing': 0.4117186435580511, 'factorization': 0.3764423082686819, 'noisy': 0.3606002748124772, 'matrix': 0.33742011635810715, 'causal': 0.3205096385960523, 'inference': 0.278697756741918, 'via': 0.23894511975306296}","confounders, causal, noisy, induced, measurements, wide, variety, matrix, accommodates, preprocessing"
Bayesian Distributed Stochastic Gradient Descent,"Michael Teng, Frank Wood","We introduce Bayesian distributed stochastic gradient descent (BDSGD), a high-throughput algorithm for training deep neural networks on parallel clusters. This algorithm uses amortized inference in a deep generative model to perform joint posterior predictive inference of mini-batch gradient computation times in a compute cluster specific manner. Specifically, our algorithm mitigates the straggler effect in synchronous, gradient-based optimization by choosing an optimal cutoff beyond which mini-batch gradient messages from slow workers are ignored. In our experiments, we show that eagerly discarding the mini-batch gradient computations of stragglers not only increases throughput but actually increases the overall rate of convergence as a function of wall-clock time by virtue of eliminating idleness.  The principal novel contribution and finding of this work goes beyond this by demonstrating that using the predicted run-times from a generative model of cluster worker performance improves substantially over the static-cutoff prior art, leading to reduced deep neural net training times on large computer clusters.",https://proceedings.neurips.cc/paper_files/paper/2018/file/86b20716fbd5b253d27cec43127089bc-Paper.pdf,2018,"gradient, algorithm, batch, deep, mini, times, beyond, cluster, clusters, cutoff","time, model, throughput, al, bdsgd, et, worker, cutoff, sgd, training","{'distributed': 0.5007278247167734, 'descent': 0.47720657300421127, 'gradient': 0.4184931372209571, 'bayesian': 0.416178463123061, 'stochastic': 0.416178463123061}","mini, cutoff, gradient, times, batch, throughput, clusters, cluster, increases, beyond"
Benefits of over-parameterization with EM,"Ji Xu, Daniel J. Hsu, Arian Maleki","Expectation Maximization (EM) is among the most popular algorithms for maximum likelihood estimation, but it is generally only guaranteed to find its stationary points of the log-likelihood objective. The goal of this article is to present theoretical and empirical evidence that over-parameterization can help EM avoid spurious local optima in the log-likelihood. We consider the problem of estimating the mean vectors of a Gaussian mixture model in a scenario where the mixing weights are known. Our study shows that the global behavior of EM, when one uses an over-parameterized model in which the mixing weights are treated as unknown, is better than that when one uses the (correct) model with the mixing weights fixed to the known values. For symmetric Gaussians mixtures with two components, we prove that introducing the (statistically redundant) weight parameters enables EM to find the global maximizer of the log-likelihood starting from almost any initial mean parameters, whereas EM without this over-parameterization may very often fail. For other Gaussian mixtures, we provide empirical evidence that shows similar behavior. Our results corroborate the value of over-parameterization in solving non-convex optimization problems, previously observed in other domains.",https://proceedings.neurips.cc/paper_files/paper/2018/file/86ba98bcbd3466d253841907ba1fc725-Paper.pdf,2018,"em, likelihood, log, mixing, model, parameterization, weights, behavior, empirical, evidence","em, population, w1, model, ﬁxed, point, whti, weights, gw, one","{'benefits': 0.5995653225303663, 'em': 0.5659158170695786, 'parameterization': 0.5659158170695786}","em, parameterization, likelihood, mixing, weights, log, mixtures, evidence, mean, shows"
How to tell when a clustering is (approximately) correct using convex relaxations,Marina Meila,"We introduce the Sublevel Set (SS) method, a generic method to obtain sufficient guarantees of near-optimality and uniqueness (up to small perturbations) for a clustering. This method can be instantiated for a variety of clustering loss functions for which convex relaxations exist. Obtaining the guarantees in practice amounts to solving a convex optimization. We demonstrate the applicability of this method by obtaining distribution free guarantees for K-means clustering on realistic data sets.",https://proceedings.neurips.cc/paper_files/paper/2018/file/882735cbdfd9f810814d17892ae50023-Paper.pdf,2018,"method, clustering, guarantees, convex, obtaining, amounts, applicability, data, demonstrate, distribution","clustering, data, loss, means, relaxations, convex, set, bounds, method, clusters","{'approximately': 0.4432700628060371, 'tell': 0.4432700628060371, 'correct': 0.41839234250019003, 'relaxations': 0.41839234250019003, 'clustering': 0.32387678466582964, 'convex': 0.2961447819212584, 'using': 0.25361603815532935}","clustering, guarantees, obtaining, method, sublevel, uniqueness, convex, ss, instantiated, amounts"
Faithful Inversion of Generative Models for Effective Amortized Inference,"Stefan Webb, Adam Golinski, Rob Zinkov, Siddharth N, Tom Rainforth, Yee Whye Teh, Frank Wood","Inference amortization methods share information across multiple posterior-inference problems, allowing each to be carried out more efficiently. Generally, they require the inversion of the dependency structure in the generative model, as the modeller must learn a mapping from observations to distributions approximating the posterior. Previous approaches have involved inverting the dependency structure in a heuristic way that fails to capture these dependencies correctly, thereby limiting the achievable accuracy of the resulting approximations. We introduce an algorithm for faithfully, and minimally, inverting the graphical model structure of any generative model. Such inverses have two crucial properties: (a) they do not encode any independence assertions that are absent from the model and; (b) they are local maxima for the number of true independencies encoded. We prove the correctness of our approach and empirically show that the resulting minimally faithful inverses lead to better inference amortization than existing heuristic approaches.",https://proceedings.neurips.cc/paper_files/paper/2018/file/894b77f805bd94d292574c38c5d628d5-Paper.pdf,2018,"model, inference, structure, amortization, approaches, dependency, generative, heuristic, inverses, inverting","inference, network, inverse, model, nami, al, et, generative, faithful, edges","{'faithful': 0.47139527970533734, 'amortized': 0.4449390831199671, 'effective': 0.42616811291643303, 'inversion': 0.411608217274928, 'generative': 0.29048661254972, 'inference': 0.28847879230904877, 'models': 0.2473310168704957}","amortization, inverses, inverting, minimally, dependency, heuristic, structure, inference, resulting, posterior"
TETRIS: TilE-matching the TRemendous Irregular Sparsity,"Yu Ji, Ling Liang, Lei Deng, Youyang Zhang, Youhui Zhang, Yuan Xie","Compressing neural networks by pruning weights with small magnitudes can significantly reduce the computation and storage cost. Although pruning makes the model smaller, it is difficult to get practical speedup in modern computing platforms such as CPU and GPU due to the irregularity. Structural pruning has attract a lot of research interest to make sparsity hardware-friendly. Increasing the sparsity granularity can lead to better hardware utilization, but it will compromise the sparsity for maintaining accuracy.In this work, we propose a novel method, TETRIS, to achieve both better hardware utilization and higher sparsity. Just like a tile-matching game, we cluster the irregularly distributed weights with small value into structured groups by reordering the input/output dimension and structurally prune them. Results show that it can achieve comparable sparsity with the irregular element-wise pruning and demonstrate negligible accuracy loss. The experiments also shows ideal speedup, which is proportional to the sparsity, on GPU platforms. Our proposed method provides a new solution toward algorithm and architecture co-optimization for accuracy-efficiency trade-off.",https://proceedings.neurips.cc/paper_files/paper/2018/file/89885ff2c83a10305ee08bd507c1049c-Paper.pdf,2018,"sparsity, pruning, accuracy, hardware, achieve, better, gpu, method, platforms, small","sparsity, block, pruning, accuracy, size, 16, 32, th, reordering, speedup","{'irregular': 0.43144994516056934, 'tetris': 0.43144994516056934, 'tile': 0.43144994516056934, 'tremendous': 0.43144994516056934, 'sparsity': 0.3658409233305492, 'matching': 0.34866057721001137}","sparsity, pruning, hardware, platforms, utilization, speedup, gpu, accuracy, weights, attract"
Stochastic Chebyshev Gradient Descent for Spectral Optimization,"Insu Han, Haim Avron, Jinwoo Shin","A large class of machine learning techniques requires the solution of optimization problems involving spectral functions of parametric matrices, e.g. log-determinant and nuclear norm. Unfortunately, computing the gradient of a spectral function is generally of cubic complexity, as such gradient descent methods are rather expensive for optimizing objectives involving the spectral function. Thus, one naturally turns to stochastic gradient methods in hope that they will provide a way to reduce or altogether avoid the computation of full gradients. However, here a new challenge appears: there is no straightforward way to compute unbiased stochastic gradients for spectral functions. In this paper, we develop unbiased stochastic gradients for spectral-sums, an important subclass of spectral functions. Our unbiased stochastic gradients are based on combining randomized trace estimators with stochastic truncation of the Chebyshev expansions. A careful design of the truncation distribution allows us to offer distributions that are variance-optimal, which is crucial for fast and stable convergence of stochastic gradient methods. We further leverage our proposed stochastic gradients to devise stochastic methods for objective functions involving spectral-sums, and rigorously analyze their convergence rate. The utility of our methods is demonstrated in numerical experiments.",https://proceedings.neurips.cc/paper_files/paper/2018/file/898aef0932f6aaecda27aba8e9903991-Paper.pdf,2018,"stochastic, spectral, gradients, methods, functions, gradient, involving, unbiased, convergence, function","stochastic, sgd, spectral, degree, gradient, chebyshev, pn, distribution, variance, gradients","{'chebyshev': 0.5804420979523355, 'spectral': 0.45113516140216947, 'descent': 0.38079744933630555, 'gradient': 0.33394577575754103, 'stochastic': 0.33209873080387514, 'optimization': 0.30454554914239534}","spectral, stochastic, gradients, involving, unbiased, functions, truncation, sums, methods, gradient"
Model-based targeted dimensionality reduction for neuronal population data,"Mikio Aoi, Jonathan W. Pillow","Summarizing high-dimensional data using a small number of parameters is a ubiquitous first step in the analysis of neuronal population activity. Recently developed methods use ""targeted"" approaches that work by identifying multiple, distinct low-dimensional subspaces of activity that capture the population response to individual experimental task variables, such as the value of a presented stimulus or the behavior of the animal. These methods have gained attention because they decompose total neural activity into what are ostensibly different parts of a neuronal computation. However, existing targeted methods have been developed outside of the confines of probabilistic modeling, making some aspects of the procedures ad hoc, or limited in flexibility or interpretability. Here we propose a new model-based method for targeted dimensionality reduction based on a probabilistic generative model of the population response data.  The low-dimensional structure of our model is expressed as a low-rank factorization of a linear regression model. We perform efficient inference using a combination of expectation maximization and direct maximization of the marginal likelihood. We also develop an efficient method for estimating the dimensionality of each subspace. We show that our approach outperforms alternative methods in both mean squared error of the parameter estimates, and in identifying the correct dimensionality of encoding using simulated data. We also show that our method provides more accurate inference of low-dimensional subspaces of activity than a competing algorithm, demixed PCA.",https://proceedings.neurips.cc/paper_files/paper/2018/file/8a1ee9f2b7abe6e88d1a479ab6a42c5e-Paper.pdf,2018,"activity, dimensional, low, methods, model, data, dimensionality, method, population, targeted","model, dimensionality, estimation, rp, task, methods, algorithm, activity, likelihood, data","{'targeted': 0.42660995975146626, 'neuronal': 0.40861225366803255, 'dimensionality': 0.39465214827550804, 'population': 0.3736020759914273, 'reduction': 0.34532539075385343, 'based': 0.3019613426504751, 'model': 0.28255771004880653, 'data': 0.25581634351034227}","activity, targeted, population, dimensionality, dimensional, low, subspaces, neuronal, response, identifying"
BourGAN: Generative Networks with Metric Embeddings,"Chang Xiao, Peilin Zhong, Changxi Zheng","This paper addresses the mode collapse for generative adversarial networks (GANs). We view modes as a geometric structure of data distribution in a metric space. Under this geometric lens, we embed subsamples of the dataset from an arbitrary metric space into the L2 space, while preserving their pairwise distance distribution. Not only does this metric embedding determine the dimensionality of the latent space automatically, it also enables us to construct a mixture of Gaussians to draw latent space random vectors. We use the Gaussian mixture model in tandem with a simple augmentation of the objective function to train GANs. Every major step of our method is supported by theoretical analysis, and our experiments on real and synthetic data confirm that the generator is able to produce samples spreading over most of the modes while avoiding unwanted samples, outperforming several recent GAN variants on a number of metrics and offering new features.",https://proceedings.neurips.cc/paper_files/paper/2018/file/8a3363abe792db2d8761d6403605aeb7-Paper.pdf,2018,"space, metric, data, distribution, gans, geometric, latent, mixture, modes, samples","space, distribution, distance, data, latent, modes, metric, samples, log, gaussian","{'bourgan': 0.5856590891665874, 'embeddings': 0.48410381402374125, 'metric': 0.47327908687325887, 'generative': 0.36089908457993025, 'networks': 0.2615448053432009}","space, metric, modes, gans, geometric, mixture, l2, unwanted, latent, offering"
Online Robust Policy Learning in the Presence of Unknown Adversaries,"Aaron Havens, Zhanhong Jiang, Soumik Sarkar","The growing prospect of deep reinforcement learning (DRL) being used in cyber-physical systems has raised concerns around safety and robustness of autonomous agents. Recent work on generating adversarial attacks have shown that it is computationally feasible for a bad actor to fool a DRL policy into behaving sub optimally. Although certain adversarial attacks with specific attack models have been addressed, most studies are only interested in off-line optimization in the data space (e.g., example fitting, distillation). This paper introduces a Meta-Learned Advantage Hierarchy (MLAH) framework that is attack model-agnostic and more suited to reinforcement learning, via handling the attacks in the decision space (as opposed to data space) and directly mitigating learned bias introduced by the adversary. In MLAH, we learn separate sub-policies (nominal and adversarial) in an online manner, as guided by a supervisory master agent that detects the presence of the adversary by leveraging the advantage function for the sub-policies. We demonstrate that the proposed algorithm enables policy learning with significantly lower bias as compared to the state-of-the-art policy learning approaches even in the presence of heavy state information attacks. We present algorithm analysis and simulation results using popular OpenAI Gym environments.",https://proceedings.neurips.cc/paper_files/paper/2018/file/8a36dfc67ebfbbea9bd01cd8a4c8ad32-Paper.pdf,2018,"attacks, learning, adversarial, policy, space, sub, advantage, adversary, algorithm, attack","policy, state, adversarial, agent, attack, attacks, learning, reward, mlah, adversary","{'presence': 0.4864175865364793, 'adversaries': 0.44997952133476793, 'unknown': 0.43697421293782857, 'online': 0.3411233158095169, 'policy': 0.3411233158095169, 'robust': 0.3296858004104635, 'learning': 0.16895839412316424}","attacks, mlah, drl, sub, attack, policy, adversary, presence, adversarial, advantage"
Representer Point Selection for Explaining Deep Neural Networks,"Chih-Kuan Yeh, Joon Kim, Ian En-Hsu Yen, Pradeep K. Ravikumar","We propose to explain the predictions of a deep neural network, by pointing to the set of what we call representer points in the training set, for a given test point prediction. Specifically, we show that we can decompose the pre-activation prediction of a neural network into a linear combination of activations of training points, with the weights corresponding to what we call representer values, which thus capture the importance of that training point on the learned parameters of the network. But it provides a deeper understanding of the network than simply training point influence: with positive representer values corresponding to excitatory training points, and negative values corresponding to inhibitory points, which as we show provides considerably more insight. Our method is also much more scalable, allowing for real-time feedback in a manner not feasible with influence functions.",https://proceedings.neurips.cc/paper_files/paper/2018/file/8a7129b8f3edd95b7d969dfc2c8e9d9d-Paper.pdf,2018,"training, network, points, corresponding, point, representer, values, call, influence, neural","xi, training, points, representer, model, test, point, inﬂuence, xt, values","{'representer': 0.5261126577427271, 'explaining': 0.4756357311677353, 'selection': 0.4165815127716844, 'point': 0.38980276036992756, 'deep': 0.24651294514720787, 'neural': 0.23562362384868346, 'networks': 0.23495244110994326}","representer, points, corresponding, values, training, influence, point, network, call, provides"
Watch Your Step: Learning Node Embeddings via Graph Attention,"Sami Abu-El-Haija, Bryan Perozzi, Rami Al-Rfou, Alexander A. Alemi","Graph embedding methods represent nodes in a continuous vector space,
preserving different types of relational information from the graph.
There are many hyper-parameters to these methods (e.g. the length of a random walk) which have to be manually tuned for every graph.
In this paper, we replace previously fixed hyper-parameters with trainable ones that we automatically learn via backpropagation. 
In particular, we propose a novel attention model on the power series of the transition matrix, which guides the random walk to optimize an upstream objective.
Unlike previous approaches to attention models, the method that we propose utilizes attention parameters exclusively on the data itself (e.g. on the random walk), and are not used by the model for inference.
We experiment on link prediction tasks, as we aim to produce embeddings that best-preserve the graph structure, generalizing to unseen information. 
We improve state-of-the-art results on a comprehensive suite of real-world graph datasets including social, collaboration, and biological networks, where we observe that our graph attention model can reduce the error by up to 20\%-40\%.
We show that our automatically-learned attention parameters can vary significantly per graph, and correspond to the optimal choice of hyper-parameter if we manually tune existing methods.",https://proceedings.neurips.cc/paper_files/paper/2018/file/8a94ecfa54dcb88a2fa993bfa6388f9e-Paper.pdf,2018,"graph, attention, parameters, hyper, methods, model, random, walk, automatically, information","graph, attention, node, random, context, embeddings, methods, matrix, model, parameters","{'watch': 0.4599885010320851, 'node': 0.43417248900514743, 'step': 0.4016481603191565, 'embeddings': 0.38022493268836705, 'attention': 0.3514469856906812, 'graph': 0.299175055548513, 'via': 0.24134612417017914, 'learning': 0.15081092572557595}","graph, attention, hyper, walk, parameters, manually, random, automatically, guides, methods"
$\ell_1$-regression with Heavy-tailed Distributions,"Lijun Zhang, Zhi-Hua Zhou","In this paper, we consider the problem of linear regression with heavy-tailed distributions. Different from previous studies that use the squared loss to measure the performance, we choose the absolute loss, which is capable of estimating the conditional median. To address the challenge that both the input and output could be heavy-tailed, we propose a truncated minimization problem, and demonstrate that it enjoys an $O(\sqrt{d/n})$ excess risk, where $d$ is the dimensionality and $n$ is the number of samples. Compared with traditional work on $\ell_1$-regression, the main advantage of our result is that we achieve a high-probability risk bound without exponential moment conditions on the input and output. Furthermore, if the input is bounded, we show that the classical empirical risk minimization is competent for $\ell_1$-regression even when the output is heavy-tailed.",https://proceedings.neurips.cc/paper_files/paper/2018/file/8b16ebc056e613024c057be590b542eb-Paper.pdf,2018,"heavy, input, output, regression, risk, tailed, ell_1, loss, minimization, problem","log, regression, ℓ1, bounded, risk, heavy, tailed, yi, probability, nα","{'ell_1': 0.5142403204377899, 'tailed': 0.48537952442360377, 'heavy': 0.4649024637741475, 'distributions': 0.40718087174577516, 'regression': 0.3435593790997907}","tailed, heavy, risk, ell_1, regression, output, input, minimization, competent, moment"
Learning Confidence Sets using Support Vector Machines,"Wenbo Wang, Xingye Qiao","The goal of confidence-set learning in the binary classification setting is to construct two sets, each with a specific probability guarantee to cover a class. An observation outside the overlap of the two sets is deemed to be from one of the two classes, while the overlap is an ambiguity region which could belong to either class. Instead of plug-in approaches, we propose a support vector classifier to construct confidence sets in a flexible manner. Theoretically, we show that the proposed learner can control the non-coverage rates and minimize the ambiguity with high probability. Efficient algorithms are developed and numerical studies illustrate the effectiveness of the proposed method.",https://proceedings.neurips.cc/paper_files/paper/2018/file/8b4224068a41c5d37f5e2d54f3995089-Paper.pdf,2018,"sets, two, ambiguity, class, confidence, construct, overlap, probability, proposed, algorithms","class, ambiguity, coverage, non, classiﬁcation, two, data, kernel, set, function","{'machines': 0.4613604195299082, 'support': 0.4354674110903204, 'vector': 0.4354674110903204, 'confidence': 0.4028460783965043, 'sets': 0.3912030207957865, 'using': 0.2639663978707618, 'learning': 0.1512607202274222}","overlap, sets, ambiguity, confidence, construct, probability, two, class, deemed, cover"
Learning to Optimize Tensor Programs,"Tianqi Chen, Lianmin Zheng, Eddie Yan, Ziheng Jiang, Thierry Moreau, Luis Ceze, Carlos Guestrin, Arvind Krishnamurthy","We introduce a learning-based framework to optimize tensor programs for deep learning workloads. Efficient implementations of tensor operators, such as matrix multiplication and high dimensional convolution are key enablers of effective deep learning systems. However, existing systems rely on manually optimized libraries such as cuDNN where only a narrow range of server class GPUs are well-supported. The reliance on hardware specific operator libraries limits the applicability of high-level graph optimizations and incurs significant engineering costs when deploying to new hardware targets. We use learning to remove this engineering burden. We learn domain specific statistical cost models to guide the search of tensor operator implementations over billions of possible program variants. We further accelerate the search by effective model transfer across workloads. Experimental results show that our framework delivers performance competitive with state-of-the-art hand-tuned libraries for low-power CPU, mobile GPU, and server-class GPU.",https://proceedings.neurips.cc/paper_files/paper/2018/file/8b5700012be65c9da25f49408d959ca0-Paper.pdf,2018,"learning, libraries, tensor, class, deep, effective, engineering, framework, gpu, hardware","latexit, sha1_base64, learning, model, cost, transfer, hardware, number, figure, loop","{'optimize': 0.6212109272718033, 'programs': 0.5440231451523386, 'tensor': 0.5211287130517317, 'learning': 0.21577920615691834}","libraries, tensor, workloads, engineering, server, implementations, hardware, operator, gpu, search"
Hybrid-MST: A Hybrid Active Sampling Strategy for Pairwise Preference Aggregation,"JING LI, Rafal Mantiuk, Junle Wang, Suiyi Ling, Patrick Le Callet","In this paper we present a hybrid active sampling strategy for pairwise preference aggregation, which aims at recovering the underlying rating of the test candidates from sparse and noisy pairwise labeling. Our method employs Bayesian optimization framework and Bradley-Terry model to construct the utility function, then to obtain the Expected Information Gain (EIG) of each pair. For computational efficiency, Gaussian-Hermite quadrature is used for estimation of EIG. In this work, a hybrid active sampling strategy is proposed, either using Global Maximum (GM) EIG sampling or Minimum Spanning Tree (MST) sampling in each trial, which is determined by the test budget. The proposed method has been validated on both simulated and real-world datasets, where it shows higher preference aggregation ability than the state-of-the-art methods.",https://proceedings.neurips.cc/paper_files/paper/2018/file/8b6a80c3cf2cbd5f967063618dc54f39-Paper.pdf,2018,"sampling, eig, active, aggregation, hybrid, method, pairwise, preference, proposed, strategy","mst, hybrid, pairwise, active, method, sampling, test, bt, aggregation, model","{'hybrid': 0.5655382112468914, 'mst': 0.3499125599188499, 'aggregation': 0.33027435845300174, 'pairwise': 0.33027435845300174, 'preference': 0.33027435845300174, 'strategy': 0.3163408327711478, 'active': 0.27706442983945145, 'sampling': 0.23162049934208032}","eig, sampling, preference, aggregation, pairwise, hybrid, active, strategy, test, hermite"
A no-regret generalization of hierarchical softmax to extreme multi-label classification,"Marek Wydmuch, Kalina Jasinska, Mikhail Kuznetsov, Róbert Busa-Fekete, Krzysztof Dembczynski","Extreme multi-label classification (XMLC) is a problem of tagging an instance with a small subset of relevant labels chosen from an extremely large pool of possible labels. Large label spaces can be efficiently handled by organizing labels as a tree, like in the hierarchical softmax (HSM) approach commonly used for multi-class problems. In this paper, we investigate probabilistic label trees (PLTs) that have been recently devised for tackling XMLC problems.  We show that PLTs are a no-regret multi-label generalization of HSM when precision@$k$ is used as a model evaluation metric.  Critically, we prove that pick-one-label heuristic---a reduction technique from multi-label to multi-class that is routinely used along with HSM---is not consistent in general.  We also show that our implementation of PLTs, referred to as extremeText (XT), obtains significantly better results than HSM with the pick-one-label heuristic and XML-CNN, a deep network specifically designed for XMLC problems. Moreover, XT is competitive to many state-of-the-art approaches in terms of statistical performance, model size and prediction time which makes it amenable to deploy in an online system.",https://proceedings.neurips.cc/paper_files/paper/2018/file/8b8388180314a337c9aa3c5aa8e2f37a-Paper.pdf,2018,"label, multi, hsm, labels, plts, problems, used, xmlc, class, heuristic","label, tree, multi, al, et, hsm, labels, one, plts, zi","{'extreme': 0.40310139494586433, 'label': 0.38932956625790305, 'softmax': 0.38932956625790305, 'hierarchical': 0.360322187704633, 'regret': 0.3465503590166717, 'generalization': 0.33035793184228546, 'classification': 0.3175429804634015, 'multi': 0.27286462798984285}","label, hsm, plts, xmlc, multi, xt, pick, labels, heuristic, problems"
The Sample Complexity of Semi-Supervised Learning with Nonparametric Mixture Models,"Chen Dan, Liu Leqi, Bryon Aragam, Pradeep K. Ravikumar, Eric P. Xing","We study the sample complexity of semi-supervised learning (SSL) and introduce new assumptions based on the mismatch between a mixture model learned from unlabeled data and the true mixture model induced by the (unknown) class conditional distributions. Under these assumptions, we establish an $\Omega(K\log K)$ labeled sample complexity bound without imposing parametric assumptions, where $K$ is the number of classes. Our results suggest that even in nonparametric settings it is possible to learn a near-optimal classifier using only a few labeled samples. Unlike previous theoretical work which focuses on binary classification, we consider general multiclass classification ($K>2$), which requires solving a difficult permutation learning problem. This permutation defines a classifier whose classification error is controlled by the Wasserstein distance between mixing measures, and we provide finite-sample results characterizing the behaviour of the excess risk of this classifier. Finally, we describe three algorithms for computing these estimators based on a connection to bipartite graph matching, and perform experiments to illustrate the superiority of the MLE over the majority vote estimator.",https://proceedings.neurips.cc/paper_files/paper/2018/file/8ba6c657b03fc7c8dd4dff8e45defcd2-Paper.pdf,2018,"assumptions, classification, classifier, sample, based, complexity, labeled, learning, mixture, model","mixture, mle, data, mv, decision, true, model, unlabeled, labeled, permutation","{'mixture': 0.4093891528179588, 'semi': 0.4002350710915872, 'complexity': 0.3921605987657389, 'nonparametric': 0.3921605987657389, 'sample': 0.37840386403659754, 'supervised': 0.35714157128485635, 'models': 0.2598579861960495, 'learning': 0.16237850759010625}","classifier, permutation, assumptions, labeled, sample, classification, mixture, defines, mle, multiclass"
Estimating Learnability in the Sublinear Data Regime,"Weihao Kong, Gregory Valiant","We consider the problem of estimating how well a model class is capable of fitting a distribution of labeled data.  We show that it is often possible to accurately estimate this ``learnability'' even when given an amount of data that is too small to reliably learn any accurate model.   Our first result applies to the setting where the data is drawn from a $d$-dimensional distribution with isotropic covariance, and the label of each datapoint is an arbitrary noisy function of the datapoint.  In this setting, we show that with $O(\sqrt{d})$ samples, one can accurately estimate the fraction of the variance of the label that can be explained via the best linear function of the data. 
We extend these techniques to a binary classification, and show that the prediction error of the best linear classifier can be accurately estimated given $O(\sqrt{d})$ labeled samples.  For comparison, in both the linear regression and binary classification settings, even if there is no noise in the labels, a sample size linear in the dimension, $d$, is required to \emph{learn} any function correlated with the underlying model.  We further extend our estimation approach to the setting where the data distribution has an (unknown) arbitrary covariance matrix, allowing these techniques to be applied to settings where the model class consists of a linear function applied to a nonlinear embedding of the data.  We demonstrate the practical viability of our approaches on synthetic and real data.  This ability to estimate the explanatory value of a set of features (or dataset), even in the regime in which there is too little data to realize that explanatory value, may be relevant to the scientific and industrial settings for which data collection is expensive and there are many potentially relevant feature sets that could be collected.",https://proceedings.neurips.cc/paper_files/paper/2018/file/8bd39eae38511daad6152e84545e504d-Paper.pdf,2018,"data, linear, function, model, accurately, distribution, estimate, even, setting, settings","data, setting, linear, variance, βt, distribution, error, algorithm, covariance, isotropic","{'estimating': 0.4872976188367458, 'learnability': 0.4872976188367458, 'regime': 0.4872976188367458, 'sublinear': 0.4599489325970652, 'data': 0.27580803365916184}","data, linear, datapoint, explanatory, accurately, estimate, settings, function, setting, covariance"
Multi-armed Bandits with Compensation,"Siwei Wang, Longbo Huang","We propose and study the known-compensation multi-arm bandit (KCMAB) problem, where a system controller offers a set of arms to many short-term players for $T$ steps. In each step, one short-term player arrives to the system. Upon arrival, the player greedily selects an arm with the current best average reward and receives a stochastic reward associated with the arm. In order to incentivize players to explore other arms,  the controller provides proper payment compensation to players. The objective of the controller is to maximize the total reward collected by players while minimizing the  compensation. We first give a compensation lower bound $\Theta(\sum_i {\Delta_i\log T\over KL_i})$, where $\Delta_i$ and $KL_i$ are the expected reward gap and Kullback-Leibler (KL) divergence between distributions of arm $i$ and the best arm, respectively. We then analyze three algorithms to solve the KCMAB problem, and obtain their regrets and compensations. We show that the algorithms all achieve $O(\log T)$ regret and $O(\log T)$ compensation that match the theoretical lower bound. Finally, we use experiments to show the behaviors of those algorithms.",https://proceedings.neurips.cc/paper_files/paper/2018/file/8bdb5058376143fa358981954e7626b8-Paper.pdf,2018,"arm, compensation, players, reward, algorithms, controller, log, arms, best, bound","arm, compensation, regret, log, µj, algorithm, time, µi, controller, policy","{'compensation': 0.6002383822377736, 'armed': 0.5426495972089138, 'bandits': 0.45860270345888965, 'multi': 0.36732663872605503}","compensation, arm, players, controller, reward, delta_i, kcmab, kl_i, log, player"
A Neural Compositional Paradigm for Image Captioning,"Bo Dai, Sanja Fidler, Dahua Lin","Mainstream captioning models often follow a sequential structure to generate cap-
tions, leading to issues such as introduction of irrelevant semantics, lack of diversity in the generated captions, and inadequate generalization performance. In this paper, we present an alternative paradigm for image captioning, which factorizes the captioning procedure into two stages: (1) extracting an explicit semantic representation from the given image; and (2) constructing the caption based on a recursive compositional procedure in a bottom-up manner. Compared to conventional ones, our paradigm better preserves the semantic content through an explicit factorization of semantics and syntax. By using the compositional generation procedure, caption construction follows a recursive structure, which naturally fits the properties of human language. Moreover, the proposed compositional procedure requires less data to train, generalizes better, and yields more diverse captions.",https://proceedings.neurips.cc/paper_files/paper/2018/file/8bf1211fd4b7b94528899de0a43b9fb3-Paper.pdf,2018,"procedure, captioning, compositional, better, caption, captions, explicit, image, paradigm, recursive","captions, phrases, image, noun, compcap, phrase, caption, module, connecting, coco","{'paradigm': 0.5459393375255865, 'compositional': 0.5152994922426806, 'captioning': 0.4935601760491971, 'image': 0.36473721015645627, 'neural': 0.24450315577131065}","captioning, compositional, procedure, captions, caption, recursive, semantics, paradigm, semantic, explicit"
Stochastic Composite Mirror Descent: Optimal Bounds with High Probabilities,"Yunwen Lei, Ke Tang","We study stochastic composite mirror descent, a class of scalable algorithms able to exploit the geometry and composite structure of a problem. We consider both convex and strongly convex objectives with non-smooth loss functions, for each of which we establish high-probability convergence rates optimal up to a logarithmic factor. We apply the derived computational error bounds to study the generalization performance of multi-pass stochastic gradient descent (SGD) in a non-parametric setting. Our high-probability generalization bounds enjoy a logarithmical dependency on the number of passes provided that the step size sequence is square-summable, which improves the existing bounds in expectation with a polynomial dependency and therefore gives a strong justification on the ability of multi-pass SGD to overcome overfitting. Our analysis removes boundedness assumptions on subgradients often imposed in the literature. Numerical results are reported to support our theoretical findings.",https://proceedings.neurips.cc/paper_files/paper/2018/file/8c6744c9d42ec2cb9e8885b54ff744d0-Paper.pdf,2018,"bounds, composite, convex, dependency, descent, generalization, high, multi, non, pass","wt, sgd, ηt, probability, bounds, log, assumption, generalization, loss, high","{'composite': 0.4265863729728035, 'mirror': 0.4265863729728035, 'probabilities': 0.4265863729728035, 'high': 0.3512686778532732, 'bounds': 0.2991638502707071, 'descent': 0.29650142130912077, 'optimal': 0.293947693744511, 'stochastic': 0.258582996997286}","composite, dependency, pass, bounds, sgd, probability, generalization, boundedness, logarithmical, subgradients"
Geometry-Aware Recurrent Neural Networks for Active Visual Recognition,"Ricson Cheng, Ziyan Wang, Katerina Fragkiadaki","We present recurrent geometry-aware neural networks that integrate visual in-
formation across multiple views of a scene into 3D latent feature tensors, while
maintaining an one-to-one mapping between 3D physical locations in the world
scene and latent feature locations. Object detection, object segmentation, and 3D
reconstruction is then carried out directly using the constructed 3D feature memory,
as opposed to any of the input 2D images. The proposed models are equipped
with differentiable egomotion-aware feature warping and (learned) depth-aware
unprojection operations to achieve geometrically consistent mapping between the
features in the input frame and the constructed latent model of the scene. We
empirically show the proposed model generalizes much better than geometry-
unaware LSTM/GRU networks, especially under the presence of multiple objects
and cross-object occlusions. Combined with active view selection policies, our
model learns to select informative viewpoints to integrate information from by
“undoing"" cross-object occlusions, seamlessly combining geometry with learning
from experience.",https://proceedings.neurips.cc/paper_files/paper/2018/file/8c9f32e03aeb2e3000825c8c875c4edd-Paper.pdf,2018,"3d, feature, object, aware, geometry, latent, model, scene, constructed, cross","3d, view, object, camera, views, active, feature, segmentation, depth, reconstruction","{'geometry': 0.45374972456425716, 'active': 0.39741283989459836, 'aware': 0.39009324185687116, 'recognition': 0.3834718600164782, 'visual': 0.3574411841019108, 'recurrent': 0.3292727417670813, 'neural': 0.22478158686624924, 'networks': 0.22414128807690484}","3d, aware, object, geometry, feature, scene, occlusions, locations, constructed, integrate"
Reward learning from human preferences and demonstrations in Atari,"Borja Ibarz, Jan Leike, Tobias Pohlen, Geoffrey Irving, Shane Legg, Dario Amodei","To solve complex real-world problems with reinforcement learning, we cannot rely on manually specified reward functions. Instead, we need humans to communicate an objective to the agent directly. In this work, we combine two approaches to this problem: learning from expert demonstrations and learning from trajectory preferences. We use both to train a deep neural network to model the reward function and use its predicted reward to train an DQN-based deep reinforcement learning agent on 9 Atari games. Our approach beats the imitation learning baseline in 7 games and achieves strictly superhuman performance on 2 games. Additionally, we investigate the fit of the reward model, present some reward hacking problems, and study the effects of noise in the human labels.",https://proceedings.neurips.cc/paper_files/paper/2018/file/8cbe9ce23f42628c98f80fa0fac8b19a-Paper.pdf,2018,"learning, reward, games, agent, deep, model, problems, reinforcement, train, use","reward, al, et, demonstrations, human, learning, model, agent, feedback, preferences","{'atari': 0.48240381520254294, 'preferences': 0.45532978472750546, 'reward': 0.43612045440307295, 'demonstrations': 0.42122054023591515, 'human': 0.40904642392803553, 'learning': 0.15815996656657855}","reward, games, learning, agent, hacking, superhuman, train, reinforcement, dqn, beats"
Rectangular Bounding Process,"Xuhui Fan, Bin Li, Scott SIsson","Stochastic partition models divide a multi-dimensional space into a number of rectangular regions, such that the data within each region exhibit certain types of homogeneity. Due to the nature of their partition strategy, existing partition models may create many unnecessary divisions in sparse regions when trying to describe data in dense regions. To avoid this problem we introduce a new parsimonious partition model -- the Rectangular Bounding Process (RBP) -- to efficiently partition multi-dimensional spaces, by employing a bounding strategy to enclose data points within rectangular bounding boxes. Unlike existing approaches, the RBP possesses several attractive theoretical properties that make it a powerful nonparametric partition prior on a hypercube. In particular, the RBP is self-consistent and as such can be directly extended from a finite hypercube to infinite (unbounded) space. We apply the RBP to regression trees and relational models as a flexible partition prior. The experimental results validate the merit of the RBP {in rich yet parsimonious expressiveness} compared to the state-of-the-art methods.",https://proceedings.neurips.cc/paper_files/paper/2018/file/8ce87bdda85cd44f14de9afb86491884-Paper.pdf,2018,"partition, rbp, bounding, data, models, rectangular, regions, dimensional, existing, hypercube","rbp, bounding, data, partition, boxes, kτ, relational, model, process, regression","{'bounding': 0.6263533324660814, 'rectangular': 0.6263533324660814, 'process': 0.46407219892735346}","partition, rbp, rectangular, bounding, regions, hypercube, parsimonious, strategy, within, divisions"
Constructing Unrestricted Adversarial Examples with Generative Models,"Yang Song, Rui Shu, Nate Kushman, Stefano Ermon","Adversarial examples are typically constructed by perturbing an existing data point within a small matrix norm, and current defense methods are focused on guarding against this type of attack. In this paper, we propose a new class of adversarial examples that are synthesized entirely from scratch using a conditional generative model, without being restricted to norm-bounded perturbations. We first train an Auxiliary Classifier Generative Adversarial Network (AC-GAN) to model the class-conditional distribution over data samples. Then, conditioned on a desired class, we search over the AC-GAN latent space to find images that are likely under the generative model and are misclassified by a target classifier. We demonstrate through human evaluation that these new kind of adversarial images, which we call Generative Adversarial Examples, are legitimate and belong to the desired class. Our empirical results on the MNIST, SVHN, and CelebA datasets show that generative adversarial examples can bypass strong adversarial training and certified defense methods designed for traditional adversarial attacks.",https://proceedings.neurips.cc/paper_files/paper/2018/file/8cea559c47e4fbdb73b23e0223d04e79-Paper.pdf,2018,"adversarial, generative, class, examples, model, ac, classifier, conditional, data, defense","adversarial, examples, unrestricted, attack, images, image, based, success, attacks, noise","{'unrestricted': 0.5322487310097331, 'constructing': 0.4811830899926932, 'examples': 0.45131160492243677, 'generative': 0.3279861669415735, 'adversarial': 0.3097275446288234, 'models': 0.27925952069557336}","adversarial, generative, examples, ac, class, defense, desired, gan, norm, classifier"
Causal Discovery from Discrete Data using Hidden Compact Representation,"Ruichu Cai, Jie Qiao, Kun Zhang, Zhenjie Zhang, Zhifeng Hao","Causal discovery from a set of observations is one of the fundamental problems across several disciplines. For continuous variables, recently a number of causal discovery methods have demonstrated their effectiveness in distinguishing the cause from effect by exploring certain properties of the conditional distribution, but causal discovery on categorical data still remains to be a challenging problem, because it is generally not easy to find a compact description of the causal mechanism for the true causal direction. In this paper we make an attempt to find a way to solve this problem by assuming a two-stage causal process: the first stage maps the cause to a hidden variable of a lower cardinality, and the second stage generates the effect from the hidden representation. In this way, the causal mechanism admits a simple yet compact representation. We show that under this model, the causal direction is identifiable under some weak conditions on the true causal mechanism. We also provide an effective solution to recover the above hidden compact representation within the likelihood framework. Empirical studies verify the effectiveness of the proposed approach on both synthetic and real-world data.",https://proceedings.neurips.cc/paper_files/paper/2018/file/8d3369c4c086f236fabf61d614a32818-Paper.pdf,2018,"causal, compact, discovery, hidden, mechanism, representation, stage, cause, data, direction","causal, representation, hidden, model, data, compact, direction, hcr, mechanism, cause","{'discovery': 0.40755755390340903, 'hidden': 0.40755755390340903, 'compact': 0.39577832523474976, 'discrete': 0.39577832523474976, 'causal': 0.3284932298506314, 'representation': 0.3248004025251194, 'using': 0.26705360979836523, 'data': 0.2641817196869916}","causal, discovery, stage, compact, hidden, mechanism, cause, direction, representation, effect"
Boosted Sparse and Low-Rank Tensor Regression,"Lifang He, Kun Chen, Wanwan Xu, Jiayu Zhou, Fei Wang","We propose a sparse and low-rank tensor regression model to relate a univariate outcome to a feature tensor, in which each unit-rank tensor from the CP decomposition of the coefficient tensor is assumed to be sparse. This structure is both parsimonious and highly interpretable, as it implies that the outcome is related to the features through a few distinct pathways, each of which may only involve subsets of feature dimensions. We take a divide-and-conquer strategy to simplify the task into a set of sparse unit-rank tensor regression problems. To make the computation efficient and scalable, for the unit-rank tensor regression, we propose a stagewise estimation procedure to efficiently trace out its entire solution path. We show that as the step size goes to zero, the stagewise solution paths converge exactly to those of the corresponding regularized regression. The superior performance of our approach is demonstrated on various real-world and synthetic examples.",https://proceedings.neurips.cc/paper_files/paper/2018/file/8d34201a5b85900908db6cae92723617-Paper.pdf,2018,"tensor, rank, regression, sparse, unit, feature, outcome, propose, solution, stagewise","tensor, rank, surf, step, sparsity, acs, time, al, data, et","{'boosted': 0.5272483935158551, 'tensor': 0.41748080025225653, 'rank': 0.39064423972401796, 'low': 0.38020073779114255, 'sparse': 0.35564498840411524, 'regression': 0.3522499568945854}","tensor, rank, unit, regression, stagewise, outcome, sparse, solution, feature, conquer"
Combinatorial Optimization with Graph Convolutional Networks and Guided Tree Search,"Zhuwen Li, Qifeng Chen, Vladlen Koltun","We present a learning-based approach to computing solutions for certain NP-hard problems. Our approach combines deep learning techniques with useful algorithmic elements from classic heuristics. The central component is a graph convolutional network that is trained to estimate the likelihood, for each vertex in a graph, of whether this vertex is part of the optimal solution. The network is designed and trained to synthesize a diverse set of solutions, which enables rapid exploration of the solution space via tree search. The presented approach is evaluated on four canonical NP-hard problems and five datasets, which include benchmark satisfiability problems and real social network graphs with up to a hundred thousand nodes. Experimental results demonstrate that the presented approach substantially outperforms recent deep learning work, and performs on par with highly optimized state-of-the-art heuristic solvers for some NP-hard problems. Experiments indicate that our approach generalizes across datasets, and scales to graphs that are orders of magnitude larger than those used during training.",https://proceedings.neurips.cc/paper_files/paper/2018/file/8d3bba7425e7c98c50f52ca1b52d3735-Paper.pdf,2018,"approach, problems, hard, learning, network, np, datasets, deep, graph, graphs","approach, graph, network, problems, solutions, mis, set, search, graphs, sat","{'combinatorial': 0.4531160076974357, 'guided': 0.39685780436267715, 'tree': 0.39685780436267715, 'search': 0.3768998891140985, 'convolutional': 0.33484927784838225, 'graph': 0.3259808503763785, 'optimization': 0.2629705028308436, 'networks': 0.22382824741348223}","np, hard, vertex, approach, problems, presented, graphs, solutions, solution, graph"
Chaining Mutual Information and Tightening Generalization Bounds,"Amir Asadi, Emmanuel Abbe, Sergio Verdu","Bounding the generalization error of learning algorithms has a long history, which yet falls short in explaining various generalization successes including those of deep learning. Two important difficulties are (i) exploiting the dependencies between the hypotheses, (ii) exploiting the dependence between the algorithm’s input and output. Progress on the first point was made with the chaining method, originating from the work of Kolmogorov, and used in the VC-dimension bound. More recently, progress on the second point was made with the mutual information method by Russo and Zou ’15. Yet, these two methods are currently disjoint. In this paper, we introduce a technique to combine chaining and mutual information methods, to obtain a generalization bound that is both algorithm-dependent and that exploits the dependencies between the hypotheses. We provide an example in which our bound significantly outperforms both the chaining and the mutual information bounds. As a corollary, we tighten Dudley’s inequality when the learning algorithm chooses its output from a small subset of hypotheses with high probability.",https://proceedings.neurips.cc/paper_files/paper/2018/file/8d7628dd7a710c8638dbd22d4421ee46-Paper.pdf,2018,"algorithm, bound, chaining, generalization, hypotheses, information, learning, mutual, dependencies, exploiting","xt, bound, bounds, algorithm, information, mutual, set, xw, generalization, random","{'chaining': 0.478789816756348, 'tightening': 0.478789816756348, 'mutual': 0.4519186153240866, 'generalization': 0.3547407374864975, 'information': 0.33317416937056427, 'bounds': 0.31692928216902905}","chaining, hypotheses, mutual, generalization, dependencies, bound, exploiting, progress, made, information"
Modeling Dynamic Missingness of Implicit Feedback for Recommendation,"Menghan Wang, Mingming Gong, Xiaolin Zheng, Kun Zhang","Implicit feedback is widely used in collaborative filtering methods for recommendation. It is well known that implicit feedback contains a large number of values that are \emph{missing not at random} (MNAR); and the missing data is a mixture of negative and unknown feedback, making it difficult to learn user's negative preferences. 
Recent studies modeled \emph{exposure}, a latent missingness variable which indicates whether an item is missing to a user, to give each missing entry a confidence of being negative feedback.
However, these studies use static models and ignore the information in temporal dependencies among items, which seems to be a essential underlying factor to subsequent missingness. To model and exploit the dynamics of missingness, we propose a latent variable named ``\emph{user intent}'' to govern the temporal changes of item missingness, and a hidden Markov model to represent such a process. The resulting framework captures the dynamic item missingness and incorporate it into matrix factorization (MF) for recommendation. We also explore two types of constraints to achieve a more compact and interpretable representation of \emph{user intents}. Experiments on real-world datasets demonstrate the superiority of our method against state-of-the-art recommender systems.",https://proceedings.neurips.cc/paper_files/paper/2018/file/8d9766a69b764fefc12f56739424d136-Paper.pdf,2018,"missingness, emph, feedback, missing, user, item, negative, implicit, latent, model","user, item, h4mf, missingness, missing, recommendation, intent, data, model, αt","{'missingness': 0.4747284546182446, 'recommendation': 0.414518645619957, 'feedback': 0.4025382274743088, 'implicit': 0.4025382274743088, 'dynamic': 0.3836345301738627, 'modeling': 0.36270881559610957}","missingness, missing, emph, feedback, user, item, negative, recommendation, implicit, studies"
Does mitigating ML's impact disparity require treatment disparity?,"Zachary Lipton, Julian McAuley, Alexandra Chouldechova","Following precedent in employment discrimination law, two notions of disparity are widely-discussed in papers on fairness and ML. Algorithms exhibit treatment disparity if they formally treat members of protected subgroups differently;
algorithms exhibit impact disparity when outcomes differ across subgroups (even unintentionally). Naturally, we can achieve impact parity through purposeful treatment disparity. One line of papers aims to reconcile the two parities proposing disparate learning processes (DLPs). Here, the sensitive feature is used during training but a group-blind classifier is produced. In this paper, we show that: (i) when sensitive and (nominally) nonsensitive features are correlated, DLPs will indirectly implement treatment disparity, undermining the policy desiderata they are designed to address; (ii) when group membership is partly revealed by other features, DLPs induce within-class discrimination; and (iii) in general, DLPs provide suboptimal trade-offs between accuracy and impact parity. Experimental results on several real-world datasets highlight the practical consequences of applying DLPs.",https://proceedings.neurips.cc/paper_files/paper/2018/file/8e0384779e58ce2af40eb365b318cc32-Paper.pdf,2018,"disparity, dlps, impact, treatment, algorithms, discrimination, exhibit, features, group, papers","treatment, rule, protected, dlp, group, parity, accuracy, dlps, optimal, data","{'disparity': 0.6827989057394636, 'impact': 0.3413994528697318, 'mitigating': 0.3413994528697318, 'require': 0.3413994528697318, 'treatment': 0.3086445003673824, 'ml': 0.298099760910175}","dlps, disparity, impact, treatment, parity, subgroups, papers, discrimination, sensitive, exhibit"
The Sparse Manifold Transform,"Yubei Chen, Dylan Paiton, Bruno Olshausen","We present a signal representation framework called the sparse manifold transform that combines key ideas from sparse coding, manifold learning, and slow feature analysis. It turns non-linear transformations in the primary sensory signal space into linear interpolations in a representational embedding space while maintaining approximate invertibility. The sparse manifold transform is an unsupervised and generative framework that explicitly and simultaneously models the sparse discreteness and low-dimensional manifold structure found in natural scenes. When stacked, it also models hierarchical composition. We provide a theoretical description of the transform and demonstrate properties of the learned representation on both synthetic data and natural videos.",https://proceedings.neurips.cc/paper_files/paper/2018/file/8e19a39c36b8e5e3afd2a3b2692aea96-Paper.pdf,2018,"manifold, sparse, transform, framework, linear, models, natural, representation, signal, space","sparse, manifold, embedding, dictionary, function, space, elements, image, data, linear","{'transform': 0.6515384400801514, 'manifold': 0.5823938072327245, 'sparse': 0.48612253022761137}","manifold, transform, sparse, signal, discreteness, natural, interpolations, invertibility, representation, space"
Probabilistic Model-Agnostic Meta-Learning,"Chelsea Finn, Kelvin Xu, Sergey Levine","Meta-learning for few-shot learning entails acquiring a prior over previous tasks and experiences, such that new tasks be learned from small amounts of data. However, a critical challenge in few-shot learning is task ambiguity: even when a powerful prior can be meta-learned from a large number of prior tasks, a small dataset for a new task can simply be too ambiguous to acquire a single model (e.g., a classifier) for that task that is accurate. In this paper, we propose a probabilistic meta-learning algorithm that can sample models for a new task from a model distribution. Our approach extends model-agnostic meta-learning, which adapts to new tasks via gradient descent, to incorporate a parameter distribution that is trained via a variational lower bound. At meta-test time, our algorithm adapts via a simple procedure that injects noise into gradient descent, and at meta-training time, the model is trained such that this stochastic adaptation procedure produces samples from the approximate model posterior. Our experimental results show that our method can sample plausible classifiers and regressors in ambiguous few-shot learning problems. We also show how reasoning about ambiguity can also be used for downstream active learning problems.",https://proceedings.neurips.cc/paper_files/paper/2018/file/8e2c381d4dd04f1c55093f22c59c3a08-Paper.pdf,2018,"learning, meta, model, new, task, tasks, prior, shot, via, adapts","learning, model, φi, meta, training, xtr, ytr, shot, inference, tasks","{'agnostic': 0.5412717841281454, 'meta': 0.5158529859468375, 'probabilistic': 0.48771528847963325, 'model': 0.39906626839040354, 'learning': 0.2092856024972875}","meta, shot, ambiguous, task, learning, adapts, ambiguity, tasks, via, prior"
Deep Neural Networks with Box Convolutions,"Egor Burkov, Victor Lempitsky","Box filters computed using integral images have been part of the computer vision toolset for a long time. Here, we show that a convolutional layer that computes box filter responses in a sliding manner can be used within deep architectures, whereas the dimensions and the offsets of the sliding boxes in such a layer can be learned as part of an end-to-end loss minimization. Crucially, the training process can make the size of the boxes in such a layer arbitrarily large without incurring extra computational cost and without the need to increase the number of learnable parameters. Due to its ability to integrate information over large boxes, the new layer facilitates long-range propagation of information and leads to the efficient increase of the receptive fields of downstream units in the network. By incorporating the new layer into existing architectures for semantic segmentation, we are able to achieve both the increase in segmentation accuracy as well as the decrease in the computational cost and the number of learnable parameters.",https://proceedings.neurips.cc/paper_files/paper/2018/file/8e489b4966fe8f703b5be647f1cbae63-Paper.pdf,2018,"layer, boxes, increase, architectures, box, computational, cost, end, information, large","box, layer, xmax, architectures, enet, boxenet, new, convolutions, ymax, block","{'box': 0.6157221818476439, 'convolutions': 0.5774986109346117, 'deep': 0.31911708581498516, 'neural': 0.3050205908937426, 'networks': 0.30415172828894843}","layer, boxes, increase, sliding, learnable, part, segmentation, box, architectures, long"
Learning Compressed Transforms with Low Displacement Rank,"Anna Thomas, Albert Gu, Tri Dao, Atri Rudra, Christopher Ré","The low displacement rank (LDR) framework for structured matrices represents a matrix through two displacement operators and a low-rank residual. Existing use of LDR matrices in deep learning has applied fixed displacement operators encoding forms of shift invariance akin to convolutions. We introduce a rich class of LDR matrices with more general displacement operators, and explicitly learn over both the operators and the low-rank component. This class generalizes several previous constructions while preserving compression and efficient computation. We prove bounds on the VC dimension of multi-layer neural networks with structured weight matrices and show empirically that our compact parameterization can reduce the sample complexity of learning. When replacing weight layers in fully-connected, convolutional, and recurrent neural networks for image classification and language modeling tasks, our new classes exceed the accuracy of existing compression approaches, and on some tasks even outperform general unstructured layers while using more than 20x fewer parameters.",https://proceedings.neurips.cc/paper_files/paper/2018/file/8e621619d71d0ae5ef4e631ad586334f-Paper.pdf,2018,"displacement, matrices, operators, ldr, low, rank, class, compression, existing, general","ldr, matrices, rank, displacement, like, matrix, operators, toeplitz, classes, layer","{'compressed': 0.48932494622876466, 'displacement': 0.48932494622876466, 'transforms': 0.48932494622876466, 'rank': 0.36254633290178734, 'low': 0.35285400176414755, 'learning': 0.16042911497961615}","displacement, operators, ldr, matrices, rank, compression, low, weight, structured, layers"
Deep Defense: Training DNNs with Improved Adversarial Robustness,"Ziang Yan, Yiwen Guo, Changshui Zhang","Despite the efficacy on a variety of computer vision tasks, deep neural networks (DNNs) are vulnerable to adversarial attacks, limiting their applications in security-critical systems. Recent works have shown the possibility of generating imperceptibly perturbed image inputs (a.k.a., adversarial examples) to fool well-trained DNN classifiers into making arbitrary predictions. To address this problem, we propose a training recipe named ""deep defense"". Our core idea is to integrate an adversarial perturbation-based regularizer into the classification objective, such that the obtained models learn to resist potential attacks, directly and precisely. The whole optimization problem is solved just like training a recursive network. Experimental results demonstrate that our method outperforms training with adversarial/Parseval regularizations by large margins on various datasets (including MNIST, CIFAR-10 and ImageNet) and different DNN architectures. Code and models for reproducing our results are available at https://github.com/ZiangYan/deepdefense.pytorch.",https://proceedings.neurips.cc/paper_files/paper/2018/file/8f121ce07d74717e0b1f21d122e04521-Paper.pdf,2018,"adversarial, training, attacks, deep, dnn, models, problem, results, 10, address","adversarial, xk, 10, training, method, accuracy, attacks, train, examples, robustness","{'defense': 0.5093700604314957, 'dnns': 0.4447666566336566, 'robustness': 0.38917619013968036, 'improved': 0.3721729735891952, 'training': 0.3285273941282273, 'adversarial': 0.29641393005406247, 'deep': 0.23866811018293743}","adversarial, dnn, attacks, training, deepdefense, parseval, recipe, ziangyan, imperceptibly, resist"
Precision and Recall for Time Series,"Nesime Tatbul, Tae Jun Lee, Stan Zdonik, Mejbah Alam, Justin Gottschlich","Classical anomaly detection is principally concerned with point-based anomalies, those anomalies that occur at a single point in time. Yet, many real-world anomalies are range-based, meaning they occur over a period of time. Motivated by this observation, we present a new mathematical model to evaluate the accuracy of time series classification algorithms. Our model expands the well-known Precision and Recall metrics to measure ranges, while simultaneously enabling customization support for domain-specific preferences.",https://proceedings.neurips.cc/paper_files/paper/2018/file/8f468c873a32bb0619eaeb2050ba45d1-Paper.pdf,2018,"anomalies, time, based, model, occur, point, accuracy, algorithms, anomaly, classical","anomaly, model, range, recall, precision, ranges, real, time, based, anomalies","{'precision': 0.5552364956671103, 'recall': 0.5552364956671103, 'series': 0.47537365003049936, 'time': 0.3967930954733141}","anomalies, occur, time, customization, point, period, principally, concerned, expands, ranges"
Neighbourhood Consensus Networks,"Ignacio Rocco, Mircea Cimpoi, Relja Arandjelović, Akihiko Torii, Tomas Pajdla, Josef Sivic","We address the problem of finding reliable dense correspondences between a pair of images. This is a challenging task due to strong appearance differences between the corresponding scene elements and ambiguities generated by repetitive patterns. The contributions of this work are threefold. First, inspired by the classic idea of disambiguating feature matches using semi-local constraints,  we develop an end-to-end trainable convolutional neural network architecture that identifies sets of spatially consistent  matches by analyzing neighbourhood consensus patterns in the 4D space of all possible correspondences between a pair of images without the need for a global geometric model. Second, we demonstrate that the model can be trained effectively from weak supervision in the form of matching and non-matching image pairs without the need for costly manual annotation of point to point correspondences.
Third, we show the proposed neighbourhood consensus network can be applied to a range of matching tasks including both category- and instance-level matching, obtaining the state-of-the-art results on the PF Pascal dataset and the InLoc indoor visual localization benchmark.",https://proceedings.neurips.cc/paper_files/paper/2018/file/8f7d807e1f53eff5f9efbe5cb81090fb-Paper.pdf,2018,"matching, correspondences, consensus, end, images, matches, model, need, neighbourhood, network","matches, matching, image, neighbourhood, consensus, images, network, feature, correspondences, pairs","{'consensus': 0.674286379009545, 'neighbourhood': 0.674286379009545, 'networks': 0.3011241573909216}","correspondences, matching, neighbourhood, consensus, pair, patterns, matches, need, end, 4d"
PAC-learning in the presence of adversaries,"Daniel Cullina, Arjun Nitin Bhagoji, Prateek Mittal","The existence of evasion attacks during the test phase of machine learning algorithms represents a significant challenge to both their deployment and understanding. These attacks can be carried out by adding imperceptible perturbations to inputs to generate adversarial examples and finding effective defenses and detectors has proven to be difficult. In this paper, we step away from the attack-defense arms race and seek to understand the limits of what can be learned in the presence of an evasion adversary. In particular, we extend the Probably Approximately Correct (PAC)-learning framework to account for the presence of an adversary. We first define corrupted hypothesis classes which arise from standard binary hypothesis classes in the presence of an evasion adversary and derive the Vapnik-Chervonenkis (VC)-dimension for these, denoted as the adversarial VC-dimension. We then show that sample complexity upper bounds from the Fundamental Theorem of Statistical learning can be extended to the case of evasion adversaries, where the sample complexity is controlled by the adversarial VC-dimension. We then explicitly derive the adversarial VC-dimension for halfspace classifiers in the presence of a sample-wise norm-constrained adversary of the type commonly studied for evasion attacks and show that it is the same as the standard VC-dimension, closing an open question. Finally, we prove that the adversarial VC-dimension can be either larger or smaller than the standard VC-dimension depending on the hypothesis class and adversary, making it an interesting object of study in its own right.",https://proceedings.neurips.cc/paper_files/paper/2018/file/8f85517967795eeef66c225f7883bdcb-Paper.pdf,2018,"dimension, vc, adversarial, adversary, evasion, presence, attacks, hypothesis, learning, sample","adversarial, dimension, vc, learning, adversary, complexity, sample, classiﬁers, hypothesis, presence","{'presence': 0.6108264060250708, 'adversaries': 0.5650687421869862, 'pac': 0.5124172239980301, 'learning': 0.21217211611299325}","vc, evasion, dimension, adversary, presence, adversarial, attacks, hypothesis, standard, sample"
Optimal Algorithms for Non-Smooth Distributed Optimization in Networks,"Kevin Scaman, Francis Bach, Sebastien Bubeck, Laurent Massoulié, Yin Tat Lee","In this work, we consider the distributed optimization of non-smooth convex functions using a network of computing units. We investigate this problem under two regularity assumptions: (1) the Lipschitz continuity of the global objective function, and (2) the Lipschitz continuity of local individual functions. Under the local regularity assumption, we provide the first optimal first-order decentralized algorithm called multi-step primal-dual (MSPD) and its corresponding optimal convergence rate. A notable aspect of this result is that, for non-smooth functions, while the dominant term of the error is in $O(1/\sqrt{t})$, the structure of the communication network only impacts a second-order term in $O(1/t)$, where $t$ is time. In other words, the error due to limits in communication resources decreases at a fast rate even in the case of non-strongly-convex objective functions. Under the global regularity assumption, we provide a simple yet efficient algorithm called distributed randomized smoothing (DRS) based on a local smoothing of the objective function, and show that DRS is within a $d^{1/4}$ multiplicative factor of the optimal convergence rate, where $d$ is the underlying dimension.",https://proceedings.neurips.cc/paper_files/paper/2018/file/8fb21ee7a2207526da55a679f0332de2-Paper.pdf,2018,"functions, local, non, objective, optimal, rate, regularity, algorithm, assumption, called","algorithm, local, communication, optimal, time, convergence, optimization, regularity, θt, fi","{'smooth': 0.5090505652737011, 'distributed': 0.4013216164941404, 'algorithms': 0.3791757219098054, 'optimal': 0.3791757219098054, 'non': 0.36181714192501446, 'optimization': 0.30588309140473086, 'networks': 0.2603534446849323}","regularity, continuity, drs, functions, smoothing, objective, rate, local, lipschitz, term"
Large-Scale Stochastic Sampling from the Probability Simplex,"Jack Baker, Paul Fearnhead, Emily Fox, Christopher Nemeth","Stochastic gradient Markov chain Monte Carlo (SGMCMC) has become a popular method for scalable Bayesian inference. These methods are based on sampling a discrete-time approximation to a continuous time process, such as the Langevin diffusion. When applied to distributions defined on a constrained space the time-discretization error can dominate when we are near the boundary of the space. We demonstrate that because of this, current SGMCMC methods for the simplex struggle with sparse simplex spaces; when many of the components are close to zero. Unfortunately, many popular large-scale Bayesian models, such as network or topic models, require inference on sparse simplex spaces. To avoid the biases caused by this discretization error, we propose the stochastic Cox-Ingersoll-Ross process (SCIR), which removes all discretization error and we prove that samples from the SCIR process are asymptotically unbiased. We discuss how this idea can be extended to target other constrained spaces. Use of the SCIR process within a SGMCMC algorithm is shown to give substantially better performance for a topic model and a Dirichlet process mixture model than existing SGMCMC approaches.",https://proceedings.neurips.cc/paper_files/paper/2018/file/900c563bfd2c48c16701acca83ad858a-Paper.pdf,2018,"process, sgmcmc, discretization, error, scir, simplex, spaces, time, bayesian, constrained","scir, process, sgrld, al, distribution, et, sgmcmc, simplex, cir, error","{'simplex': 0.5254290135482388, 'probability': 0.4750176779779505, 'scale': 0.3951176014877731, 'large': 0.3616966497600622, 'sampling': 0.34780154937874735, 'stochastic': 0.3006231097683602}","sgmcmc, scir, simplex, discretization, process, spaces, topic, error, constrained, popular"
Transfer of Value Functions via Variational Methods,"Andrea Tirinzoni, Rafael Rodriguez Sanchez, Marcello Restelli","We consider the problem of transferring value functions in reinforcement learning. We propose an approach that uses the given source tasks to learn a prior distribution over optimal value functions and provide an efficient variational approximation of the corresponding posterior in a new target task. We show our approach to be general, in the sense that it can be combined with complex parametric function approximators and distribution models, while providing two practical algorithms based on Gaussians and Gaussian mixtures. We theoretically analyze them by deriving a finite-sample analysis and provide a comprehensive empirical evaluation in four different domains.",https://proceedings.neurips.cc/paper_files/paper/2018/file/9023effe3c16b0477df9b93e26d57e2c-Paper.pdf,2018,"approach, distribution, functions, provide, value, algorithms, analysis, analyze, approximation, approximators","tasks, distribution, algorithm, task, source, transfer, function, error, variational, weights","{'value': 0.46885028630707165, 'methods': 0.45093041891157415, 'transfer': 0.42986087510945714, 'functions': 0.41836895375604183, 'variational': 0.35265335319589736, 'via': 0.30440733465687986}","value, functions, approximators, distribution, comprehensive, gaussians, transferring, provide, mixtures, deriving"
Adaptive Methods for Nonconvex Optimization,"Manzil Zaheer, Sashank Reddi, Devendra Sachan, Satyen Kale, Sanjiv Kumar","Adaptive gradient methods that rely on scaling gradients down by the square root of exponential moving averages of past squared gradients, such RMSProp, Adam, Adadelta have found wide application in optimizing the nonconvex problems that arise in deep learning. However, it has been recently demonstrated that such methods can fail to converge even in simple convex optimization settings. In this work, we provide a new analysis of such methods applied to nonconvex stochastic optimization problems, characterizing the effect of increasing minibatch size. Our analysis shows that under this scenario such methods do converge to stationarity up to the statistical limit of variance in the stochastic gradients (scaled by a constant factor). In particular, our result implies that increasing minibatch sizes enables convergence,  thus providing a way to circumvent the non-convergence issues. Furthermore, we provide a new adaptive optimization algorithm, Yogi, which controls the increase in effective learning rate,  leading to even better performance with similar theoretical guarantees on convergence. Extensive experiments show that Yogi with very little hyperparameter tuning outperforms methods such as Adam in several challenging machine learning tasks.",https://proceedings.neurips.cc/paper_files/paper/2018/file/90365351ccc7437a1309dc64e4db32a3-Paper.pdf,2018,"methods, convergence, gradients, learning, optimization, adam, adaptive, analysis, converge, even","adam, learning, yogi, rate, 10, convergence, methods, results, size, β2","{'methods': 0.5657842442888014, 'nonconvex': 0.5318810200303152, 'adaptive': 0.5011105093542275, 'optimization': 0.38194113009835445}","yogi, adam, minibatch, gradients, methods, increasing, convergence, nonconvex, converge, optimization"
How Does Batch Normalization Help Optimization?,"Shibani Santurkar, Dimitris Tsipras, Andrew Ilyas, Aleksander Madry","Batch Normalization (BatchNorm) is a widely adopted technique that enables faster and more stable training of deep neural networks (DNNs).
Despite its pervasiveness, the exact reasons for BatchNorm's effectiveness are still poorly understood.
The popular belief is that this effectiveness stems from controlling the change of the layers' input distributions during training to reduce the so-called ""internal covariate shift"".
In this work, we demonstrate that such distributional stability of layer inputs has little to do with the success of BatchNorm.
Instead, we uncover a more fundamental impact of BatchNorm on the training process: it makes the optimization landscape significantly smoother.
This smoothness induces a more predictive and stable behavior of the gradients, allowing for faster training.",https://proceedings.neurips.cc/paper_files/paper/2018/file/905056c1ac1dad141560467e0a99e1cf-Paper.pdf,2018,"batchnorm, training, effectiveness, faster, stable, adopted, allowing, batch, behavior, belief","batchnorm, training, layer, loss, standard, gradient, network, networks, yj, landscape","{'help': 0.6111276474288555, 'normalization': 0.5181956917968606, 'batch': 0.5051560377841461, 'optimization': 0.3206456003775322}","batchnorm, training, stable, effectiveness, faster, pervasiveness, smoother, adopted, stems, covariate"
Compact Generalized Non-local Network,"Kaiyu Yue, Ming Sun, Yuchen Yuan, Feng Zhou, Errui Ding, Fuxin Xu","The non-local module is designed for capturing long-range spatio-temporal dependencies in images and videos. Although having shown excellent performance, it lacks the mechanism to model the interactions between positions across channels, which are of vital importance in recognizing fine-grained objects and actions. To address this limitation, we generalize the non-local module and take the correlations between the positions of any two channels into account. This extension utilizes the compact representation for multiple kernel functions with Taylor expansion that makes the generalized non-local module in a fast and low-complexity computation flow. Moreover, we implement our generalized non-local method within channel groups to ease the optimization. Experimental results illustrate the clear-cut improvements and practical applicability of the generalized non-local module on both fine-grained object recognition and video classification. Code is available at: https://github.com/KaiyuYue/cgnl-network.pytorch.",https://proceedings.neurips.cc/paper_files/paper/2018/file/907edb0aa6986220dbffb79a788596ee-Paper.pdf,2018,"local, non, module, generalized, channels, fine, grained, positions, account, across","cgnl, block, local, non, channels, feature, operation, nl, results, module","{'compact': 0.5299895087501001, 'generalized': 0.47754934532120086, 'local': 0.4451325410197435, 'non': 0.38791129737058927, 'network': 0.37741169787871276}","module, local, non, generalized, grained, positions, channels, fine, cgnl, cut"
Stacked Semantics-Guided Attention Model for Fine-Grained Zero-Shot Learning,"yunlong yu, Zhong Ji, Yanwei Fu, Jichang Guo, Yanwei Pang, Zhongfei (Mark) Zhang","Zero-Shot Learning (ZSL) is generally achieved via aligning the semantic relationships between the visual features and the corresponding class semantic descriptions. However, using the global features to represent fine-grained images may lead to sub-optimal results since they neglect the discriminative differences of local regions. Besides, different regions contain distinct discriminative information. The important regions should contribute more to the prediction. To this end, we propose a novel stacked semantics-guided attention (S2GA) model to obtain semantic relevant features by using individual class semantic features to progressively guide the visual features to generate an attention map for weighting the importance of different local regions. Feeding both the integrated visual features and the class semantic features into a multi-class classification architecture, the proposed framework can be trained end-to-end. Extensive experimental results on CUB and NABird datasets show that the proposed approach has a consistent improvement on both fine-grained zero-shot classification and retrieval tasks.",https://proceedings.neurips.cc/paper_files/paper/2018/file/9087b0efc7c7acd1ef7e153678809c77-Paper.pdf,2018,"features, semantic, class, regions, end, visual, attention, classification, different, discriminative","features, semantic, class, attention, visual, image, region, local, different, regions","{'stacked': 0.3892413221622963, 'fine': 0.3673958659568116, 'grained': 0.3673958659568116, 'semantics': 0.3673958659568116, 'zero': 0.3145512140662967, 'guided': 0.3082053557033271, 'attention': 0.2973937154369267, 'shot': 0.292705757860812, 'model': 0.24333828170967398, 'learning': 0.12761589473265653}","features, semantic, regions, class, grained, visual, end, discriminative, fine, shot"
MacNet: Transferring Knowledge from Machine Comprehension to Sequence-to-Sequence Models,"Boyuan Pan, Yazheng Yang, Hao Li, Zhou Zhao, Yueting Zhuang, Deng Cai, Xiaofei He","Machine Comprehension (MC) is one of the core problems in natural language processing, requiring both understanding of the natural language and knowledge about the world. Rapid progress has been made since the release of several benchmark datasets, and recently the state-of-the-art models even surpass human performance on the well-known SQuAD evaluation. In this paper, we transfer knowledge learned from machine comprehension to the sequence-to-sequence tasks to deepen the understanding of the text. We propose MacNet: a novel encoder-decoder supplementary architecture to the widely used attention-based sequence-to-sequence models. Experiments on neural machine translation (NMT) and abstractive text summarization show that our proposed framework can significantly improve the performance of the baseline models, and our method for the abstractive text summarization achieves the state-of-the-art results on the Gigaword dataset.",https://proceedings.neurips.cc/paper_files/paper/2018/file/908c9a564a86426585b29f5335b619bc-Paper.pdf,2018,"sequence, machine, models, text, abstractive, art, comprehension, knowledge, language, natural","al, et, model, 2017, layer, attention, sequence, machine, models, mc","{'sequence': 0.6602377928270485, 'macnet': 0.3651528230160788, 'transferring': 0.3651528230160788, 'comprehension': 0.33011889641352427, 'machine': 0.28380656456338565, 'knowledge': 0.2668001567293925, 'models': 0.19158787310333847}","sequence, text, abstractive, comprehension, summarization, machine, understanding, language, deepen, macnet"
Multiplicative Weights Updates with Constant Step-Size in Graphical Constant-Sum Games,Yun Kuen Cheung,"Since Multiplicative Weights (MW) updates are the discrete analogue of the continuous Replicator Dynamics (RD), some researchers had expected their qualitative behaviours would be similar. We show that this is false in the context of graphical constant-sum games, which include two-person zero-sum games as special cases. In such games which have a fully-mixed Nash Equilibrium (NE), it was known that RD satisfy the permanence and Poincare recurrence properties, but we show that MW updates with any constant step-size eps > 0 converge to the boundary of the state space, and thus do not satisfy the two properties. Using this result, we show that MW updates have a regret lower bound of Omega( 1 / (eps T) ), while it was known that the regret of RD is upper bounded by O( 1 / T ).Interestingly, the regret perspective can be useful for better understanding of the behaviours of MW updates. In a two-person zero-sum game, if it has a unique NE which is fully mixed, then we show, via regret, that for any sufficiently small eps, there exist at least two probability densities and a constant Z > 0, such that for any arbitrarily small z > 0, each of the two densities fluctuates above Z and below z infinitely often.",https://proceedings.neurips.cc/paper_files/paper/2018/file/90e1357833654983612fb05e3ec9148c-Paper.pdf,2018,"two, mw, regret, show, updates, constant, eps, games, rd, sum","game, sum, player, si, updates, games, constant, mw, two, ui","{'constant': 0.5930884391530913, 'updates': 0.30960578505034053, 'weights': 0.29654421957654564, 'multiplicative': 0.28641288229610323, 'size': 0.28641288229610323, 'step': 0.28641288229610323, 'sum': 0.28641288229610323, 'graphical': 0.2711361077953954, 'games': 0.250614727242769}","mw, rd, eps, updates, regret, behaviours, ne, games, sum, two"
Efficient Online Portfolio with Logarithmic Regret,"Haipeng Luo, Chen-Yu Wei, Kai Zheng","We study the decades-old problem of online portfolio management and propose the first algorithm with logarithmic regret that is not based on Cover's Universal Portfolio algorithm and admits much faster implementation. Specifically Universal Portfolio enjoys optimal regret $\mathcal{O}(N\ln T)$ for $N$ financial instruments over $T$ rounds, but requires log-concave sampling and has a large polynomial running time. Our algorithm, on the other hand, ensures a slightly larger but still logarithmic regret of $\mathcal{O}(N^2(\ln T)^4)$, and is based on the well-studied Online Mirror Descent framework with a novel regularizer that can be implemented via standard optimization methods in time $\mathcal{O}(TN^{2.5})$ per round. The regret of all other existing works is either polynomial in $T$ or has a potentially unbounded factor such as the inverse of the smallest price relative.",https://proceedings.neurips.cc/paper_files/paper/2018/file/91c77393975889bd08f301c9e13a44b7-Paper.pdf,2018,"regret, algorithm, mathcal, portfolio, based, ln, logarithmic, online, polynomial, time","xt, ln, ut, regret, ui, algorithm, time, xs, barrons, lemma","{'logarithmic': 0.5541079888705663, 'portfolio': 0.5230096929940201, 'regret': 0.43066758575095987, 'online': 0.3667852594413905, 'efficient': 0.3153081060551229}","portfolio, mathcal, regret, ln, universal, logarithmic, polynomial, online, old, tn"
Banach Wasserstein GAN,"Jonas Adler, Sebastian Lunz","Wasserstein Generative Adversarial Networks (WGANs) can be used to generate realistic samples from complicated image distributions. The Wasserstein metric used in WGANs is based on a notion of distance between individual images, which  induces a notion of distance between probability distributions of images. So far the community has considered $\ell^2$ as the underlying distance. We generalize the theory of WGAN with gradient penalty to Banach spaces, allowing practitioners to select the features to emphasize in the generator. We further discuss the effect of some particular choices of underlying norms, focusing on Sobolev norms. Finally, we demonstrate a boost in performance for an appropriate choice of norm on CIFAR-10 and CelebA.",https://proceedings.neurips.cc/paper_files/paper/2018/file/91d0dbfd38d950cb716c4dd26c5da08a-Paper.pdf,2018,"distance, distributions, images, norms, notion, underlying, used, wasserstein, wgans, 10","norm, spaces, space, 10, wasserstein, banach, distance, gradient, images, lipschitz","{'banach': 0.6528597115536139, 'gan': 0.5700575155762727, 'wasserstein': 0.49880720320093597}","wgans, distance, norms, wasserstein, notion, underlying, banach, emphasize, wgan, images"
SplineNets: Continuous Neural Decision Graphs,"Cem Keskin, Shahram Izadi","We present SplineNets, a practical and novel approach for using conditioning in convolutional neural networks (CNNs). SplineNets are continuous generalizations of neural decision graphs, and they can dramatically reduce runtime complexity and computation costs of CNNs, while maintaining or even increasing accuracy. Functions of SplineNets are both dynamic (i.e., conditioned on the input) and hierarchical (i.e.,conditioned on the computational path). SplineNets employ a unified loss function with a desired level of smoothness over both the network and decision parameters, while allowing for sparse activation of a subset of nodes for individual samples. In particular, we embed infinitely many function weights (e.g. filters) on smooth, low dimensional manifolds parameterized by compact B-splines, which are indexed by a position parameter. Instead of sampling from a categorical distribution to pick a branch, samples choose a continuous position to pick a function weight. We further show that by maximizing the mutual information between spline positions and class labels, the network can be optimally utilized and specialized for classification tasks. Experiments show that our approach can significantly increase the accuracy of ResNets with negligible cost in speed, matching the precision of a 110 level ResNet with a 32 level SplineNet.",https://proceedings.neurips.cc/paper_files/paper/2018/file/9246444d94f081e3549803b928260f56-Paper.pdf,2018,"splinenets, function, level, accuracy, approach, cnns, conditioned, continuous, decision, network","decision, φi, knots, spline, model, splinenets, number, parameters, position, figure","{'splinenets': 0.5907538973389557, 'continuous': 0.4442412903883021, 'decision': 0.4442412903883021, 'graphs': 0.43163635186134824, 'neural': 0.2645737030752176}","splinenets, pick, position, level, conditioned, cnns, function, decision, continuous, indexed"
Post: Device Placement with Cross-Entropy Minimization and Proximal Policy Optimization,"Yuanxiang Gao, Li Chen, Baochun Li","Training deep neural networks requires an exorbitant amount of computation resources, including a heterogeneous mix of GPU and CPU devices. It is critical to place operations in a neural network on these devices in an optimal way, so that the training process can complete within the shortest amount of time. The state-of-the-art uses reinforcement learning to learn placement skills by repeatedly performing Monte-Carlo experiments. However, due to its equal treatment of placement samples, we argue that there remains ample room for significant improvements. In this paper, we propose a new joint learning algorithm, called Post, that integrates cross-entropy minimization and proximal policy optimization to achieve theoretically guaranteed optimal efficiency. In order to incorporate the cross-entropy method as a sampling technique, we propose to represent placements using discrete probability distributions, which allows us to estimate an optimal probability mass by maximal likelihood estimation, a powerful tool with the best possible efficiency. We have implemented Post in the Google Cloud platform, and our extensive experiments with several popular neural network training benchmarks have demonstrated clear evidence of superior performance: with the same amount of learning time, it leads to placements that have training times up to 63.7% shorter over the state-of-the-art.",https://proceedings.neurips.cc/paper_files/paper/2018/file/92650b2e92217715fe312e6fa7b90d82-Paper.pdf,2018,"training, amount, learning, neural, optimal, art, cross, devices, efficiency, entropy","placement, policy, method, cross, entropy, post, training, optimization, placements, gradient","{'device': 0.4075900113839321, 'placement': 0.3847147686064756, 'post': 0.3847147686064756, 'proximal': 0.3369125191068558, 'cross': 0.3293790397429966, 'minimization': 0.31678988519316864, 'entropy': 0.31141274309490113, 'policy': 0.26979940927380486, 'optimization': 0.21385375781628438}","placement, placements, amount, post, devices, training, cross, entropy, optimal, efficiency"
Implicit Reparameterization Gradients,"Mikhail Figurnov, Shakir Mohamed, Andriy Mnih","By providing a simple and efficient way of computing low-variance gradients of continuous random variables, the reparameterization trick has become the technique of choice for training a variety of latent variable models. However, it is not applicable to a number of important continuous distributions.  We introduce an alternative approach to computing reparameterization gradients based on implicit differentiation and demonstrate its broader applicability by applying it to Gamma, Beta, Dirichlet, and von Mises distributions, which cannot be used with the classic reparameterization trick. Our experiments show that the proposed approach is faster and more accurate than the existing gradient estimators for these distributions.",https://proceedings.neurips.cc/paper_files/paper/2018/file/92c8c96e4c37100777c7190b76d28233-Paper.pdf,2018,"distributions, reparameterization, approach, computing, continuous, gradients, trick, accurate, alternative, applicability","reparameterization, distribution, distributions, gradients, implicit, 10, gamma, gradient, function, mises","{'reparameterization': 0.6233018884621567, 'implicit': 0.5599444996896442, 'gradients': 0.5458542965909705}","reparameterization, trick, distributions, computing, gradients, continuous, mises, von, differentiation, dirichlet"
Visual Object Networks: Image Generation with Disentangled 3D Representations,"Jun-Yan Zhu, Zhoutong Zhang, Chengkai Zhang, Jiajun Wu, Antonio Torralba, Josh Tenenbaum, Bill Freeman","Recent progress in deep generative models has led to tremendous breakthroughs in image generation. While being able to synthesize photorealistic images, existing models lack an understanding of our underlying 3D world. Different from previous works built on 2D datasets and models, we present a new generative model, Visual Object Networks (VONs), synthesizing natural images of objects with a disentangled 3D representation. Inspired by classic graphics rendering pipelines, we unravel the image formation process into three conditionally independent factors---shape, viewpoint, and texture---and present an end-to-end adversarial learning framework that jointly models 3D shape and 2D texture. Our model first learns to synthesize 3D shapes that are indistinguishable from real shapes. It then renders the object's 2.5D sketches (i.e., silhouette and depth map) from its shape under a sampled viewpoint. Finally, it learns to add realistic textures to these 2.5D sketches to generate realistic images. The VON not only generates images that are more realistic than the state-of-the-art 2D image synthesis methods but also enables many 3D operations such as changing the viewpoint of a generated image,  shape and texture editing, linear interpolation in texture and shape space, and transferring appearance across different objects and viewpoints.",https://proceedings.neurips.cc/paper_files/paper/2018/file/92cc227532d17e56e07902b254dfad10-Paper.pdf,2018,"3d, shape, image, images, models, texture, 2d, realistic, viewpoint, 5d","al, et, 3d, 5d, shape, texture, image, 2d, images, 2017","{'disentangled': 0.43893704977172726, '3d': 0.3780208548391931, 'generation': 0.35800358128464477, 'visual': 0.35800358128464477, 'object': 0.35378524529763594, 'representations': 0.3460460283721429, 'image': 0.3358451777527919, 'networks': 0.22449395149274917}","shape, texture, 3d, viewpoint, 2d, images, realistic, 5d, sketches, image"
Neural Architecture Optimization,"Renqian Luo, Fei Tian, Tao Qin, Enhong Chen, Tie-Yan Liu","Automatic neural architecture design has shown its potential in discovering powerful neural network architectures. Existing methods, no matter based on reinforcement learning or evolutionary algorithms (EA), conduct architecture search in a discrete space, which is highly inefficient. In this paper, we propose a simple and efficient method to automatic neural architecture design based on continuous optimization. We call this new approach neural architecture optimization (NAO). There are three key components in our proposed approach: (1) An encoder embeds/maps neural network architectures into a continuous space. (2) A predictor takes the continuous representation of a network as input and predicts its accuracy. (3) A decoder maps a continuous representation of a network back to its architecture. The performance predictor and the encoder enable us to perform gradient based optimization in the continuous space to find the embedding of a new architecture with potentially better accuracy. Such a better embedding is then decoded to a network by the decoder. Experiments show that the architecture discovered by our method is very competitive for image classification task on CIFAR-10 and language modeling task on PTB, outperforming or on par with the best results of previous architecture search methods with a significantly reduction of computational resources. Specifically we obtain $2.11\%$ test set error rate for CIFAR-10 image classification task and $56.0$ test set perplexity of PTB language modeling task. The best discovered architectures on both tasks are successfully transferred to other tasks such as CIFAR-100 and WikiText-2. Furthermore, combined with the recent proposed weight sharing mechanism, we discover powerful architecture on CIFAR-10 (with error rate $3.53\%$) and on PTB (with test set perplexity $56.6$), with very limited computational resources (less than $10$ GPU hours) for both tasks.",https://proceedings.neurips.cc/paper_files/paper/2018/file/933670f1ac8ba969f32989c312faba75-Paper.pdf,2018,"architecture, continuous, network, neural, 10, cifar, task, architectures, based, optimization","architecture, architectures, performance, nao, weight, decoder, space, 34, continuous, ex","{'architecture': 0.7949974665788971, 'optimization': 0.46138489919583237, 'neural': 0.3938311858235541}","architecture, ptb, cifar, continuous, 10, 56, perplexity, network, task, architectures"
MiME: Multilevel Medical Embedding of Electronic Health Records for Predictive Healthcare,"Edward Choi, Cao Xiao, Walter Stewart, Jimeng Sun","Deep learning models exhibit state-of-the-art performance for many predictive healthcare tasks using electronic health records (EHR) data, but these models typically require training data volume that exceeds the capacity of most healthcare systems.
External resources such as medical ontologies are used to bridge the data volume constraint, but this approach is often not directly applicable or useful because of inconsistencies with terminology.
To solve the data insufficiency challenge, we leverage the inherent multilevel structure of EHR data and, in particular, the encoded relationships among medical codes.
We propose Multilevel Medical Embedding (MiME) which learns the multilevel embedding of EHR data while jointly performing auxiliary prediction tasks that rely on this inherent EHR structure without the need for external labels. 
We conducted two prediction tasks, heart failure prediction and sequential disease prediction, where MiME outperformed baseline methods in diverse evaluation settings.
In particular, MiME consistently outperformed all baselines when predicting heart failure on datasets of different volumes, especially demonstrating the greatest performance improvement (15% relative gain in PR-AUC over the best baseline) on the smallest dataset, demonstrating its ability to effectively model the multilevel structure of EHR data.",https://proceedings.neurips.cc/paper_files/paper/2018/file/934b535800b1cba8f96a5d72f72f1611-Paper.pdf,2018,"data, ehr, multilevel, prediction, medical, mime, structure, tasks, baseline, demonstrating","codes, visit, mime, dx, data, prediction, treatment, code, diagnosis, embedding","{'electronic': 0.3505072452513174, 'health': 0.3505072452513174, 'healthcare': 0.3505072452513174, 'mime': 0.3505072452513174, 'multilevel': 0.3505072452513174, 'records': 0.3505072452513174, 'medical': 0.33083566816051174, 'predictive': 0.2897280984925559, 'embedding': 0.26357810184966585}","ehr, multilevel, mime, medical, data, prediction, outperformed, heart, healthcare, volume"
Learning Optimal Reserve Price against Non-myopic Bidders,"Jinyan Liu, Zhiyi Huang, Xiangning Wang","We consider the problem of learning optimal reserve price in repeated auctions against non-myopic bidders, who may bid strategically in order to gain in future rounds even if the single-round auctions are truthful. Previous algorithms, e.g., empirical pricing, do not provide non-trivial regret rounds in this setting in general. We introduce algorithms that obtain small regret against non-myopic bidders either when the market is large, i.e., no bidder appears in a constant fraction of the rounds, or when the bidders are impatient, i.e., they discount future utility by some factor mildly bounded away from one. Our approach carefully controls what information is revealed to each bidder, and builds on techniques from differentially private online learning as well as the recent line of works on jointly differentially private algorithms.",https://proceedings.neurips.cc/paper_files/paper/2018/file/93d65641ff3f1586614cf2c1ad240b6c-Paper.pdf,2018,"algorithms, bidders, non, rounds, auctions, bidder, differentially, future, learning, myopic","bidder, bidders, price, algorithm, day, regret, bid, round, bids, learning","{'bidders': 0.4623438189526346, 'myopic': 0.4623438189526346, 'reserve': 0.4623438189526346, 'price': 0.40370475327231264, 'optimal': 0.30070694681999854, 'non': 0.28694064985861534, 'learning': 0.151583135629039}","bidders, rounds, auctions, bidder, myopic, differentially, private, future, non, regret"
A Bayesian Approach to Generative Adversarial Imitation Learning,"Wonseok Jeon, Seokin Seo, Kee-Eung Kim","Generative adversarial training for imitation learning has shown promising results on high-dimensional and continuous control tasks. This paradigm is based on reducing the imitation learning problem to the density matching problem, where the agent iteratively refines the policy to match the empirical state-action visitation frequency of the expert demonstration. Although this approach has shown to robustly learn to imitate even with scarce demonstration, one must still address the inherent challenge that collecting trajectory samples in each iteration is a costly operation. To address this issue, we first propose a Bayesian formulation of generative adversarial imitation learning (GAIL), where the imitation policy and the cost function are represented as stochastic neural networks. Then, we show that we can significantly enhance the sample efficiency of GAIL leveraging the predictive density of the cost, on an extensive set of imitation learning tasks with high-dimensional states and actions.",https://proceedings.neurips.cc/paper_files/paper/2018/file/943aa0fcda4ee2901a7de9321663b114-Paper.pdf,2018,"imitation, learning, address, adversarial, cost, demonstration, density, dimensional, gail, generative","gail, policy, log, imitation, learning, cost, function, state, 0a, discriminator","{'imitation': 0.5523749486676479, 'approach': 0.45331336819829376, 'generative': 0.40143293492128723, 'adversarial': 0.37908561335289853, 'bayesian': 0.3727180459323703, 'learning': 0.21357869988092307}","imitation, gail, demonstration, density, shown, address, refines, policy, cost, dimensional"
Credit Assignment For Collective Multiagent RL With Global Rewards,"Duc Thien Nguyen, Akshat Kumar, Hoong Chuin Lau","Scaling decision theoretic planning to large multiagent systems is challenging due to uncertainty and partial observability in the environment. We focus on a multiagent planning model subclass, relevant to urban settings, where agent interactions are dependent on their ``collective influence'' on each other, rather than their identities. Unlike previous work, we address a general setting where system reward is not decomposable among agents. We develop collective actor-critic RL approaches for this setting, and address the problem of multiagent credit assignment, and computing low variance policy gradient estimates that result in faster convergence to high quality solutions. We also develop difference rewards based credit assignment methods for the collective setting. Empirically our new approaches provide significantly better solutions than previous methods in the presence of global rewards on two real world problems modeling taxi fleet optimization and multiagent patrolling, and a synthetic grid navigation domain.",https://proceedings.neurips.cc/paper_files/paper/2018/file/94bb077f18daa6620efa5cf6e6f178d2-Paper.pdf,2018,"multiagent, collective, setting, address, approaches, assignment, credit, develop, methods, planning","nsa, dt, agents, agent, state, action, critic, ns, qw, global","{'collective': 0.4098565956529493, 'credit': 0.386854144985257, 'multiagent': 0.386854144985257, 'rewards': 0.386854144985257, 'rl': 0.386854144985257, 'assignment': 0.3578744844041157, 'global': 0.3245287451723898}","multiagent, collective, credit, assignment, planning, setting, rewards, solutions, address, fleet"
Knowledge Distillation by On-the-Fly Native Ensemble,"xu lan, Xiatian Zhu, Shaogang Gong","Knowledge distillation is effective to train the small and generalisable network models for meeting the low-memory and fast running requirements. Existing offline distillation methods rely on a strong pre-trained teacher, which enables favourable knowledge discovery and transfer but requires a complex two-phase training procedure. Online counterparts address this limitation at the price of lacking a high-capacity teacher. In this work, we present an On-the-fly Native Ensemble (ONE) learning strategy for one-stage online distillation. Specifically, ONE only trains a single multi-branch network while simultaneously establishing a strong teacher on-the-fly to enhance the learning of target network. Extensive evaluations show that ONE improves the generalisation performance of a variety of deep neural networks more significantly than alternative methods on four image classification dataset: CIFAR10, CIFAR100, SVHN, and ImageNet, whilst having the computational efficiency advantages.",https://proceedings.neurips.cc/paper_files/paper/2018/file/94ef7214c4a90790186e255304f8fd1f-Paper.pdf,2018,"one, distillation, network, teacher, fly, knowledge, learning, methods, online, strong","one, training, model, teacher, distillation, branch, network, ensemble, branches, knowledge","{'fly': 0.48979068575455914, 'native': 0.48979068575455914, 'distillation': 0.4427986050317312, 'ensemble': 0.4427986050317312, 'knowledge': 0.35786723663959985}","distillation, teacher, fly, one, strong, online, network, favourable, generalisable, knowledge"
Flexible and accurate inference and learning for deep generative models,"Eszter Vértes, Maneesh Sahani","We introduce a new approach to learning in hierarchical latent-variable generative
models called the “distributed distributional code Helmholtz machine”, which
emphasises flexibility and accuracy in the inferential process. Like the original
Helmholtz machine and later variational autoencoder algorithms (but unlike adver-
sarial methods) our approach learns an explicit inference or “recognition” model
to approximate the posterior distribution over the latent variables. Unlike these
earlier methods, it employs a posterior representation that is not limited to a narrow
tractable parametrised form (nor is it represented by samples). To train the genera-
tive and recognition models we develop an extended wake-sleep algorithm inspired
by the original Helmholtz machine. This makes it possible to learn hierarchical
latent models with both discrete and continuous variables, where an accurate poste-
rior representation is essential. We demonstrate that the new algorithm outperforms
current state-of-the-art methods on synthetic, natural image patch and the MNIST
data sets.",https://proceedings.neurips.cc/paper_files/paper/2018/file/955cb567b6e38f4c6b3f28cc857fc38c-Paper.pdf,2018,"helmholtz, latent, machine, methods, models, algorithm, approach, hierarchical, new, original","model, generative, posterior, ddc, recognition, latent, models, zl, hm, distribution","{'accurate': 0.5143939911255495, 'flexible': 0.5143939911255495, 'generative': 0.3630262024098162, 'inference': 0.3605169943237888, 'models': 0.3090938993867956, 'deep': 0.27603112760432436, 'learning': 0.1931447511863976}","helmholtz, latent, hierarchical, machine, original, recognition, unlike, posterior, variables, adver"
A loss framework for calibrated anomaly detection,,"Given samples from a probability distribution, anomaly detection is the problem of determining if a given point lies in a low-density region. This paper concerns calibrated anomaly detection, which is the practically relevant extension where we additionally wish to produce a confidence score for a point being anomalous. Building on a classification framework for anomaly detection, we show how minimisation of a suitably modified proper loss produces density estimates only for anomalous instances. We then show how to incorporate quantile control by relating our objective to a generalised version of the pinball loss. Finally, we show how to efficiently optimise the objective with kernelised scorer, by leveraging a recent result from the point process literature. The resulting objective captures a close relative of the one-class SVM as a special case.",https://proceedings.neurips.cc/paper_files/paper/2018/file/959a557f5f6beb411fd954f3f34b21c3-Paper.pdf,2018,"anomaly, detection, objective, point, show, anomalous, density, given, loss, additionally","loss, density, al, et, estimation, one, quantile, set, proper, sublevel","{'calibrated': 0.5198602478026112, 'anomaly': 0.49792852368191043, 'framework': 0.41417474151072875, 'loss': 0.4080726074166269, 'detection': 0.37914184499904163}","anomaly, anomalous, detection, point, objective, density, generalised, kernelised, minimisation, pinball"
Persistence Fisher Kernel: A Riemannian Manifold Kernel for Persistence Diagrams,"Tam Le, Makoto Yamada","Algebraic topology methods have recently played an important role for statistical analysis with complicated geometric structured data such as shapes, linked twist maps, and material data. Among them, \textit{persistent homology} is a well-known tool to extract robust topological features, and outputs as \textit{persistence diagrams} (PDs). However, PDs are point multi-sets which can not be used in machine learning algorithms for vector data. To deal with it, an emerged approach is to use kernel methods, and an appropriate geometry for PDs is an important factor to measure the similarity of PDs. A popular geometry for PDs is the \textit{Wasserstein metric}. However, Wasserstein distance is not \textit{negative definite}. Thus, it is limited to build positive definite kernels upon the Wasserstein distance \textit{without approximation}. In this work, we rely upon the alternative \textit{Fisher information geometry} to propose a positive definite kernel for PDs \textit{without approximation}, namely the Persistence Fisher (PF) kernel. Then, we analyze eigensystem of the integral operator induced by the proposed kernel for kernel machines. Based on that, we derive generalization error bounds via covering numbers and Rademacher averages for kernel machines with the PF kernel. Additionally, we show some nice properties such as stability and infinite divisibility for the proposed kernel. Furthermore, we also propose a linear time complexity over the number of points in PDs for an approximation of our proposed kernel with a bounded error. Throughout experiments with many different tasks on various benchmark datasets, we illustrate that the PF kernel compares favorably with other baseline kernels for PDs.",https://proceedings.neurips.cc/paper_files/paper/2018/file/959ab9a0695c467e7caf75431a872e5c-Paper.pdf,2018,"kernel, pds, textit, approximation, data, definite, geometry, pf, proposed, wasserstein","kernel, al, et, pds, dgi, dgj, fisher, persistence, kpf, latexit","{'persistence': 0.6158081852917866, 'kernel': 0.5165963974542555, 'diagrams': 0.3079040926458933, 'fisher': 0.3079040926458933, 'riemannian': 0.3079040926458933, 'manifold': 0.26361648328708304}","pds, kernel, textit, pf, definite, geometry, wasserstein, persistence, fisher, kernels"
Constructing Deep Neural Networks by Bayesian Network Structure Learning,"Raanan Y. Rohekar, Shami Nisimov, Yaniv Gurwicz, Guy Koren, Gal Novik","We introduce a principled approach for unsupervised structure learning of deep neural networks. We propose a new interpretation for depth and inter-layer connectivity where conditional independencies in the input distribution are encoded hierarchically in the network structure. Thus, the depth of the network is determined inherently. The proposed method casts the problem of neural network structure learning as a problem of Bayesian network structure learning. Then, instead of directly learning the discriminative structure, it learns a generative graph, constructs its stochastic inverse, and then constructs a discriminative graph. We prove that conditional-dependency relations among the latent variables in the generative graph are preserved in the class-conditional discriminative graph. We demonstrate on image classification benchmarks that the deepest layers (convolutional and dense) of common networks can be replaced by significantly smaller learned structures, while maintaining classification accuracy---state-of-the-art on tested benchmarks. Our structure learning algorithm requires a small computational cost and runs efficiently on a standard desktop CPU.",https://proceedings.neurips.cc/paper_files/paper/2018/file/95d309f0b035d97f69902e7972c2b2e6-Paper.pdf,2018,"structure, learning, graph, network, conditional, discriminative, benchmarks, classification, constructs, depth","network, structure, al, et, layer, graph, layers, learning, set, algorithm","{'constructing': 0.5458113557570388, 'structure': 0.4299627013226694, 'network': 0.36454974232833953, 'bayesian': 0.34542610669361706, 'deep': 0.28288363549165, 'neural': 0.2703876962008199, 'networks': 0.2696174866968091, 'learning': 0.19793959422597518}","structure, discriminative, graph, conditional, constructs, network, depth, benchmarks, learning, deepest"
"Training Deep Models Faster with Robust, Approximate Importance Sampling","Tyler B. Johnson, Carlos Guestrin","In theory, importance sampling speeds up stochastic gradient algorithms for supervised learning by prioritizing training examples. In practice, the cost of computing importances greatly limits the impact of importance sampling. We propose a robust, approximate importance sampling procedure (RAIS) for stochastic gradient de- scent. By approximating the ideal sampling distribution using robust optimization, RAIS provides much of the benefit of exact importance sampling with drastically reduced overhead. Empirically, we find RAIS-SGD and standard SGD follow similar learning curves, but RAIS moves faster through these paths, achieving speed-ups of at least 20% and sometimes much more.",https://proceedings.neurips.cc/paper_files/paper/2018/file/967990de5b3eac7b87d49a13c6834978-Paper.pdf,2018,"sampling, importance, rais, gradient, learning, much, robust, sgd, stochastic, 20","rais, sgd, sampling, training, gradient, learning, importance, examples, epochs, kg","{'importance': 0.43132964536366747, 'approximate': 0.4131777572192906, 'faster': 0.4055677990161864, 'sampling': 0.3454086058597553, 'training': 0.3365533099179983, 'robust': 0.3338274090743309, 'models': 0.27378461852089886, 'deep': 0.24449876597684791}","rais, importance, sampling, sgd, much, robust, importances, prioritizing, scent, speeds"
Learning Beam Search Policies via Imitation Learning,"Renato Negrinho, Matthew Gormley, Geoffrey J. Gordon","Beam search is widely used for approximate decoding in structured prediction problems. Models often use a beam at test time but ignore its existence at train time, and therefore do not explicitly learn how to use the beam. We develop an unifying meta-algorithm for learning beam search policies using imitation learning. In our setting, the beam is part of the model and not just an artifact of approximate decoding. Our meta-algorithm captures existing learning algorithms and suggests new ones. It also lets us show novel no-regret guarantees for learning beam search policies.",https://proceedings.neurips.cc/paper_files/paper/2018/file/967c2ae04b169f07e7fa8fdfd110551e-Paper.pdf,2018,"beam, learning, search, algorithm, approximate, decoding, meta, policies, time, use","beam, cost, search, policy, loss, learning, vk, θt, algorithm, function","{'beam': 0.5258283404479149, 'imitation': 0.4458675397704429, 'policies': 0.4163563873597847, 'search': 0.395417891503629, 'learning': 0.34479409210345446, 'via': 0.27589087914415}","beam, search, decoding, meta, policies, approximate, artifact, lets, learning, unifying"
Multivariate Time Series Imputation with Generative Adversarial Networks,"Yonghong Luo, Xiangrui Cai, Ying ZHANG, Jun Xu, Yuan xiaojie","Multivariate time series usually contain a large number of missing values, which hinders the application of advanced analysis methods on multivariate time series data. Conventional approaches to addressing the challenge of missing values, including mean/zero imputation, case deletion, and matrix factorization-based imputation, are all incapable of modeling the temporal dependencies and the nature of complex distribution in multivariate time series. In this paper, we treat the problem of missing value imputation as data generation.  Inspired by the success of Generative Adversarial Networks (GAN) in image generation, we propose to learn the overall distribution of a multivariate time series dataset with GAN, which is further used to generate the missing values for each sample. Different from the image data, the time series data are usually incomplete due to the nature of data recording process. A modified Gate Recurrent Unit is employed in GAN to model the temporal irregularity of the incomplete time series. Experiments on two multivariate time series datasets show that the proposed model outperformed the baselines in terms of accuracy of imputation. Experimental results also showed that a simple model on the imputed data can achieve state-of-the-art results on the prediction tasks, demonstrating the benefits of our model in downstream applications.",https://proceedings.neurips.cc/paper_files/paper/2018/file/96b9bff013acedfb1d140579e2fbeb63-Paper.pdf,2018,"series, time, data, multivariate, imputation, missing, model, gan, values, distribution","dataset, imputation, time, missing, series, gan, values, methods, model, method","{'imputation': 0.48845472845195315, 'multivariate': 0.46784793192319146, 'series': 0.41819748692829983, 'time': 0.34906830731319366, 'generative': 0.3188965969385165, 'adversarial': 0.30114398080041305, 'networks': 0.23110545838035002}","series, multivariate, imputation, missing, time, gan, values, data, incomplete, usually"
An Efficient Pruning Algorithm for Robust Isotonic Regression,Cong Han Lim,"We study a generalization of the classic isotonic regression problem  where we allow separable nonconvex objective functions, focusing on the case of estimators used in robust regression. A simple dynamic programming approach allows us to solve this problem to within ε-accuracy (of the global minimum) in time linear in 1/ε and the dimension. We can combine techniques from the convex case with branch-and-bound ideas to form a new algorithm for this problem that naturally exploits the shape of the objective function. Our algorithm achieves the best bounds for both the general nonconvex and convex case (linear in log (1/ε)), while performing much faster in practice than a straightforward dynamic programming approach, especially as the desired accuracy increases.",https://proceedings.neurips.cc/paper_files/paper/2018/file/96da2f590cd7246bbde0051047b0d6f7-Paper.pdf,2018,"case, problem, accuracy, algorithm, approach, convex, dynamic, linear, nonconvex, objective","algorithm, functions, fi, xi, problem, points, gk, function, convex, intervals","{'isotonic': 0.5079355867411582, 'pruning': 0.48650693700965747, 'algorithm': 0.4046743162837443, 'regression': 0.35952492023784693, 'robust': 0.3442703452893318, 'efficient': 0.3062203435208291}","case, programming, nonconvex, dynamic, regression, objective, convex, isotonic, problem, accuracy"
Bilinear Attention Networks,"Jin-Hwa Kim, Jaehyun Jun, Byoung-Tak Zhang","Attention networks in multimodal learning provide an efficient way to utilize given visual information selectively.  However, the computational cost to learn attention distributions for every pair of multimodal input channels is prohibitively expensive. To solve this problem, co-attention builds two separate attention distributions for each modality neglecting the interaction between multimodal inputs. In this paper, we propose bilinear attention networks (BAN) that find bilinear attention distributions to utilize given vision-language information seamlessly. BAN considers bilinear interactions among two groups of input channels, while low-rank bilinear pooling extracts the joint representations for each pair of channels. Furthermore, we propose a variant of multimodal residual networks to exploit eight-attention maps of the BAN efficiently. We quantitatively and qualitatively evaluate our model on visual question answering (VQA 2.0) and Flickr30k Entities datasets, showing that BAN significantly outperforms previous methods and achieves new state-of-the-arts on both datasets.",https://proceedings.neurips.cc/paper_files/paper/2018/file/96ea64f3a1aa2fd00c72faacf0cb8ac9-Paper.pdf,2018,"attention, ban, bilinear, multimodal, channels, distributions, networks, datasets, given, information","attention, ban, bilinear, model, channel, rank, visual, maps, used, co","{'bilinear': 0.7295016351988248, 'attention': 0.5905052881966697, 'networks': 0.345153399018478}","ban, attention, bilinear, multimodal, channels, utilize, pair, distributions, visual, arts"
Unsupervised Video Object Segmentation for Deep Reinforcement Learning,"Vikash Goel, Jameson Weng, Pascal Poupart","We present a new technique for deep reinforcement learning that automatically detects moving objects and uses the relevant information for action selection. The detection of moving objects is done in an unsupervised way by exploiting structure from motion. Instead of directly learning a policy from raw images, the agent first learns to detect and segment moving objects by exploiting flow information in video sequences. The learned representation is then used to focus the policy of the agent on the moving objects. Over time, the agent identifies which objects are critical for decision making and gradually builds a policy based on relevant moving objects. This approach, which we call Motion-Oriented REinforcement Learning (MOREL), is demonstrated on a suite of Atari games where the ability to detect moving objects reduces the amount of interaction needed with the environment to obtain a good policy. Furthermore, the resulting policy is more interpretable than policies that directly map images to actions or values with a black box neural network. We can gain insight into the policy by inspecting the segmentation and motion of each object detected by the agent. This allows practitioners to confirm whether a policy is making decisions based on sensible information. Our code is available at https://github.com/vik-goel/MOREL.",https://proceedings.neurips.cc/paper_files/paper/2018/file/96f2b50b5d3613adf9c27049b2a888c7-Paper.pdf,2018,"objects, policy, moving, agent, information, learning, motion, based, detect, directly","object, policy, network, objects, learning, segmentation, masks, reinforcement, model, moving","{'segmentation': 0.4874994829437156, 'video': 0.464605899103209, 'object': 0.40462114646553526, 'unsupervised': 0.40007251078489314, 'reinforcement': 0.3385424773697173, 'deep': 0.26938491519752794, 'learning': 0.18849425740772954}","moving, objects, policy, motion, agent, morel, detect, exploiting, relevant, information"
Constructing Fast Network through Deconstruction of Convolution,"Yunho Jeon, Junmo Kim","Convolutional neural networks have achieved great success in various vision tasks; however, they incur heavy resource costs. By using deeper and wider networks, network accuracy can be improved rapidly. However, in an environment with limited resources (e.g., mobile applications), heavy networks may not be usable. This study shows that naive convolution can be deconstructed into a shift operation and pointwise convolution. To cope with various convolutions, we propose a new shift operation called active shift layer (ASL) that formulates the amount of shift as a learnable function with shift parameters. This new layer can be optimized end-to-end through backpropagation and it can provide optimal shift values. Finally, we apply this layer to a light and fast network that surpasses existing state-of-the-art networks.",https://proceedings.neurips.cc/paper_files/paper/2018/file/9719a00ed0c5709d80dfef33795dcef3-Paper.pdf,2018,"shift, networks, layer, convolution, end, heavy, however, network, new, operation","shift, convolution, network, asl, parameters, values, number, time, inference, flops","{'deconstruction': 0.5478887925785413, 'constructing': 0.49532259416601077, 'convolution': 0.46457333928921246, 'fast': 0.35944094246415137, 'network': 0.33082808220831944}","shift, operation, layer, heavy, convolution, networks, end, asl, deconstructed, usable"
Improving Simple Models with Confidence Profiles,"Amit Dhurandhar, Karthikeyan Shanmugam, Ronny Luss, Peder A. Olsen","In this paper, we propose a new method called ProfWeight for transferring information from a pre-trained deep neural network that has a high test accuracy to a simpler interpretable model or a very shallow network of low complexity and a priori low test accuracy. We are motivated by applications in interpretability and model deployment in severely memory constrained environments (like sensors). Our method uses linear probes to generate confidence scores through flattened intermediate representations. Our transfer method involves a theoretically justified weighting of samples during the training of the simple model using  confidence scores of these intermediate layers. The value of our method is first demonstrated on CIFAR-10, where our weighting method significantly  improves (3-4\%) networks with only a fraction of the number of Resnet blocks of a complex Resnet model. We further demonstrate operationally significant results on a real manufacturing problem, where we dramatically increase the test accuracy of a CART model (the domain standard) by roughly $13\%$.",https://proceedings.neurips.cc/paper_files/paper/2018/file/972cda1e62b72640cb7ac702714a115f-Paper.pdf,2018,"method, model, accuracy, test, confidence, intermediate, low, network, resnet, scores","model, simple, complex, training, conﬁdence, models, network, neural, probes, set","{'profiles': 0.5531062426015061, 'confidence': 0.4829557789020476, 'simple': 0.4379553159868504, 'improving': 0.42988900169172156, 'models': 0.2902030107419862}","weighting, method, test, intermediate, resnet, scores, confidence, model, accuracy, flattened"
Turbo Learning for CaptionBot and DrawingBot,"Qiuyuan Huang, Pengchuan Zhang, Dapeng Wu, Lei Zhang","We study in this paper the problems of both image captioning and
text-to-image generation, and present a novel turbo learning
approach to jointly training an image-to-text generator (a.k.a.
CaptionBot) and a text-to-image generator (a.k.a. DrawingBot). The
key idea behind the joint training is that image-to-text
generation and text-to-image generation as dual problems can form
a closed loop to provide informative feedback to each other. Based
on such feedback, we introduce a new loss metric by comparing the
original input with the output produced by the closed loop. In
addition to the old loss metrics used in CaptionBot and
DrawingBot, this extra loss metric makes the jointly trained
CaptionBot and DrawingBot better than the separately trained
CaptionBot and DrawingBot. Furthermore, the turbo-learning
approach enables semi-supervised learning since the closed loop
can provide peudo-labels for unlabeled samples. Experimental
results on the COCO dataset demonstrate that the proposed turbo
learning can significantly improve the performance of both
CaptionBot and DrawingBot by a large margin.",https://proceedings.neurips.cc/paper_files/paper/2018/file/976abf49974d4686f87192efa0513ae0-Paper.pdf,2018,"image, captionbot, drawingbot, text, learning, closed, generation, loop, loss, turbo","captionbot, image, turbo, learning, drawingbot, lstm, training, loss, sentence, θcap","{'captionbot': 0.5672768592605216, 'drawingbot': 0.5672768592605216, 'turbo': 0.5672768592605216, 'learning': 0.18598627595216588}","captionbot, drawingbot, text, turbo, image, closed, loop, generation, generator, loss"
Online Reciprocal Recommendation with Theoretical Performance Guarantees,"Fabio Vitale, Nikos Parotsidis, Claudio Gentile","A reciprocal recommendation problem is one where the goal of learning is not just to predict a user's preference towards a passive item (e.g., a book), but to recommend the targeted user on one side another user from the other side such that a mutual interest between the two exists. The problem thus is sharply different from the more traditional items-to-users recommendation, since a good match requires meeting the preferences of both users. We initiate a rigorous theoretical investigation of the reciprocal recommendation task in a specific framework of sequential learning. We point out general limitations, formulate reasonable assumptions enabling effective learning and, under these assumptions, we design and analyze a computationally efficient algorithm that uncovers mutual likes at a pace comparable to those achieved by a clairvoyant algorithm knowing all user preferences in advance. Finally, we validate our algorithm against synthetic and real-world datasets, showing improved empirical performance over simple baselines.",https://proceedings.neurips.cc/paper_files/paper/2018/file/97af07a14cacba681feacf3012730892-Paper.pdf,2018,"user, algorithm, learning, recommendation, assumptions, mutual, one, preferences, problem, reciprocal","matches, number, cb, cg, user, algorithm, log, random, smile, two","{'reciprocal': 0.4733230225241967, 'theoretical': 0.42791090194014797, 'performance': 0.41329146447572995, 'recommendation': 0.41329146447572995, 'guarantees': 0.39124720286554276, 'online': 0.313310602090369}","user, recommendation, reciprocal, side, users, preferences, mutual, assumptions, likes, book"
Learning semantic similarity in a continuous space,Michel Deudon,"We address the problem of learning semantic representation of questions to measure similarity between pairs as a continuous distance metric. Our work naturally extends Word Mover’s Distance (WMD) [1] by representing text documents as normal distributions instead of bags of embedded words. Our learned metric measures the dissimilarity between two questions as the minimum amount of distance the intent (hidden representation) of one question needs to ""travel"" to match the intent of another question. We first learn to repeat, reformulate questions to infer intents as normal distributions with a deep generative model [2] (variational auto encoder). Semantic similarity between pairs is then learned discriminatively as an optimal transport distance metric (Wasserstein 2) with our novel variational siamese framework. Among known models that can read sentences individually, our proposed framework achieves competitive results on Quora duplicate questions dataset. Our work sheds light on how deep generative models can approximate distributions (semantic representations) to effectively measure semantic similarity with meaningful distance metrics from Information Theory.",https://proceedings.neurips.cc/paper_files/paper/2018/file/97e8527feaf77a97fc38f34216141515-Paper.pdf,2018,"distance, questions, semantic, distributions, metric, similarity, deep, framework, generative, intent","sentences, semantic, similarity, siamese, model, words, pairs, variational, repeat, word","{'similarity': 0.5116337781142579, 'semantic': 0.49010240596990956, 'space': 0.49010240596990956, 'continuous': 0.46545523468098876, 'learning': 0.20293241467446216}","questions, distance, semantic, similarity, metric, intent, normal, distributions, pairs, measure"
Representation Balancing MDPs for Off-policy Policy Evaluation,"Yao Liu, Omer Gottesman, Aniruddh Raghu, Matthieu Komorowski, Aldo A. Faisal, Finale Doshi-Velez, Emma Brunskill","We study the problem of off-policy policy evaluation (OPPE) in RL. In contrast to prior work, we consider how to estimate both the individual policy value and average policy value accurately. We draw inspiration from recent work in causal reasoning, and propose a new finite sample generalization error bound for value estimates from MDP models. Using this upper bound as an objective, we develop a learning algorithm of an MDP model with a balanced representation, and show that our approach can yield substantially lower MSE in common synthetic benchmarks and a HIV treatment simulation domain.",https://proceedings.neurips.cc/paper_files/paper/2018/file/980ecd059122ce2e50136bda65c25e07-Paper.pdf,2018,"policy, value, bound, mdp, work, accurately, algorithm, approach, average, balanced","policy, model, s0, evaluation, bound, value, function, mse, distribution, data","{'policy': 0.6013412898703526, 'balancing': 0.4542276498002743, 'mdps': 0.42873495499616315, 'evaluation': 0.3851549053815585, 'representation': 0.31608216096284264}","policy, value, mdp, hiv, oppe, mse, bound, balanced, inspiration, draw"
See and Think: Disentangling Semantic Scene Completion,"Shice Liu, YU HU, Yiming Zeng, Qiankun Tang, Beibei Jin, Yinhe Han, Xiaowei Li","Semantic scene completion predicts volumetric occupancy and object category of a 3D scene, which helps intelligent agents to understand and interact with the surroundings. In this work, we propose a disentangled framework, sequentially carrying out 2D semantic segmentation, 2D-3D reprojection and 3D semantic scene completion. This three-stage framework has three advantages: (1) explicit semantic segmentation significantly boosts performance; (2) flexible fusion ways of sensor data bring good extensibility; (3) progress in any subtask will promote the holistic performance. Experimental results show that regardless of inputing a single depth or RGB-D, our framework can generate high-quality semantic scene completion, and outperforms state-of-the-art approaches on both synthetic and real datasets.",https://proceedings.neurips.cc/paper_files/paper/2018/file/9872ed9fc22fc182d371c3e9ed316094-Paper.pdf,2018,"semantic, scene, 3d, completion, framework, 2d, performance, segmentation, three, advantages","semantic, scene, completion, segmentation, depth, 3d, rgb, 2d, satnet, snet","{'see': 0.46492780955467755, 'think': 0.43883458788459034, 'disentangling': 0.4059610163512024, 'completion': 0.3942279308118397, 'scene': 0.3681347091417524, 'semantic': 0.3681347091417524}","semantic, scene, completion, 3d, 2d, segmentation, three, framework, extensibility, inputing"
L4: Practical loss-based stepsize adaptation for deep learning,"Michal Rolinek, Georg Martius","We propose a stepsize adaptation scheme for stochastic gradient descent.
It operates directly with the loss function and rescales the gradient in order to make fixed predicted progress on the loss.
We demonstrate its capabilities by conclusively improving the performance of Adam and Momentum optimizers.
The enhanced optimizers with default hyperparameters
 consistently outperform their constant stepsize counterparts, even the best ones,
 without a measurable increase in computational cost.
The performance is validated on multiple architectures including dense nets, CNNs, ResNets, and the recurrent Differential Neural Computer on classical datasets MNIST, fashion MNIST, CIFAR10 and others.",https://proceedings.neurips.cc/paper_files/paper/2018/file/98b17f068d5d9b7668e19fb8ae470841-Paper.pdf,2018,"gradient, loss, mnist, optimizers, performance, stepsize, adam, adaptation, architectures, best","loss, learning, 10, gradient, stepsize, adam, rate, see, also, l4adam","{'l4': 0.4692784235847466, 'stepsize': 0.4692784235847466, 'practical': 0.37923028586402807, 'adaptation': 0.35289289395083073, 'loss': 0.34769363976191286, 'based': 0.3135207361695168, 'deep': 0.21988295584495277, 'learning': 0.15385670147201802}","stepsize, optimizers, mnist, conclusively, rescales, adam, cifar10, measurable, default, loss"
Generalisation of structural knowledge in the hippocampal-entorhinal system,"James Whittington, Timothy Muller, Shirely Mark, Caswell Barry, Tim Behrens","A central problem to understanding intelligence is the concept of generalisation. This allows previously learnt structure to be exploited to solve tasks in novel situations differing in their particularities. We take inspiration from neuroscience, specifically the hippocampal-entorhinal system known to be important for generalisation. We propose that to generalise structural knowledge, the representations of the structure of the world, i.e. how entities in the world relate to each other, need to be separated from representations of the entities themselves. We show, under these principles, artificial neural networks embedded with hierarchy and fast Hebbian memory, can learn the statistics of memories and generalise structural knowledge. Spatial neuronal representations mirroring those found in the brain emerge, suggesting spatial cognition is an instance of more general organising principles. We further unify many entorhinal cell types as basis functions for constructing transition graphs, and show these representations effectively utilise memories. We experimentally support model assumptions, showing a preserved relationship between entorhinal grid and hippocampal place cells across environments.",https://proceedings.neurips.cc/paper_files/paper/2018/file/99064ba6631e279d4a74622df99657d6-Paper.pdf,2018,"representations, entorhinal, entities, generalisation, generalise, hippocampal, knowledge, memories, principles, show","cells, grid, place, cell, representations, model, across, sensory, environments, gt","{'entorhinal': 0.44701237556418105, 'hippocampal': 0.44701237556418105, 'generalisation': 0.4219246248098423, 'system': 0.4041245823750278, 'structural': 0.3903177968197205, 'knowledge': 0.32661111825842315}","entorhinal, generalise, hippocampal, representations, memories, principles, generalisation, entities, structural, spatial"
Pelee: A Real-Time Object Detection System on Mobile Devices,"Robert J. Wang, Xiang Li, Charles X. Ling","An increasing need of running Convolutional Neural Network (CNN) models on mobile devices with limited computing power and memory resource encourages studies on efficient model design. A number of efficient architectures have been proposed in recent years, for example, MobileNet, ShuffleNet, and MobileNetV2. However, all these models are heavily dependent on depthwise separable convolution which lacks efficient implementation in most deep learning frameworks. In this study, we propose an efficient architecture named PeleeNet, which is built with conventional convolution instead. On ImageNet ILSVRC 2012 dataset, our proposed PeleeNet achieves a higher accuracy and 1.8 times faster speed than MobileNet and MobileNetV2 on NVIDIA TX2. Meanwhile, PeleeNet is only 66% of the model size of MobileNet. We then propose a real-time object detection system by combining PeleeNet with Single Shot MultiBox Detector (SSD) method and optimizing the architecture for fast speed. Our proposed detection system, named Pelee, achieves 76.4% mAP (mean average precision) on PASCAL VOC2007 and 22.4 mAP on MS COCO dataset at the speed of 23.6 FPS on iPhone 8 and 125 FPS on NVIDIA TX2. The result on COCO outperforms YOLOv2 in consideration of a higher precision, 13.6 times lower computational cost and 11.3 times smaller model size. The code and models are open sourced.",https://proceedings.neurips.cc/paper_files/paper/2018/file/9908279ebbf1f9b250ba689db6a0222b-Paper.pdf,2018,"efficient, peleenet, mobilenet, model, models, proposed, speed, times, achieves, architecture","model, layer, speed, peleenet, conv, mobilenet, accuracy, feature, map, block","{'pelee': 0.4137812612810915, 'mobile': 0.39055854594410744, 'real': 0.39055854594410744, 'devices': 0.37408176719667663, 'system': 0.37408176719667663, 'object': 0.29121054847902295, 'detection': 0.2848401821745296, 'time': 0.27910797582302865}","peleenet, mobilenet, mobilenetv2, tx2, speed, fps, nvidia, times, coco, efficient"
Co-regularized Alignment for Unsupervised Domain Adaptation,"Abhishek Kumar, Prasanna Sattigeri, Kahini Wadhawan, Leonid Karlinsky, Rogerio Feris, Bill Freeman, Gregory Wornell","Deep neural networks, trained with large amount of labeled data, can fail to
generalize well when tested with examples from a target domain whose distribution differs from the training data distribution, referred as the source domain. It can be expensive or even infeasible to obtain required amount of labeled data in all possible domains. Unsupervised domain adaptation sets out to address this problem, aiming to learn a good predictive model for the target domain using labeled examples from the source domain but only unlabeled examples from the target domain. 
Domain alignment approaches this problem by matching the source and target feature distributions, and has been used as a key component in many state-of-the-art domain adaptation methods. However, matching the marginal feature distributions does not guarantee that the corresponding class conditional distributions will be aligned across the two domains. We propose co-regularized domain alignment for unsupervised domain adaptation, which constructs multiple diverse feature  spaces and aligns source and target distributions in each of them individually, while encouraging that alignments agree with each other with regard to the class predictions on the unlabeled target examples.
The proposed method is generic and can be used to improve any domain adaptation method which uses domain alignment. We instantiate it in the context of a recent state-of-the-art method and 
observe that it provides significant performance improvements on several domain adaptation benchmarks.",https://proceedings.neurips.cc/paper_files/paper/2018/file/99607461cdb9c26e2bd5f31b12dcf27a-Paper.pdf,2018,"domain, target, adaptation, distributions, examples, source, alignment, data, feature, labeled","domain, co, target, pt, ps, feature, alignment, adaptation, source, 35","{'co': 0.48683145674861233, 'alignment': 0.4373460140716917, 'regularized': 0.416807699692356, 'adaptation': 0.38786057139477104, 'unsupervised': 0.35891344309718615, 'domain': 0.3445870799873945}","domain, adaptation, target, source, alignment, examples, labeled, distributions, feature, unlabeled"
How To Make the Gradients Small Stochastically: Even Faster Convex and Nonconvex SGD,Zeyuan Allen-Zhu,"Stochastic gradient descent (SGD) gives an optimal convergence rate when minimizing convex stochastic objectives $f(x)$. However, in terms of making the gradients small, the original SGD does not give an optimal rate, even when $f(x)$ is convex.

If $f(x)$ is convex, to find a point with gradient norm $\varepsilon$, we design an algorithm SGD3 with a near-optimal rate $\tilde{O}(\varepsilon^{-2})$, improving the best known rate $O(\varepsilon^{-8/3})$. If $f(x)$ is nonconvex, to find its $\varepsilon$-approximate local minimum, we design an algorithm SGD5 with rate $\tilde{O}(\varepsilon^{-3.5})$, where previously SGD variants only achieve $\tilde{O}(\varepsilon^{-4})$. This is no slower than the best known stochastic version of Newton's method in all parameter regimes.",https://proceedings.neurips.cc/paper_files/paper/2018/file/996a7fa078cc36c46d02f9af3bef918b-Paper.pdf,2018,"varepsilon, rate, convex, optimal, sgd, stochastic, tilde, algorithm, best, design","convex, x0, xs, sgd, σs, gradient, rate, theorem, gf, stochastic","{'even': 0.3859931977354004, 'make': 0.3859931977354004, 'stochastically': 0.3859931977354004, 'small': 0.3489597791057657, 'gradients': 0.3190606662945104, 'sgd': 0.30563345676207665, 'faster': 0.30000426256952667, 'nonconvex': 0.2820272476648758, 'convex': 0.25787861838181075}","varepsilon, rate, tilde, sgd, convex, stochastic, optimal, design, sgd3, sgd5"
Entropy Rate Estimation for Markov Chains with Large State Space,"Yanjun Han, Jiantao Jiao, Chuan-Zheng Lee, Tsachy Weissman, Yihong Wu, Tiancheng Yu","Entropy estimation is one of the prototypical problems in distribution property testing. To consistently estimate the Shannon entropy of a distribution on $S$ elements with independent samples, the optimal sample complexity scales sublinearly with $S$ as $\Theta(\frac{S}{\log S})$ as shown by Valiant and Valiant \cite{Valiant--Valiant2011}. Extending the theory and algorithms for entropy estimation to dependent data, this paper considers the problem of estimating the entropy rate of a stationary reversible Markov chain with $S$ states from a sample path of $n$ observations. We show that
\begin{itemize}
	\item Provided the Markov chain mixes not too slowly, \textit{i.e.}, the relaxation time is at most $O(\frac{S}{\ln^3 S})$, consistent estimation is achievable when $n \gg \frac{S^2}{\log S}$.
	\item Provided the Markov chain has some slight dependency, \textit{i.e.}, the relaxation time is at least $1+\Omega(\frac{\ln^2 S}{\sqrt{S}})$, consistent estimation is impossible when $n \lesssim \frac{S^2}{\log S}$.
\end{itemize}
Under both assumptions, the optimal estimation accuracy is shown to be $\Theta(\frac{S^2}{n \log S})$. In comparison, the empirical entropy rate requires at least $\Omega(S^2)$ samples to be consistent, even when the Markov chain is memoryless. In addition to synthetic experiments, we also apply the estimators that achieve the optimal sample complexity to estimate the entropy rate of the English language in the Penn Treebank and the Google One Billion Words corpora, which provides a natural benchmark for language modeling and relates it directly to the widely used perplexity measure.",https://proceedings.neurips.cc/paper_files/paper/2018/file/99cad265a1768cc2dd013f0e740300ae-Paper.pdf,2018,"entropy, frac, estimation, chain, log, markov, consistent, optimal, rate, sample","entropy, rate, markov, ln, xn, s2, sample, chain, theorem, x0","{'chains': 0.4138016483586469, 'rate': 0.39634429586659997, 'state': 0.3717395394591372, 'markov': 0.3471347830516744, 'space': 0.3471347830516744, 'entropy': 0.33495752419209024, 'large': 0.3017917239979318, 'estimation': 0.29867910348261656}","frac, entropy, valiant, chain, estimation, markov, log, itemize, theta, consistent"
Data-dependent PAC-Bayes priors via differential privacy,"Gintare Karolina Dziugaite, Daniel M. Roy","The Probably Approximately Correct (PAC) Bayes framework (McAllester, 1999) can incorporate knowledge about the learning algorithm and (data) distribution through the use of distribution-dependent priors, yielding tighter generalization bounds on data-dependent posteriors. Using this flexibility, however, is difficult, especially when the data distribution is presumed to be unknown. We show how a differentially private data-dependent prior yields a valid PAC-Bayes bound, and then show how non-private mechanisms for choosing priors can also yield generalization bounds. As an application of this result, we show that a Gaussian prior mean chosen via stochastic gradient Langevin dynamics (SGLD; Welling and Teh, 2011) leads to a valid PAC-Bayes bound due to control of the 2-Wasserstein distance to a differentially private stationary distribution. We study our data-dependent bounds empirically, and show that they can be nonvacuous even when other distribution-dependent bounds are vacuous.",https://proceedings.neurips.cc/paper_files/paper/2018/file/9a0ee0a9e7a42d2d69b8f86b3a0756b1-Paper.pdf,2018,"data, dependent, distribution, bounds, show, bayes, pac, private, bound, differentially","bound, data, private, kl, bayes, pac, dependent, bounds, let, distribution","{'dependent': 0.39950595411274226, 'bayes': 0.380744656151904, 'priors': 0.380744656151904, 'privacy': 0.380744656151904, 'pac': 0.3730633897877936, 'differential': 0.3661922628846899, 'data': 0.2666699090206517, 'via': 0.24720357297199577}","dependent, pac, bayes, private, distribution, bounds, valid, differentially, priors, data"
"Unsupervised Depth Estimation, 3D Face Rotation and Replacement","Joel Ruben Antony Moniz, Christopher Beckham, Simon Rajotte, Sina Honari, Chris Pal","We present an unsupervised approach for learning to estimate three dimensional (3D) facial structure from a single image while also predicting 3D viewpoint transformations that match a desired pose and facial geometry.
We achieve this by inferring the depth of facial keypoints of an input image in an unsupervised manner, without using any form of ground-truth depth information. We show how it is possible to use these depths as intermediate computations within a new backpropable loss to predict the parameters of a 3D affine transformation matrix that maps inferred 3D keypoints of an input face to the corresponding 2D keypoints on a desired target facial geometry or pose.
Our resulting approach, called DepthNets, can therefore be used to infer plausible 3D transformations from one face pose to another, allowing faces to be frontalized, transformed into 3D models or even warped to another pose and facial geometry.
Lastly, we identify certain shortcomings with our formulation, and explore adversarial image translation techniques as a post-processing step to re-synthesize complete head shots for faces re-targeted to different poses or identities.",https://proceedings.neurips.cc/paper_files/paper/2018/file/9a1335ef5ffebb0de9d089c4182e4868-Paper.pdf,2018,"3d, facial, pose, geometry, image, keypoints, another, approach, depth, desired","face, depth, target, depthnet, 3d, keypoints, model, source, afﬁne, models","{'rotation': 0.4422444184448559, 'face': 0.41742426055003473, 'replacement': 0.41742426055003473, 'depth': 0.3998140782705331, '3d': 0.3325635802013892, 'unsupervised': 0.307743422306568, 'estimation': 0.3012938793441451}","facial, 3d, pose, keypoints, geometry, faces, transformations, face, desired, another"
Information Constraints on Auto-Encoding Variational Bayes,"Romain Lopez, Jeffrey Regier, Michael I. Jordan, Nir Yosef","Parameterizing the approximate posterior of a generative model with neural networks has become a common theme in recent machine learning research. While providing appealing flexibility, this approach makes it difficult to impose or assess structural constraints such as conditional independence. We propose a framework for learning representations that relies on Auto-Encoding Variational Bayes and whose search space is constrained via kernel-based measures of independence.  In particular, our method employs the $d$-variable Hilbert-Schmidt Independence Criterion (dHSIC) to enforce independence between the latent representations and arbitrary nuisance factors.
We show how to apply this method to a range of problems, including the problems of learning invariant representations and the learning of interpretable representations. We also present a full-fledged application to single-cell RNA sequencing (scRNA-seq). In this setting the biological signal in mixed in complex ways with sequencing errors and sampling effects.  We show that our method out-performs the state-of-the-art in this domain.",https://proceedings.neurips.cc/paper_files/paper/2018/file/9a96a2c73c0d477ff2a6da3bf538f4f4-Paper.pdf,2018,"independence, learning, representations, method, problems, sequencing, show, also, appealing, application","qφ, variational, model, posterior, independence, data, representations, hcv, learning, hsic","{'auto': 0.4833648653223237, 'encoding': 0.4471554827540592, 'bayes': 0.4138397279679842, 'constraints': 0.4138397279679842, 'information': 0.3563577202748975, 'variational': 0.31127626881303316}","independence, representations, sequencing, dhsic, fledged, schmidt, scrna, seq, method, parameterizing"
On Misinformation Containment in Online Social Networks,"Amo Tong, Ding-Zhu Du, Weili Wu","The widespread online misinformation could cause public panic and serious economic damages. The misinformation containment problem aims at limiting the spread of misinformation in online social networks by launching competing campaigns. Motivated by realistic scenarios, we present the first analysis of the misinformation containment problem for the case when an arbitrary number of cascades are allowed. This paper makes four contributions. First, we provide a formal model for multi-cascade diffusion and introduce an important concept called as cascade priority. Second, we show that the misinformation containment problem cannot be approximated within a factor of $\Omega(2^{\log^{1-\epsilon}n^4})$ in polynomial time unless $NP \subseteq DTIME(n^{\polylog{n}})$. Third, we introduce several types of cascade priority that are frequently seen in real social networks. Finally, we design novel algorithms for solving the misinformation containment problem. The effectiveness of the proposed algorithm is supported by encouraging experimental results.",https://proceedings.neurips.cc/paper_files/paper/2018/file/9b04d152845ec0a378394003c96da594-Paper.pdf,2018,"misinformation, containment, problem, cascade, first, introduce, networks, online, priority, social","cascade, cascades, problem, fv, priority, active, misinformation, set, nodes, seed","{'containment': 0.5323593267763231, 'misinformation': 0.5323593267763231, 'social': 0.5024816347212528, 'online': 0.3523889886260215, 'networks': 0.23774209103881264}","misinformation, containment, cascade, priority, social, problem, online, cascades, damages, dtime"
Autoconj: Recognizing and Exploiting Conjugacy Without a Domain-Specific Language,"Matthew D. Hoffman, Matthew J. Johnson, Dustin Tran","Deriving conditional and marginal distributions using conjugacy relationships can be time consuming and error prone. In this paper, we propose a strategy for automating such derivations. Unlike previous systems which focus on relationships between pairs of random variables, our system (which we call Autoconj) operates directly on Python functions that compute log-joint distribution functions. Autoconj provides support for conjugacy-exploiting algorithms in any Python-embedded PPL. This paves the way for accelerating development of novel inference algorithms and structure-exploiting modeling strategies. The package can be downloaded at https://github.com/google-research/autoconj.",https://proceedings.neurips.cc/paper_files/paper/2018/file/9b89bedda1fc8a2d88c448e361194f02-Paper.pdf,2018,"autoconj, algorithms, conjugacy, exploiting, functions, python, relationships, accelerating, automating, call","log, np, autoconj, joint, function, functions, python, conjugacy, inference, term","{'autoconj': 0.39976637623803, 'conjugacy': 0.39976637623803, 'recognizing': 0.39976637623803, 'specific': 0.37733022065203115, 'exploiting': 0.330445528866764, 'without': 0.3230566536204637, 'language': 0.30543521760676035, 'domain': 0.2670803562979772}","autoconj, conjugacy, python, exploiting, relationships, derivations, downloaded, package, ppl, paves"
Size-Noise Tradeoffs in Generative Networks,"Bolton Bailey, Matus J. Telgarsky","This paper investigates the ability of generative networks to convert their input noise distributions into other distributions. Firstly, we demonstrate a construction that allows ReLU networks to increase the dimensionality of their noise distribution by implementing a ``space-filling'' function based on iterated tent maps. We show this construction is optimal by analyzing the number of affine pieces in functions computed by multivariate ReLU networks. Secondly, we provide efficient ways (using polylog$(1/\epsilon)$ nodes) for networks to pass between univariate uniform and normal distributions, using a Taylor series approximation and a binary search gadget for computing function inverses. Lastly, we indicate how high dimensional distributions can be efficiently transformed into low dimensional distributions.",https://proceedings.neurips.cc/paper_files/paper/2018/file/9bd5ee6fe55aaeb673025dbcb8f939c1-Paper.pdf,2018,"distributions, networks, construction, dimensional, function, noise, relu, using, ability, affine","network, distribution, uniform, function, normal, distributions, networks, afﬁne, theorem, number","{'tradeoffs': 0.5716069587967624, 'size': 0.49911004929737035, 'noise': 0.4846847703516319, 'generative': 0.3522397790544505, 'networks': 0.2552693769067308}","distributions, construction, relu, networks, noise, filling, gadget, pieces, tent, dimensional"
Global Convergence of Langevin Dynamics Based Algorithms for Nonconvex Optimization,"Pan Xu, Jinghui Chen, Difan Zou, Quanquan Gu","We present a unified framework to analyze the global convergence of Langevin dynamics based algorithms for nonconvex finite-sum optimization with $n$ component functions.  At the core of our analysis is a direct analysis of the ergodicity of the numerical approximations to Langevin dynamics, which leads to faster convergence rates. Specifically, we show that gradient Langevin dynamics (GLD) and stochastic gradient Langevin dynamics (SGLD)  converge to the \textit{almost minimizer}\footnote{Following \citet{raginsky2017non}, an almost minimizer is defined to be a point which is within the ball of the global minimizer with radius $O(d\log(\beta+1)/\beta)$, where $d$ is the problem dimension and $\beta$ is the inverse temperature parameter.} within $\tilde O\big(nd/(\lambda\epsilon) \big)$\footnote{$\tilde O(\cdot)$ notation hides polynomials of logarithmic terms and constants.} and $\tilde O\big(d^7/(\lambda^5\epsilon^5) \big)$ stochastic gradient evaluations respectively, where $d$ is the problem dimension, and $\lambda$ is the spectral gap of the Markov chain generated by GLD. Both results improve upon the best known gradient complexity\footnote{Gradient complexity is defined as the total number of stochastic gradient evaluations of an algorithm, which is the number of stochastic gradients calculated per iteration times the total number of iterations.} results \citep{raginsky2017non}. 
Furthermore, for the first time we prove the global convergence guarantee for variance reduced stochastic gradient Langevin dynamics (VR-SGLD) to the almost minimizer within $\tilde O\big(\sqrt{n}d^5/(\lambda^4\epsilon^{5/2})\big)$ stochastic gradient evaluations, which outperforms the gradient complexities of GLD and SGLD in a wide regime.  
Our theoretical analyses shed some light on using Langevin dynamics based algorithms for nonconvex optimization with provable guarantees.",https://proceedings.neurips.cc/paper_files/paper/2018/file/9c19a2aa1d84e04b0bd4bc888792bd1e-Paper.pdf,2018,"gradient, big, dynamics, langevin, stochastic, lambda, minimizer, tilde, almost, beta","fn, langevin, gradient, optimization, dynamics, stochastic, gld, sgld, ld, svrg","{'langevin': 0.42519704340696196, 'global': 0.385578377228071, 'dynamics': 0.37205254248201597, 'convergence': 0.366187702558688, 'nonconvex': 0.35579746288500935, 'based': 0.325332246838669, 'algorithms': 0.3167158763512309, 'optimization': 0.2554963985228922}","langevin, big, lambda, gradient, minimizer, dynamics, tilde, gld, footnote, sgld"
Online convex optimization for cumulative constraints,"Jianjun Yuan, Andrew Lamperski","We propose the algorithms for online convex
  optimization which lead to cumulative squared constraint violations
  of the form
  $\sum\limits_{t=1}^T\big([g(x_t)]_+\big)^2=O(T^{1-\beta})$, where
  $\beta\in(0,1)$.  Previous literature has
  focused on long-term constraints of the form
  $\sum\limits_{t=1}^Tg(x_t)$. There, strictly feasible solutions
  can cancel out the effects of violated constraints.
  In contrast, the new form heavily penalizes large constraint
  violations and cancellation effects cannot occur. 
  Furthermore, useful bounds on the single step constraint violation
  $[g(x_t)]_+$ are derived.
  For convex objectives, our regret bounds generalize
  existing bounds, and for strongly convex objectives we give improved
  regret bounds.
  In numerical experiments, we show that our algorithm closely follows
  the constraint boundary leading to low cumulative violation.",https://proceedings.neurips.cc/paper_files/paper/2018/file/9cb9ed4f35cf7c2f295cc2bc6f732a84-Paper.pdf,2018,"bounds, constraint, convex, form, x_t, beta, big, constraints, cumulative, effects","xt, constraint, algorithm, gi, ft, ogd, violation, clipped, convex, 10","{'cumulative': 0.5962484172968684, 'constraints': 0.4818364671663446, 'convex': 0.39834825838112453, 'online': 0.3946796198977654, 'optimization': 0.31283878669646425}","x_t, constraint, limits_, bounds, violation, violations, cumulative, form, beta, big"
Semi-supervised Deep Kernel Learning: Regression with Unlabeled Data by Minimizing Predictive Variance,"Neal Jean, Sang Michael Xie, Stefano Ermon","Large amounts of labeled data are typically required to train deep learning models. For many real-world problems, however, acquiring additional data can be expensive or even impossible. We present semi-supervised deep kernel learning (SSDKL), a semi-supervised regression model based on minimizing predictive variance in the posterior regularization framework. SSDKL combines the hierarchical representation learning of neural networks with the probabilistic modeling capabilities of Gaussian processes. By leveraging unlabeled data, we show improvements  on a diverse set of real-world regression tasks over supervised deep kernel learning and semi-supervised methods such as VAT and mean teacher adapted for regression.",https://proceedings.neurips.cc/paper_files/paper/2018/file/9d28de8ff9bb6a3fa41fddfdc28f3bc1-Paper.pdf,2018,"learning, supervised, data, deep, regression, semi, kernel, real, ssdkl, world","data, supervised, ssdkl, labeled, learning, semi, unlabeled, kernel, training, deep","{'minimizing': 0.38579399668683617, 'unlabeled': 0.38579399668683617, 'predictive': 0.3378576490600474, 'semi': 0.33030303626660407, 'kernel': 0.3236393955261652, 'variance': 0.3236393955261652, 'supervised': 0.29473915179566945, 'regression': 0.2730711521454337, 'data': 0.23134108176509427, 'deep': 0.1915142609810022}","supervised, semi, ssdkl, regression, kernel, deep, vat, learning, world, acquiring"
Stimulus domain transfer in recurrent models for large scale cortical population prediction on video,"Fabian Sinz, Alexander S. Ecker, Paul Fahey, Edgar Walker, Erick Cobos, Emmanouil Froudarakis, Dimitri Yatsenko, Zachary Pitkow, Jacob Reimer, Andreas Tolias","To better understand the representations in visual cortex, we need to generate better predictions of neural activity in awake animals presented with their ecological input: natural video. Despite recent advances in models for static images, models for predicting responses to natural video are scarce and standard linear-nonlinear models perform poorly. We developed a new deep recurrent network architecture that predicts inferred spiking activity of thousands of mouse V1 neurons simultaneously recorded with two-photon microscopy, while accounting for confounding factors such as the animal's gaze position and brain state changes related to running state and pupil dilation. Powerful system identification models provide an opportunity to gain insight into cortical functions through in silico experiments that can subsequently be tested in the brain. However, in many cases this approach requires that the model is able to generalize to stimulus statistics that it was not trained on, such as band-limited noise and other parameterized stimuli. We investigated these domain transfer properties in our model and find that our model trained on natural images is able to correctly predict the orientation tuning of neurons in responses to artificial noise stimuli. Finally, we show that we can fully generalize from movies to noise and maintain high predictive performance on both stimulus domains by fine-tuning only the final layer's weights on a network otherwise trained on natural movies. The converse, however, is not true.",https://proceedings.neurips.cc/paper_files/paper/2018/file/9d684c589d67031a627ad33d59db65e5-Paper.pdf,2018,"models, natural, model, noise, trained, able, activity, better, brain, generalize","trained, movies, noise, network, natural, model, readout, neurons, neural, performance","{'cortical': 0.3753766576407931, 'stimulus': 0.3753766576407931, 'population': 0.3287347033693851, 'video': 0.32138408276738784, 'scale': 0.29906408655921835, 'transfer': 0.2946579048778877, 'large': 0.27376780423022956, 'prediction': 0.2709442169144765, 'domain': 0.2656975931994096, 'recurrent': 0.260907801018431}","natural, movies, stimulus, stimuli, noise, neurons, activity, trained, brain, responses"
Sigsoftmax: Reanalysis of the Softmax Bottleneck,"Sekitoshi Kanai, Yasuhiro Fujiwara, Yuki Yamanaka, Shuichi Adachi","Softmax is an output activation function for modeling categorical probability distributions in many applications of deep learning. However, a recent study revealed that softmax can be a bottleneck of representational capacity of neural networks in language modeling (the softmax bottleneck). In this paper, we propose an output activation function for breaking the softmax bottleneck without additional parameters. We re-analyze the softmax bottleneck from the perspective of the output set of log-softmax and identify the cause of the softmax bottleneck. On the basis of this analysis, we propose sigsoftmax, which is composed of a multiplication of an exponential function and sigmoid function. Sigsoftmax can break the softmax bottleneck. The experiments on language modeling demonstrate that sigsoftmax and mixture of sigsoftmax outperform softmax and mixture of softmax, respectively.",https://proceedings.neurips.cc/paper_files/paper/2018/file/9dcb88e0137649590b755372b040afad-Paper.pdf,2018,"softmax, bottleneck, function, sigsoftmax, modeling, output, activation, language, mixture, propose","softmax, log, sigsoftmax, function, output, bottleneck, exp, vector, pd, vectors","{'reanalysis': 0.5231853667685109, 'sigsoftmax': 0.5231853667685109, 'bottleneck': 0.49382254641428736, 'softmax': 0.45682976769416705}","softmax, bottleneck, sigsoftmax, modeling, output, function, activation, mixture, language, breaking"
Parsimonious Quantile Regression of Financial Asset Tail Dynamics via Sequential Learning,"Xing Yan, Weizhong Zhang, Lin Ma, Wei Liu, Qi Wu","We propose a parsimonious quantile regression framework to learn the dynamic tail behaviors of financial asset returns. Our model captures well both the time-varying characteristic and the asymmetrical heavy-tail property of financial time series. It combines the merits of a popular sequential neural network model, i.e., LSTM, with a novel parametric quantile function that we construct to represent the conditional distribution of asset returns. Our model also captures individually the serial dependences of higher moments, rather than just the volatility. Across a wide range of asset classes, the out-of-sample forecasts of conditional quantiles or VaR of our model outperform the GARCH family. Further, the proposed approach does not suffer from the issue of quantile crossing, nor does it expose to the ill-posedness comparing to the parametric probability density function approach.",https://proceedings.neurips.cc/paper_files/paper/2018/file/9e3cfc48eccf81a0d57663e129aef3cb-Paper.pdf,2018,"model, asset, quantile, approach, captures, conditional, financial, function, parametric, returns","rt, garch, quantile, distribution, htqf, conditional, model, parameters, lstm, time","{'asset': 0.3835174329553313, 'financial': 0.3835174329553313, 'quantile': 0.3835174329553313, 'parsimonious': 0.36199321954673463, 'tail': 0.36199321954673463, 'sequential': 0.2980800318019954, 'dynamics': 0.29302046783681995, 'regression': 0.25622458197736553, 'via': 0.20122339099300543, 'learning': 0.12573927166900126}","asset, quantile, financial, tail, returns, captures, parametric, conditional, model, asymmetrical"
Adaptive Learning with Unknown Information Flows,"Yonatan Gur, Ahmadreza Momeni","An agent facing sequential decisions that are characterized by partial feedback needs to strike a balance between maximizing immediate payoffs based on available information, and acquiring new information that may be essential for maximizing future payoffs. This trade-off is captured by the multi-armed bandit (MAB) framework that has been studied and applied when at each time epoch payoff observations are collected on the actions that are selected at that epoch. In this paper we introduce a new, generalized MAB formulation in which additional information on each arm may appear arbitrarily throughout the decision horizon, and study the impact of such information flows on the achievable performance and the design of efficient decision-making policies. By obtaining matching lower and upper bounds, we characterize the (regret) complexity of this family of MAB problems as a function of the information flows. We introduce an adaptive exploration policy that, without any prior knowledge of the information arrival process, attains the best performance (in terms of regret rate) that is achievable when the information arrival process is a priori known. Our policy uses dynamically customized virtual time indexes to endogenously control the exploration rate based on the realized information arrival process.",https://proceedings.neurips.cc/paper_files/paper/2018/file/9e740b84bb48a64dde25061566299467-Paper.pdf,2018,"information, arrival, mab, process, achievable, based, decision, epoch, exploration, flows","information, time, arm, exploration, policy, performance, rate, regret, arrival, ﬂows","{'flows': 0.5770504028561179, 'unknown': 0.5183943849748087, 'information': 0.42542679619130414, 'adaptive': 0.4208514794525704, 'learning': 0.20043993493105897}","information, arrival, mab, epoch, payoffs, flows, achievable, maximizing, process, exploration"
DVAE#: Discrete Variational Autoencoders with Relaxed Boltzmann Priors,"Arash Vahdat, Evgeny Andriyash, William Macready","Boltzmann machines are powerful distributions that have been shown to be an effective prior over binary latent variables in variational autoencoders (VAEs). However, previous methods for training discrete VAEs have used the evidence lower bound and not the tighter importance-weighted bound. We propose two approaches for relaxing Boltzmann machines to continuous distributions that permit training with importance-weighted bounds. These relaxations are based on generalized overlapping transformations and the Gaussian integral trick. Experiments on the MNIST and OMNIGLOT datasets show that these relaxations outperform previous discrete VAEs with Boltzmann priors. An implementation which reproduces these results is available.",https://proceedings.neurips.cc/paper_files/paper/2018/file/9f53d83ec0691550f7d2507d57f4f5a2-Paper.pdf,2018,"boltzmann, vaes, bound, discrete, distributions, importance, machines, previous, relaxations, training","dvae, smoothing, function, log, 10, 20, 83, power, gaussian, 84","{'dvae': 0.4386177320140731, 'relaxed': 0.4386177320140731, 'boltzmann': 0.41400111525192146, 'discrete': 0.37191873094215167, 'priors': 0.3544529633945336, 'autoencoders': 0.32983634663238187, 'variational': 0.2666075498766728}","boltzmann, vaes, relaxations, machines, weighted, importance, discrete, reproduces, previous, bound"
Reinforcement Learning for Solving the Vehicle Routing Problem,"MohammadReza Nazari, Afshin Oroojlooy, Lawrence Snyder, Martin Takac","We present an end-to-end framework for solving the Vehicle Routing Problem (VRP) using reinforcement learning. In this approach, we train a single policy model that finds near-optimal solutions for a broad range of problem instances of similar size, only by observing the reward signals and following feasibility rules. We consider a parameterized stochastic policy, and by applying a policy gradient algorithm to optimize its parameters, the trained model produces the solution as a sequence of consecutive actions in real time, without the need to re-train for every new problem instance. On capacitated VRP, our approach outperforms classical heuristics and Google's OR-Tools on medium-sized instances in solution quality with comparable computation time (after training). We demonstrate how our approach can handle problems with split delivery and explore the effect of such deliveries on the solution quality. Our proposed framework can be applied to other variants of the VRP such as the stochastic VRP, and has the potential to be applied more generally to combinatorial optimization problems",https://proceedings.neurips.cc/paper_files/paper/2018/file/9fb4651c05b2ed70fba5afe0b039a550-Paper.pdf,2018,"vrp, approach, policy, problem, solution, applied, end, framework, instances, model","vrp, 10, problem, rl, sequence, problems, time, customer, rnd, policy","{'routing': 0.5141326654973447, 'vehicle': 0.48527791141949184, 'problem': 0.4359503835142329, 'solving': 0.4359503835142329, 'reinforcement': 0.3027443679611586, 'learning': 0.16856252505321667}","vrp, solution, policy, instances, quality, applied, end, capacitated, deliveries, delivery"
GumBolt: Extending Gumbel trick to Boltzmann priors,"Amir H. Khoshaman, Mohammad Amin","Boltzmann machines (BMs) are appealing candidates for powerful priors in variational autoencoders (VAEs), as they are capable of capturing nontrivial and multi-modal distributions over discrete variables. However, non-differentiability of the discrete units prohibits using the reparameterization trick, essential for low-noise back propagation. The Gumbel trick resolves this problem in a consistent way by relaxing the variables and distributions, but it is incompatible with BM priors. Here, we propose the GumBolt, a model that extends the Gumbel trick to BM priors in VAEs. GumBolt is significantly simpler than the recently proposed methods with BM prior and outperforms them by a considerable margin. It achieves state-of-the-art performance on permutation invariant MNIST and OMNIGLOT datasets in the scope of models with only discrete latent variables.  Moreover, the performance can be further improved by allowing multi-sampled (importance-weighted) estimation of log-likelihood in training, which was not possible with previous models.",https://proceedings.neurips.cc/paper_files/paper/2018/file/a00e5eb0973d24649a4a920fc53d9564-Paper.pdf,2018,"bm, discrete, priors, trick, variables, distributions, gumbel, gumbolt, models, multi","discrete, al, et, variables, log, gumbolt, qφ, dvae, bm, 2016","{'extending': 0.42894937194262117, 'gumbel': 0.42894937194262117, 'gumbolt': 0.42894937194262117, 'boltzmann': 0.40487537417926045, 'trick': 0.40487537417926045, 'priors': 0.3466398299337515}","bm, trick, priors, gumbel, gumbolt, variables, vaes, discrete, bms, incompatible"
Adding One Neuron Can Eliminate All Bad Local Minima,"SHIYU LIANG, Ruoyu Sun, Jason D. Lee, R. Srikant","One of the main difficulties in analyzing neural networks is the non-convexity of the loss function which may have many bad local minima. In this paper, we study the landscape of neural networks for binary classification tasks. Under mild assumptions, we prove that after adding one special neuron with a skip connection to the output, or one special neuron per layer, every local minimum is a global minimum.",https://proceedings.neurips.cc/paper_files/paper/2018/file/a012869311d64a44b5a0d567cd20de04-Paper.pdf,2018,"one, local, minimum, networks, neural, neuron, special, adding, analyzing, assumptions","loss, minimum, neural, function, local, network, ln, neuron, xi, every","{'adding': 0.42127277936961766, 'bad': 0.42127277936961766, 'eliminate': 0.42127277936961766, 'neuron': 0.3808545251433879, 'minima': 0.3482226486528658, 'one': 0.3335681990094704, 'local': 0.30001801669092854}","neuron, minimum, special, one, local, bad, landscape, adding, difficulties, mild"
Norm matters: efficient and accurate normalization schemes in deep networks,"Elad Hoffer, Ron Banner, Itay Golan, Daniel Soudry","Over the past few years, Batch-Normalization has been commonly used in deep networks, allowing faster training and high performance for a wide variety of applications. However, the reasons behind its merits remained unanswered, with several shortcomings that hindered its use for certain tasks. In this work, we present a novel view on the purpose and function of normalization methods and weight-decay, as tools to decouple weights' norm from the underlying optimized objective. This property highlights the connection between practices such as normalization, weight decay and learning-rate adjustments. We suggest several alternatives to the widely used $L^2$ batch-norm, using normalization in $L^1$ and $L^\infty$ spaces that can substantially improve numerical stability in low-precision implementations as well as provide computational and memory benefits. We demonstrate that such methods enable the first batch-norm alternative to work for half-precision implementations. Finally, we suggest a modification to weight-normalization, which improves its performance on large-scale tasks.",https://proceedings.neurips.cc/paper_files/paper/2018/file/a0160709701140704575d499c997b6ca-Paper.pdf,2018,"normalization, batch, norm, weight, decay, implementations, methods, performance, precision, several","norm, normalization, bn, weight, batch, wt, precision, l1, l2, learning","{'matters': 0.43309218480091166, 'norm': 0.4148210083529451, 'schemes': 0.4148210083529451, 'accurate': 0.4006487828661756, 'normalization': 0.3890692315022002, 'efficient': 0.2610993225672628, 'deep': 0.2149938319961016, 'networks': 0.20491145250364243}","normalization, norm, weight, batch, decay, implementations, precision, suggest, unanswered, adjustments"
Local Differential Privacy for Evolving Data,"Matthew Joseph, Aaron Roth, Jonathan Ullman, Bo Waggoner","There are now several large scale deployments of differential privacy used to collect statistical information about users. However, these deployments periodically recollect the data and recompute the statistics using algorithms designed for a single use. As a result, these systems do not provide meaningful privacy guarantees over long time scales. Moreover, existing techniques to mitigate this effect do not apply in the ``local model'' of differential privacy that these systems use.In this paper, we introduce a new technique for local differential privacy that makes it possible to maintain up-to-date statistics over time, with privacy guarantees that degrade only in the number of changes in the underlying distribution rather than the number of collection periods. We use our technique for tracking a changing statistic in the setting where users are partitioned into an unknown collection of groups, and at every time period each user draws a single bit from a common (but changing) group-specific distribution. We also provide an application to frequency and heavy-hitter estimation.",https://proceedings.neurips.cc/paper_files/paper/2018/file/a01610228fe998f515a72dd730294d87-Paper.pdf,2018,"privacy, differential, time, use, changing, collection, deployments, distribution, guarantees, local","user, privacy, pt, number, local, users, data, global, epoch, estimate","{'evolving': 0.5693717422997228, 'privacy': 0.4601170600297805, 'differential': 0.4425309841694372, 'local': 0.4054897188948093, 'data': 0.3222615801810189}","privacy, deployments, differential, changing, users, collection, statistics, use, time, technique"
Dialog-based Interactive Image Retrieval,"Xiaoxiao Guo, Hui Wu, Yu Cheng, Steven Rennie, Gerald Tesauro, Rogerio Feris","Existing methods for interactive image retrieval have demonstrated the merit of integrating user feedback, improving retrieval results. However, most current systems rely on restricted forms of user feedback, such as binary relevance responses, or feedback based on a fixed set of relative attributes, which limits their impact. In this paper, we introduce a new approach to interactive image search that enables users to provide feedback via natural language, allowing for more natural and effective interaction. We formulate the task of dialog-based interactive image retrieval as a reinforcement learning problem, and reward the dialog system for improving the rank of the target image during each dialog turn. To mitigate the cumbersome and costly process of collecting human-machine conversations as the dialog system learns, we train our system with a user simulator, which is itself trained to describe the differences between target and candidate images. The efficacy of our approach is demonstrated in a footwear retrieval application. Experiments on both simulated and real-world data show that 1) our proposed learning framework achieves better accuracy than other supervised and reinforcement learning baselines and 2) user feedback based on natural language rather than pre-specified attributes leads to more effective retrieval results, and a more natural and expressive communication interface.",https://proceedings.neurips.cc/paper_files/paper/2018/file/a01a0380ca3c61428c26a231f0e49a09-Paper.pdf,2018,"feedback, retrieval, dialog, image, natural, user, based, interactive, learning, system","image, dialog, user, based, feedback, retrieval, relative, candidate, attribute, images","{'dialog': 0.509540652334454, 'interactive': 0.4779087510780559, 'retrieval': 0.4779087510780559, 'based': 0.37654666039191254, 'image': 0.37654666039191254}","dialog, retrieval, feedback, user, interactive, natural, image, system, attributes, improving"
Byzantine Stochastic Gradient Descent,"Dan Alistarh, Zeyuan Allen-Zhu, Jerry Li","This paper studies the problem of distributed stochastic optimization in an adversarial setting where, out of $m$ machines which allegedly compute stochastic gradients every iteration, an $\alpha$-fraction are Byzantine, and may behave adversarially. Our main result is a variant of stochastic gradient descent (SGD) which finds $\varepsilon$-approximate minimizers of convex functions in $T = \tilde{O}\big( \frac{1}{\varepsilon^2 m} + \frac{\alpha^2}{\varepsilon^2} \big)$ iterations. In contrast, traditional mini-batch SGD needs $T = O\big( \frac{1}{\varepsilon^2 m} \big)$ iterations, but cannot tolerate Byzantine failures.
Further, we provide a lower bound showing that, up to logarithmic factors, our algorithm is information-theoretically optimal both in terms of sample complexity and time complexity.",https://proceedings.neurips.cc/paper_files/paper/2018/file/a07c2f3b3b907aaf8436a26c6d77f0a2-Paper.pdf,2018,"big, varepsilon, frac, stochastic, alpha, byzantine, complexity, iterations, sgd, adversarial","xk, byzantine, stochastic, machines, machine, algorithm, setting, rk, convex, theorem","{'byzantine': 0.6919204331824323, 'descent': 0.45393250597956863, 'gradient': 0.3980826108870133, 'stochastic': 0.395880826852126}","varepsilon, big, frac, byzantine, alpha, iterations, sgd, stochastic, allegedly, tolerate"
Robust Hypothesis Testing Using Wasserstein Uncertainty Sets,"RUI GAO, Liyan Xie, Yao Xie, Huan Xu","We develop a novel computationally efficient and general framework for robust hypothesis testing. The new framework features a new way to construct uncertainty sets under the null and the alternative distributions, which are sets centered around the empirical distribution defined via Wasserstein metric, thus our approach is data-driven and free of distributional assumptions. We develop a convex safe approximation of the minimax formulation and show that such approximation renders a nearly-optimal detector among the family of all possible tests. By exploiting the structure of the least favorable distribution, we also develop a tractable reformulation of such approximation, with complexity independent of the dimension of observation space and can be nearly sample-size-independent in general. Real-data example using human activity data demonstrated the excellent performance of the new robust detector.",https://proceedings.neurips.cc/paper_files/paper/2018/file/a08e32d2f9a8b78894d964ec7fd4172e-Paper.pdf,2018,"approximation, data, develop, new, detector, distribution, framework, general, independent, nearly","p1, p2, detector, distributions, test, problem, robust, distribution, optimal, n1","{'hypothesis': 0.48243949836090316, 'sets': 0.4090766808784611, 'uncertainty': 0.38986592964989325, 'testing': 0.37496491334386334, 'wasserstein': 0.368600317391928, 'robust': 0.308637830269082, 'using': 0.2760267486809181}","detector, approximation, develop, nearly, independent, sets, robust, null, new, centered"
Life-Long Disentangled Representation Learning with Cross-Domain Latent Homologies,"Alessandro Achille, Tom Eccles, Loic Matthey, Chris Burgess, Nicholas Watters, Alexander Lerchner, Irina Higgins","Intelligent behaviour in the real-world requires the ability to acquire new knowledge from an ongoing sequence of experiences while preserving and reusing past knowledge. We propose a novel algorithm for unsupervised representation learning from piece-wise stationary visual data: Variational Autoencoder with Shared Embeddings (VASE). Based on the Minimum Description Length principle, VASE automatically detects shifts in the data distribution and allocates spare representational capacity to new knowledge, while simultaneously protecting previously learnt representations from catastrophic forgetting. Our approach encourages the learnt representations to be disentangled, which imparts a number of desirable properties: VASE can deal sensibly with ambiguous inputs, it can enhance its own representations through imagination-based exploration, and most importantly, it exhibits semantically meaningful sharing of latents between different datasets. Compared to baselines with entangled representations, our approach is able to reason beyond surface-level statistics and perform semantically meaningful cross-domain inference.",https://proceedings.neurips.cc/paper_files/paper/2018/file/a0afdf1ac166b8652ffe9dee6eac779e-Paper.pdf,2018,"representations, knowledge, vase, approach, based, data, learnt, meaningful, new, semantically","data, vase, learning, fashion, environment, mnist, moving, latent, generative, factors","{'homologies': 0.41806684173479114, 'life': 0.41806684173479114, 'disentangled': 0.36504342499097897, 'long': 0.3544929397017813, 'cross': 0.33784550904826544, 'latent': 0.309750405220339, 'representation': 0.2909190376687714, 'domain': 0.2793067343422967, 'learning': 0.13706657291590404}","vase, representations, learnt, meaningful, semantically, knowledge, imagination, imparts, protecting, sensibly"
Computationally and statistically efficient learning of causal Bayes nets using path queries,"Kevin Bello, Jean Honorio","Causal discovery from empirical data is a fundamental problem in many scientific domains. Observational data allows for identifiability only up to Markov equivalence class. In this paper we first propose a polynomial time algorithm for learning the exact correctly-oriented structure of the transitive reduction of any causal Bayesian network with high probability, by using interventional path queries. Each path query takes as input an origin node and a target node, and answers whether there is a directed path from the origin to the target. This is done by intervening on the origin node and observing samples from the target node. We theoretically  show the logarithmic sample complexity for the size of interventional data per path query, for continuous and discrete networks. We then show how to learn the transitive edges using also logarithmic sample complexity (albeit in time exponential in the maximum number of parents for discrete networks), which allows us to learn the full network. We further extend our work by reducing the number of interventional path queries for learning rooted trees. We also provide an analysis of imperfect interventions.",https://proceedings.neurips.cc/paper_files/paper/2018/file/a0b45d1bb84fe1bedbb8449764c4d5d5-Paper.pdf,2018,"path, node, data, interventional, origin, target, allows, also, causal, complexity","xi, path, query, xj, let, algorithm, transitive, variables, noisy, σ2","{'statistically': 0.403748201142377, 'computationally': 0.38108857292741205, 'queries': 0.38108857292741205, 'path': 0.35254081755772665, 'nets': 0.333736891809175, 'bayes': 0.32627442055975986, 'causal': 0.2841494917823707, 'using': 0.23100368776963134, 'efficient': 0.22974778054517958, 'learning': 0.13237209155815557}","path, interventional, origin, node, transitive, target, query, queries, logarithmic, causal"
Predictive Approximate Bayesian Computation via Saddle Points,"Yingxiang Yang, Bo Dai, Negar Kiyavash, Niao He","Approximate Bayesian computation (ABC) is an important methodology for Bayesian inference when the likelihood function is intractable. Sampling-based ABC algorithms such as rejection- and K2-ABC are inefficient when the parameters have high dimensions, while the regression-based algorithms such as K- and DR-ABC are hard to scale. In this paper, we introduce an optimization-based ABC framework that addresses these deficiencies. Leveraging a generative model for posterior and joint distribution matching, we show that ABC can be framed as saddle point problems, whose objectives can be accessed directly with samples. We present the predictive ABC algorithm (P-ABC), and provide a probabilistically approximately correct (PAC) bound that guarantees its learning consistency. Numerical experiment shows that P-ABC outperforms both K2- and DR-ABC significantly.",https://proceedings.neurips.cc/paper_files/paper/2018/file/a14185bf0c82b3369f86efb3cac5ad28-Paper.pdf,2018,"abc, based, algorithms, bayesian, dr, k2, accessed, addresses, algorithm, approximate","abc, based, et, al, algorithms, samples, yi, distribution, dr, function","{'saddle': 0.4463607652346869, 'points': 0.41865102394918435, 'predictive': 0.408116269240135, 'computation': 0.39899064962669273, 'approximate': 0.3909412826636818, 'bayesian': 0.28248708952188767, 'via': 0.25905002887495654}","abc, k2, dr, framed, probabilistically, bayesian, accessed, deficiencies, rejection, based"
Co-teaching: Robust training of deep neural networks with extremely noisy labels,"Bo Han, Quanming Yao, Xingrui Yu, Gang Niu, Miao Xu, Weihua Hu, Ivor Tsang, Masashi Sugiyama","Deep learning with noisy labels is practically challenging, as the capacity of deep models is so high that they can totally memorize these noisy labels sooner or later during training. Nonetheless, recent studies on the memorization effects of deep neural networks show that they would first memorize training data of clean labels and then those of noisy labels. Therefore in this paper, we propose a new deep learning paradigm called ''Co-teaching'' for combating with noisy labels. Namely, we train two deep neural networks simultaneously, and let them teach each other given every mini-batch: firstly, each network feeds forward all data and selects some data of possibly clean labels; secondly, two networks communicate with each other what data in this mini-batch should be used for training; finally, each network back propagates the data selected by its peer network and updates itself. Empirical results on noisy versions of MNIST, CIFAR-10 and CIFAR-100 demonstrate that Co-teaching is much superior to the state-of-the-art methods in the robustness of trained deep models.",https://proceedings.neurips.cc/paper_files/paper/2018/file/a19744e268754fb0148b017647355b7b-Paper.pdf,2018,"deep, labels, data, noisy, network, networks, training, batch, cifar, clean","co, noisy, teaching, instances, 50, labels, networks, symmetry, training, 10","{'extremely': 0.426440590615153, 'co': 0.40250739360828414, 'teaching': 0.38552652009377175, 'labels': 0.3615933230869029, 'noisy': 0.337660126080034, 'training': 0.2750405390270083, 'robust': 0.27281285855178394, 'deep': 0.19981105638834887, 'neural': 0.19098471750902146, 'networks': 0.19044069037090414}","labels, noisy, deep, memorize, clean, teaching, co, mini, data, cifar"
On the Global Convergence of Gradient Descent for Over-parameterized Models using Optimal Transport,"Lénaïc Chizat, Francis Bach","Many tasks in machine learning and signal processing can be solved by minimizing a convex function of a measure. This includes sparse spikes deconvolution or training a neural network with a single hidden layer. For these problems, we study a simple minimization method: the unknown measure is discretized into a mixture of particles and a continuous-time gradient descent is performed on their weights and positions. This is an idealization of the usual way to train neural networks with a large hidden layer. We show that, when initialized correctly and in the many-particle limit, this gradient flow, although non-convex, converges to global minimizers. The proof involves Wasserstein gradient flows, a by-product of optimal transport theory. Numerical experiments show that this asymptotic behavior is already at play for a reasonable number of particles, even in high dimension.",https://proceedings.neurips.cc/paper_files/paper/2018/file/a1afc58c6ca9540d057299ec3016d726-Paper.pdf,2018,"gradient, convex, hidden, layer, many, measure, neural, particles, show, already","gradient, particle, ﬂow, rd, convex, global, function, wasserstein, set, 10","{'parameterized': 0.4233978590891912, 'transport': 0.408932608273056, 'global': 0.3708294164751609, 'convergence': 0.35218046467345315, 'descent': 0.3072472915647302, 'optimal': 0.3046010112394749, 'gradient': 0.26944491174984575, 'using': 0.267954619311172, 'models': 0.24572327176870426}","particles, hidden, gradient, measure, layer, idealization, convex, discretized, spikes, particle"
Smoothed analysis of the low-rank approach for smooth semidefinite programs,"Thomas Pumir, Samy Jelassi, Nicolas Boumal","We consider semidefinite programs (SDPs) of size $n$ with equality constraints. In order to overcome scalability issues, Burer and Monteiro proposed a factorized approach based on optimizing over a matrix $Y$ of size $n\times k$ such that $X=YY^*$ is the SDP variable. The advantages of such formulation are twofold: the dimension of the optimization variable is reduced, and positive semidefiniteness is naturally enforced. However, optimization in $Y$ is non-convex. In prior work, it has been shown that, when the constraints on the factorized variable regularly define a smooth manifold, provided $k$ is large enough, for almost all cost matrices, all second-order stationary points (SOSPs) are optimal. Importantly, in practice, one can only compute points which approximately satisfy necessary optimality conditions, leading to the question: are such points also approximately optimal? To this end, and under similar assumptions, we use smoothed analysis to show that approximate SOSPs for a randomly perturbed objective function are approximate global optima, with $k$ scaling like the square root of the number of constraints (up to log factors). We particularize our results to an SDP relaxation of phase retrieval.",https://proceedings.neurips.cc/paper_files/paper/2018/file/a1d50185e7426cbb0acad1e6ca74b9aa-Paper.pdf,2018,"constraints, points, variable, approximate, approximately, factorized, optimal, optimization, order, sdp","sdp, matrix, al, et, optimality, approximate, rank, problem, order, perturbed","{'semidefinite': 0.41426356994202074, 'smoothed': 0.39678673002662346, 'smooth': 0.38323063982178335, 'programs': 0.36278977131270984, 'rank': 0.3251828079921742, 'low': 0.3164893551302982, 'analysis': 0.30541322294709433, 'approach': 0.30541322294709433}","sdp, sosps, factorized, variable, constraints, points, approximately, burer, monteiro, particularize"
Differentially Private Contextual Linear Bandits,"Roshan Shariff, Or Sheffet","We study the contextual linear bandit problem, a version of the standard stochastic multi-armed bandit (MAB) problem where a learner sequentially selects actions to maximize a reward which depends also on a user provided per-round context. Though the context is chosen arbitrarily or adversarially, the reward is assumed to be a stochastic function of a feature vector that encodes the context and selected action. Our goal is to devise private learners for the contextual linear bandit problem.We first show that using the standard definition of differential privacy results in linear regret. So instead, we adopt the notion of joint differential privacy, where we assume that the action chosen on day t is only revealed to user t and thus needn't be kept private that day, only on following days. We give a general scheme converting the classic linear-UCB algorithm into a joint differentially private algorithm using the tree-based algorithm. We then apply either Gaussian noise or Wishart noise to achieve joint-differentially private algorithms and bound the resulting algorithms' regrets. In addition, we give the first lower bound on the additional regret any private algorithms for the MAB problem must incur.",https://proceedings.neurips.cc/paper_files/paper/2018/file/a1d7311f2a312426d710e1c617fcbc8c-Paper.pdf,2018,"private, linear, problem, algorithm, algorithms, bandit, context, joint, action, bound","algorithm, regret, private, privacy, ht, log, noise, bounds, reward, problem","{'contextual': 0.4721497312830554, 'differentially': 0.4721497312830554, 'bandits': 0.45558703062893796, 'private': 0.44840539712105193, 'linear': 0.3814738783378174}","private, bandit, joint, day, mab, context, linear, differentially, contextual, privacy"
Learning to Reason with Third Order Tensor Products,"Imanol Schlag, Jürgen Schmidhuber","We combine Recurrent Neural Networks with Tensor Product Representations to
learn combinatorial representations of sequential data. This improves symbolic
interpretation and systematic generalisation. Our architecture is trained end-to-end
through gradient descent on a variety of simple natural language reasoning tasks,
significantly outperforming the latest state-of-the-art models in single-task and
all-tasks settings. We also augment a subset of the data such that training and test
data exhibit large systematic differences and show that our approach generalises
better than the previous state-of-the-art.",https://proceedings.neurips.cc/paper_files/paper/2018/file/a274315e1abede44d63005826249d1df-Paper.pdf,2018,"data, art, end, representations, state, systematic, tasks, also, approach, architecture","tasks, entity, tpr, representations, task, tensor, memory, order, data, vector","{'products': 0.4884825499457632, 'reason': 0.4884825499457632, 'third': 0.46106736161811107, 'tensor': 0.386785599289811, 'order': 0.36733421361681523, 'learning': 0.16015292859021255}","systematic, end, generalises, representations, latest, augment, data, generalisation, combinatorial, differences"
Diversity-Driven Exploration Strategy for Deep Reinforcement Learning,"Zhang-Wei Hong, Tzu-Yun Shann, Shih-Yang Su, Yi-Hsiang Chang, Tsu-Jui Fu, Chun-Yi Lee","Efficient exploration remains a challenging research problem in reinforcement learning, especially when an environment contains large state spaces, deceptive local optima, or sparse rewards.
To tackle this problem, we present a diversity-driven approach for exploration, which can be easily combined with both off- and on-policy reinforcement learning algorithms. We show that by simply adding a distance measure to the loss function, the proposed methodology significantly enhances an agent's exploratory behaviors, and thus preventing the policy from being trapped in local optima. We further propose an adaptive scaling method for stabilizing the learning process. We demonstrate the effectiveness of our method in huge 2D gridworlds and a variety of benchmark environments, including Atari 2600 and MuJoCo. Experimental results show that our method outperforms baseline approaches in most tasks in terms of mean scores and exploration efficiency.",https://proceedings.neurips.cc/paper_files/paper/2018/file/a2802cade04644083dcde1c8c483ed9a-Paper.pdf,2018,"exploration, learning, method, local, optima, policy, problem, reinforcement, show, 2600","policy, methods, exploration, dqn, rewards, reward, agent, distance, performance, ddpg","{'diversity': 0.49036244875436447, 'strategy': 0.4696751698352896, 'driven': 0.44051808781102714, 'exploration': 0.40378450873258576, 'reinforcement': 0.3059163957942785, 'deep': 0.24342369967529112, 'learning': 0.17032865211510267}","exploration, optima, gridworlds, stabilizing, policy, local, method, reinforcement, 2600, deceptive"
On Neuronal Capacity,"Pierre Baldi, Roman Vershynin","We define the capacity of a learning machine to be the logarithm of the number (or volume) of the functions it can implement. We review known results, and derive new results, estimating the capacity of several neuronal models:  linear and polynomial threshold gates, linear and polynomial threshold gates with constrained weights (binary weights, positive weights), and ReLU neurons. We also derive capacity estimates and bounds for fully recurrent networks and layered feedforward networks.",https://proceedings.neurips.cc/paper_files/paper/2018/file/a292f1c5874b2be8395ffd75f313937f-Paper.pdf,2018,"capacity, weights, derive, gates, linear, networks, polynomial, results, threshold, also","threshold, functions, capacity, polynomial, linear, function, number, degree, gates, weights","{'capacity': 0.7417959371586874, 'neuronal': 0.6706256687712339}","capacity, gates, weights, threshold, polynomial, derive, layered, logarithm, review, volume"
GroupReduce: Block-Wise Low-Rank Approximation for Neural Language Model Shrinking,"Patrick Chen, Si Si, Yang Li, Ciprian Chelba, Cho-Jui Hsieh","Model compression is essential for serving large deep neural nets on devices with limited resources or applications that require real-time responses. For advanced NLP problems, a neural language model usually consists of recurrent layers (e.g., using LSTM cells), an embedding matrix for representing input tokens, and a softmax layer for generating output tokens. For problems with a very large vocabulary size, the embedding and the softmax matrices can account for more than half of the model size. For instance, the bigLSTM model achieves state-of-the-art performance on the One-Billion-Word (OBW) dataset with around 800k vocabulary, and its word embedding and softmax matrices use more than 6GBytes space, and are responsible for over 90\% of the model parameters. In this paper, we propose GroupReduce, a novel compression method for neural language models, based on vocabulary-partition (block) based low-rank matrix approximation and the inherent frequency distribution of tokens (the power-law distribution of words). We start by grouping words into $c$ blocks based on their frequency, and then refine the clustering iteratively by constructing weighted low-rank approximation for each block, where the weights are based the frequencies of the words in the block. The experimental results show our method can significantly outperform traditional compression methods such as low-rank approximation and pruning. On the OBW dataset, our method achieved 6.6x compression rate for the embedding and softmax matrices, and when combined with quantization, our method can achieve 26x compression rate without losing prediction accuracy.",https://proceedings.neurips.cc/paper_files/paper/2018/file/a2b8a85a29b2d64ad6f47275bf1360c6-Paper.pdf,2018,"compression, model, based, embedding, method, softmax, approximation, block, low, matrices","embedding, rank, compression, approximation, model, low, matrix, word, matrices, softmax","{'groupreduce': 0.4017080001879558, 'shrinking': 0.4017080001879558, 'wise': 0.3507593752864828, 'block': 0.33205046862941073, 'language': 0.3069186848739099, 'rank': 0.29762995630589956, 'low': 0.2896731027089843, 'approximation': 0.2860845594761957, 'model': 0.2511319558050642, 'neural': 0.17990803555154006}","compression, softmax, tokens, vocabulary, embedding, words, block, obw, matrices, rank"
Deep Structured Prediction with Nonlinear Output Transformations,"Colin Graber, Ofer Meshi, Alexander Schwing","Deep structured models are widely used for tasks like semantic segmentation, where explicit correlations between variables provide important prior information which generally helps to reduce the data needs of deep nets. However, current deep structured models are restricted by oftentimes very local neighborhood structure, which cannot be increased for computational complexity reasons, and by the fact that the output configuration, or a representation thereof, cannot be transformed further. Very recent approaches which address those issues include graphical model inference inside deep nets so as to permit subsequent non-linear output space transformations. However, optimization of those formulations is challenging and not well understood. Here, we develop a novel model which generalizes existing approaches, such as structured prediction energy networks, and discuss a formulation which maintains applicability of existing inference techniques.",https://proceedings.neurips.cc/paper_files/paper/2018/file/a2d10d355cdebc879e4fc6ecc6f63dd7-Paper.pdf,2018,"deep, structured, approaches, cannot, existing, however, inference, model, models, nets","structured, deep, inference, model, output, models, nets, structure, variables, approach","{'transformations': 0.49703825531842843, 'nonlinear': 0.47606933927001915, 'output': 0.4465153120064526, 'structured': 0.37060457247567236, 'prediction': 0.358758697757541, 'deep': 0.24673767597235272}","structured, nets, deep, cannot, oftentimes, output, configuration, thereof, approaches, permit"
Training Neural Networks Using Features Replay,"Zhouyuan Huo, Bin Gu, Heng Huang","Training a neural network using backpropagation algorithm requires passing error gradients sequentially through the network.
The backward locking prevents us from updating network layers in parallel and fully leveraging the computing resources.  Recently, there are several works trying to decouple and parallelize the backpropagation algorithm. However, all of them suffer from severe accuracy loss or memory explosion when the neural network is deep.  To address these challenging issues, we propose a novel parallel-objective formulation for the objective function of the neural network. After that,  we introduce features replay algorithm and prove that it is guaranteed to converge to critical points for the non-convex problem under certain conditions. Finally, we apply our method to training deep convolutional neural networks, and the experimental results show that the proposed method achieves {faster} convergence, {lower} memory consumption, and {better} generalization error than compared methods.",https://proceedings.neurips.cc/paper_files/paper/2018/file/a36b598abb934e4528412e5a2127b931-Paper.pdf,2018,"network, neural, algorithm, backpropagation, deep, error, memory, method, objective, parallel","ht, lk, wt, layer, algorithm, 10, module, backward, fr, error","{'replay': 0.5889911635800895, 'features': 0.4559370353601021, 'training': 0.4024682519766727, 'using': 0.35702757971869903, 'neural': 0.27946893095118064, 'networks': 0.27867285320903934}","network, backpropagation, parallel, neural, memory, locking, objective, decouple, consumption, explosion"
Mallows Models for Top-k Lists,"Flavio Chierichetti, Anirban Dasgupta, Shahrzad Haddadan, Ravi Kumar, Silvio Lattanzi","The classic Mallows model is a widely-used tool to realize distributions on per- mutations. Motivated by common practical situations, in this paper, we generalize Mallows to model distributions on top-k lists by using a suitable distance measure between top-k lists. Unlike many earlier works, our model is both analytically tractable and computationally efficient. We demonstrate this by studying two basic problems in this model, namely, sampling and reconstruction, from both algorithmic and experimental points of view.",https://proceedings.neurips.cc/paper_files/paper/2018/file/a381c2c35c9157f6b67fd07d5a200ae1-Paper.pdf,2018,"model, distributions, lists, mallows, top, algorithmic, analytically, basic, classic, common","top, model, lists, list, elements, center, number, algorithm, mallows, time","{'lists': 0.5619936874456778, 'mallows': 0.5619936874456778, 'top': 0.5304528211813955, 'models': 0.29486606288808204}","mallows, lists, top, model, mutations, realize, distributions, earlier, studying, analytically"
Information-based Adaptive Stimulus Selection to Optimize Communication Efficiency in Brain-Computer Interfaces,"Boyla Mainsah, Dmitry Kalika, Leslie Collins, Siyuan Liu, Chandra Throckmorton","Stimulus-driven brain-computer interfaces (BCIs), such as the P300 speller, rely on using a sequence of sensory stimuli to elicit specific neural responses as control signals, while a user attends to relevant target stimuli that occur within the sequence. In current BCIs, the stimulus presentation schedule is typically generated in a pseudo-random fashion. Given the non-stationarity of brain electrical signals, a better strategy could be to adapt the stimulus presentation schedule in real-time by selecting the optimal stimuli that will maximize the signal-to-noise ratios of the elicited neural responses and provide the most information about the user's intent based on the uncertainties of the data being measured. However, the high-dimensional stimulus space limits the development of algorithms with tractable solutions for optimized stimulus selection to allow for real-time decision-making within the stringent time requirements of BCI processing. We derive a simple analytical solution of an information-based objective function for BCI stimulus selection by transforming the high-dimensional stimulus space into a one-dimensional space that parameterizes the objective function - the prior probability mass of the stimulus under consideration, irrespective of its contents. We demonstrate the utility of our adaptive stimulus selection algorithm in improving BCI performance with results from simulation and real-time human experiments.",https://proceedings.neurips.cc/paper_files/paper/2018/file/a3eb043e7bf775de87763e9f8121c953-Paper.pdf,2018,"stimulus, time, bci, dimensional, real, selection, space, stimuli, based, bcis","stimulus, character, bci, ﬂash, selection, adaptive, target, yt, data, time","{'efficiency': 0.3508119111796303, 'interfaces': 0.3508119111796303, 'computer': 0.33112323527167625, 'optimize': 0.33112323527167625, 'stimulus': 0.33112323527167625, 'brain': 0.3171538974032234, 'selection': 0.2777765455873152, 'communication': 0.27266042337408525, 'information': 0.24411853181090828, 'adaptive': 0.2414931221873751}","stimulus, bci, stimuli, bcis, presentation, selection, schedule, dimensional, time, brain"
Doubly Robust Bayesian Inference for Non-Stationary Streaming Data with $\beta$-Divergences,"Jeremias Knoblauch, Jack E. Jewson, Theodoros Damoulas","We present the very first robust Bayesian Online Changepoint Detection algorithm through General Bayesian Inference (GBI) with $\beta$-divergences. The resulting inference procedure is doubly robust for both the predictive and the changepoint (CP) posterior, with linear time and constant space complexity. We provide a construction for exponential models and demonstrate it on the Bayesian Linear Regression model. In so doing, we make two additional contributions: Firstly, we make GBI scalable using Structural Variational approximations that are exact as $\beta \to 0$. Secondly, we give a principled way of choosing the divergence parameter $\beta$ by minimizing expected predictive loss on-line. Reducing False Discovery Rates of \CPs from up to 99\% to 0\% on real world data, this offers the state of the art.",https://proceedings.neurips.cc/paper_files/paper/2018/file/a3f390d88e4c41f2747bfa2f1b5f87db-Paper.pdf,2018,"bayesian, beta, changepoint, gbi, inference, linear, make, predictive, robust, 99","rt, yt, mt, θm, inference, βp, bocpd, robust, model, y1","{'beta': 0.40313262536645733, 'doubly': 0.40313262536645733, 'divergences': 0.3805075452143222, 'stationary': 0.35200331525629863, 'streaming': 0.33322805896348756, 'robust': 0.25790172493443836, 'non': 0.25019289273490103, 'inference': 0.24670423721422566, 'bayesian': 0.23065148737855407, 'data': 0.22817106509077364}","beta, changepoint, gbi, bayesian, predictive, make, robust, cps, 99, cp"
Clebsch–Gordan Nets: a Fully Fourier Space Spherical Convolutional Neural Network,"Risi Kondor, Zhen Lin, Shubhendu Trivedi","Recent work by Cohen et al. has achieved state-of-the-art results for learning spherical images in a rotation invariant way by using ideas from group representation theory and noncommutative harmonic analysis. In this paper we propose a generalization of this work that generally exhibits improved performace, but from an implementation point of view is actually simpler. An unusual feature of the proposed architecture is that it uses the Clebsch--Gordan transform as its only source of nonlinearity, thus avoiding repeated forward and backward Fourier transforms. The underlying ideas of the paper generalize to constructing neural networks that are invariant to the action of other compact groups.",https://proceedings.neurips.cc/paper_files/paper/2018/file/a3fc981af450752046be179185ebc8b5-Paper.pdf,2018,"ideas, invariant, paper, work, achieved, action, actually, al, analysis, architecture","spherical, fourier, network, neural, layer, covariant, space, ℓ2, group, sphere","{'clebsch': 0.3821751495434443, 'gordan': 0.3821751495434443, 'spherical': 0.3821751495434443, 'fourier': 0.333703876052145, 'fully': 0.333703876052145, 'nets': 0.31590468062632476, 'space': 0.3026102862144141, 'convolutional': 0.25532781438206836, 'network': 0.23076630422771546, 'neural': 0.17116009728162387}","ideas, invariant, clebsch, cohen, gordan, harmonic, noncommutative, performace, unusual, repeated"
Visualizing the Loss Landscape of Neural Nets,"Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, Tom Goldstein","Neural network training relies on our ability to find ""good"" minimizers of highly non-convex loss functions. It is well known that certain network architecture designs (e.g., skip connections) produce loss functions that train easier, and well-chosen training parameters (batch size, learning rate, optimizer) produce minimizers that generalize better. However, the reasons for these differences, and their effect on the underlying loss landscape, is not well understood. In this paper, we explore the structure of neural loss functions, and the effect of loss landscapes on generalization, using a range of visualization methods. First, we introduce a simple ""filter normalization"" method that helps us visualize loss function curvature, and make meaningful side-by-side comparisons between loss functions. Then, using a variety of visualizations, we explore how network architecture affects the loss landscape, and how training parameters affect the shape of minimizers.",https://proceedings.neurips.cc/paper_files/paper/2018/file/a41b3bb3e6b050b6c9067c67f663b915-Paper.pdf,2018,"loss, functions, minimizers, network, training, well, architecture, effect, explore, landscape","loss, batch, non, network, networks, neural, plots, figure, resnet, sharpness","{'visualizing': 0.5485164478249764, 'landscape': 0.5177319669471925, 'nets': 0.45340183283878743, 'loss': 0.40640197935522654, 'neural': 0.24565733455576585}","loss, minimizers, functions, landscape, side, effect, explore, produce, well, training"
Non-monotone Submodular Maximization in Exponentially Fewer Iterations,"Eric Balkanski, Adam Breuer, Yaron Singer","In this paper we consider parallelization for applications whose objective can be
expressed as maximizing a non-monotone submodular function under a cardinality constraint. Our main result is an algorithm whose approximation is arbitrarily close
to 1/2e in O(log^2 n) adaptive rounds, where n is the size of the ground set. This is an exponential speedup in parallel running time over any previously studied algorithm for constrained non-monotone submodular maximization. Beyond its provable guarantees, the algorithm performs well in practice. Specifically, experiments on traffic monitoring and personalized data summarization applications show that the algorithm finds solutions whose values are competitive with state-of-the-art algorithms while running in exponentially fewer parallel iterations.",https://proceedings.neurips.cc/paper_files/paper/2018/file/a42a596fc71e17828440030074d15e74-Paper.pdf,2018,"algorithm, whose, applications, monotone, non, parallel, running, submodular, 2e, adaptive","set, sieve, algorithm, blits, monotone, non, fs, submodular, elements, random","{'fewer': 0.4345612512426387, 'iterations': 0.4345612512426387, 'exponentially': 0.41017229703324093, 'monotone': 0.3928680586374196, 'maximization': 0.3511748660322005, 'submodular': 0.32197129795539847, 'non': 0.2696981828748318}","whose, monotone, submodular, running, parallel, algorithm, 2e, monitoring, traffic, applications"
Representation Learning for Treatment Effect Estimation from Observational Data,"Liuyi Yao, Sheng Li, Yaliang Li, Mengdi Huai, Jing Gao, Aidong Zhang","Estimating individual treatment effect (ITE) is a challenging problem in causal inference, due to the missing counterfactuals and the selection bias. Existing ITE estimation methods mainly focus on balancing the distributions of control and treated groups, but ignore the local similarity information that is helpful. In this paper, we propose a local similarity preserved individual treatment effect (SITE) estimation method based on deep representation learning. SITE preserves local similarity and balances data distributions simultaneously, by focusing on several hard samples in each mini-batch. Experimental results on synthetic and three real-world datasets demonstrate the advantages of the proposed SITE method, compared with the state-of-the-art ITE estimation methods.",https://proceedings.neurips.cc/paper_files/paper/2018/file/a50abba8132a77191791390c3eb19fe7-Paper.pdf,2018,"estimation, ite, local, similarity, site, distributions, effect, individual, method, methods","treatment, site, similarity, pddm, control, units, mpdm, space, outcome, effect","{'observational': 0.4812399643110969, 'treatment': 0.4609375423087623, 'effect': 0.44518976036576335, 'representation': 0.35479115030985536, 'estimation': 0.3473556030300459, 'data': 0.2885751849132474, 'learning': 0.16715993378622201}","ite, site, similarity, estimation, treatment, local, effect, individual, counterfactuals, balances"
Memory Replay GANs: Learning to Generate New Categories without Forgetting,"Chenshen Wu, Luis Herranz, Xialei Liu, yaxing wang, Joost van de Weijer, Bogdan Raducanu","Previous works on sequential learning address the problem of forgetting in discriminative models. In this paper we consider the case of generative models. In particular, we investigate generative adversarial networks (GANs) in the task of learning new categories in a sequential fashion. We first show that sequential fine tuning renders the network unable to properly generate images from previous categories (i.e. forgetting). Addressing this problem, we propose Memory Replay GANs (MeRGANs), a conditional GAN framework that integrates a memory replay generator. We study two methods to prevent forgetting by leveraging these replays, namely joint training with replay and replay alignment. Qualitative and quantitative experimental results in MNIST, SVHN and LSUN datasets show that our memory replay approach can generate competitive images while significantly mitigating the forgetting of previous categories.",https://proceedings.neurips.cc/paper_files/paper/2018/file/a57e8915461b83adefb011530b711704-Paper.pdf,2018,"replay, forgetting, categories, memory, previous, sequential, gans, generate, generative, images","task, images, tasks, forgetting, replay, category, mergan, previous, training, generated","{'generate': 0.3913128890607531, 'categories': 0.3693511699577282, 'forgetting': 0.3693511699577282, 'replay': 0.3693511699577282, 'new': 0.35376908227664305, 'gans': 0.31622527549253304, 'without': 0.31622527549253304, 'memory': 0.30984564407059323, 'learning': 0.12829507458380013}","replay, forgetting, categories, sequential, memory, gans, previous, generate, lsun, mergans"
HOGWILD!-Gibbs can be PanAccurate,"Constantinos Daskalakis, Nishanth Dikkala, Siddhartha Jayanti","Asynchronous Gibbs sampling has been recently shown to be fast-mixing and an accurate method for estimating probabilities of events on a small number of variables of a graphical model satisfying Dobrushin's condition~\cite{DeSaOR16}. We investigate whether it can be used to accurately estimate expectations of functions of {\em all the variables} of the model. Under the same condition, we show that the synchronous (sequential) and asynchronous Gibbs samplers can be coupled so that the expected Hamming distance between their (multivariate) samples remains bounded by $O(\tau \log n),$ where $n$ is the number of variables in the graphical model, and $\tau$ is a measure of the asynchronicity. A similar bound holds for any constant power of the Hamming distance. Hence, the expectation of any function that is Lipschitz with respect to a power of the Hamming distance, can be estimated with a bias that grows logarithmically in $n$. Going beyond Lipschitz functions, we consider the bias arising from asynchronicity in estimating the expectation of polynomial functions of all variables in the model. Using recent concentration of measure results~\cite{DaskalakisDK17,GheissariLP17,GotzeSS18}, we show that the bias introduced by the asynchronicity is of smaller order than the standard deviation of the function value already present in the true model. We perform experiments on a multi-processor machine to empirically illustrate our theoretical findings.",https://proceedings.neurips.cc/paper_files/paper/2018/file/a5bfc9e07964f8dddeb95fc584cd965d-Paper.pdf,2018,"model, variables, asynchronicity, bias, distance, functions, hamming, asynchronous, cite, condition","model, gibbs, hogwild, functions, ising, models, graphical, dobrushin, function, hamming","{'gibbs': 0.5773502691896257, 'hogwild': 0.5773502691896257, 'panaccurate': 0.5773502691896257}","asynchronicity, hamming, variables, bias, tau, distance, asynchronous, cite, gibbs, expectation"
DeepExposure: Learning to Expose Photos with Asynchronously Reinforced Adversarial Learning,"Runsheng Yu, Wenyu Liu, Yasen Zhang, Zhi Qu, Deli Zhao, Bo Zhang","The accurate exposure is the key of capturing high-quality photos in computational photography, especially for mobile phones that are limited by sizes of camera modules. Inspired by luminosity masks usually applied by professional photographers, in this paper, we develop a novel algorithm for learning local exposures with deep reinforcement adversarial learning. To be specific, we segment an image into sub-images that can reflect variations of dynamic range exposures according to raw low-level features. Based on these sub-images, a local exposure for each sub-image is automatically learned by virtue of policy network sequentially while the reward of learning is globally designed for striking a balance of overall exposures. The aesthetic evaluation function is approximated by discriminator in generative adversarial networks. The reinforcement learning and the adversarial learning are trained collaboratively by asynchronous deterministic policy gradient and generative loss approximation. To further simply the algorithmic architecture, we also prove the feasibility of leveraging the discriminator as the value function. Further more, we employ each local exposure to retouch the raw input image respectively, thus delivering multiple retouched images under different exposures which are fused with exposure blending. The extensive experiments verify that our algorithms are superior to state-of-the-art methods in terms of quantitative accuracy and visual illustration.",https://proceedings.neurips.cc/paper_files/paper/2018/file/a5e0ff62be0b08456fc7f1e88812af3d-Paper.pdf,2018,"learning, exposure, exposures, adversarial, image, images, local, sub, discriminator, function","image, function, learning, exposure, value, images, method, st, policy, methods","{'asynchronously': 0.42310951703079724, 'deepexposure': 0.42310951703079724, 'expose': 0.42310951703079724, 'photos': 0.42310951703079724, 'reinforced': 0.38251504033453865, 'learning': 0.27743970905161836, 'adversarial': 0.24621697372659346}","exposures, exposure, sub, raw, discriminator, images, local, adversarial, image, learning"
Data Amplification: A Unified and Competitive Approach to Property Estimation,"Yi Hao, Alon Orlitsky, Ananda Theertha Suresh, Yihong Wu","Estimating properties of discrete distributions is a fundamental problem in statistical learning. We design the first unified, linear-time, competitive, property estimator that for a wide class of properties and for all underlying distributions uses just 2n samples to achieve the performance attained by the empirical estimator with n\sqrt{\log n} samples. This provides off-the-shelf, distribution-independent, ``amplification'' of the amount of data available relative to common-practice estimators.We illustrate the estimator's practical advantages by comparing it to existing estimators for a wide variety of properties and distributions. In most cases, its performance with n samples is even as good as that of the empirical estimator with n\log n samples, and for essentially all properties, its performance is comparable to that of the best existing estimator designed specifically for that property.",https://proceedings.neurips.cc/paper_files/paper/2018/file/a753a43564c29148df3150afb4475440-Paper.pdf,2018,"estimator, properties, samples, distributions, performance, empirical, estimators, existing, log, property","estimator, properties, xn, samples, log, distribution, fx, support, size, property","{'amplification': 0.4372222842745291, 'competitive': 0.4372222842745291, 'property': 0.4372222842745291, 'unified': 0.3927795146350317, 'approach': 0.32233939132823963, 'estimation': 0.31558395286177027, 'data': 0.2621800159787878}","estimator, properties, samples, distributions, estimators, property, wide, log, 2n, performance"
Insights on representational similarity in neural networks with canonical correlation,"Ari Morcos, Maithra Raghu, Samy Bengio","Comparing different neural network representations and determining how representations evolve over time remain challenging open questions in our understanding of the function of neural networks. Comparing representations in neural networks is fundamentally difficult as the structure of representations varies greatly, even across groups of networks trained on identical tasks, and over the course of training. Here, we develop projection weighted CCA (Canonical Correlation Analysis) as a tool for understanding neural networks, building off of SVCCA, a recently proposed method (Raghu et al, 2017). We first improve the core method, showing how to differentiate between signal and noise, and then apply this technique to compare across a group of CNNs, demonstrating that networks which generalize converge to more similar representations than networks which memorize, that wider networks converge to more similar solutions than narrow networks, and that trained networks with identical topology but different learning rates converge to distinct clusters with diverse representations. We also investigate the representational dynamics of RNNs, across both training and sequential timesteps, finding that RNNs converge in a bottom-up pattern over the course of training and that the hidden state is highly variable over the course of a sequence, even when accounting for linear transforms. Together, these results provide new insights into the function of CNNs and RNNs, and demonstrate the utility of using CCA to understand representations.",https://proceedings.neurips.cc/paper_files/paper/2018/file/a7a3d70c6d17a73140918996d03c014f-Paper.pdf,2018,"networks, representations, converge, neural, across, course, rnns, training, cca, cnns","networks, cca, solutions, training, l1, similar, representations, converge, distance, figure","{'canonical': 0.4483729074279882, 'insights': 0.4483729074279882, 'representational': 0.4483729074279882, 'correlation': 0.4232087993149757, 'similarity': 0.37062352246540775, 'neural': 0.20080727526501582, 'networks': 0.20023526820235438}","representations, networks, converge, course, rnns, cca, identical, across, comparing, cnns"
Efficient nonmyopic batch active search,"Shali Jiang, Gustavo Malkomes, Matthew Abbott, Benjamin Moseley, Roman Garnett","Active search is a learning paradigm for actively identifying as many members of a given class as possible. A critical target scenario is high-throughput screening for scientific discovery, such as drug or materials discovery. In these settings, specialized instruments can often evaluate \emph{multiple} points simultaneously; however, all existing work on active search focuses on sequential acquisition. We bridge this gap, addressing batch active search from both the theoretical and practical perspective. We first derive the Bayesian optimal policy for this problem, then prove a lower bound on the performance gap between sequential and batch optimal policies: the ``cost of parallelization.''  We also propose novel, efficient batch policies inspired by state-of-the-art sequential policies, and develop an aggressive pruning technique that can dramatically speed up computation. We conduct thorough experiments on data from three application domains: a citation network, material science, and drug discovery, testing all proposed policies (14 total) with a wide range of batch sizes. Our results demonstrate that the empirical performance gap matches our theoretical bound, that nonmyopic policies usually significantly outperform myopic alternatives, and that diversity is an important consideration for batch policy design.",https://proceedings.neurips.cc/paper_files/paper/2018/file/a7aeed74714116f3b292a982238f83d2-Paper.pdf,2018,"batch, policies, active, discovery, gap, search, sequential, bound, drug, optimal","batch, policy, ens, policies, search, points, active, sequential, di, optimal","{'nonmyopic': 0.5590590263248987, 'batch': 0.46211629242093066, 'active': 0.44266879248698465, 'search': 0.42040705000258577, 'efficient': 0.31812543097028156}","batch, policies, discovery, active, gap, drug, sequential, search, citation, nonmyopic"
Learning Safe Policies with Expert Guidance,"Jessie Huang, Fa Wu, Doina Precup, Yang Cai","We propose a framework for ensuring safe behavior of a reinforcement learning agent when the reward function may be difficult to specify. In order to do this, we rely on the existence of demonstrations from expert policies, and we provide a theoretical framework for the agent to optimize in the space of rewards consistent with its existing knowledge. We propose two methods to solve the resulting optimization: an exact ellipsoid-based method and a method in the spirit of the ""follow-the-perturbed-leader"" algorithm. Our experiments demonstrate the behavior of our algorithm in both discrete and continuous problems. The trained agent safely avoids states with potential negative effects while imitating the behavior of the expert in the other states.",https://proceedings.neurips.cc/paper_files/paper/2018/file/a89b71bb5227c75d463dd82a03115738-Paper.pdf,2018,"agent, behavior, algorithm, expert, framework, method, propose, states, avoids, based","policy, expert, maxmin, reward, algorithm, agent, mdp, learning, time, separation","{'guidance': 0.5433030473531758, 'expert': 0.5128111590271172, 'safe': 0.47439592288845844, 'policies': 0.4301930433130334, 'learning': 0.1781262691067655}","behavior, agent, expert, states, ellipsoid, leader, spirit, imitating, safely, safe"
Fast Similarity Search via Optimal Sparse Lifting,"Wenye Li, Jingwei Mao, Yin Zhang, Shuguang Cui","Similarity search is a fundamental problem in computing science with various applications and has attracted significant research attention, especially in large-scale search with high dimensions. Motivated by the evidence in biological science, our work develops a novel approach for similarity search. Fundamentally different from existing methods that typically reduce the dimension of the data to lessen the computational complexity and speed up the search, our approach projects the data into an even higher-dimensional space while ensuring the sparsity of the data in the output space, with the objective of further improving precision and speed. Specifically, our approach has two key steps. Firstly, it computes the optimal sparse lifting for given input samples and increases the dimension of the data while approximately preserving their pairwise similarity. Secondly, it seeks the optimal lifting operator that best maps input samples to the optimal sparse lifting. Computationally, both steps are modeled as optimization problems that can be efficiently and effectively solved by the Frank-Wolfe algorithm. Simple as it is, our approach has reported significantly improved results in empirical evaluations, and exhibited its high potentials in solving practical problems.",https://proceedings.neurips.cc/paper_files/paper/2018/file/a8baa56554f96369ab93e4f3bb068c22-Paper.pdf,2018,"approach, data, search, lifting, optimal, similarity, dimension, high, input, problems","lifting, optimal, algorithm, sparse, input, data, samples, results, space, vectors","{'lifting': 0.510813156199092, 'similarity': 0.4222364200329968, 'search': 0.38412661631075795, 'sparse': 0.34455892373929553, 'fast': 0.3351175727161252, 'optimal': 0.33223124934185017, 'via': 0.26801273324697295}","lifting, search, similarity, science, approach, steps, speed, optimal, dimension, data"
Differentially Private Robust Low-Rank Approximation,"Raman Arora, Vladimir braverman, Jalaj Upadhyay","In this paper, we study the following robust low-rank matrix approximation problem: given a matrix $A \in \R^{n \times d}$, find a rank-$k$ matrix $B$, while satisfying differential privacy, such that 
$ \norm{  A - B }_p \leq \alpha \mathsf{OPT}_k(A) + \tau,$ where 
$\norm{  M }_p$ is the entry-wise $\ell_p$-norm 
and $\mathsf{OPT}_k(A):=\min_{\mathsf{rank}(X) \leq k} \norm{  A - X}_p$. 
It is well known that low-rank approximation w.r.t. entrywise $\ell_p$-norm, for $p \in [1,2)$, yields robustness to gross outliers in the data.  We propose an algorithm that guarantees $\alpha=\widetilde{O}(k^2), \tau=\widetilde{O}(k^2(n+kd)/\varepsilon)$, runs in $\widetilde O((n+d)\poly~k)$ time and uses $O(k(n+d)\log k)$ space. We study extensions to the streaming setting where entries of the matrix arrive in an arbitrary order and output is produced at the very end or continually. We also study the related problem of differentially private robust principal component analysis (PCA), wherein we return a rank-$k$ projection matrix $\Pi$ such that $\norm{  A - A \Pi }_p \leq \alpha \mathsf{OPT}_k(A) + \tau.$",https://proceedings.neurips.cc/paper_files/paper/2018/file/a9813e9550fee3110373c21fa012eee7-Paper.pdf,2018,"norm, matrix, rank, _p, mathsf, _k, alpha, leq, opt, study","matrix, rank, robust, algorithm, private, log, approximation, low, privacy, differentially","{'differentially': 0.4441752628005386, 'private': 0.42183776016600055, 'rank': 0.41562272501175795, 'low': 0.4045114470492984, 'approximation': 0.3995002575314625, 'robust': 0.35887187672814774}","_p, mathsf, norm, rank, _k, opt, tau, matrix, widetilde, leq"
Evidential Deep Learning to Quantify Classification Uncertainty,"Murat Sensoy, Lance Kaplan, Melih Kandemir","Deterministic neural nets have been shown to learn effective predictors on a wide range of machine learning problems. However, as the standard approach is to train the network to minimize a prediction loss, the resultant model remains ignorant to its prediction confidence. Orthogonally to Bayesian neural nets that indirectly infer prediction uncertainty through weight uncertainties, we propose explicit modeling of the same using the theory of subjective logic. By placing a Dirichlet distribution on the class probabilities, we treat predictions of a neural net as subjective opinions and learn the function that collects the evidence leading to these opinions by a deterministic neural net from data. The resultant predictor for a multi-class classification problem is another Dirichlet distribution whose parameters are set by the continuous output of a neural net. We provide a preliminary analysis on how the peculiarities of our new loss function drive improved uncertainty estimation. We observe that our method achieves unprecedented success on detection of out-of-distribution queries and endurance against adversarial perturbations.",https://proceedings.neurips.cc/paper_files/paper/2018/file/a981f2b708044d6fb4a71a1463242520-Paper.pdf,2018,"neural, distribution, net, prediction, class, deterministic, dirichlet, function, learn, loss","uncertainty, distribution, evidence, class, neural, dirichlet, loss, predictions, classiﬁcation, model","{'evidential': 0.5354972544817501, 'quantify': 0.5354972544817501, 'uncertainty': 0.4327426250396164, 'classification': 0.38136531031854953, 'deep': 0.25091014895348224, 'learning': 0.1755670772001841}","net, opinions, subjective, resultant, neural, dirichlet, nets, prediction, deterministic, distribution"
A Game-Theoretic Approach to Recommendation Systems with Strategic Content Providers,"Omer Ben-Porat, Moshe Tennenholtz","We introduce a game-theoretic approach to the study of recommendation systems with strategic content providers. Such systems should be fair and stable. Showing that traditional approaches fail to satisfy these requirements, we propose the Shapley mediator. We show that the Shapley mediator satisfies the fairness and stability requirements, runs in linear time, and is the only economically efficient mechanism satisfying these properties.",https://proceedings.neurips.cc/paper_files/paper/2018/file/a9a1d5317a33ae8cef33961c34144f84-Paper.pdf,2018,"mediator, requirements, shapley, systems, approach, approaches, content, economically, efficient, fail","mediator, shapley, user, ui, content, game, σi, fairness, player, item","{'providers': 0.3977350104208442, 'strategic': 0.3977350104208442, 'content': 0.37541286152033837, 'game': 0.3595750448282009, 'recommendation': 0.34729027980399396, 'theoretic': 0.34729027980399396, 'systems': 0.30913031421135073, 'approach': 0.2767707814345459}","mediator, shapley, requirements, systems, economically, providers, strategic, fair, satisfying, recommendation"
Frequency-Domain Dynamic Pruning for Convolutional Neural Networks,"Zhenhua Liu, Jizheng Xu, Xiulian Peng, Ruiqin Xiong","Deep convolutional neural networks have demonstrated their powerfulness in a variety of applications. However, the storage and computational requirements have largely restricted their further extensions on mobile devices. Recently, pruning of unimportant parameters has been used for both network compression and acceleration. Considering that there are spatial redundancy within most filters in a CNN, we propose a frequency-domain dynamic pruning scheme to exploit the spatial correlations. The frequency-domain coefficients are pruned dynamically in each iteration and different frequency bands are pruned discriminatively, given their different importance on accuracy. Experimental results demonstrate that the proposed scheme can outperform previous spatial-domain counterparts by a large margin. Specifically, it can achieve a compression ratio of 8.4x and a theoretical inference speed-up of 9.2x for ResNet-110, while the accuracy is even better than the reference model on CIFAR-110.",https://proceedings.neurips.cc/paper_files/paper/2018/file/a9a6653e48976138166de32772b1bf40-Paper.pdf,2018,"domain, frequency, spatial, 110, accuracy, compression, different, pruned, pruning, scheme","domain, frequency, pruning, fdnp, layer, compression, coefﬁcients, layers, spatial, convolutional","{'frequency': 0.4937790311040746, 'pruning': 0.47294761432901594, 'dynamic': 0.4227559646316883, 'convolutional': 0.3495046840718411, 'domain': 0.3495046840718411, 'neural': 0.2342919664702256, 'networks': 0.23362457700752343}","frequency, spatial, pruned, 110, pruning, domain, compression, scheme, powerfulness, unimportant"
Adaptive Path-Integral Autoencoders: Representation Learning and Planning for Dynamical Systems,"Jung-Su Ha, Young-Jin Park, Hyeok-Joo Chae, Soon-Seo Park, Han-Lim Choi","We present a representation learning algorithm that learns a low-dimensional latent dynamical system from high-dimensional sequential raw data, e.g., video. The framework builds upon recent advances in amortized inference methods that use both an inference network and a refinement procedure to output samples from a variational distribution given an observation sequence, and takes advantage of the duality between control and inference to approximately solve the intractable inference problem using the path integral control approach. The learned dynamical model can be used to predict and plan the future states; we also present the efficient planning method that exploits the learned low-dimensional latent dynamics. Numerical experiments show that the proposed path-integral control based variational inference method leads to tighter lower bounds in statistical model learning of sequential data. Supplementary video: https://youtu.be/xCp35crUoLQ",https://proceedings.neurips.cc/paper_files/paper/2018/file/aa0d2a804a3510442f2fd40f2100b054-Paper.pdf,2018,"inference, control, dimensional, data, dynamical, integral, latent, learned, learning, low","inference, al, distribution, et, variational, model, 10, control, pθ, data","{'integral': 0.40984050802025185, 'path': 0.37913891423150736, 'planning': 0.37913891423150736, 'dynamical': 0.350890800133479, 'systems': 0.3374794473150364, 'autoencoders': 0.3265215741873098, 'representation': 0.3021523482411407, 'adaptive': 0.298902805173022, 'learning': 0.1423591498300615}","inference, control, integral, dimensional, path, dynamical, video, sequential, xcp35cruolq, youtu"
Testing for Families of Distributions via the Fourier Transform,"Clément L. Canonne, Ilias Diakonikolas, Alistair Stewart","We study the general problem of testing whether an unknown discrete distribution belongs to a specified family of distributions. More specifically, given a distribution family P and sample access to an unknown discrete distribution D , we want to distinguish (with high probability) between the case that D in P and the case that D is ε-far, in total variation distance, from every distribution in P . This is the prototypical hypothesis testing problem that has received significant attention in statistics and, more recently, in computer science. The main contribution of this work is a simple and general testing technique that is applicable to all distribution families whose Fourier spectrum satisfies a certain approximate sparsity property. We apply our Fourier-based framework to obtain near sample-optimal and  computationally efficient testers for the following fundamental distribution families: Sums of Independent Integer Random Variables (SIIRVs), Poisson Multinomial Distributions (PMDs), and Discrete Log-Concave Distributions. For the first two, ours are the first non-trivial testers in the literature, vastly generalizing previous work on testing Poisson Binomial Distributions. For the third, our tester improves on prior work in both sample and time complexity.",https://proceedings.neurips.cc/paper_files/paper/2018/file/aa8fdbb7d8159b3048daca36fe5c06d2-Paper.pdf,2018,"distribution, distributions, testing, discrete, sample, work, case, families, family, first","fourier, testing, distribution, algorithm, distributions, sample, ϵ2, theorem, probability, samples","{'families': 0.4575877620436667, 'transform': 0.4575877620436667, 'fourier': 0.4419544242592135, 'distributions': 0.40077435240191456, 'testing': 0.3933928416178381, 'via': 0.26556573113659265}","testing, distribution, testers, distributions, families, poisson, discrete, fourier, sample, family"
A Unified Framework for Extensive-Form Game Abstraction with Bounds,"Christian Kroer, Tuomas Sandholm","Abstraction has long been a key component in the practical solving of large-scale extensive-form games. Despite this, abstraction remains poorly understood. There have been some recent theoretical results but they have been confined to specific assumptions on abstraction structure and are specific to various disjoint types of abstraction, and specific solution concepts, for example, exact Nash equilibria or strategies with bounded immediate regret. In this paper we present a unified framework for analyzing abstractions that can express all types of abstractions and solution concepts used in prior papers with performance guarantees---while maintaining comparable bounds on abstraction quality. Moreover, our framework gives an exact decomposition of abstraction error in a much broader class of games, albeit only in an ex-post sense, as our results depend on the specific strategy chosen. Nonetheless, we use this ex-post decomposition along with slightly weaker assumptions than prior work to derive generalizations of prior bounds on abstraction quality. We also show, via counterexample, that such assumptions are necessary for some games. Finally, we prove the first bounds for how $\epsilon$-Nash equilibria computed in abstractions perform in the original game. This is important because often one cannot afford to compute an exact Nash equilibrium in the abstraction. All our results apply to general-sum n-player games.",https://proceedings.neurips.cc/paper_files/paper/2018/file/aa942ab2bfa6ebda4840e7360ce6e7ef-Paper.pdf,2018,"abstraction, games, specific, abstractions, assumptions, bounds, exact, nash, prior, results","σ0, information, game, set, abstraction, player, strategy, abstract, i0, equilibrium","{'abstraction': 0.41341393179226416, 'extensive': 0.41341393179226416, 'form': 0.41341393179226416, 'game': 0.39597293618229634, 'unified': 0.3713912335053076, 'framework': 0.3293685352183511, 'bounds': 0.2899260534944706}","abstraction, abstractions, games, nash, specific, ex, exact, equilibria, post, assumptions"
Model-Agnostic Private Learning,"Raef Bassily, Om Thakkar, Abhradeep Guha Thakurta","We design differentially private learning algorithms that are agnostic to the learning model assuming access to limited amount of unlabeled public data. First, we give a new differentially private algorithm for answering a sequence of $m$ online classification queries (given by a sequence of $m$ unlabeled public feature vectors) based on a private training set. Our private algorithm follows the paradigm of subsample-and-aggregate, in which any generic non-private learner is trained on disjoint subsets of the private training set, then for each classification query, the votes of the resulting classifiers ensemble are aggregated in a differentially private fashion. Our private aggregation is based on a novel combination of distance-to-instability framework [Smith & Thakurta 2013] and the sparse-vector technique [Dwork et al. 2009, Hardt & Talwar 2010].  We show that our algorithm makes a conservative use of the privacy budget. In particular, if the underlying non-private learner yields classification error at most $\alpha\in (0, 1)$, then our construction answers more queries, by at least a factor of $1/\alpha$ in some cases, than what is implied by a straightforward application of the advanced composition theorem for differential privacy. Next, we apply the knowledge transfer technique to construct a private learner that outputs a classifier, which can be used to answer unlimited number of queries. In the PAC model, we analyze our construction and prove upper bounds on the sample complexity for both the realizable and the non-realizable cases. As in non-private sample complexity, our bounds are completely characterized by the VC dimension of the concept class.",https://proceedings.neurips.cc/paper_files/paper/2018/file/aa97d584861474f4097cf13ccb5325da-Paper.pdf,2018,"private, non, algorithm, classification, differentially, learner, queries, alpha, based, bounds","private, learner, algorithm, queries, non, theorem, pac, privacy, rate, agnostic","{'agnostic': 0.6350542806070331, 'private': 0.5631982645726421, 'model': 0.4682097781901041, 'learning': 0.24554710153496317}","private, differentially, queries, learner, realizable, public, non, alpha, unlabeled, construction"
Towards Text Generation with Adversarially Learned Neural Outlines,"Sandeep Subramanian, Sai Rajeswar Mudumba, Alessandro Sordoni, Adam Trischler, Aaron C. Courville, Chris Pal","Recent progress in deep generative models has been fueled by two paradigms -- autoregressive and adversarial models. We propose a combination of both approaches with the goal of learning generative models of text. Our method first produces a high-level sentence outline and then generates words sequentially, conditioning on both the outline and the previous outputs.
We generate outlines with an adversarial model trained to approximate the distribution of sentences in a latent space induced by general-purpose sentence encoders. This provides strong, informative conditioning for the autoregressive stage. Our quantitative evaluations suggests that conditioning information from generated outlines is able to guide the autoregressive model to produce realistic samples, comparable to maximum-likelihood trained language models, even at high temperatures with multinomial sampling. Qualitative results also demonstrate that this generative procedure yields natural-looking sentences and interpolations.",https://proceedings.neurips.cc/paper_files/paper/2018/file/aaaccd2766ec67aecbe26459bb828d81-Paper.pdf,2018,"models, autoregressive, conditioning, generative, adversarial, high, model, outline, outlines, sentence","sentence, model, distribution, samples, models, trained, table, data, text, representations","{'outlines': 0.47720995845568176, 'adversarially': 0.41668542033040146, 'learned': 0.4046423780607655, 'text': 0.38563986722399884, 'towards': 0.35357020277776174, 'generation': 0.33985482160815733, 'neural': 0.2137221716550913}","autoregressive, conditioning, outline, outlines, sentence, sentences, generative, models, fueled, interpolations"
"FastGRNN: A Fast, Accurate, Stable and Tiny Kilobyte Sized Gated Recurrent Neural Network","Aditya Kusupati, Manish Singh, Kush Bhatia, Ashish Kumar, Prateek Jain, Manik Varma","This paper develops the FastRNN and FastGRNN algorithms to address the twin RNN limitations of inaccurate training and inefficient prediction. Previous approaches have improved accuracy at the expense of prediction costs making them infeasible for resource-constrained and real-time applications. Unitary RNNs have increased accuracy somewhat by restricting the range of the state transition matrix's singular values but have also increased the model size as they require a larger number of hidden units to make up for the loss in expressive power. Gated RNNs have obtained state-of-the-art accuracies by adding extra parameters thereby resulting in even larger models. FastRNN addresses these limitations by adding a residual connection that does not constrain the range of the singular values explicitly and has only two extra scalar parameters. FastGRNN then extends the residual connection to a gate by reusing the RNN matrices to match state-of-the-art gated RNN accuracies but with a 2-4x smaller model. Enforcing FastGRNN's matrices to be low-rank, sparse and quantized resulted in accurate models that could be up to 35x smaller than leading gated and unitary RNNs. This allowed FastGRNN to accurately recognize the ""Hey Cortana"" wakeword with a 1 KB model and to be deployed on severely resource-constrained IoT microcontrollers too tiny to store other RNN models. FastGRNN's code is available at (https://github.com/Microsoft/EdgeML/).",https://proceedings.neurips.cc/paper_files/paper/2018/file/ab013ca67cf2d50796b0c11d1b8bc95d-Paper.pdf,2018,"fastgrnn, rnn, gated, model, models, rnns, state, accuracies, accuracy, adding","fastgrnn, rnn, fastrnn, prediction, rnns, training, model, accuracy, time, unitary","{'fastgrnn': 0.3655981385044265, 'kilobyte': 0.3655981385044265, 'sized': 0.3655981385044265, 'tiny': 0.34507961267284715, 'accurate': 0.31922932729174586, 'stable': 0.31000296108187575, 'gated': 0.3022021796021846, 'fast': 0.23984965789993284, 'recurrent': 0.23984965789993284, 'network': 0.2207567167985325}","fastgrnn, rnn, gated, rnns, fastrnn, unitary, accuracies, singular, resource, adding"
Bipartite Stochastic Block Models with Tiny Clusters,Stefan Neumann,"We study the problem of finding clusters in random bipartite graphs. We present a simple two-step algorithm which provably finds even tiny clusters of size $O(n^\epsilon)$, where $n$ is the number of vertices in the graph and $\epsilon > 0$. Previous algorithms were only able to identify clusters of size $\Omega(\sqrt{n})$. We evaluate the algorithm on synthetic and on real-world data; the experiments show that the algorithm can find extremely small clusters even in presence of high destructive noise.",https://proceedings.neurips.cc/paper_files/paper/2018/file/ab7314887865c4265e896c6e209d1cd6-Paper.pdf,2018,"clusters, algorithm, epsilon, even, size, able, algorithms, bipartite, data, destructive","clusters, algorithm, ui, right, vi, pcv, side, graph, left, cluster","{'bipartite': 0.4958211194067391, 'clusters': 0.46799406731066673, 'tiny': 0.46799406731066673, 'block': 0.40984405333808177, 'stochastic': 0.28368301513901617, 'models': 0.26014673232492225}","clusters, epsilon, destructive, size, algorithm, tiny, bipartite, even, vertices, finds"
Conditional Adversarial Domain Adaptation,"Mingsheng Long, ZHANGJIE CAO, Jianmin Wang, Michael I. Jordan","Adversarial learning has been embedded into deep networks to learn disentangled and transferable representations for domain adaptation. Existing adversarial domain adaptation methods may struggle to align different domains of multimodal distributions that are native in classification problems. In this paper, we present conditional adversarial domain adaptation, a principled framework that conditions the adversarial adaptation models on discriminative information conveyed in the classifier predictions. Conditional domain adversarial networks (CDANs) are designed with two novel conditioning strategies: multilinear conditioning that captures the cross-covariance between feature representations and classifier predictions to improve the discriminability, and entropy conditioning that controls the uncertainty of classifier predictions to guarantee the transferability. Experiments testify that the proposed approach exceeds the state-of-the-art results on five benchmark datasets.",https://proceedings.neurips.cc/paper_files/paper/2018/file/ab88b15733f543179858600245108dd8-Paper.pdf,2018,"adversarial, adaptation, domain, classifier, conditioning, predictions, conditional, networks, representations, align","domain, cdan, adaptation, adversarial, conditional, discriminator, classiﬁer, distributions, feature, conditioning","{'conditional': 0.5709208816970758, 'adaptation': 0.5312706544521048, 'domain': 0.4719969416904512, 'adversarial': 0.4111200865867863}","adaptation, conditioning, adversarial, domain, classifier, predictions, conditional, cdans, conveyed, testify"
Stochastic Expectation Maximization with Variance Reduction,"Jianfei Chen, Jun Zhu, Yee Whye Teh, Tong Zhang","Expectation-Maximization (EM) is a popular tool for learning latent variable models, but the vanilla batch EM does not scale to large data sets because the whole data set is needed at every E-step. Stochastic Expectation Maximization (sEM) reduces the cost of E-step by stochastic approximation. However, sEM has a slower asymptotic convergence rate than batch EM, and requires a decreasing sequence of step sizes, which is difficult to tune. In this paper, we propose a variance reduced stochastic EM (sEM-vr) algorithm inspired by variance reduced stochastic gradient descent algorithms. We show that sEM-vr has the same exponential asymptotic convergence rate as batch EM. Moreover, sEM-vr only requires a constant step size to achieve this rate, which alleviates the burden of parameter tuning. We compare sEM-vr with batch EM, sEM and other algorithms on Gaussian mixture models and probabilistic latent semantic analysis, and sEM-vr converges significantly faster than these baselines.",https://proceedings.neurips.cc/paper_files/paper/2018/file/aba22f748b1a6dff75bda4fd1ee9fe07-Paper.pdf,2018,"sem, em, vr, batch, step, stochastic, rate, algorithms, asymptotic, convergence","sem, vr, step, st, se, algorithm, bem, stochastic, convergence, variance","{'expectation': 0.559794856742144, 'maximization': 0.4523778483695798, 'variance': 0.4432514307182613, 'reduction': 0.42770246335668805, 'stochastic': 0.3202854549841238}","sem, vr, em, batch, step, stochastic, rate, expectation, maximization, reduced"
Bayesian Nonparametric Spectral Estimation,Felipe Tobar,"Spectral estimation (SE) aims to identify how the energy of a signal (e.g., a time series) is distributed across different frequencies. This can become particularly challenging when only partial and noisy observations of the signal are available, where current methods fail to handle uncertainty appropriately. In this context, we propose a joint probabilistic model for signals, observations and spectra, where  SE is addressed as an inference problem. Assuming a Gaussian process prior over the signal, we apply Bayes' rule to find the analytic posterior distribution of the spectrum given a set of observations. Besides its expressiveness and natural account of spectral uncertainty, the proposed model also provides a functional-form representation of the power spectral density, which can be optimised efficiently. Comparison with previous approaches is addressed theoretically, showing that the proposed method is an infinite-dimensional variant of the Lomb-Scargle approach, and also empirically through three experiments.",https://proceedings.neurips.cc/paper_files/paper/2018/file/abd1c782880cc59759f4112fda0b8f98-Paper.pdf,2018,"observations, signal, spectral, addressed, also, model, proposed, se, uncertainty, account","spectrum, signal, bnse, gp, frequency, fc, model, fourier, local, covariance","{'nonparametric': 0.5567651511311866, 'spectral': 0.5465105828369775, 'estimation': 0.47904769072211306, 'bayesian': 0.4023084132190932}","spectral, signal, observations, se, addressed, uncertainty, lomb, optimised, scargle, spectra"
A Simple Unified Framework for Detecting Out-of-Distribution Samples and Adversarial Attacks,"Kimin Lee, Kibok Lee, Honglak Lee, Jinwoo Shin","Detecting test samples drawn sufficiently far away from the training distribution statistically or adversarially is a fundamental requirement for deploying a good classifier in many real-world machine learning applications. However, deep neural networks with the softmax classifier are known to produce highly overconfident posterior distributions even for such abnormal samples. In this paper, we propose a simple yet effective method for detecting any abnormal samples, which is applicable to any pre-trained softmax neural classifier. We obtain the class conditional Gaussian distributions with respect to (low- and upper-level) features of the deep models under Gaussian discriminant analysis, which result in a confidence score based on the Mahalanobis distance. While most prior methods have been evaluated for detecting either out-of-distribution or adversarial samples, but not both, the proposed method achieves the state-of-the-art performances for both cases in our experiments. Moreover, we found that our proposed method is more robust in harsh cases, e.g., when the training dataset has noisy labels or small number of samples. Finally, we show that the proposed method enjoys broader usage by applying it to class-incremental learning: whenever out-of-distribution samples are detected, our classification rule can incorporate new classes well without further training deep models.",https://proceedings.neurips.cc/paper_files/paper/2018/file/abdeb6f575ac5c6676b747bca8d09cc2-Paper.pdf,2018,"samples, method, classifier, deep, detecting, distribution, proposed, training, abnormal, cases","samples, 99, class, distribution, method, classiﬁer, cifar, training, 95, conﬁdence","{'detecting': 0.433443769437085, 'distribution': 0.3784700969819734, 'unified': 0.36753155401079624, 'samples': 0.35828314770614705, 'attacks': 0.3432053164207646, 'simple': 0.3432053164207646, 'framework': 0.32594557617453895, 'adversarial': 0.2522307083526991}","samples, detecting, abnormal, classifier, softmax, method, distribution, cases, proposed, deep"
Breaking the Span Assumption Yields Fast Finite-Sum Minimization,"Robert Hannah, Yanli Liu, Daniel O'Connor, Wotao Yin","In this paper, we show that SVRG and SARAH can be modified to be fundamentally faster than all of the other standard algorithms that minimize the sum of $n$ smooth functions, such as SAGA, SAG, SDCA, and SDCA without duality. Most finite sum algorithms follow what we call the ``span assumption'': Their updates are in the span of a sequence of component gradients chosen in a random IID fashion. In the big data regime, where the condition number $\kappa=O(n)$, the span assumption prevents algorithms from converging to an approximate solution of accuracy $\epsilon$ in less than $n\ln(1/\epsilon)$ iterations. SVRG and SARAH do not follow the span assumption since they are updated with a hybrid of full-gradient and component-gradient information. We show that because of this, they can be up to $\Omega(1+(\ln(n/\kappa))_+)$ times faster. In particular, to obtain an accuracy $\epsilon = 1/n^\alpha$ for $\kappa=n^\beta$ and $\alpha,\beta\in(0,1)$, modified SVRG requires $O(n)$ iterations, whereas algorithms that follow the span assumption require $O(n\ln(n))$ iterations. Moreover, we present lower bound results that show this speedup is optimal, and provide analysis to help explain why this speedup exists. With the understanding that the span assumption is a point of weakness of finite sum algorithms, future work may purposefully exploit this to yield faster algorithms in the big data regime.",https://proceedings.neurips.cc/paper_files/paper/2018/file/abea47ba24142ed16b7d8fbf2c740e0d-Paper.pdf,2018,"algorithms, span, assumption, epsilon, faster, follow, iterations, kappa, ln, show","svrg, ln, algorithms, gradient, sarah, complexity, span, assumption, xk, convex","{'assumption': 0.3920776541833494, 'span': 0.3920776541833494, 'yields': 0.3920776541833494, 'finite': 0.3700730140386932, 'breaking': 0.35446047339393405, 'sum': 0.34235044604736403, 'minimization': 0.30473326525794875, 'fast': 0.25722147167044707}","span, assumption, kappa, ln, svrg, follow, algorithms, iterations, sarah, sdca"
Differential Privacy for Growing Databases,"Rachel Cummings, Sara Krehbiel, Kevin A. Lai, Uthaipon Tantipongpipat","The large majority of differentially private algorithms focus on the static setting, where queries are made on an unchanging database. This is unsuitable for the myriad applications involving databases that grow over time. To address this gap in the literature, we consider the dynamic setting, in which new data arrive over time. Previous results in this setting have been limited to answering a single non-adaptive query repeatedly as the database grows. In contrast, we provide tools for richer and more adaptive analysis of growing databases. Our first contribution is a novel modification of the private multiplicative weights algorithm, which provides accurate analysis of exponentially many adaptive linear queries (an expressive query class including all counting queries) for a static database. Our modification maintains the accuracy guarantee of the static setting even as the database grows without bound. Our second contribution is a set of general results which show that many other private and accurate algorithms can be immediately extended to the dynamic setting by rerunning them at appropriate points of data growth with minimal loss of accuracy, even when data growth is unbounded.",https://proceedings.neurips.cc/paper_files/paper/2018/file/ac27b77292582bc293a51055bfc994ee-Paper.pdf,2018,"setting, database, adaptive, data, private, queries, static, accuracy, accurate, algorithms","database, queries, algorithm, accuracy, static, data, setting, private, log, query","{'databases': 0.5540928051785087, 'growing': 0.5540928051785087, 'privacy': 0.44776994283671784, 'differential': 0.4306557845783859}","database, setting, static, private, queries, databases, adaptive, growth, grows, modification"
Learning Pipelines with Limited Data and Domain Knowledge: A Study in Parsing Physics Problems,"Mrinmaya Sachan, Kumar Avinava Dubey, Tom M. Mitchell, Dan Roth, Eric P. Xing","As machine learning becomes more widely used in practice, we need new methods to build complex intelligent systems that integrate learning with existing software, and with domain knowledge encoded as rules. As a case study, we present such a system that learns to parse Newtonian physics problems in textbooks. This system, Nuts&Bolts, learns a pipeline process that incorporates existing code, pre-learned machine learning models, and human engineered rules.  It jointly trains the entire pipeline to prevent propagation of errors, using a combination of labelled and unlabelled data.  Our approach achieves a good performance on the parsing task, outperforming the simple pipeline and its variants. Finally, we also show how Nuts&Bolts can be used to achieve improvements on a relation extraction task and on the end task of answering Newtonian physics problems.",https://proceedings.neurips.cc/paper_files/paper/2018/file/ac627ab1ccbdb62ec96e702f07f6425b-Paper.pdf,2018,"learning, pipeline, task, bolts, existing, learns, machine, newtonian, nuts, physics","pipeline, rules, diagram, function, various, data, nuts, elements, level, text","{'pipelines': 0.4043338881770497, 'physics': 0.3655408052034134, 'parsing': 0.35305222190690333, 'study': 0.35305222190690333, 'limited': 0.33422101872285914, 'problems': 0.33422101872285914, 'knowledge': 0.29542793574922294, 'domain': 0.2701318703536323, 'data': 0.22885097387934572, 'learning': 0.13256411375802857}","pipeline, bolts, newtonian, nuts, physics, rules, task, learns, system, parse"
Theoretical guarantees for EM under misspecified Gaussian mixture models,"Raaz Dwivedi, nhật Hồ, Koulik Khamaru, Martin J. Wainwright, Michael I. Jordan","Recent years have witnessed substantial progress in understanding
  the behavior of EM for mixture models that are correctly specified.
  Given that model misspecification is common in practice, it is
  important to understand EM in this more general setting.  We provide
  non-asymptotic guarantees for population and sample-based EM for
  parameter estimation under a few specific univariate settings of
  misspecified Gaussian mixture models.  Due to misspecification, the
  EM iterates no longer converge to the true model and instead
  converge to the projection of the true model over the set of models
  being searched over.  We provide two classes of theoretical
  guarantees: first, we characterize the bias introduced due to the
  misspecification; and second, we prove that population EM converges
  at a geometric rate to the model projection under a suitable
  initialization condition.  This geometric convergence rate for
  population EM imply a statistical complexity of order $1/\sqrt{n}$
  when running EM with $n$ samples. We validate our theoretical
  findings in different cases via several numerical examples.",https://proceedings.neurips.cc/paper_files/paper/2018/file/acc21473c4525b922286130ffbfe00b5-Paper.pdf,2018,"em, model, misspecification, models, population, converge, due, geometric, guarantees, mixture","em, model, mixture, convergence, true, component, σ2, two, gaussian, population","{'misspecified': 0.45772224594295086, 'em': 0.43203342333163103, 'theoretical': 0.413806913627375, 'guarantees': 0.37835165392859943, 'mixture': 0.37835165392859943, 'gaussian': 0.29770109045882576, 'models': 0.2401570686157114}","em, misspecification, population, projection, geometric, converge, mixture, true, model, misspecified"
Online Improper Learning with an Approximation Oracle,"Elad Hazan, Wei Hu, Yuanzhi Li, Zhiyuan Li","We study the following question: given an efficient approximation algorithm for an optimization problem, can we learn efficiently in the same setting? We give a formal affirmative answer to this question in the form of a reduction from online learning to offline approximate optimization using an efficient algorithm that guarantees near optimal regret. The algorithm is efficient in terms of the number of oracle calls to a given approximation oracle – it makes only logarithmically many such calls per iteration. This resolves an open question by Kalai and Vempala, and by Garber. Furthermore, our result applies to the more general improper learning problems.",https://proceedings.neurips.cc/paper_files/paper/2018/file/ad47a008a2f806aa6eb1b53852cd8b37-Paper.pdf,2018,"algorithm, efficient, question, approximation, calls, given, learning, optimization, oracle, affirmative","rd, algorithm, oracle, zi, regret, vi, ft, setting, approximation, log","{'improper': 0.5902650326047696, 'oracle': 0.533633122552634, 'approximation': 0.42036930244836285, 'online': 0.39071898884622186, 'learning': 0.1935231332757889}","question, calls, oracle, efficient, affirmative, garber, kalai, vempala, approximation, improper"
Using Trusted Data to Train Deep Networks on Labels Corrupted by Severe Noise,"Dan Hendrycks, Mantas Mazeika, Duncan Wilson, Kevin Gimpel","The growing importance of massive datasets with the advent of deep learning makes robustness to label noise a critical property for classifiers to have. Sources of label noise include automatic labeling for large datasets, non-expert labeling, and label corruption by data poisoning adversaries. In the latter case, corruptions may be arbitrarily bad, even so bad that a classifier predicts the wrong labels with high confidence. To protect against such sources of noise, we leverage the fact that a small set of clean labels is often easy to procure. We demonstrate that robustness to label noise up to severe strengths can be achieved by using a set of trusted data with clean labels, and propose a loss correction that utilizes trusted examples in a data-efficient manner to mitigate the effects of label noise on deep neural network classifiers. Across vision and natural language processing tasks, we experiment with various label noises at several strengths, and show that our method significantly outperforms existing methods.",https://proceedings.neurips.cc/paper_files/paper/2018/file/ad554d8c3b06d6b97ee76a2448bd7913-Paper.pdf,2018,"label, noise, data, labels, bad, classifiers, clean, datasets, deep, labeling","trusted, label, labels, corruption, 10, glc, data, 25, noise, method","{'corrupted': 0.3954215940769805, 'severe': 0.3954215940769805, 'trusted': 0.3954215940769805, 'train': 0.3732292814311256, 'labels': 0.33529127238183487, 'noise': 0.33529127238183487, 'using': 0.22623963697442556, 'data': 0.22380665965306232, 'deep': 0.18527693697570555, 'networks': 0.17658816496561655}","label, noise, trusted, labels, clean, bad, strengths, labeling, sources, classifiers"
Multi-Task Zipping via Layer-wise Neuron Sharing,"Xiaoxi He, Zimu Zhou, Lothar Thiele","Future mobile devices are anticipated to perceive, understand and react to the world on their own by running multiple correlated deep neural networks on-device. Yet the complexity of these neural networks needs to be trimmed down both within-model and cross-model to fit in mobile storage and memory. Previous studies focus on squeezing the redundancy within a single neural network. In this work, we aim to reduce the redundancy across multiple models. We propose Multi-Task Zipping (MTZ), a framework to automatically merge correlated, pre-trained deep neural networks for cross-model compression. Central in MTZ is a layer-wise neuron sharing and incoming weight updating scheme that induces a minimal change in the error function. MTZ inherits information from each model and demands light retraining to re-boost the accuracy of individual tasks. Evaluations show that MTZ is able to fully merge the hidden layers of two VGG-16 networks with a 3.18% increase in the test error averaged on ImageNet and CelebA, or share 39.61% parameters between the two networks with <0.5% increase in the test errors for both tasks. The number of iterations to retrain the combined network is at least 17.8 times lower than that of training a single VGG-16 network. Moreover, experiments show that MTZ is also able to effectively merge multiple residual networks.",https://proceedings.neurips.cc/paper_files/paper/2018/file/ad8e88c0f76fa4fc8e5474384142a00a-Paper.pdf,2018,"networks, mtz, model, neural, merge, multiple, network, 16, able, correlated","layers, layer, networks, two, mtz, wa, model, wb, 10, neurons","{'sharing': 0.42186482883872395, 'zipping': 0.42186482883872395, 'neuron': 0.38138977149791214, 'layer': 0.36835971339772783, 'wise': 0.36835971339772783, 'task': 0.33403698998008524, 'multi': 0.25816774494884853, 'via': 0.22134344909817694}","mtz, merge, networks, vgg, redundancy, 16, mobile, correlated, multiple, cross"
Practical Deep Stereo (PDS): Toward applications-friendly deep stereo matching,"Stepan Tulyakov, Anton Ivanov, François Fleuret","End-to-end deep-learning networks recently demonstrated extremely good performance for stereo matching. However, existing networks are difficult to use for practical applications since (1) they are memory-hungry and unable to process even modest-size images, (2) they have to be fully re-trained to handle a different disparity range.The Practical Deep Stereo (PDS) network that we propose addresses both issues: First, its architecture relies on novel bottleneck modules that drastically reduce the memory footprint in inference, and additional design choices allow to handle greater image size during training. This results in a model that leverages large image context to resolve matching ambiguities. Second, a novel sub-pixel cross-entropy loss combined with a MAP estimator make this network less sensitive to ambiguous matches, and applicable to any disparity range without re-training.We compare PDS to state-of-the-art methods published over the recent months, and demonstrate its superior performance on FlyingThings3D and KITTI sets.",https://proceedings.neurips.cc/paper_files/paper/2018/file/ade55409d1224074754035a5a937d2e0-Paper.pdf,2018,"deep, disparity, end, handle, image, matching, memory, network, networks, novel","disparity, network, pixel, sub, matching, training, 12, module, softargmin, map","{'stereo': 0.6341478613897478, 'friendly': 0.3170739306948739, 'pds': 0.3170739306948739, 'toward': 0.3170739306948739, 'deep': 0.2971334269749397, 'applications': 0.27685944466658907, 'matching': 0.25623176207191056, 'practical': 0.25623176207191056}","stereo, pds, disparity, handle, matching, memory, practical, range, end, flyingthings3d"
Masking: A New Perspective of Noisy Supervision,"Bo Han, Jiangchao Yao, Gang Niu, Mingyuan Zhou, Ivor Tsang, Ya Zhang, Masashi Sugiyama","It is important to learn various types of classifiers given training data with noisy labels. Noisy labels, in the most popular noise model hitherto, are corrupted from ground-truth labels by an unknown noise transition matrix. Thus, by estimating this matrix, classifiers can escape from overfitting those noisy labels. However, such estimation is practically difficult, due to either the indirect nature of two-step approaches, or not big enough data to afford end-to-end approaches. In this paper, we propose a human-assisted approach called ''Masking'' that conveys human cognition of invalid class transitions and naturally speculates the structure of the noise transition matrix. To this end, we derive a structure-aware probabilistic model incorporating a structure prior, and solve the challenges from structure extraction and structure alignment. Thanks to Masking, we only estimate unmasked noise transition probabilities and the burden of estimation is tremendously reduced. We conduct extensive experiments on CIFAR-10 and CIFAR-100 with three noise structures as well as the industrial-level Clothing1M with agnostic noise structure, and the results show that Masking can improve the robustness of classifiers significantly.",https://proceedings.neurips.cc/paper_files/paper/2018/file/aee92f16efd522b9326c25cc3237ac15-Paper.pdf,2018,"noise, structure, labels, classifiers, end, masking, matrix, noisy, transition, approaches","latexit, structure, sha1_base64, noise, transition, matrix, noisy, masking, model, labels","{'masking': 0.4907326252008744, 'perspective': 0.46319115552115125, 'new': 0.4436501718029739, 'supervision': 0.4436501718029739, 'noisy': 0.38856723244352753}","noise, masking, structure, labels, transition, classifiers, noisy, matrix, end, cifar"
Learning to Multitask,"Yu Zhang, Ying Wei, Qiang Yang","Multitask learning has shown promising performance in many applications and many multitask models have been proposed. In order to identify an effective multitask model for a given multitask problem, we propose a learning framework called Learning to MultiTask (L2MT). To achieve the goal, L2MT exploits historical multitask experience which is organized as a training set consisting of several tuples, each of which contains a multitask problem with multiple tasks, a multitask model, and the relative test error. Based on such training set, L2MT first uses a proposed layerwise graph neural network to learn task embeddings for all the tasks in a multitask problem and then learns an estimation function to estimate the relative test error based on task embeddings and the representation of the multitask model based on a unified formulation. Given a new multitask problem, the estimation function is used to identify a suitable multitask model. Experiments on benchmark datasets show the effectiveness of the proposed L2MT framework.",https://proceedings.neurips.cc/paper_files/paper/2018/file/aeefb050911334869a7a5d9e4d0e1689-Paper.pdf,2018,"multitask, l2mt, model, problem, based, learning, proposed, embeddings, error, estimation","multitask, task, problem, training, test, function, l2mt, learning, matrix, tasks","{'multitask': 0.940090265884265, 'learning': 0.34092563997102365}","multitask, l2mt, relative, embeddings, identify, problem, model, test, proposed, estimation"
Thwarting Adversarial Examples: An $L_0$-Robust Sparse Fourier Transform,"Mitali Bafna, Jack Murtagh, Nikhil Vyas","We give a new algorithm for approximating the Discrete Fourier transform of an approximately sparse signal that is robust to worst-case $L_0$ corruptions, namely that some coordinates of the signal can be corrupt arbitrarily. Our techniques generalize to a wide range of linear transformations that are used in data analysis such as the Discrete Cosine and Sine transforms, the Hadamard transform, and their high-dimensional analogs. We use our algorithm to successfully defend against worst-case $L_0$ adversaries in the setting of image classification. We give experimental results on the Jacobian-based Saliency Map Attack (JSMA) and the CW $L_0$ attack on the MNIST and Fashion-MNIST datasets as well as the Adversarial Patch on the ImageNet dataset.",https://proceedings.neurips.cc/paper_files/paper/2018/file/aef546f29283b6ccef3c61f58fb8e79b-Paper.pdf,2018,"l_0, algorithm, attack, case, discrete, give, mnist, signal, transform, worst","images, sparse, algorithm, l0, image, adversarial, fourier, network, top, noise","{'l_0': 0.4263381268820523, 'thwarting': 0.4263381268820523, 'transform': 0.385433887058063, 'fourier': 0.37226566305871034, 'examples': 0.36150644064052434, 'sparse': 0.2875779614616126, 'robust': 0.2727473079814577, 'adversarial': 0.24809577463964885}","l_0, attack, transform, worst, signal, mnist, give, discrete, analogs, cw"
"Constant Regret, Generalized Mixability, and Mirror Descent","Zakaria Mhammedi, Robert C. Williamson","We consider the setting of prediction with expert advice; a learner makes predictions by aggregating those of a group of experts. Under this setting, and for the right choice of loss function and ``mixing'' algorithm, it is possible for the learner to achieve a constant regret regardless of the number of prediction rounds. For example, a constant regret can be achieved for \emph{mixable} losses using the \emph{aggregating algorithm}. The \emph{Generalized Aggregating Algorithm} (GAA) is a name for a family of algorithms parameterized by convex functions on simplices (entropies), which reduce to the aggregating algorithm when using the \emph{Shannon entropy} $\operatorname{S}$. For a given entropy $\Phi$, losses for which a constant regret is possible using the \textsc{GAA} are called $\Phi$-mixable. Which losses are $\Phi$-mixable was previously left as an open question. We fully characterize $\Phi$-mixability and answer other open questions posed by \cite{Reid2015}. We show that the Shannon entropy $\operatorname{S}$ is fundamental in nature when it comes to mixability; any $\Phi$-mixable loss is necessarily $\operatorname{S}$-mixable, and the lowest worst-case regret of the \textsc{GAA} is achieved using the Shannon entropy. Finally, by leveraging the connection between the \emph{mirror descent algorithm} and the update step of the GAA, we suggest a new \emph{adaptive} generalized aggregating algorithm and analyze its performance in terms of the regret bound.",https://proceedings.neurips.cc/paper_files/paper/2018/file/af1b5754061ebbd4412adfb34c8d3534-Paper.pdf,2018,"algorithm, emph, aggregating, mixable, phi, regret, entropy, gaa, using, constant","loss, mixable, mixability, entropy, algorithm, theorem, ℓis, regret, ηℓ, gaa","{'mixability': 0.48076644674129265, 'mirror': 0.4537843105718592, 'constant': 0.43464018029486357, 'regret': 0.3736645727670688, 'generalized': 0.3673220485932564, 'descent': 0.31540551123257204}","mixable, phi, aggregating, gaa, emph, operatorname, regret, shannon, entropy, losses"
Dual Principal Component Pursuit: Improved Analysis and Efficient Algorithms,"Zhihui Zhu, Yifan Wang, Daniel Robinson, Daniel Naiman, René Vidal, Manolis Tsakiris","Recent methods for learning a linear subspace from data corrupted by outliers are based on convex L1 and nuclear norm optimization and require the dimension of the subspace and the number of outliers to be sufficiently small [27]. In sharp contrast, the recently proposed Dual Principal Component Pursuit (DPCP) method [22] can provably handle subspaces of high dimension by solving a non-convex L1 optimization problem on the sphere. However, its geometric analysis is based on quantities that are difficult to interpret and are not amenable to  statistical analysis. In this paper we provide a refined geometric analysis and a new statistical analysis that show that DPCP can tolerate as many outliers as the square of the number of inliers, thus improving upon other provably correct robust PCA methods. We also propose a scalable Projected Sub-Gradient Descent method (DPCP-PSGD) for solving the DPCP problem and show it admits linear convergence even though the underlying optimization problem is non-convex and non-smooth. Experiments on road plane detection from 3D point cloud data demonstrate that DPCP-PSGD can be more efficient than the traditional RANSAC algorithm, which is one of the most popular methods for such computer vision applications.",https://proceedings.neurips.cc/paper_files/paper/2018/file/af21d0c97db2e27e13572cbf59eb343d-Paper.pdf,2018,"dpcp, analysis, convex, methods, non, optimization, outliers, problem, based, data","psgm, step, dpcp, outliers, size, subspace, points, method, convergence, point","{'component': 0.4385391422569756, 'principal': 0.41392693620098264, 'pursuit': 0.41392693620098264, 'dual': 0.35438945394139826, 'improved': 0.32042012141580095, 'analysis': 0.30516504182941223, 'algorithms': 0.2852244609388104, 'efficient': 0.24954512324922395}","dpcp, outliers, psgd, l1, analysis, subspace, convex, geometric, provably, optimization"
Generative modeling for protein structures,"Namrata Anand, Possu Huang","Analyzing the structure and function of proteins is a key part of understanding biology at the molecular and cellular level. In addition, a major engineering challenge is to design new proteins in a principled and methodical way. Current computational modeling methods for protein design are slow and often require human oversight and intervention. Here, we apply Generative Adversarial Networks (GANs) to the task of generating protein structures, toward application in fast de novo protein design. We encode protein structures in terms of pairwise distances between alpha-carbons on the protein backbone, which eliminates the need for the generative model to learn translational and rotational symmetries. We then introduce a convex formulation of corruption-robust 3D structure recovery to fold the protein structures from generated pairwise distance maps, and solve these problems using the Alternating Direction Method of Multipliers. We test the effectiveness of our models by predicting completions of corrupted protein structures and show that the method is capable of quickly producing structurally plausible solutions.",https://proceedings.neurips.cc/paper_files/paper/2018/file/afa299a4d1d8c52e75dd8a24c3ce534f-Paper.pdf,2018,"protein, structures, design, generative, method, pairwise, proteins, structure, 3d, addition","structure, protein, structures, maps, gan, figure, rosetta, backbone, generated, sampling","{'protein': 0.6056813809294491, 'structures': 0.5288628125356353, 'modeling': 0.462761341074864, 'generative': 0.3732373661879079}","protein, structures, proteins, design, pairwise, carbons, cellular, completions, methodical, multipliers"
Found Graph Data and Planted Vertex Covers,"Austin R. Benson, Jon Kleinberg","A typical way in which network data is recorded is to measure all interactions involving a specified set of core nodes, which produces a graph containing this core together with a potentially larger set of fringe nodes that link to the core. Interactions between nodes in the fringe, however, are not present in the resulting graph data. For example, a phone service provider may only record calls in which at least one of the participants is a customer; this can include calls between a customer and a non-customer, but not between pairs of non-customers. Knowledge of which nodes belong to the core is crucial for interpreting the dataset, but this metadata is unavailable in many cases, either because it has been lost due to difficulties in data provenance, or because the network consists of ""found data"" obtained in settings such as counter-surveillance. This leads to an algorithmic problem of recovering the core set. Since the core is a vertex cover, we essentially have a planted vertex cover problem, but with an arbitrary underlying graph. We develop a framework for analyzing this planted vertex cover problem, based on the theory of fixed-parameter tractability, together with algorithms for recovering the core. Our algorithms are fast, simple to implement, and out-perform several baselines based on core-periphery structure on various real-world datasets.",https://proceedings.neurips.cc/paper_files/paper/2018/file/afd4836712c5e77550897e25711e1d96-Paper.pdf,2018,"core, data, nodes, cover, customer, graph, problem, set, vertex, algorithms","vertex, cover, nodes, core, size, planted, graph, minimal, algorithm, set","{'covers': 0.45915219378510996, 'found': 0.45915219378510996, 'planted': 0.45915219378510996, 'vertex': 0.45915219378510996, 'graph': 0.29863112397955416, 'data': 0.2598778627740181}","core, customer, vertex, nodes, cover, fringe, planted, calls, graph, recovering"
Fast Estimation of Causal Interactions using Wold Processes,"Flavio Figueiredo, Guilherme Resende Borges, Pedro O.S. Vaz de Melo, Renato Assunção","We here focus on the task of learning Granger causality matrices for multivariate point processes. In order to accomplish this task, our work is the first to explore the use of Wold processes. By doing so, we are able to develop asymptotically fast MCMC learning algorithms. With $N$ being the total number of events and $K$ the number of processes, our learning algorithm has a $O(N(\,\log(N)\,+\,\log(K)))$ cost per iteration. This is much faster than the $O(N^3\,K^2)$ or $O(K^3)$ for the state of the art. Our approach, called GrangerBusca, is validated on nine datasets. This is an advance in relation to most prior efforts which focus mostly on subsets of the Memetracker data. Regarding accuracy, GrangerBusca is three times more accurate (in Precision@10) than the state of the art for the commonly explored subsets Memetracker. Due to GrangerBusca's much lower training complexity, our approach is the only one able to train models for larger, full, sets of data.",https://proceedings.neurips.cc/paper_files/paper/2018/file/aff0a6a4521232970b2c1cf539ad0a19-Paper.pdf,2018,"grangerbusca, learning, processes, able, approach, art, data, focus, log, memetracker","process, granger, busca, processes, pa, tai, events, pb, event, point","{'wold': 0.5000831748334115, 'interactions': 0.4521035998774354, 'causal': 0.35194802001785014, 'estimation': 0.3406984768064007, 'processes': 0.3406984768064007, 'fast': 0.3280781978156976, 'using': 0.2861215412259539}","grangerbusca, memetracker, processes, subsets, focus, log, much, granger, nine, wold"
Relating Leverage Scores and Density using Regularized Christoffel Functions,"Edouard Pauwels, Francis Bach, Jean-Philippe Vert","Statistical leverage scores emerged as a fundamental tool for matrix sketching and column sampling with applications to low rank approximation, regression, random feature learning and quadrature. Yet, the very nature of this quantity is barely understood. Borrowing ideas from the orthogonal polynomial literature, we introduce the regularized Christoffel function associated to a positive definite kernel. This uncovers a variational formulation for leverage scores for kernel methods and allows to elucidate their relationships with the chosen kernel as well as population density. Our main result quantitatively describes a decreasing relation between leverage score and population density for a broad class of kernels on Euclidean spaces. Numerical simulations support our findings.",https://proceedings.neurips.cc/paper_files/paper/2018/file/aff1621254f7c1be92f64550478c56e6-Paper.pdf,2018,"kernel, leverage, density, population, scores, allows, applications, approximation, associated, barely","rd, function, density, christoffel, leverage, cλ, given, assumption, kernel, 2ν","{'christoffel': 0.4037306295487061, 'relating': 0.4037306295487061, 'scores': 0.4037306295487061, 'density': 0.36499542513214406, 'leverage': 0.36499542513214406, 'regularized': 0.326260220715582, 'functions': 0.2911316281112284, 'using': 0.2309936342191103}","leverage, kernel, scores, population, density, christoffel, barely, elucidate, borrowing, quadrature"
"Online Adaptive Methods, Universality and Acceleration","Kfir Y. Levy, Alp Yurtsever, Volkan Cevher","We present a novel method for convex unconstrained optimization that, without  any modifications ensures: (1) accelerated convergence rate for smooth objectives, (2) standard convergence rate in the general (non-smooth) setting, and (3)  standard convergence rate in the stochastic optimization setting.To the best of our knowledge, this is the first method that simultaneously applies to all of the above settings.At the heart of our method is an adaptive learning rate rule that employs importance weights, in the spirit of adaptive online learning algorithms  [duchi2011adaptive,levy2017online],  combined with an update  that linearly couples two sequences, in the spirit of [AllenOrecchia2017]. An empirical examination of our method demonstrates its applicability to the above mentioned scenarios and corroborates our theoretical findings.",https://proceedings.neurips.cc/paper_files/paper/2018/file/b0169350cd35566c47ba83c6ec1d6f82-Paper.pdf,2018,"method, rate, convergence, adaptive, learning, optimization, setting, smooth, spirit, standard","10, yt, smooth, α2, adagrad, rate, method, 101, learning, αt","{'universality': 0.5617301056609793, 'acceleration': 0.45394174937977694, 'methods': 0.4365916992854531, 'adaptive': 0.38668572166369086, 'online': 0.3718306299117975}","rate, spirit, convergence, method, smooth, adaptive, allenorecchia2017, duchi2011adaptive, levy2017online, mentioned"
Reparameterization Gradient for Non-differentiable Models,"Wonyeol Lee, Hangyeol Yu, Hongseok Yang","We present a new algorithm for stochastic variational inference that targets at models with non-differentiable densities. One of the key challenges in stochastic variational inference is to come up with a low-variance estimator of the gradient of a variational objective. We tackle the challenge by generalizing the reparameterization trick, one of the most effective techniques for addressing the variance issue for differentiable models, so that the trick works for non-differentiable models as well. Our algorithm splits the space of latent variables into regions where the density of the variables is differentiable, and their boundaries where the density may fail to be differentiable. For each differentiable region, the algorithm applies the standard reparameterization trick and estimates the gradient restricted to the region. For each potentially non-differentiable boundary, it uses a form of manifold sampling and computes the direction for variational parameters that, if followed, would increase the boundary’s contribution to the variational objective. The sum of all the estimates becomes the gradient estimate of our algorithm. Our estimator enjoys the reduced variance of the reparameterization gradient while remaining unbiased even for non-differentiable models. The experiments with our preliminary implementation confirm the benefit of reduced variance and unbiasedness.",https://proceedings.neurips.cc/paper_files/paper/2018/file/b096577e264d1ebd6b41041f392eec23-Paper.pdf,2018,"differentiable, variational, algorithm, gradient, models, non, variance, reparameterization, trick, boundary","estimator, differentiable, gradient, rk, reparameterization, 10, rn, non, variational, models","{'reparameterization': 0.5927762781606983, 'differentiable': 0.5075138705754167, 'non': 0.38976470675270897, 'gradient': 0.3613204484713086, 'models': 0.32951018513845176}","differentiable, reparameterization, variational, trick, variance, gradient, non, boundary, region, reduced"
Improving Exploration in Evolution Strategies for Deep Reinforcement Learning via a Population of Novelty-Seeking Agents,"Edoardo Conti, Vashisht Madhavan, Felipe Petroski Such, Joel Lehman, Kenneth Stanley, Jeff Clune","Evolution strategies (ES) are a family of black-box optimization algorithms able to train deep neural networks roughly as well as Q-learning and policy gradient methods on challenging deep reinforcement learning (RL) problems, but are much faster (e.g. hours vs. days) because they parallelize better. However, many RL problems require directed exploration because they have reward functions that are sparse or deceptive (i.e. contain local optima), and it is unknown how to encourage such exploration with ES. Here we show that algorithms that have been invented to promote directed exploration in small-scale evolved neural networks via populations of exploring agents, specifically novelty search (NS) and quality diversity (QD) algorithms, can be hybridized with ES to improve its performance on sparse or deceptive deep RL tasks, while retaining scalability. Our experiments confirm that the resultant new algorithms, NS-ES and two QD algorithms, NSR-ES and NSRA-ES, avoid local optima encountered by ES to achieve higher performance on Atari and simulated robots learning to walk around a deceptive trap. This paper thus introduces a family of fast, scalable algorithms for reinforcement learning that are capable of directed exploration. It also adds this new family of exploration algorithms to the RL toolbox and raises the interesting possibility that analogous algorithms with multiple simultaneous paths of exploration might also combine well with existing RL algorithms outside ES.",https://proceedings.neurips.cc/paper_files/paper/2018/file/b1301141feffabac455e1f90a7de2054-Paper.pdf,2018,"algorithms, es, exploration, rl, learning, deceptive, deep, directed, family, also","es, reward, ns, exploration, novelty, performance, local, algorithms, nsr, nsra","{'seeking': 0.37405349378706476, 'novelty': 0.35306042662851994, 'strategies': 0.35306042662851994, 'agents': 0.3266122898799338, 'evolution': 0.3266122898799338, 'population': 0.30919134756178346, 'exploration': 0.290724404532302, 'improving': 0.290724404532302, 'reinforcement': 0.22025947028804949, 'via': 0.19625786461024014}","es, exploration, rl, algorithms, deceptive, directed, qd, ns, family, optima"
Optimistic optimization of a Brownian,"Jean-Bastien Grill, Michal Valko, Remi Munos","We address the problem of optimizing a Brownian motion. We consider a (random) realization $W$ of a Brownian motion  with input space in $[0,1]$. Given $W$, our goal is to return an $\epsilon$-approximation of its maximum using the smallest possible number of function evaluations, the sample complexity of the algorithm. We provide an algorithm with sample complexity of order $\log^2(1/\epsilon)$. This improves over previous results of Al-Mharmah and Calvin (1996) and Calvin et al. (2017) which provided only polynomial rates. Our algorithm is adaptive---each query depends on previous values---and is an instance of the  optimism-in-the-face-of-uncertainty principle.",https://proceedings.neurips.cc/paper_files/paper/2018/file/b132ecc1609bfcf302615847c1caa69a-Paper.pdf,2018,"algorithm, al, brownian, calvin, complexity, epsilon, motion, previous, sample, 1996","2h, brownian, u2h, motion, algorithm, sample, bound, lemma, near, number","{'brownian': 0.691283562579803, 'optimistic': 0.6249596125335319, 'optimization': 0.3627016939702838}","brownian, calvin, motion, al, epsilon, 1996, mharmah, optimism, previous, realization"
Generalizing Tree Probability Estimation via Bayesian Networks,"Cheng Zhang, Frederick A Matsen IV","Probability estimation is one of the fundamental tasks in statistics and machine learning. However, standard methods for probability estimation on discrete objects do not handle object structure in a satisfactory manner. In this paper, we derive a general Bayesian network formulation for probability estimation on leaf-labeled trees that enables flexible approximations which can generalize beyond observations. We show that efficient algorithms for learning Bayesian networks can be easily extended to probability estimation on this challenging structured space. Experiments on both synthetic and real data show that our methods greatly outperform the current practice of using the empirical distribution, as well as a previous effort for probability estimation on trees.",https://proceedings.neurips.cc/paper_files/paper/2018/file/b137fdd1f79d56c7edf3365fea7520f2-Paper.pdf,2018,"estimation, probability, bayesian, learning, methods, show, trees, algorithms, approximations, beyond","s1, sbn, trees, tree, 10, probability, si, em, data, log","{'probability': 0.48511820777050507, 'generalizing': 0.4685443012184738, 'tree': 0.4248866593137782, 'estimation': 0.3655777890361169, 'bayesian': 0.30701540381828113, 'via': 0.28154330648806464, 'networks': 0.23963655308807175}","probability, estimation, trees, bayesian, leaf, satisfactory, greatly, effort, extended, labeled"
Safe Active Learning for Time-Series Modeling with Gaussian Processes,"Christoph Zimmer, Mona Meister, Duy Nguyen-Tuong","Learning time-series models is useful for many applications, such as simulation
and forecasting. In this study, we consider the problem of actively learning time-series models while taking given safety constraints into account. For time-series modeling we employ a Gaussian process with a nonlinear exogenous input structure. The proposed approach generates data appropriate for time series model learning, i.e. input and output trajectories, by dynamically exploring the input space. The approach parametrizes the input trajectory as consecutive trajectory sections, which are determined stepwise given safety requirements and past observations. We analyze the proposed algorithm and evaluate it empirically on a technical application. The results show the effectiveness of our approach in a realistic technical use case.",https://proceedings.neurips.cc/paper_files/paper/2018/file/b197ffdef2ddc3308584dce7afa3661b-Paper.pdf,2018,"input, series, time, approach, learning, given, models, proposed, safety, technical","safety, safe, trajectories, input, model, learning, trajectory, exploration, time, information","{'safe': 0.43250459075828257, 'series': 0.40028041495078187, 'active': 0.3922050278431899, 'modeling': 0.378446734571363, 'processes': 0.3374581050846198, 'time': 0.3341129759621156, 'gaussian': 0.3221590380552893, 'learning': 0.1623969039494366}","series, input, safety, technical, trajectory, time, parametrizes, sections, stepwise, approach"
Computing Kantorovich-Wasserstein Distances on $d$-dimensional histograms using $(d+1)$-partite graphs,"Gennaro Auricchio, Federico Bassetti, Stefano Gualandi, Marco Veneroni","This paper presents a novel method to compute the exact Kantorovich-Wasserstein distance between a pair of $d$-dimensional histograms having $n$ bins each. We prove that this problem is equivalent to an uncapacitated minimum cost flow problem on a $(d+1)$-partite graph with $(d+1)n$ nodes and $dn^{\frac{d+1}{d}}$ arcs, whenever the cost is separable along the principal $d$-dimensional directions. We show numerically the benefits of our approach by computing the Kantorovich-Wasserstein distance of order 2 among two sets of instances: gray scale images and $d$-dimensional biomedical histograms. On these types of instances, our approach is competitive with state-of-the-art optimal transport algorithms.",https://proceedings.neurips.cc/paper_files/paper/2018/file/b19aa25ff58940d974234b48391b9549-Paper.pdf,2018,"dimensional, approach, cost, distance, histograms, instances, kantorovich, problem, wasserstein, algorithms","distance, problem, ﬂow, cost, graph, partite, dimensional, runtime, algorithm, kantorovich","{'histograms': 0.3812516736944436, 'kantorovich': 0.3812516736944436, 'partite': 0.3812516736944436, 'computing': 0.35985462187401274, 'distances': 0.35985462187401274, 'dimensional': 0.3080946703171258, 'wasserstein': 0.2912893500789781, 'graphs': 0.2785628369372946, 'using': 0.21813234670166023}","histograms, kantorovich, dimensional, wasserstein, instances, distance, arcs, biomedical, dn, partite"
SimplE Embedding for Link Prediction in Knowledge Graphs,"Seyed Mehran Kazemi, David Poole","Knowledge graphs contain knowledge about the world and provide a structured representation of this knowledge. Current knowledge graphs contain only a small subset of what is true in the world. Link prediction approaches aim at predicting new links for a knowledge graph given the existing links among the entities. Tensor factorization approaches have proved promising for such link prediction problems. Proposed in 1927, Canonical Polyadic (CP) decomposition is among the first tensor factorization approaches. CP generally performs poorly for link prediction as it learns two independent embedding vectors for each entity, whereas they are really tied. We present a simple enhancement of CP (which we call SimplE) to allow the two embeddings of each entity to be learned dependently. The complexity of SimplE grows linearly with the size of embeddings. The embeddings learned through SimplE are interpretable, and certain types of background knowledge can be incorporated into these embeddings through weight tying. 
We prove SimplE is fully expressive and derive a bound on the size of its embeddings for full expressivity. 
We show empirically that, despite its simplicity, SimplE outperforms several state-of-the-art tensor factorization techniques.
SimplE's code is available on GitHub at https://github.com/Mehran-k/SimplE.",https://proceedings.neurips.cc/paper_files/paper/2018/file/b2ab001909a8a6f04b51920306046ce5-Paper.pdf,2018,"simple, knowledge, embeddings, approaches, cp, factorization, link, prediction, tensor, among","simple, vr, ei, relation, ej, embedding, model, triple, entity, embeddings","{'link': 0.48039029775883374, 'simple': 0.42074575951200144, 'embedding': 0.39958652283521584, 'graphs': 0.3882486223169308, 'knowledge': 0.3882486223169308, 'prediction': 0.36201490712168216}","simple, embeddings, knowledge, cp, link, factorization, tensor, links, entity, contain"
Bounded-Loss Private Prediction Markets,"Rafael Frongillo, Bo Waggoner","Prior work has investigated variations of prediction markets that preserve participants' (differential) privacy, which formed the basis of useful mechanisms for purchasing data for machine learning objectives.
  Such markets required potentially unlimited financial subsidy, however, making them impractical.
  In this work, we design an adaptively-growing prediction market with a bounded financial subsidy, while achieving privacy, incentives to produce accurate predictions, and precision in the sense that market prices are
 not heavily impacted by the added privacy-preserving noise.
  We briefly discuss how our mechanism can extend to the data-purchasing setting, and its relationship to traditional learning algorithms.",https://proceedings.neurips.cc/paper_files/paper/2018/file/b2dc449578a4744a1684d3b0ea933702-Paper.pdf,2018,"privacy, data, financial, learning, market, markets, prediction, purchasing, subsidy, work","market, loss, noise, qt, participants, price, privacy, trader, prediction, zt","{'markets': 0.5368675895944235, 'bounded': 0.5067368795467914, 'private': 0.4037193014610636, 'loss': 0.39777120982972325, 'prediction': 0.3657590961393076}","privacy, markets, purchasing, subsidy, market, financial, briefly, incentives, prices, prediction"
Statistical mechanics of low-rank tensor decomposition,"Jonathan Kadmon, Surya Ganguli","Often, large, high dimensional datasets collected across multiple
modalities can be organized as a higher order tensor. Low-rank tensor
decomposition then arises as a powerful and widely used tool to discover
simple low dimensional structures underlying such data. However, we
currently lack a theoretical understanding of the algorithmic behavior
of low-rank tensor decompositions. We derive Bayesian approximate
message passing (AMP) algorithms for recovering arbitrarily shaped
low-rank tensors buried within noise, and we employ dynamic mean field
theory to precisely characterize their performance. Our theory reveals
the existence of phase transitions between easy, hard and impossible
inference regimes, and displays an excellent match with simulations.
Moreover, it reveals several qualitative surprises compared to the
behavior of symmetric, cubic tensor decomposition. Finally, we compare
our AMP algorithm to the most commonly used algorithm, alternating
least squares (ALS), and demonstrate that AMP significantly outperforms
ALS in the presence of noise.",https://proceedings.neurips.cc/paper_files/paper/2018/file/b3848d61bbbc6207c6668a8a9e2730ed-Paper.pdf,2018,"low, tensor, amp, rank, algorithm, als, behavior, decomposition, dimensional, noise","tensor, rank, noise, amp, αi, low, tensors, order, decomposition, mean","{'mechanics': 0.49961734791699713, 'decomposition': 0.41298200293148224, 'statistical': 0.3956022489464935, 'tensor': 0.3956022489464935, 'rank': 0.3701720885833224, 'low': 0.3602758901257498}","amp, tensor, als, rank, low, reveals, decomposition, behavior, theory, noise"
A theory on the absence of spurious solutions for nonconvex and nonsmooth optimization,"Cedric Josz, Yi Ouyang, Richard Zhang, Javad Lavaei, Somayeh Sojoudi","We study the set of continuous functions that admit no spurious local optima (i.e. local minima that are not global minima) which we term global functions. They satisfy various powerful properties for analyzing nonconvex and nonsmooth optimization problems. For instance, they satisfy a theorem akin to the fundamental uniform limit theorem in the analysis regarding continuous functions. Global functions are also endowed with useful properties regarding the composition of functions and change of variables. Using these new results, we show that a class of non-differentiable nonconvex optimization problems arising in tensor decomposition applications are global functions. This is the first result concerning nonconvex methods for nonsmooth objective functions. Our result provides a theoretical guarantee for the widely-used $\ell_1$ norm to avoid outliers in nonconvex optimization.",https://proceedings.neurips.cc/paper_files/paper/2018/file/b3ba8f1bee1238a2f37603d90b58898d-Paper.pdf,2018,"functions, global, nonconvex, optimization, continuous, local, minima, nonsmooth, problems, properties","global, function, functions, local, minima, minimum, proposition, rn, set, x1","{'absence': 0.43668552711336744, 'spurious': 0.43668552711336744, 'nonsmooth': 0.41217735181191145, 'solutions': 0.41217735181191145, 'theory': 0.3528915222968476, 'nonconvex': 0.3190657711830797, 'optimization': 0.22911955236609616}","functions, nonconvex, global, nonsmooth, regarding, satisfy, theorem, minima, optimization, continuous"
A Structured Prediction Approach for Label Ranking,"Anna Korba, Alexandre Garcia, Florence d'Alché-Buc","We propose to solve a label ranking problem as a structured output regression task. In this view, we adopt a least square surrogate loss
approach that solves a supervised learning problem in two steps:
a regression step in a well-chosen feature space and a pre-image (or decoding) step. We use specific feature maps/embeddings for ranking data, which convert any ranking/permutation into a vector representation. These embeddings are all well-tailored for our approach, either by resulting in consistent estimators, or by solving trivially the pre-image problem which is often the bottleneck in structured prediction. Their extension to the case of incomplete or partial rankings is also discussed. Finally, we provide empirical results on synthetic and real-world datasets showing the relevance of our method.",https://proceedings.neurips.cc/paper_files/paper/2018/file/b3dd760eb02d2e669c604f6b2f1e803f-Paper.pdf,2018,"problem, ranking, approach, embeddings, feature, image, pre, regression, step, structured","ranking, al, et, embedding, sk, problem, rankings, label, step, prediction","{'ranking': 0.5363444922818791, 'label': 0.4961663488074547, 'structured': 0.399912318890754, 'approach': 0.39541661850849597, 'prediction': 0.3871296616337979}","ranking, pre, embeddings, structured, feature, regression, step, discussed, rankings, trivially"
Geometrically Coupled Monte Carlo Sampling,"Mark Rowland, Krzysztof M. Choromanski, François Chalus, Aldo Pacchiano, Tamas Sarlos, Richard E. Turner, Adrian Weller","Monte Carlo sampling in high-dimensional, low-sample settings is important in many machine learning tasks.  We improve current methods for sampling in Euclidean spaces by avoiding independence, and instead consider ways to couple samples. We show fundamental connections to optimal transport theory, leading to novel sampling algorithms, and providing new theoretical grounding for existing strategies.  We compare our new strategies against prior methods for improving sample efficiency, including QMC, by studying discrepancy. We explore our findings empirically, and observe benefits of our sampling schemes for reinforcement learning and generative modelling.",https://proceedings.neurips.cc/paper_files/paper/2018/file/b3e3e393c77e35a4a3f3cbd1e429b5dc-Paper.pdf,2018,"sampling, learning, methods, new, sample, strategies, algorithms, avoiding, benefits, carlo","samples, al, et, coupling, optimal, orthogonal, rd, antithetic, function, sampling","{'geometrically': 0.5373204602516338, 'coupled': 0.485768221353353, 'carlo': 0.41761986839937043, 'monte': 0.41761986839937043, 'sampling': 0.3556729525200889}","sampling, strategies, couple, qmc, grounding, discrepancy, sample, avoiding, studying, independence"
The Lingering of Gradients: How to Reuse Gradients Over Time,"Zeyuan Allen-Zhu, David Simchi-Levi, Xinshang Wang","Classically, the time complexity of a first-order method is estimated by its number of gradient computations. In this paper, we study a more refined complexity by taking into account the ``lingering'' of gradients: once a gradient is computed at $x_k$, the additional time to compute gradients at $x_{k+1},x_{k+2},\dots$ may be reduced.

We show how this improves the running time of gradient descent and SVRG. For instance, if the ""additional time'' scales linearly with respect to the traveled distance, then the ""convergence rate'' of gradient descent can be improved from $1/T$ to $\exp(-T^{1/3})$. On the empirical side, we solve a hypothetical revenue management problem on the Yahoo! Front Page Today Module application with 4.6m users to $10^{-6}$ error (or $10^{-12}$ dual error) using 6 passes of the dataset.",https://proceedings.neurips.cc/paper_files/paper/2018/file/b4288d9c0ec0a1841b3b3728321e7088-Paper.pdf,2018,"gradient, time, 10, additional, complexity, descent, error, gradients, x_, 12","fi, gradient, xk, svrg, x0, gradients, figure, full, epoch, rd","{'gradients': 0.7335622474360293, 'lingering': 0.4437244504536279, 'reuse': 0.41882122847339376, 'time': 0.2993055625715385}","x_, gradient, time, additional, 10, gradients, classically, lingering, page, traveled"
Submodular Maximization via Gradient Ascent: The Case of Deep Submodular   Functions,"Wenruo Bai, William Stafford Noble, Jeff A. Bilmes","We study the problem of maximizing deep submodular functions (DSFs) subject to a matroid constraint. DSFs are an expressive class of submodular functions that include, as strict subfamilies, the facility location, weighted coverage, and sums of concave composed with modular functions. We use a strategy similar to the continuous greedy approach, but we show that the multilinear extension of any DSF has a natural and computationally attainable concave relaxation that we can optimize using gradient ascent. Our results show a guarantee of $\max_{0<\delta<1}(1-\epsilon-\delta-e^{-\delta^2\Omega(k)})$ with a running time of $O(\nicefrac{n^2}{\epsilon^2})$ plus time for pipage rounding
to recover a discrete solution, where $k$ is the rank of the matroid constraint. This bound is often better than the standard $1-1/e$ guarantee of the continuous greedy algorithm, but runs much faster. Our bound also holds even for fully curved ($c=1$) functions where the guarantee of $1-c/e$ degenerates to $1-1/e$ where $c$ is the curvature of $f$.  We perform computational experiments that support our theoretical results.",https://proceedings.neurips.cc/paper_files/paper/2018/file/b43a6403c17870707ca3c44984a2da22-Paper.pdf,2018,"functions, delta, guarantee, bound, concave, constraint, continuous, dsfs, epsilon, greedy","dsf, concave, extension, function, functions, submodular, algorithm, continuous, greedy, matroid","{'submodular': 0.6127628348329142, 'ascent': 0.39031171582075896, 'case': 0.36107304488507713, 'maximization': 0.3341709459794287, 'functions': 0.2981906018364132, 'gradient': 0.2379103371706295, 'via': 0.21696496718942448, 'deep': 0.19375692843742526}","delta, dsfs, guarantee, functions, matroid, concave, submodular, constraint, greedy, epsilon"
Sparsified SGD with Memory,"Sebastian U. Stich, Jean-Baptiste Cordonnier, Martin Jaggi","Huge scale machine learning problems are nowadays tackled by distributed optimization algorithms, i.e. algorithms that leverage the compute power of many devices for training. The communication overhead is a key bottleneck that hinders perfect scalability. Various recent works proposed to use quantization or sparsification techniques to reduce the amount of data that needs to be communicated, for instance by only sending the most significant entries of the stochastic gradient (top-k sparsification). Whilst such schemes showed very promising performance in practice, they have eluded theoretical analysis so far.In this work we analyze Stochastic Gradient Descent (SGD) with k-sparsification or compression (for instance top-k or random-k) and show that this scheme converges at the same rate as vanilla SGD when equipped with error compensation (keeping track of accumulated errors in memory).  That is, communication can be reduced by a factor of the dimension of the problem (sometimes even more) whilst still converging at the same rate. We present numerical experiments to illustrate the theoretical findings and the good scalability for distributed applications.",https://proceedings.neurips.cc/paper_files/paper/2018/file/b440509a0106086a67bc2ea9df0a1dab-Paper.pdf,2018,"sparsification, algorithms, communication, distributed, gradient, instance, rate, scalability, sgd, stochastic","sgd, xt, convergence, memory, sparsiﬁcation, 10, gradient, gt, rd, communication","{'sparsified': 0.6444938452544978, 'memory': 0.5406605605317774, 'sgd': 0.5406605605317774}","sparsification, whilst, scalability, top, sgd, communication, instance, distributed, accumulated, nowadays"
Convergence of Cubic Regularization for Nonconvex Optimization under KL Property,"Yi Zhou, Zhe Wang, Yingbin Liang","Cubic-regularized Newton's method (CR) is a popular algorithm that guarantees to produce a second-order stationary solution for solving nonconvex optimization problems. However, existing understandings of convergence rate of CR are conditioned on special types of geometrical properties of the objective function. In this paper, we explore the asymptotic convergence rate of CR by exploiting the ubiquitous Kurdyka-Lojasiewicz (KL) property of the nonconvex objective functions. In specific, we characterize the asymptotic convergence rate of various types of optimality measures for CR including function value gap, variable distance gap, gradient norm and least eigenvalue of the Hessian matrix. Our results fully characterize the diverse convergence behaviors of these optimality measures in the full parameter regime of the KL property. Moreover, we show that the obtained asymptotic convergence rates of CR are order-wise faster than those of first-order gradient descent algorithms under the KL property.",https://proceedings.neurips.cc/paper_files/paper/2018/file/b4568df26077653eeadf29596708c94b-Paper.pdf,2018,"convergence, cr, asymptotic, kl, order, property, rate, characterize, function, gap","xk, cr, convergence, kł, property, order, al, et, gradient, rate","{'kl': 0.46306096501116845, 'cubic': 0.4370725165714544, 'property': 0.4370725165714544, 'convergence': 0.3482174244666618, 'nonconvex': 0.3383370749260329, 'regularization': 0.3297783268539796, 'optimization': 0.24295818027884375}","cr, kl, convergence, asymptotic, property, optimality, rate, characterize, order, nonconvex"
Model Agnostic Supervised Local Explanations,"Gregory Plumb, Denali Molitor, Ameet S. Talwalkar","Model interpretability is an increasingly important component of practical machine learning. Some of the most common forms of interpretability systems are example-based, local, and global explanations. One of the main challenges in interpretability is designing explanation systems that can capture aspects of each of these explanation types, in order to develop a more thorough understanding of the model. We address this challenge in a novel model called MAPLE that uses local linear modeling techniques along with a dual interpretation of random forests (both as a supervised neighborhood approach and as a feature selection method). MAPLE has two fundamental advantages over existing interpretability systems. First, while it is effective as a black-box explanation system, MAPLE itself is a highly accurate predictive model that provides faithful self explanations, and thus sidesteps the typical accuracy-interpretability trade-off. Specifically, we demonstrate, on several UCI datasets, that MAPLE is at least as accurate as random forests and that it produces more faithful local explanations than LIME, a popular interpretability system. Second, MAPLE provides both example-based and local explanations and can detect global patterns, which allows it to diagnose limitations in its local explanations.",https://proceedings.neurips.cc/paper_files/paper/2018/file/b495ce63ede0f4efc9eec62cb947c162-Paper.pdf,2018,"interpretability, explanations, local, maple, model, explanation, systems, accurate, based, example","local, model, maple, explanations, training, explanation, feature, global, point, based","{'explanations': 0.5259778704167647, 'agnostic': 0.4933255589989252, 'supervised': 0.4195362002399624, 'local': 0.41433888030172344, 'model': 0.3637167051824418}","maple, interpretability, explanations, explanation, local, faithful, forests, systems, accurate, example"
Mental Sampling in Multimodal Representations,"Jianqiao Zhu, Adam Sanborn, Nick Chater","Both resources in the natural environment and concepts in a semantic space are distributed ""patchily"", with large gaps in between the patches. To describe people's internal and external foraging behavior, various random walk models have been proposed. In particular, internal foraging has been modeled as sampling: in order to gather relevant information for making a decision, people draw samples from a mental representation using random-walk algorithms such as Markov chain Monte Carlo (MCMC). However, two common empirical observations argue against people using simple sampling algorithms such as MCMC for internal foraging. First, the distance between samples is often best described by a Levy flight distribution: the probability of the distance between two successive locations follows a power-law on the distances. Second, humans and other animals produce long-range, slowly decaying autocorrelations characterized as 1/f-like fluctuations, instead of the 1/f^2 fluctuations produced by random walks. We propose that mental sampling is not done by simple MCMC, but is instead adapted to multimodal representations and is implemented by Metropolis-coupled Markov chain Monte Carlo (MC3), one of the first algorithms developed for sampling from multimodal distributions. MC3 involves running multiple Markov chains in parallel but with target distributions of different temperatures, and it swaps the states of the chains whenever a better location is found. Heated chains more readily traverse valleys in the probability landscape to propose moves to far-away peaks, while the colder chains make the local steps that explore the current peak or patch. We show that MC3 generates distances between successive samples that follow a Levy flight distribution and produce 1/f-like autocorrelations, providing a single mechanistic account of these two puzzling empirical phenomena of internal foraging.",https://proceedings.neurips.cc/paper_files/paper/2018/file/b4a721cfb62f5d19ec61575114d8a2d1-Paper.pdf,2018,"chains, foraging, internal, sampling, algorithms, markov, mc3, mcmc, people, random","distribution, noise, mc3, power, sampling, lévy, samples, algorithms, ﬂight, law","{'mental': 0.5973010707393732, 'multimodal': 0.5637786422627648, 'representations': 0.4111721861828529, 'sampling': 0.3953764114506141}","foraging, chains, mc3, internal, people, mcmc, autocorrelations, flight, levy, sampling"
Nonparametric learning from Bayesian models with randomized objective functions,"Simon Lyddon, Stephen Walker, Chris C. Holmes","Bayesian learning is built on an assumption that the model space contains a true reflection of the data generating mechanism. This assumption is problematic, particularly in complex data environments. Here we present a Bayesian nonparametric approach to learning that makes use of statistical models, but does not assume that the model is true. Our approach has provably better properties than using a parametric model and admits a Monte Carlo sampling scheme that can afford massive scalability on modern computer architectures. The model-based aspect of learning is particularly attractive for regularizing nonparametric inference when the sample size is small, and also for correcting approximate approaches such as variational Bayes (VB). We demonstrate the approach on a number of examples including VB classifiers and Bayesian random forests.",https://proceedings.neurips.cc/paper_files/paper/2018/file/b4d168b48157c623fbd095b4a565b5bb-Paper.pdf,2018,"model, approach, bayesian, learning, assumption, data, nonparametric, particularly, true, vb","posterior, model, bayesian, x1, prior, fθ, distribution, data, parametric, samples","{'randomized': 0.5037743743852008, 'objective': 0.4660360935152457, 'nonparametric': 0.4226121593590925, 'functions': 0.3848738784891375, 'bayesian': 0.30537189135027304, 'models': 0.2800361510530519, 'learning': 0.17498731882331278}","vb, bayesian, nonparametric, particularly, true, assumption, model, reflection, approach, afford"
On the Dimensionality of Word Embedding,"Zi Yin, Yuanyuan Shen","In this paper, we provide a theoretical understanding of word embedding and its dimensionality. Motivated by the unitary-invariance of word embedding, we propose the Pairwise Inner Product (PIP) loss, a novel metric on the dissimilarity between word embeddings. Using techniques from matrix perturbation theory, we reveal a fundamental bias-variance trade-off in dimensionality selection for word embeddings. This bias-variance trade-off sheds light on many empirical observations which were previously unexplained, for example the existence of an optimal dimensionality. Moreover, new insights and discoveries, like when and how word embeddings are robust to over-fitting, are revealed. By optimizing over the bias-variance trade-off of the PIP loss, we can explicitly answer the open question of dimensionality selection for word embedding.",https://proceedings.neurips.cc/paper_files/paper/2018/file/b534ba68236ba543ae44b22bd110a1d6-Paper.pdf,2018,"word, dimensionality, bias, embedding, embeddings, trade, variance, loss, pip, selection","pip, word, matrix, loss, embedding, et, embeddings, dimensionality, al, variance","{'dimensionality': 0.6039350035695964, 'word': 0.6039350035695964, 'embedding': 0.5201202004602238}","word, dimensionality, pip, bias, embeddings, trade, embedding, variance, selection, unexplained"
Large Scale computation of Means and Clusters for Persistence Diagrams using Optimal Transport,"Theo Lacombe, Marco Cuturi, Steve OUDOT","Persistence diagrams (PDs) are now routinely used to summarize the underlying topology of complex data. Despite several appealing properties, incorporating PDs in learning pipelines can be challenging because their natural geometry is not Hilbertian. Indeed, this was recently exemplified in a string of papers which show that the simple task of averaging a few PDs can be computationally prohibitive. We propose in this article a tractable framework to carry out standard tasks on PDs at scale, notably evaluating distances, estimating barycenters and performing clustering. This framework builds upon a reformulation of PD metrics as optimal transport (OT) problems. Doing so, we can exploit recent computational advances: the OT problem on a planar grid, when regularized with entropy, is convex can be solved in linear time using the Sinkhorn algorithm and convolutions. This results in scalable computations that can stream on GPUs. We demonstrate the efficiency of our approach by carrying out clustering with diagrams metrics on several thousands of PDs, a scale never seen before in the literature.",https://proceedings.neurips.cc/paper_files/paper/2018/file/b58f7d184743106a8a66028b7a28937c-Paper.pdf,2018,"pds, clustering, diagrams, framework, metrics, ot, scale, several, advances, algorithm","diagrams, et, al, pds, sinkhorn, cost, transport, algorithm, persistence, diagram","{'clusters': 0.36628585982816053, 'diagrams': 0.36628585982816053, 'persistence': 0.36628585982816053, 'means': 0.33884699163690424, 'transport': 0.33884699163690424, 'computation': 0.31360086647738483, 'scale': 0.2918214115324427, 'large': 0.267137749576573, 'optimal': 0.25239644435284697, 'using': 0.2220307571759495}","pds, diagrams, ot, metrics, clustering, barycenters, hilbertian, planar, scale, carry"
Probabilistic Matrix Factorization for Automated Machine Learning,"Nicolo Fusi, Rishit Sheth, Melih Elibol","In order to achieve state-of-the-art performance, modern machine learning techniques require careful data pre-processing and hyperparameter tuning. Moreover, given the ever increasing number of machine learning models being developed, model selection is becoming increasingly important. Automating the selection and tuning of machine learning pipelines, which can include different data pre-processing methods and machine learning models, has long been one of the goals of the machine learning community. 
In this paper, we propose to solve this meta-learning task by combining ideas from collaborative filtering and Bayesian optimization. Specifically, we use a probabilistic matrix factorization model to transfer knowledge across experiments performed in hundreds of different datasets and use an acquisition function to guide the exploration of the space of possible ML pipelines. In our experiments, we show that our approach quickly identifies high-performing pipelines across a wide range of datasets, significantly outperforming the current state-of-the-art.",https://proceedings.neurips.cc/paper_files/paper/2018/file/b59a51a3c0bf9c5228fde841714f523a-Paper.pdf,2018,"learning, machine, pipelines, across, art, data, datasets, different, experiments, model","model, pipelines, dataset, pipeline, datasets, performance, random, learning, space, function","{'automated': 0.5105069082615762, 'factorization': 0.4470745147291054, 'machine': 0.42037228111700314, 'probabilistic': 0.41323694758714424, 'matrix': 0.4007305541038442, 'learning': 0.17732588170347477}","pipelines, machine, pre, learning, tuning, selection, processing, across, automating, becoming"
REFUEL: Exploring Sparse Features in Deep Reinforcement Learning for Fast Disease Diagnosis,"Yu-Shao Peng, Kai-Fu Tang, Hsuan-Tien Lin, Edward Chang","This paper proposes REFUEL, a reinforcement learning method with two techniques: {\em reward shaping} and {\em feature rebuilding}, to improve the performance of online symptom checking for disease diagnosis. Reward shaping can guide the search of policy towards better directions. Feature rebuilding can guide the agent to learn correlations between features. Together, they can find symptom queries that can yield positive responses from a patient with high probability. Experimental results justify that the two techniques in REFUEL allows the symptom checker to identify the disease more rapidly and accurately.",https://proceedings.neurips.cc/paper_files/paper/2018/file/b5a1d925221b37e2e399f7b319038ba0-Paper.pdf,2018,"symptom, disease, em, feature, guide, rebuilding, refuel, reward, shaping, techniques","reward, agent, symptoms, positive, state, feature, symptom, disease, st, refuel","{'diagnosis': 0.40512011933828984, 'disease': 0.40512011933828984, 'exploring': 0.40512011933828984, 'refuel': 0.40512011933828984, 'features': 0.2960023982312922, 'sparse': 0.27326577362062127, 'fast': 0.26577794523013504, 'reinforcement': 0.2385528924889001, 'deep': 0.18982123369726264, 'learning': 0.13282188596101815}","symptom, rebuilding, refuel, shaping, disease, guide, em, reward, feature, checker"
Cluster Variational Approximations for Structure Learning of Continuous-Time Bayesian Networks from Incomplete Data,"Dominik Linzner, Heinz Koeppl","Continuous-time Bayesian networks (CTBNs) constitute a general and powerful framework for modeling continuous-time stochastic processes on networks. This makes them particularly attractive for learning the directed structures among interacting entities. However, if the available data is incomplete, one needs to simulate the prohibitively complex CTBN dynamics. Existing approximation techniques, such as sampling and low-order variational methods, either scale unfavorably in system size, or are unsatisfactory in terms of accuracy. Inspired by recent advances in statistical physics, we present a new approximation scheme based on cluster-variational methods  that significantly improves upon existing variational approximations. We can analytically marginalize the parameters of the approximate CTBN, as these are of secondary importance for structure learning. This recovers a scalable scheme for direct structure learning from incomplete and noisy time-series data. Our approach outperforms existing methods in terms of scalability.",https://proceedings.neurips.cc/paper_files/paper/2018/file/b607aa5b2fd58dd860bfb55619389982-Paper.pdf,2018,"existing, learning, methods, time, variational, approximation, continuous, ctbn, data, incomplete","variational, approximation, dynamics, ctbn, time, lower, state, bound, structure, 10","{'incomplete': 0.42930210802263435, 'cluster': 0.40520831359291454, 'approximations': 0.3548596644473391, 'continuous': 0.32283108633470076, 'structure': 0.3057362671353137, 'time': 0.2895772563885124, 'variational': 0.2609451803310427, 'bayesian': 0.24562430207716004, 'data': 0.24298286238727512, 'networks': 0.19171859252792678}","ctbn, incomplete, variational, scheme, existing, time, continuous, constitute, ctbns, unfavorably"
Norm-Ranging LSH for Maximum Inner Product Search,"Xiao Yan, Jinfeng Li, Xinyan Dai, Hongzhi Chen, James Cheng","Neyshabur and Srebro proposed SIMPLE-LSH, which is the state-of-the-art hashing based algorithm for maximum inner product search (MIPS). We found that the performance of SIMPLE-LSH, in both theory and practice, suffers from long tails in the 2-norm distribution of real datasets. We propose NORM-RANGING LSH, which addresses the excessive normalization problem caused by long tails by partitioning a dataset into sub-datasets and building a hash index for each sub-dataset independently. We prove that NORM-RANGING LSH achieves lower query time complexity than SIMPLE-LSH under mild conditions. We also show that the idea of dataset partitioning can improve another hashing based MIPS algorithm. Experiments show that NORM-RANGING LSH probes much less items than SIMPLE-LSH at the same recall, thus significantly benefiting MIPS based applications.",https://proceedings.neurips.cc/paper_files/paper/2018/file/b60c5ab647a27045b462934977ccad9a-Paper.pdf,2018,"lsh, norm, simple, based, dataset, mips, ranging, algorithm, datasets, hashing","lsh, sub, dataset, items, datasets, simple, norm, range, query, alsh","{'lsh': 0.4148691211039525, 'ranging': 0.4148691211039525, 'inner': 0.3915853516271202, 'norm': 0.3750652542780833, 'product': 0.3750652542780833, 'maximum': 0.351781484801251, 'search': 0.3119776179753818}","lsh, mips, norm, ranging, tails, hashing, partitioning, simple, dataset, sub"
Generalizing Point Embeddings using the Wasserstein Space of Elliptical Distributions,"Boris Muzellec, Marco Cuturi","Embedding complex objects as vectors in low dimensional spaces is a longstanding problem in machine learning. We propose in this work an extension of that approach, which consists in embedding objects as elliptical probability distributions, namely distributions whose densities have elliptical level sets. We endow these measures with the 2-Wasserstein metric, with two important benefits: (i) For such measures, the squared 2-Wasserstein metric has a closed form, equal to a weighted sum of the squared Euclidean distance between means and the squared Bures metric between covariance matrices. The latter is a Riemannian metric between positive semi-definite matrices, which turns out to be Euclidean on a suitable factor representation of such matrices, which is valid on the entire geodesic between these matrices. (ii) The 2-Wasserstein distance boils down to the usual Euclidean metric when comparing Diracs, and therefore provides a natural framework to extend point embeddings. We show that for these reasons Wasserstein elliptical embeddings are more intuitive and yield tools that are better behaved numerically than the alternative choice of Gaussian embeddings with the Kullback-Leibler divergence. In particular, and unlike previous work based on the KL geometry, we learn elliptical distributions that are not necessarily diagonal. We demonstrate the advantages of elliptical embeddings by using them for visualization, to compute embeddings of words, and to reflect entailment or hypernymy.",https://proceedings.neurips.cc/paper_files/paper/2018/file/b613e70fd9f59310cf0a8d33de3f2800-Paper.pdf,2018,"elliptical, embeddings, metric, matrices, wasserstein, distributions, euclidean, squared, distance, embedding","embeddings, elliptical, metric, distributions, using, scale, al, et, matrices, measures","{'elliptical': 0.440239681356478, 'generalizing': 0.3844040834055271, 'embeddings': 0.3639006254176758, 'distributions': 0.3485863906572036, 'space': 0.3485863906572036, 'wasserstein': 0.3363582103618023, 'point': 0.3261785104228557, 'using': 0.2518822117550726}","elliptical, embeddings, metric, wasserstein, matrices, squared, euclidean, distributions, measures, embedding"
Dirichlet-based Gaussian Processes for Large-scale Calibrated Classification,"Dimitrios Milios, Raffaello Camoriano, Pietro Michiardi, Lorenzo Rosasco, Maurizio Filippone","This paper studies the problem of deriving fast and accurate classification algorithms with uncertainty quantification. Gaussian process classification provides a principled approach, but the corresponding computational burden is hardly sustainable in large-scale problems and devising efficient alternatives is a challenge. In this work, we investigate if and how Gaussian process regression directly applied to classification labels can be used to tackle this question. While in this case training is remarkably faster, predictions need to be calibrated for classification and uncertainty estimation. To this aim, we propose a novel regression approach where the labels are obtained through the interpretation of classification labels as the coefficients of a degenerate Dirichlet distribution. Extensive experimental results show that the proposed approach provides essentially the same accuracy and uncertainty quantification as Gaussian process classification while requiring only a fraction of computational resources.",https://proceedings.neurips.cc/paper_files/paper/2018/file/b6617980ce90f637e68c3ebe8b9be745-Paper.pdf,2018,"classification, approach, gaussian, labels, process, uncertainty, computational, provides, quantification, regression","classiﬁcation, gp, class, gpc, regression, labels, likelihood, dirichlet, gpd, based","{'calibrated': 0.4371550664431396, 'dirichlet': 0.4371550664431396, 'scale': 0.34828319228005294, 'classification': 0.32984061207561716, 'large': 0.3188237206874545, 'processes': 0.31553543550642116, 'based': 0.30942533757657864, 'gaussian': 0.3012302589372253}","classification, quantification, uncertainty, labels, gaussian, process, regression, provides, devising, sustainable"
Latent Alignment and Variational Attention,"Yuntian Deng, Yoon Kim, Justin Chiu, Demi Guo, Alexander Rush","Neural attention has become central to many state-of-the-art models in natural language processing and related domains. Attention networks are an easy-to-train and effective method for softly simulating alignment; however, the approach does not marginalize over latent alignments in a probabilistic sense. This property makes it difficult to compare attention to other alignment approaches, to compose it with probabilistic models, and to perform posterior inference conditioned on observed data. A related latent approach, hard attention, fixes these issues, but is generally harder to train and less accurate. This work considers variational attention networks, alternatives to soft and hard attention for learning latent variable alignment models, with tighter approximation bounds based on amortized variational inference. We further propose methods for reducing the variance of gradients to make these approaches computationally feasible. Experiments show that for machine translation and visual question answering, inefficient exact latent variable models outperform standard neural attention, but these gains go away when using hard attention based training. On the other hand, variational attention retains most of the performance gain but with training speed comparable to neural attention.",https://proceedings.neurips.cc/paper_files/paper/2018/file/b691334ccf10d4ab144d672f7783c8a3-Paper.pdf,2018,"attention, latent, models, alignment, hard, neural, variational, approach, approaches, based","attention, variational, alignment, log, soft, models, latent, inference, hard, model","{'alignment': 0.5689479950283325, 'attention': 0.512653058337617, 'latent': 0.49713788984790835, 'variational': 0.40784671042917275}","attention, alignment, latent, hard, variational, models, variable, probabilistic, related, softly"
The Pessimistic Limits and Possibilities of Margin-based Losses in Semi-supervised Learning,"Jesse Krijthe, Marco Loog","Consider a classification problem where we have both labeled and unlabeled data available.  We show that for linear classifiers defined by convex margin-based surrogate losses that are decreasing,  it is impossible to construct \emph{any} semi-supervised approach that is able to guarantee an improvement over the supervised classifier measured by this surrogate loss on the labeled and unlabeled data. For convex margin-based loss functions that also increase, we demonstrate safe improvements \emph{are} possible.",https://proceedings.neurips.cc/paper_files/paper/2018/file/b6a1085a27ab7bff7550f8a3bd017df8-Paper.pdf,2018,"based, convex, data, emph, labeled, loss, margin, supervised, surrogate, unlabeled","supervised, wsup, semi, loss, safe, classiﬁer, unlabeled, losses, possible, φ0","{'pessimistic': 0.40737977547086085, 'possibilities': 0.40737977547086085, 'limits': 0.3682944603545024, 'losses': 0.3557117993210209, 'margin': 0.34543101668256304, 'semi': 0.32920914523814404, 'supervised': 0.2937630405327231, 'based': 0.2721668005329283, 'learning': 0.13356273237872182}","surrogate, unlabeled, margin, labeled, emph, supervised, convex, loss, decreasing, safe"
Porcupine Neural Networks: Approximating Neural Network Landscapes,"Soheil Feizi, Hamid Javadi, Jesse Zhang, David Tse","Neural networks have been used prominently in several machine learning and statistics applications. In general, the underlying optimization of neural networks is non-convex which makes analyzing their performance challenging. In this paper, we take another approach to this problem by constraining the network such that the corresponding optimization landscape has good theoretical properties without significantly compromising performance. In particular, for two-layer neural networks we introduce Porcupine Neural Networks (PNNs) whose weight vectors are constrained to lie over a finite set of lines. We show that most local optima of PNN optimizations are global while we have a characterization of regions where bad local optimizers may exist. Moreover, our theoretical and empirical results suggest that an unconstrained neural network can be approximated using a polynomially-large PNN.",https://proceedings.neurips.cc/paper_files/paper/2018/file/b6cda17abb967ed28ec9610137aa45f7-Paper.pdf,2018,"neural, networks, local, network, optimization, performance, pnn, theoretical, analyzing, another","pnn, neural, kl, network, neurons, function, networks, optimization, lines, unconstrained","{'landscapes': 0.484658162210528, 'porcupine': 0.484658162210528, 'approximating': 0.45745761063911433, 'neural': 0.4341158146540195, 'network': 0.29264794699689467, 'networks': 0.21643960972880918}","pnn, neural, networks, pnns, porcupine, prominently, local, compromising, optimizers, optimizations"
On the Local Hessian in Back-propagation,"Huishuai Zhang, Wei Chen, Tie-Yan Liu","Back-propagation (BP) is the foundation for successfully training deep neural networks. However, BP sometimes has difficulties in propagating a learning signal deep enough effectively, e.g., the vanishing gradient phenomenon. Meanwhile, BP often works well when combining with ``designing tricks'' like orthogonal initialization, batch normalization and skip connection. There is no clear understanding on what is essential to the efficiency of BP. In this paper, we take one step towards clarifying this problem. We view BP as a solution of back-matching propagation which minimizes a sequence of back-matching losses each corresponding to one block of the network. We study the Hessian of the local back-matching loss (local Hessian)  and connect it to the efficiency of BP. It turns out that those designing tricks facilitate BP by improving the spectrum of local Hessian. In addition, we can utilize the local Hessian to balance the training pace of each block and design new training algorithms. Based on a scalar approximation of local Hessian, we propose a scale-amended SGD algorithm. We apply it to train neural networks with batch normalization, and achieve favorable results over vanilla SGD. This corroborates the importance of local Hessian from another side.",https://proceedings.neurips.cc/paper_files/paper/2018/file/b6d67a24906e8a8541291882f81d31ca-Paper.pdf,2018,"bp, hessian, local, back, matching, training, batch, block, deep, designing","hessian, local, back, matching, block, zb, loss, zk, sgd, al","{'back': 0.5664034764209052, 'hessian': 0.5346151188281107, 'propagation': 0.48027256258971096, 'local': 0.4033758077057057}","bp, hessian, local, back, tricks, matching, designing, normalization, block, sgd"
Infinite-Horizon Gaussian Processes,"Arno Solin, James Hensman, Richard E. Turner","Gaussian processes provide a flexible framework for forecasting, removing noise, and interpreting long temporal datasets. State space modelling (Kalman filtering) enables these non-parametric models to be deployed on long datasets by reducing the complexity to linear in the number of data points. The complexity is still cubic in the state dimension m which is an impediment to practical application. In certain special cases (Gaussian likelihood, regular spacing) the GP posterior will reach a steady posterior state when the data are very long. We leverage this and formulate an inference scheme for GPs with general likelihoods, where inference is based on single-sweep EP (assumed density filtering). The infinite-horizon model tackles the cubic cost in the state dimensionality and reduces the cost in the state dimension m to O(m^2) per data point. The model is extended to online-learning of hyperparameters. We show examples for large finite-length modelling problems, and present how the method runs in real-time on a smartphone on a continuous data stream updated at 100 Hz.",https://proceedings.neurips.cc/paper_files/paper/2018/file/b865367fc4c0845c0682bd466e6ebf4c-Paper.pdf,2018,"state, data, long, complexity, cost, cubic, datasets, dimension, filtering, gaussian","state, data, time, gp, ihgp, likelihood, covariance, pp, model, space","{'infinite': 0.5858877025172842, 'horizon': 0.5611704299202089, 'processes': 0.4228896004244148, 'gaussian': 0.40371739431818543}","state, cubic, filtering, long, modelling, dimension, data, posterior, gaussian, impediment"
Constrained Graph Variational Autoencoders for Molecule Design,"Qi Liu, Miltiadis Allamanis, Marc Brockschmidt, Alexander Gaunt","Graphs are ubiquitous data structures for representing interactions between entities. With an emphasis on applications in chemistry, we explore the task of learning to generate graphs that conform to a distribution observed in training data. We propose a variational autoencoder model in which both encoder and decoder are graph-structured. Our decoder assumes a sequential ordering of graph extension steps and we discuss and analyze design choices that mitigate the potential downsides of this linearization. Experiments compare our approach with a wide range of baselines on the molecule generation task and show that our method is successful at matching the statistics of the original dataset on semantically important metrics. Furthermore, we show that by using appropriate shaping of the latent space, our model allows us to design molecules that are (locally) optimal in desired properties.",https://proceedings.neurips.cc/paper_files/paper/2018/file/b8a03c5c15fcfa8dae0b03351eb1742f-Paper.pdf,2018,"data, decoder, design, graph, graphs, model, show, task, allows, analyze","graph, node, model, generation, molecules, edge, graphs, molecule, focus, using","{'molecule': 0.5212121807781871, 'design': 0.4419533426897457, 'constrained': 0.40510008169575096, 'autoencoders': 0.3919465835062453, 'graph': 0.3389947417968151, 'variational': 0.316811410804275}","decoder, graphs, graph, design, downsides, molecule, conform, emphasis, linearization, chemistry"
Hardware Conditioned Policies for Multi-Robot Transfer Learning,"Tao Chen, Adithyavairavan Murali, Abhinav Gupta","Deep reinforcement learning could be used to learn dexterous robotic policies but it is challenging to transfer them to new robots with vastly different hardware properties. It is also prohibitively expensive to learn a new policy from scratch for each robot hardware due to the high sample complexity of modern state-of-the-art algorithms. We propose a novel approach called Hardware Conditioned Policies where we train a universal policy conditioned on a vector representation of robot hardware. We considered robots in simulation with varied dynamics, kinematic structure, kinematic lengths and degrees-of-freedom. First, we use the kinematic structure directly as the hardware encoding and show great zero-shot transfer to completely novel robots not seen during training. For robots with lower zero-shot success rate, we also demonstrate that fine-tuning the policy network is significantly more sample-efficient than training a model from scratch. In tasks where knowing the agent dynamics is important for success, we learn an embedding for robot hardware and show that policies conditioned on the encoding of hardware tend to generalize and transfer well. Videos of experiments are available at: https://sites.google.com/view/robot-transfer-hcp.",https://proceedings.neurips.cc/paper_files/paper/2018/file/b8cfbf77a3d250a4523ba67a65a7d031-Paper.pdf,2018,"hardware, robot, robots, transfer, conditioned, kinematic, learn, policies, policy, also","robot, policy, hcp, dynamics, hardware, robots, dof, learning, vh, different","{'hardware': 0.47857677215042155, 'robot': 0.45171752745641885, 'conditioned': 0.43266059006885216, 'policies': 0.3789421006808467, 'transfer': 0.3545828903519534, 'multi': 0.2928736353563305, 'learning': 0.1569052397545233}","hardware, robot, robots, kinematic, transfer, conditioned, policies, scratch, policy, encoding"
Learning without the Phase: Regularized PhaseMax Achieves Optimal Sample Complexity,"Fariborz Salehi, Ehsan Abbasi, Babak Hassibi","The problem of estimating an unknown signal, $\mathbf x_0\in \mathbb R^n$, from a vector $\mathbf y\in \mathbb R^m$ consisting of $m$ magnitude-only measurements of the form $y_i=|\mathbf a_i\mathbf x_0|$, where  $\mathbf a_i$'s are the rows of a known measurement matrix $\mathbf A$ is a classical problem known as phase retrieval. This problem arises when measuring the phase is costly or altogether infeasible. In many applications in machine learning, signal processing, statistics, etc., the underlying signal has certain structure (sparse, low-rank, finite alphabet, etc.), opening of up the possibility of recovering $\mathbf x_0$ from a number of measurements smaller than the ambient dimension, i.e., $m",https://proceedings.neurips.cc/paper_files/paper/2018/file/b91f4f4d36fa98a94ac5584af95594a0-Paper.pdf,2018,"mathbf, problem, signal, x_0, a_i, etc, known, mathbb, measurements, phase","x0, signal, phase, measurements, phasemax, recovery, function, regularized, convex, transition","{'phasemax': 0.41883025994069645, 'achieves': 0.3953241788025502, 'phase': 0.36571001886727816, 'regularized': 0.3384624376985275, 'without': 0.3384624376985275, 'complexity': 0.3316341865433195, 'sample': 0.3200006783689904, 'optimal': 0.2724058666727754, 'learning': 0.13731686570820215}","mathbf, x_0, a_i, signal, mathbb, etc, phase, measurements, ambient, rows"
Learning Disentangled Joint Continuous and Discrete Representations,Emilien Dupont,"We present a framework for learning disentangled and interpretable jointly continuous and discrete representations in an unsupervised manner. By augmenting the continuous latent distribution of variational autoencoders with a relaxed discrete distribution and controlling the amount of information encoded in each latent unit, we show how continuous and categorical factors of variation can be discovered automatically from data. Experiments show that the framework disentangles continuous and discrete generative factors on various datasets and outperforms current disentangling methods when a discrete generative factor is prominent.",https://proceedings.neurips.cc/paper_files/paper/2018/file/b9228e0962a78b84f3d5d92f4faa000b-Paper.pdf,2018,"continuous, discrete, distribution, factors, framework, generative, latent, show, amount, augmenting","latent, discrete, continuous, model, al, et, qφ, factors, data, variables","{'disentangled': 0.4772588724788838, 'discrete': 0.4634651362587287, 'joint': 0.4634651362587287, 'continuous': 0.41102433036327823, 'representations': 0.3762579108156265, 'learning': 0.17920124994993583}","discrete, continuous, factors, augmenting, latent, generative, disentangles, prominent, relaxed, discovered"
Attacks Meet Interpretability: Attribute-steered Detection of Adversarial Samples,"Guanhong Tao, Shiqing Ma, Yingqi Liu, Xiangyu Zhang","Adversarial sample attacks perturb benign inputs to induce DNN misbehaviors. Recent research has demonstrated the widespread presence and the devastating consequences of such attacks. Existing defense techniques either assume prior knowledge of specific attacks or may not work well on complex models due to their underlying assumptions. We argue that adversarial sample attacks are deeply entangled with interpretability of DNN models: while classification results on benign inputs can be reasoned based on the human perceptible features/attributes, results on adversarial samples can hardly be explained. Therefore, we propose a novel adversarial sample detection technique for face recognition models, based on interpretability. It features a novel bi-directional correspondence inference between attributes and internal neurons to identify neurons critical for individual attributes. The activation values of critical neurons are enhanced to amplify the reasoning part of the computation and the values of other neurons are weakened to suppress the uninterpretable part. The classification results after such transformation are compared with those of the original model to detect adversaries. Results show that our technique can achieve 94% detection accuracy for 7 different kinds of attacks with 9.91% false positives on benign inputs. In contrast, a state-of-the-art feature squeezing technique can only achieve 55% accuracy with 23.3% false positives.",https://proceedings.neurips.cc/paper_files/paper/2018/file/b994697479c5716eda77e8e9713e5f0f-Paper.pdf,2018,"attacks, adversarial, neurons, results, attributes, benign, inputs, models, sample, technique","attribute, neurons, adversarial, witnesses, model, samples, face, attributes, layer, witness","{'meet': 0.4203669424286299, 'steered': 0.4203669424286299, 'attribute': 0.38003559708795204, 'interpretability': 0.3564432724600206, 'samples': 0.34747388691395076, 'attacks': 0.33285094783208913, 'detection': 0.28937365624244754, 'adversarial': 0.2446210076904149}","attacks, neurons, benign, attributes, adversarial, inputs, positives, false, technique, dnn"
Learning To Learn Around A Common Mean,"Giulia Denevi, Carlo Ciliberto, Dimitris Stamos, Massimiliano Pontil","The problem of learning-to-learn (LTL) or meta-learning is gaining increasing attention due to recent empirical evidence of its effectiveness in applications. The goal addressed in LTL is to select an algorithm that works well on tasks sampled from a meta-distribution. In this work, we consider the family of algorithms given by a variant of Ridge Regression, in which the regularizer is the square distance to an unknown mean vector. We show that, in this setting, the LTL problem can be reformulated as a Least Squares (LS) problem and we exploit a novel meta- algorithm to efficiently solve it. At each iteration the meta-algorithm processes only one dataset. Specifically, it firstly estimates the stochastic LS objective function, by splitting this dataset into two subsets used to train and test the inner algorithm, respectively. Secondly, it performs a stochastic gradient step with the estimated value. Under specific assumptions, we present a bound for the generalization error of our meta-algorithm, which suggests the right splitting parameter to choose. When the hyper-parameters of the problem are fixed, this bound is consistent as the number of tasks grows, even if the sample size is kept constant. Preliminary experiments confirm our theoretical findings, highlighting the advantage of our approach, with respect to independent task learning.",https://proceedings.neurips.cc/paper_files/paper/2018/file/b9a25e422ba96f7572089a00b838c3f8-Paper.pdf,2018,"algorithm, meta, problem, learning, ltl, bound, dataset, ls, splitting, stochastic","algorithm, ltl, problem, tasks, risk, ex, environment, learning, en, transfer","{'around': 0.531083561475022, 'common': 0.531083561475022, 'learn': 0.45032361847534913, 'mean': 0.45032361847534913, 'learning': 0.17412001248721917}","meta, ltl, ls, splitting, algorithm, problem, dataset, gaining, kept, reformulated"
Recurrent Relational Networks,"Rasmus Palm, Ulrich Paquet, Ole Winther","This paper is concerned with learning to solve tasks that require a chain of interde-
pendent steps of relational inference, like answering complex questions about the
relationships between objects, or solving puzzles where the smaller elements of a
solution mutually constrain each other. We introduce the recurrent relational net-
work, a general purpose module that operates on a graph representation of objects.
As a generalization of Santoro et al. [2017]’s relational network, it can augment
any neural network model with the capacity to do many-step relational reasoning.
We achieve state of the art results on the bAbI textual question-answering dataset
with the recurrent relational network, consistently solving 20/20 tasks. As bAbI is
not particularly challenging from a relational reasoning point of view, we introduce
Pretty-CLEVR, a new diagnostic dataset for relational reasoning. In the Pretty-
CLEVR set-up, we can vary the question to control for the number of relational
reasoning steps that are required to obtain the answer. Using Pretty-CLEVR, we
probe the limitations of multi-layer perceptrons, relational and recurrent relational
networks. Finally, we show how recurrent relational networks can learn to solve
Sudoku puzzles from supervised training data, a challenging task requiring upwards
of 64 steps of relational reasoning. We achieve state-of-the-art results amongst
comparable methods by solving 96.6% of the hardest Sudoku puzzles.",https://proceedings.neurips.cc/paper_files/paper/2018/file/b9f94c77652c9a76fc8a442748cd54bd-Paper.pdf,2018,"relational, reasoning, recurrent, clevr, network, pretty, puzzles, solving, steps, 20","relational, network, al, et, reasoning, steps, step, node, sudoku, graph","{'relational': 0.7400119561365287, 'recurrent': 0.5560003400282697, 'networks': 0.3784784361934464}","relational, reasoning, puzzles, clevr, pretty, recurrent, babi, sudoku, steps, solving"
Experimental Design for Cost-Aware Learning of Causal Graphs,"Erik Lindgren, Murat Kocaoglu, Alexandros G. Dimakis, Sriram Vishwanath","We consider the minimum cost intervention design problem: Given the essential graph of a causal graph and a cost to intervene on a variable, identify the set of interventions with minimum total cost that can learn any causal graph with the given essential graph. We first show that this problem is NP-hard. We then prove that we can achieve a constant factor approximation to this problem with a greedy algorithm. We then constrain the sparsity of each intervention. We develop an algorithm that returns an intervention design that is nearly optimal in terms of size for sparse graphs with sparse interventions and we discuss how to use it when there are costs on the vertices.",https://proceedings.neurips.cc/paper_files/paper/2018/file/ba3e9b6a519cfddc560b5d53210df1bd-Paper.pdf,2018,"graph, cost, intervention, problem, algorithm, causal, design, essential, given, interventions","graph, cost, intervention, minimum, algorithm, design, problem, size, causal, separating","{'cost': 0.4623323903604186, 'experimental': 0.4428276359955516, 'design': 0.4153372287213912, 'aware': 0.38070341150114856, 'graphs': 0.35789069929436285, 'causal': 0.3447269823555286, 'learning': 0.16059233956287458}","intervention, graph, interventions, cost, essential, causal, minimum, design, sparse, intervene"
Reinforcement Learning with Multiple Experts: A Bayesian Model Combination Approach,"Michael Gimelfarb, Scott Sanner, Chi-Guhn Lee","Potential based reward shaping is a powerful technique for accelerating convergence of reinforcement learning algorithms. Typically, such information includes an estimate of the optimal value function and is often provided by a human expert or other sources of domain knowledge. However, this information is often biased or inaccurate and can mislead many reinforcement learning algorithms. In this paper, we apply Bayesian Model Combination with multiple experts in a way that learns to trust a good combination of experts as training progresses. This approach is both computationally efficient and general, and is shown numerically to improve convergence across discrete and continuous domains and different reinforcement learning algorithms.",https://proceedings.neurips.cc/paper_files/paper/2018/file/ba4002d88b8860b6a684ade8357aba56-Paper.pdf,2018,"algorithms, learning, reinforcement, combination, convergence, experts, information, often, accelerating, across","αt, learning, reward, policy, posterior, using, 100, expert, al, bayesian","{'combination': 0.48666964322485406, 'experts': 0.48666964322485406, 'multiple': 0.3659709250719947, 'approach': 0.33865748281322405, 'model': 0.3042466151453929, 'reinforcement': 0.28657290896205845, 'bayesian': 0.27844701720622067, 'learning': 0.15955855255641826}","experts, reinforcement, combination, algorithms, mislead, progresses, often, shaping, convergence, accelerating"
Differentiable MPC for End-to-end Planning and Control,"Brandon Amos, Ivan Jimenez, Jacob Sacks, Byron Boots, J. Zico Kolter","We present foundations for using Model Predictive Control (MPC) as a differentiable policy class for reinforcement learning. This provides one way of leveraging and combining the advantages of model-free and model-based approaches. Specifically, we differentiate through MPC by using the KKT conditions of the convex approximation at a fixed point of the controller. Using this strategy, we are able to learn the cost and dynamics of a controller via end-to-end learning. Our experiments focus on imitation learning in the pendulum and cartpole domains, where we learn the cost and dynamics terms of an MPC policy class. We show that our MPC policies are significantly more data-efficient than a generic neural network and that our method is superior to traditional system identification in a setting where the expert is unrealizable.",https://proceedings.neurips.cc/paper_files/paper/2018/file/ba6d843eb4251a4526ce65d1807a9309-Paper.pdf,2018,"mpc, learning, model, using, class, controller, cost, dynamics, end, learn","mpc, model, al, et, learning, dynamics, loss, cost, imitation, control","{'end': 0.6996502419670692, 'mpc': 0.4125619077550915, 'planning': 0.36023668177747736, 'differentiable': 0.33339689691980456, 'control': 0.315211442386057}","mpc, controller, dynamics, end, cartpole, unrealizable, policy, cost, kkt, pendulum"
Zeroth-Order Stochastic Variance Reduction for Nonconvex Optimization,"Sijia Liu, Bhavya Kailkhura, Pin-Yu Chen, Paishun Ting, Shiyu Chang, Lisa Amini","As application demands for zeroth-order (gradient-free) optimization accelerate, the need for variance reduced and faster converging approaches is also intensifying. This paper addresses these challenges by  presenting: a) a comprehensive theoretical analysis of variance reduced zeroth-order (ZO) optimization, b) a novel variance reduced ZO algorithm, called ZO-SVRG, and c) an experimental evaluation of our approach in the context of two compelling applications, black-box chemical material classification and generation of adversarial examples from black-box deep neural network models. Our theoretical analysis uncovers an essential difficulty in the analysis of ZO-SVRG: the unbiased assumption on gradient estimates no longer holds. We prove that compared to its first-order counterpart, ZO-SVRG with a two-point random gradient estimator could suffer an additional error of order $O(1/b)$, where $b$ is the mini-batch size. To mitigate this error, we propose two accelerated versions of ZO-SVRG utilizing 
 variance reduced gradient estimators, which achieve  the best rate  known for ZO stochastic optimization (in terms of iterations). Our extensive experimental results show that our approaches outperform other state-of-the-art ZO algorithms, and strike a balance  between the convergence rate and the function query complexity.",https://proceedings.neurips.cc/paper_files/paper/2018/file/ba9a56ce0a9bfa26e8ed9e10b2cc8f46-Paper.pdf,2018,"zo, gradient, order, reduced, svrg, variance, analysis, optimization, two, approaches","zo, svrg, gradient, convergence, randgradest, fi, rate, function, optimization, algorithms","{'zeroth': 0.4839693654373215, 'variance': 0.4059979010260762, 'reduction': 0.3917557628750675, 'order': 0.38558033178413614, 'nonconvex': 0.3746398440705936, 'stochastic': 0.2933667292218823, 'optimization': 0.2690270192684037}","zo, svrg, reduced, variance, zeroth, order, gradient, black, box, analysis"
Near-Optimal Time and Sample Complexities for Solving Markov Decision Processes with a Generative Model,"Aaron Sidford, Mengdi Wang, Xian Wu, Lin Yang, Yinyu Ye","In this paper we consider the problem of computing an $\epsilon$-optimal policy of a discounted Markov Decision Process (DMDP) provided we can only access its transition function through a generative sampling model that given any state-action pair samples from the transition function in $O(1)$ time. Given such a DMDP with states $\states$, actions $\actions$, discount factor $\gamma\in(0,1)$, and rewards in range $[0, 1]$ we provide an algorithm which computes an $\epsilon$-optimal policy with probability $1 - \delta$ where {\it both} the run time spent and number of sample taken is upper bounded by 
\[
O\left[\frac{|\cS||\cA|}{(1-\gamma)^3 \epsilon^2} \log \left(\frac{|\cS||\cA|}{(1-\gamma)\delta \epsilon}
		\right) 
		\log\left(\frac{1}{(1-\gamma)\epsilon}\right)\right] ~.
\]
For fixed values of $\epsilon$, this improves upon the previous best known bounds by a factor of $(1 - \gamma)^{-1}$ and matches the sample complexity lower bounds proved in \cite{azar2013minimax} up to logarithmic factors. 
We also extend our method to computing $\epsilon$-optimal policies for finite-horizon MDP with a generative model and provide a nearly matching sample complexity lower bound.",https://proceedings.neurips.cc/paper_files/paper/2018/file/bb03e43ffe34eeb242a2ee4a4f125e56-Paper.pdf,2018,"epsilon, gamma, frac, left, optimal, right, sample, actions, bounds, ca","policy, algorithm, optimal, value, error, sample, state, samples, time, vector","{'complexities': 0.3975616519581451, 'near': 0.3371058994550512, 'solving': 0.3371058994550512, 'markov': 0.31479347998071133, 'sample': 0.30375073266701497, 'decision': 0.2989625664262971, 'processes': 0.27085224233728183, 'time': 0.2681673587618501, 'optimal': 0.25857283180268087, 'model': 0.248539822862942}","epsilon, gamma, left, frac, right, ca, cs, dmdp, delta, transition"
Algebraic tests of general Gaussian latent tree models,"Dennis Leung, Mathias Drton","We consider general Gaussian latent tree models in which the observed variables are not restricted to be leaves of the tree. Extending related recent work, we give a full semi-algebraic description of the set of covariance matrices of any such model.  In other words, we find polynomial constraints that characterize when a matrix is the covariance matrix of a distribution in a given latent tree model. However, leveraging these constraints to test a given such model is often complicated by the number of constraints being large and by singularities of individual polynomials, which may invalidate standard approximations to relevant probability distributions. Illustrating with the star tree, we propose a new testing methodology that circumvents singularity issues by trading off some statistical estimation efficiency and handles cases with many constraints through recent advances on Gaussian approximation for maxima of sums of high-dimensional random vectors. Our test avoids the need to maximize the possibly multimodal likelihood function of such models and is applicable to models with larger number of variables.  These points are illustrated in numerical experiments.",https://proceedings.neurips.cc/paper_files/paper/2018/file/bbb001ba009ed11717eaec9305b2feb6-Paper.pdf,2018,"constraints, tree, model, models, covariance, gaussian, given, latent, matrix, number","test, tree, model, latent, variables, pht, one, observed, al, et","{'algebraic': 0.4795256578139518, 'tests': 0.4526131586502245, 'tree': 0.3796934382876186, 'general': 0.3663740433117898, 'latent': 0.35528593036719747, 'gaussian': 0.3118819600740834, 'models': 0.25159685229931505}","tree, constraints, covariance, variables, test, matrix, gaussian, circumvents, singularities, singularity"
Binary Classification from Positive-Confidence Data,"Takashi Ishida, Gang Niu, Masashi Sugiyama","Can we learn a binary classifier from only positive data, without any negative data or unlabeled data?  We show that if one can equip positive data with confidence (positive-confidence), one can successfully learn a binary classifier, which we name positive-confidence (Pconf) classification.  Our work is related to one-class classification which is aimed at ""describing"" the positive class by clustering-related methods, but one-class classification does not have the ability to tune hyper-parameters and their aim is not on ""discriminating"" positive and negative classes.  For the Pconf classification problem, we provide a simple empirical risk minimization framework that is model-independent and optimization-independent.  We theoretically establish the consistency and an estimation error bound, and demonstrate the usefulness of the proposed method for training deep neural networks through experiments.",https://proceedings.neurips.cc/paper_files/paper/2018/file/bd1354624fbae3b2149878941c60df99-Paper.pdf,2018,"positive, classification, data, one, class, confidence, binary, classifier, independent, learn","positive, classiﬁcation, data, conﬁdence, method, pconf, negative, class, used, fully","{'positive': 0.5077923182061268, 'binary': 0.49044375800990947, 'confidence': 0.49044375800990947, 'classification': 0.4000132180173886, 'data': 0.31790914966457917}","positive, confidence, pconf, classification, one, negative, class, binary, classifier, data"
Transfer Learning with Neural AutoML,"Catherine Wong, Neil Houlsby, Yifeng Lu, Andrea Gesmundo","We reduce the computational cost of Neural AutoML with transfer learning. AutoML relieves human effort by automating the design of ML algorithms. Neural AutoML has become popular for the design of deep learning architectures, however, this method has a high computation cost. To address this we propose Transfer Neural AutoML that uses knowledge from prior tasks to speed up network design. We extend RL-based architecture search methods to support parallel training on multiple tasks and then transfer the search strategy to new tasks.
On language and image classification data, Transfer Neural AutoML reduces convergence time over single-task training by over an order of magnitude on many tasks.",https://proceedings.neurips.cc/paper_files/paper/2018/file/bdb3c278f45e6734c35733d24299d3f4-Paper.pdf,2018,"automl, neural, tasks, transfer, design, cost, learning, search, training, address","task, automl, controller, tasks, naml, transfer, accuracy, models, neural, learning","{'automl': 0.7338242814874185, 'transfer': 0.543698628688294, 'neural': 0.32864888142792864, 'learning': 0.2405901864127335}","automl, transfer, design, tasks, neural, search, relieves, cost, automating, ml"
"Loss Surfaces, Mode Connectivity, and Fast Ensembling of DNNs","Timur Garipov, Pavel Izmailov, Dmitrii Podoprikhin, Dmitry P. Vetrov, Andrew G. Wilson","The loss functions of deep neural networks are complex and their geometric properties are not well understood.  We show that the optima of these complex loss functions are in fact connected by simple curves, over which training and test accuracy are nearly constant.  We introduce a training procedure to discover these high-accuracy pathways between modes.  Inspired by this new geometric insight, we also propose a new ensembling method entitled Fast Geometric Ensembling (FGE). Using FGE we can train high-performing ensembles in the time required to train a single model.  We achieve improved performance compared to the recent state-of-the-art Snapshot Ensembles, on  CIFAR-10, CIFAR-100, and ImageNet.",https://proceedings.neurips.cc/paper_files/paper/2018/file/be3087e74e9100d4bc4c6268cdbe8456-Paper.pdf,2018,"geometric, accuracy, cifar, complex, ensembles, ensembling, fge, functions, high, loss","loss, networks, two, cifar, fge, resnet, ensembling, training, 10, 100","{'connectivity': 0.41732747768363126, 'ensembling': 0.41732747768363126, 'mode': 0.41732747768363126, 'surfaces': 0.41732747768363126, 'dnns': 0.36439783448102453, 'loss': 0.30920260211425715, 'fast': 0.2737865492535841}","geometric, ensembling, fge, ensembles, cifar, complex, entitled, snapshot, train, loss"
Fairness Behind a Veil of Ignorance: A Welfare Analysis for Automated Decision Making,"Hoda Heidari, Claudio Ferrari, Krishna Gummadi, Andreas Krause","We draw attention to an important, yet largely overlooked aspect of evaluating fairness for automated decision making systems---namely risk and welfare considerations. Our proposed family of measures corresponds to the long-established formulations of cardinal social welfare in economics, and is justified by the Rawlsian conception of fairness behind a veil of ignorance. The convex formulation of our welfare-based measures of fairness allows us to integrate them as a constraint into any convex loss minimization pipeline. Our empirical analysis reveals interesting trade-offs between our proposal and (a) prediction accuracy, (b) group discrimination, and (c) Dwork et al's notion of individual fairness. Furthermore and perhaps most importantly, our work provides both heuristic justification and empirical evidence suggesting that a lower-bound on our measures often leads to bounded inequality in algorithmic outcomes; hence presenting the first computationally feasible mechanism for bounding individual-level inequality.",https://proceedings.neurips.cc/paper_files/paper/2018/file/be3159ad04564bfb90db9e32851ebf9c-Paper.pdf,2018,"fairness, measures, welfare, convex, empirical, individual, inequality, accuracy, al, algorithmic","beneﬁt, fairness, welfare, individual, measures, social, al, et, b0, inequality","{'ignorance': 0.3693053147751204, 'veil': 0.3693053147751204, 'welfare': 0.3693053147751204, 'automated': 0.3485787304660455, 'behind': 0.3485787304660455, 'making': 0.33387298486760886, 'fairness': 0.30526651891681056, 'decision': 0.2777140706510224, 'analysis': 0.25698748634194746}","fairness, welfare, measures, inequality, individual, conception, considerations, ignorance, rawlsian, veil"
A Unified View of Piecewise Linear Neural Network Verification,"Rudy R. Bunel, Ilker Turkaslan, Philip Torr, Pushmeet Kohli, Pawan K. Mudigonda","The success of Deep Learning and its potential use in many safety-critical
  applications has motivated research on formal verification of Neural Network
  (NN) models. Despite the reputation of learned NN models to behave as black
  boxes and the theoretical hardness of proving their properties, researchers
  have been successful in verifying some classes of models by exploiting their
  piecewise linear structure and taking insights from formal methods such as
  Satisifiability Modulo Theory. These methods are however still far from
  scaling to realistic neural networks. To facilitate progress on this crucial
  area, we make two key contributions. First, we present a unified framework
  that encompasses previous methods. This analysis results in the identification
  of new methods that combine the strengths of multiple existing approaches,
  accomplishing a speedup of two orders of magnitude compared to the previous
  state of the art. Second, we propose a new data set of benchmarks which
  includes a collection of previously released testcases. We use the benchmark
  to provide the first experimental comparison of existing algorithms and
  identify the factors impacting the hardness of verification problems.",https://proceedings.neurips.cc/paper_files/paper/2018/file/be53d253d6bc3258a8160556dda3e9b2-Paper.pdf,2018,"methods, models, existing, first, formal, hardness, neural, new, nn, previous","xi, bound, bounds, problem, veriﬁcation, methods, linear, network, lower, 10","{'piecewise': 0.48623080926727064, 'verification': 0.4395802744499068, 'unified': 0.4122914609431624, 'view': 0.4122914609431624, 'linear': 0.3110632991123097, 'network': 0.2935975481145305, 'neural': 0.21776222947758206}","nn, hardness, formal, verification, methods, accomplishing, impacting, modulo, reputation, satisifiability"
Lifted Weighted Mini-Bucket,"Nicholas Gallo, Alexander T. Ihler","Many graphical models, such as Markov Logic Networks (MLNs) with evidence, possess highly symmetric substructures but no exact symmetries.  Unfortunately, there are few principled methods that exploit these symmetric substructures to perform efficient approximate inference.  In this paper, we present a lifted variant of the Weighted Mini-Bucket elimination algorithm which provides a principled way to (i) exploit the highly symmetric substructure of MLN models, and (ii) incorporate high-order inference terms which are necessary for high quality approximate inference.  Our method has significant control over the accuracy-time trade-off of the approximation, allowing us to generate any-time approximations.  Experimental results demonstrate the utility of this class of approximations, especially in models with strong repulsive potentials.",https://proceedings.neurips.cc/paper_files/paper/2018/file/bea6cfd50b4f5e3c735a972cf0eb8450-Paper.pdf,2018,"inference, models, symmetric, approximate, approximations, exploit, high, highly, principled, substructures","lifted, inference, lwmb, factors, order, bucket, ground, mini, domain, xv","{'bucket': 0.5118241680562461, 'lifted': 0.5118241680562461, 'mini': 0.5118241680562461, 'weighted': 0.4627181247591181}","symmetric, substructures, principled, inference, approximations, highly, exploit, bucket, mln, mlns"
Can We Gain More from Orthogonality Regularizations in Training Deep Networks?,"Nitin Bansal, Xiaohan Chen, Zhangyang Wang","This paper seeks to answer the question: as the (near-) orthogonality of weights is found to be a favorable property for training deep convolutional neural networks, how can we enforce it in more effective and easy-to-use ways? We develop novel orthogonality regularizations on training deep CNNs, utilizing various advanced analytical tools such as mutual coherence and restricted isometry property. These plug-and-play regularizations can be conveniently incorporated into training almost any CNN without extra hassle. We then benchmark their effects on state-of-the-art models: ResNet, WideResNet, and ResNeXt, on several most popular computer vision datasets: CIFAR-10, CIFAR-100, SVHN and ImageNet. We observe consistent performance gains after applying those proposed regularizations, in terms of both the final accuracies achieved, and faster and more stable convergences. We have made our codes and pre-trained models publicly available: https://github.com/nbansal90/Can-we-Gain-More-from-Orthogonality.",https://proceedings.neurips.cc/paper_files/paper/2018/file/bf424cb7b0dea050a42b9739eb261a3a-Paper.pdf,2018,"orthogonality, regularizations, training, cifar, deep, models, property, 10, 100, accuracies","training, orthogonality, 10, cifar, resnet, srip, norm, orthogonal, regularization, experiments","{'gain': 0.5106453573200739, 'orthogonality': 0.5106453573200739, 'regularizations': 0.5106453573200739, 'training': 0.32934991982435774, 'deep': 0.23926565747117284, 'networks': 0.22804502320581005}","orthogonality, regularizations, cifar, property, training, convergences, hassle, nbansal90, resnext, wideresnet"
Understanding the Role of Adaptivity in Machine Teaching: The Case of Version Space Learners,"Yuxin Chen, Adish Singla, Oisin Mac Aodha, Pietro Perona, Yisong Yue","In real-world applications of education, an effective teacher adaptively chooses the next example to teach based on the learner’s current state. However, most existing work in algorithmic machine teaching focuses on the batch setting, where adaptivity plays no role. In this paper, we study the case of teaching consistent, version space learners in an interactive setting. At any time step, the teacher provides an example, the learner performs an update, and the teacher observes the learner’s new state. We highlight that adaptivity does not speed up the teaching process when considering existing models of version space learners, such as the “worst-case” model (the learner picks the next hypothesis randomly from the version space) and the “preference-based” model (the learner picks hypothesis according to some global preference). Inspired by human teaching, we propose a new model where the learner picks hypotheses according to some local preference defined by the current hypothesis. We show that our model exhibits several desirable properties, e.g., adaptivity plays a key role, and the learner’s transitions over hypotheses are smooth/interpretable. We develop adaptive teaching algorithms, and demonstrate our results via simulation and user studies.",https://proceedings.neurips.cc/paper_files/paper/2018/file/bf62768ca46b6c3b5bea9515d1a1fc45-Paper.pdf,2018,"learner, teaching, model, adaptivity, hypothesis, picks, preference, space, teacher, version","teaching, learner, hypothesis, preference, teacher, adaptive, model, non, version, examples","{'adaptivity': 0.3699702954944465, 'role': 0.3699702954944465, 'version': 0.3699702954944465, 'learners': 0.34920639036059165, 'teaching': 0.3344741652155711, 'case': 0.3230469636190192, 'space': 0.29294635494786136, 'machine': 0.2875508333401438, 'understanding': 0.2875508333401438}","learner, teaching, picks, adaptivity, preference, teacher, hypothesis, version, learners, hypotheses"
Wavelet regression and additive models for irregularly spaced data,"Asad Haris, Ali Shojaie, Noah Simon","We present a novel approach for nonparametric regression using wavelet basis functions. Our proposal, waveMesh, can be applied to non-equispaced data with sample size not necessarily a power of 2. We develop an efficient proximal gradient descent algorithm for computing the estimator and establish adaptive minimax convergence rates. The main appeal of our approach is that it naturally extends to additive and sparse additive models for a potentially large number of covariates. We prove minimax optimal convergence rates under a weak compatibility condition for sparse additive models. The compatibility condition holds when we have a small number of covariates. Additionally, we establish convergence rates for when the condition is not met. We complement our theoretical results with empirical studies comparing waveMesh to existing methods.",https://proceedings.neurips.cc/paper_files/paper/2018/file/bf764716fe1a58cb07f8a377ec25c16d-Paper.pdf,2018,"additive, condition, convergence, rates, approach, compatibility, covariates, establish, minimax, models","wavelet, wavemesh, additive, data, interpolation, xi, functions, sparse, proposal, methods","{'irregularly': 0.45882152253810204, 'spaced': 0.45882152253810204, 'wavelet': 0.4330710050415021, 'additive': 0.4148007221195212, 'regression': 0.3065345736923414, 'data': 0.259690704489431, 'models': 0.2407338355240916}","additive, wavemesh, condition, compatibility, rates, covariates, minimax, convergence, establish, sparse"
Self-Erasing Network for Integral Object Attention,"Qibin Hou, PengTao Jiang, Yunchao Wei, Ming-Ming Cheng","Recently, adversarial erasing for weakly-supervised object attention has been deeply studied due to its capability in localizing integral object regions. However, such a strategy raises one key problem that attention regions will gradually expand to non-object regions as training iterations continue, which significantly decreases the quality of the produced attention maps. To tackle such an issue as well as promote the quality of object attention, we introduce a simple yet effective Self-Erasing Network (SeeNet) to prohibit attentions from spreading to unexpected background regions. In particular, SeeNet leverages two self-erasing strategies to encourage networks to use reliable object and background cues for learning to attention. In this way, integral object regions can be effectively highlighted without including much more background regions. To test the quality of the generated attention maps, we employ the mined object regions as heuristic cues for learning semantic segmentation models. Experiments on Pascal VOC well demonstrate the superiority of our SeeNet over other state-of-the-art methods.",https://proceedings.neurips.cc/paper_files/paper/2018/file/c042f4db68f23406c6cecf84a7ebb0fe-Paper.pdf,2018,"object, regions, attention, background, erasing, quality, seenet, cues, integral, learning","attention, background, semantic, erasing, segmentation, regions, maps, results, zone, network","{'erasing': 0.4988898731557122, 'integral': 0.4708905928767321, 'self': 0.4123806747645358, 'attention': 0.38116896774328607, 'object': 0.3511081993962303, 'network': 0.30124138731236433}","regions, object, attention, erasing, seenet, background, cues, quality, integral, self"
Training deep learning based denoisers without ground truth data,"Shakarim Soltanayev, Se Young Chun","Recently developed deep-learning-based denoisers often outperform state-of-the-art conventional denoisers, such as the BM3D. They are typically trained to minimizethe mean squared error (MSE) between the output image of a deep neural networkand a ground truth image.  In deep learning based denoisers, it is important to use high quality noiseless ground truth data for high performance, but it is often challenging or even infeasible to obtain noiseless images in application areas such as hyperspectral remote sensing and medical imaging. In this article, we propose a method based on Stein’s unbiased risk estimator (SURE) for training deep neural network denoisers only based on the use of noisy images. We demonstrate that our SURE-based method, without the use of ground truth data, is able to train deep neural network denoisers to yield performances close to those networks trained with ground truth, and to outperform the state-of-the-art denoiser BM3D. Further improvements were achieved when noisy test images were used for training of denoiser networks using our proposed SURE-based method.",https://proceedings.neurips.cc/paper_files/paper/2018/file/c0560792e4a3c79e62f76cbf9fb277dd-Paper.pdf,2018,"based, deep, denoisers, ground, truth, images, method, neural, sure, use","sure, image, 25, based, dncnn, deep, bm3d, 26, images, training","{'denoisers': 0.44010765752395686, 'ground': 0.44010765752395686, 'truth': 0.44010765752395686, 'without': 0.35565699249246907, 'based': 0.29403200711151883, 'training': 0.28385535997098355, 'data': 0.24909874977388055, 'deep': 0.20621483486740746, 'learning': 0.14429283145378677}","denoisers, truth, sure, ground, bm3d, denoiser, noiseless, based, deep, images"
Structural Causal Bandits: Where to Intervene?,"Sanghack Lee, Elias Bareinboim","We study the problem of identifying the best action in a sequential decision-making setting when the reward distributions of the arms exhibit a non-trivial dependence structure, which is governed by the underlying causal model of the domain where the agent is deployed. In this setting, playing an arm corresponds to intervening on a set of variables and setting them to specific values. In this paper, we show that whenever the underlying causal model is not taken into account during the decision-making process, the standard strategies of simultaneously intervening on all variables or on all the subsets of the variables may, in general, lead to suboptimal policies, regardless of the number of interventions performed by the agent in the environment. We formally acknowledge this phenomenon and investigate structural properties implied by the underlying causal model, which lead to a complete characterization of the relationships between the arms' distributions. We leverage this characterization to build a new algorithm that takes as input a causal structure and finds a minimal, sound, and complete set of qualified arms that an agent should play to maximize its expected reward. We empirically demonstrate that the new strategy learns an optimal policy and leads to orders of magnitude faster convergence rates when compared with its causal-insensitive counterparts.",https://proceedings.neurips.cc/paper_files/paper/2018/file/c0a271bc0ecb776a094786474322cb82-Paper.pdf,2018,"causal, agent, arms, model, setting, underlying, variables, characterization, complete, decision","causal, arms, set, variables, pomis, mab, reward, variable, optimal, scm","{'intervene': 0.5932363700984388, 'structural': 0.5179962024047036, 'bandits': 0.4532529261174618, 'causal': 0.4175072795205998}","causal, arms, intervening, agent, variables, underlying, characterization, setting, complete, lead"
Scalar Posterior Sampling with Applications,"Georgios Theocharous, Zheng Wen, Yasin Abbasi Yadkori, Nikos Vlassis","We propose a practical  non-episodic PSRL algorithm that unlike recent state-of-the-art PSRL algorithms  uses a deterministic,  model-independent episode switching schedule. Our algorithm termed deterministic schedule PSRL (DS-PSRL) is efficient in terms of time, sample, and space complexity.  We prove a Bayesian regret bound under mild assumptions.  Our result is more generally applicable to multiple parameters and continuous state action problems.  We compare our algorithm with state-of-the-art PSRL algorithms on standard discrete and continuous problems from the literature.  Finally, we show how the assumptions of our algorithm satisfy a sensible  parameterization  for a  large class of problems in sequential recommendations.",https://proceedings.neurips.cc/paper_files/paper/2018/file/c157297d1a1ff043255bfb18530caaa2-Paper.pdf,2018,"psrl, algorithm, problems, state, algorithms, art, assumptions, continuous, deterministic, schedule","psrl, state, algorithm, xt, model, time, ds, optimal, policy, regret","{'scalar': 0.5687428745974049, 'posterior': 0.5368232225671956, 'applications': 0.4966092168915819, 'sampling': 0.3764726497444158}","psrl, schedule, deterministic, algorithm, problems, assumptions, continuous, ds, state, episode"
Ex ante coordination and collusion in zero-sum multi-player extensive-form games,"Gabriele Farina, Andrea Celli, Nicola Gatti, Tuomas Sandholm","Recent milestones in equilibrium computation, such as the success of Libratus, show that it is possible to compute strong solutions to two-player zero-sum games in theory and practice. This is not the case for games with more than two players, which remain one of the main open challenges in computational game theory. This paper focuses on zero-sum games where a team of players faces an opponent, as is the case, for example, in Bridge, collusion in poker, and many non-recreational applications such as war, where the colluders do not have time or means of communicating during battle, collusion in bidding, where communication during the auction is illegal, and coordinated swindling in public. The possibility for the team members to communicate before game play—that is, coordinate their strategies ex ante—makes the use of behavioral strategies unsatisfactory. The reasons for this are closely related to the fact that the team can be represented as a single player with imperfect recall. We propose a new game representation, the realization form, that generalizes the sequence form but can also be applied to imperfect-recall games. Then, we use it to derive an auxiliary game that is equivalent to the original one. It provides a sound way to map the problem of finding an optimal ex-ante-correlated strategy for the team to the well-understood Nash equilibrium-finding problem in a (larger) two-player zero-sum perfect-recall game. By reasoning over the auxiliary game, we devise an anytime algorithm, fictitious team-play, that is guaranteed to converge to an optimal coordinated strategy for the team against an optimal opponent, and that is dramatically faster than the prior state-of-the-art algorithm for this problem.",https://proceedings.neurips.cc/paper_files/paper/2018/file/c17028c9b6e0c5deaad29665d582284a-Paper.pdf,2018,"game, team, games, optimal, player, problem, recall, sum, two, zero","player, team, game, form, realization, strategies, recall, strategy, normal, play","{'ante': 0.33449978871984415, 'collusion': 0.33449978871984415, 'ex': 0.33449978871984415, 'coordination': 0.3157266008048766, 'extensive': 0.3157266008048766, 'form': 0.3157266008048766, 'player': 0.3157266008048766, 'sum': 0.29207518115132336, 'zero': 0.2703138352892445, 'games': 0.25556930705004516}","team, game, games, player, recall, ante, collusion, ex, sum, opponent"
Online Learning of Quantum States,"Scott Aaronson, Xinyi Chen, Elad Hazan, Satyen Kale, Ashwin Nayak","Suppose we have many copies of an unknown n-qubit state $\rho$. We measure some copies of $\rho$ using a known two-outcome measurement E_1, then other copies using a measurement E_2, and so on. At each stage t, we generate a current hypothesis $\omega_t$ about the state $\rho$, using the outcomes of the previous measurements. We show that it is possible to do this in a way that guarantees that $|\trace(E_i \omega_t)  - \trace(E_i\rho)|$, the error in our prediction for the next measurement, is at least $eps$ at most $O(n / eps^2)  $\ times. Even in the non-realizable setting---where there could be arbitrary noise in the measurement outcomes---we show how to output hypothesis states that incur at most  $O(\sqrt {Tn})  $ excess loss over the best possible state on the first $T$ measurements. These results generalize a 2007 theorem by Aaronson on the PAC-learnability of quantum states, to the online and regret-minimization settings. We give three different ways to prove our results---using convex optimization, quantum postselection, and sequential fat-shattering dimension---which have different advantages in terms of parameters and portability.",https://proceedings.neurips.cc/paper_files/paper/2018/file/c1a3d34711ab5d85335331ca0e57f067-Paper.pdf,2018,"measurement, rho, using, copies, state, different, e_i, eps, hypothesis, measurements","et, tr, state, quantum, loss, measurements, learning, theorem, algorithm, states","{'states': 0.6406366045451484, 'quantum': 0.6046820486144546, 'online': 0.4240618578423557, 'learning': 0.21003785783419932}","rho, measurement, copies, e_i, omega_t, eps, quantum, trace, outcomes, hypothesis"
Realistic Evaluation of Deep Semi-Supervised Learning Algorithms,"Avital Oliver, Augustus Odena, Colin A. Raffel, Ekin Dogus Cubuk, Ian Goodfellow","Semi-supervised learning (SSL) provides a powerful framework for leveraging unlabeled data when labels are limited or expensive to obtain. SSL algorithms based on deep neural networks have recently proven successful on standard benchmark tasks. However, we argue that these benchmarks fail to address many issues that SSL algorithms would face in real-world applications. After creating a unified reimplementation of various widely-used SSL techniques, we test them in a suite of experiments designed to address these issues. We find that the performance of simple baselines which do not use unlabeled data is often underreported, SSL methods differ in sensitivity to the amount of labeled and unlabeled data, and performance can degrade substantially when the unlabeled dataset contains out-of-distribution examples. To help guide SSL research towards real-world applicability, we make our unified reimplemention and evaluation platform publicly available.",https://proceedings.neurips.cc/paper_files/paper/2018/file/c1fea270c48e8079d8ddf7d06d26ab52-Paper.pdf,2018,"ssl, unlabeled, data, address, algorithms, issues, performance, real, unified, world","ssl, data, unlabeled, model, validation, labeled, dataset, 10, learning, supervised","{'realistic': 0.5021627374087696, 'evaluation': 0.45111890075431316, 'semi': 0.42993379443564983, 'supervised': 0.38364261900994595, 'algorithms': 0.3460250676013717, 'deep': 0.24928155018727138, 'learning': 0.17442751259306064}","ssl, unlabeled, issues, unified, address, reimplementation, reimplemention, underreported, creating, world"
Long short-term memory and Learning-to-learn in networks of spiking neurons,"Guillaume Bellec, Darjan Salaj, Anand Subramoney, Robert Legenstein, Wolfgang Maass","Recurrent networks of spiking neurons (RSNNs) underlie the astounding computing and learning capabilities of the brain. But computing and learning capabilities of RSNN models have remained poor, at least in comparison with ANNs. We address two possible reasons for that. One is that RSNNs in the brain are not randomly connected or designed according to simple rules, and they do not start learning as a tabula rasa network. Rather, RSNNs in the brain were optimized for their tasks through evolution, development, and prior experience. Details of these optimization processes are largely unknown. But their functional contribution can be approximated through powerful optimization methods, such as backpropagation through time (BPTT).A second major mismatch between RSNNs in the brain and models is that the latter only show a small fraction of the dynamics of neurons and synapses in the brain. We include neurons in our RSNN model that reproduce one prominent dynamical process of biological neurons that takes place at the behaviourally relevant time scale of seconds: neuronal adaptation. We denote these networks as LSNNs because of their Long short-term memory. The inclusion of adapting neurons drastically increases the computing and learning capability of RSNNs if they are trained and configured by deep learning (BPTT combined with a rewiring algorithm that optimizes the network architecture). In fact, the computational performance of these RSNNs approaches for the first time that of LSTM networks. In addition RSNNs with adapting neurons can acquire abstract knowledge from prior learning in a Learning-to-Learn (L2L) scheme, and transfer that knowledge in order to learn new but related tasks from very few examples. We demonstrate this for supervised learning and reinforcement learning.",https://proceedings.neurips.cc/paper_files/paper/2018/file/c203d8a151612acf12457e4d67635a95-Paper.pdf,2018,"learning, rsnns, neurons, brain, computing, networks, time, adapting, bptt, capabilities","learning, neurons, lsnn, rsnns, fig, network, input, training, performance, networks","{'short': 0.40555037418380907, 'term': 0.40555037418380907, 'neurons': 0.3827896023080617, 'spiking': 0.3666405776623442, 'learn': 0.3438798057865968, 'long': 0.3438798057865968, 'memory': 0.32111903391084934, 'networks': 0.18111149580843564, 'learning': 0.13296294846889625}","rsnns, neurons, brain, learning, rsnn, bptt, computing, adapting, capabilities, time"
Revisiting Decomposable Submodular Function Minimization with Incidence Relations,"Pan Li, Olgica Milenkovic","We introduce a new approach to decomposable submodular function minimization (DSFM) that exploits incidence relations. Incidence relations describe which variables effectively influence the component functions, and when properly utilized, they allow for improving the convergence rates of DSFM solvers. Our main results include the precise parametrization of the DSFM problem based on incidence relations, the development of new scalable alternative projections and parallel coordinate descent methods and an accompanying rigorous analysis of their convergence rates.",https://proceedings.neurips.cc/paper_files/paper/2018/file/c21002f464c5fc5bee3b98ced83963b8-Paper.pdf,2018,"dsfm, incidence, relations, convergence, new, rates, accompanying, allow, alternative, analysis","rcdm, ap, problem, submodular, incidence, iterations, 10, convergence, projections, algorithms","{'incidence': 0.42332447951567914, 'relations': 0.42332447951567914, 'decomposable': 0.39956616853626703, 'revisiting': 0.38270937863293486, 'function': 0.35895106765352275, 'minimization': 0.3290191356993507, 'submodular': 0.3136458479356092}","dsfm, incidence, relations, rates, accompanying, convergence, decomposable, parametrization, projections, utilized"
DifNet: Semantic Segmentation by Diffusion Networks,"Peng Jiang, Fanglin Gu, Yunhai Wang, Changhe Tu, Baoquan Chen","Deep Neural Networks (DNNs) have recently shown state of the art performance on semantic segmentation tasks, however, they still suffer from problems of poor boundary localization and spatial fragmented predictions. The difficulties lie in the requirement of making dense predictions from a long path model all at once since details are hard to keep when data goes through deeper layers. Instead, in this work, we decompose this difficult task into two relative simple sub-tasks: seed detection which is required to predict initial predictions without the need of wholeness and preciseness, and similarity estimation which measures the possibility of any two nodes belong to the same class without the need of knowing which class they are. We use one branch network for one sub-task each, and apply a cascade of random walks base on hierarchical semantics to approximate a complex diffusion process which propagates seed information to the whole image according to the estimated similarities. 
The proposed DifNet consistently produces improvements over the baseline models with the same depth and with the equivalent number of parameters, and also achieves promising performance on Pascal VOC and Pascal Context dataset. OurDifNet is trained end-to-end without complex loss functions.",https://proceedings.neurips.cc/paper_files/paper/2018/file/c2626d850c80ea07e7511bbae4c76f4b-Paper.pdf,2018,"predictions, without, class, complex, end, need, one, pascal, performance, seed","random, model, difnet, seed, walks, branch, map, diffusion, two, walk","{'difnet': 0.545324896582846, 'diffusion': 0.49300468653044327, 'segmentation': 0.4623993256199337, 'semantic': 0.431793964709424, 'networks': 0.2435322810895405}","seed, predictions, pascal, without, sub, need, end, complex, difnet, fragmented"
Out of the Box: Reasoning with Graph Convolution Nets for Factual Visual Question Answering,"Medhini Narasimhan, Svetlana Lazebnik, Alexander Schwing","Accurately answering a question about a given image requires combining observations with general knowledge. While this is effortless for humans, reasoning with general knowledge remains an algorithmic challenge. To advance research in this direction a novelfact-based' visual question answering (FVQA) task has been introduced recently along with a large set of curated facts which link two entities, i.e., two possible answers, via a relation. Given a question-image pair, deep network techniques have been employed to successively reduce the large set of facts until one of the two entities of the final remaining fact is predicted as the answer. We observe that a successive process which considers one fact at a time to form a local decision is sub-optimal. Instead, we develop an entity graph and use a graph convolutional network toreason' about the correct answer by jointly considering all entities. We show on the challenging FVQA dataset that this leads to an improvement in accuracy of around 7% compared to the state-of-the-art.",https://proceedings.neurips.cc/paper_files/paper/2018/file/c26820b8a4c1b3c2aa868d6d57e14a79-Paper.pdf,2018,"entities, question, two, answer, answering, fact, facts, fvqa, general, given","question, fact, answer, facts, image, visual, relation, model, gcn, entity","{'factual': 0.40047154573802235, 'box': 0.3620490282180643, 'convolution': 0.3395732962855872, 'reasoning': 0.3395732962855872, 'answering': 0.33102841958048157, 'nets': 0.33102841958048157, 'question': 0.33102841958048157, 'visual': 0.28520399317814815, 'graph': 0.26046541744619534}","entities, fvqa, facts, question, answering, answer, fact, two, graph, novelfact"
Distributed Multi-Player Bandits - a Game of Thrones Approach,"Ilai Bistritz, Amir Leshem","We consider a multi-armed bandit game where N players compete for K arms for T turns. Each player has different expected rewards for the arms, and the instantaneous rewards are independent and identically distributed. Performance is measured using the expected sum of regrets, compared to the optimal assignment of arms to players. We assume that each player only knows her actions and the reward she received each turn. Players cannot observe the actions of other players, and no communication between players is possible. We present a distributed algorithm and prove that it achieves an expected sum of regrets of near-O\left(\log^{2}T\right). This is the first algorithm to achieve a poly-logarithmic regret in this fully distributed scenario. All other works have assumed that either all players have the same vector of expected rewards or that communication between players is possible.",https://proceedings.neurips.cc/paper_files/paper/2018/file/c2964caac096f26db222cb325aa267cb-Paper.pdf,2018,"players, expected, arms, distributed, rewards, actions, algorithm, communication, player, possible","players, player, algorithm, expected, optimal, rewards, dynamics, phase, game, got","{'thrones': 0.46501174956494257, 'player': 0.43891381691553805, 'game': 0.4203970390198557, 'bandits': 0.35528491979400517, 'approach': 0.32358646317595985, 'distributed': 0.3201064036812334, 'multi': 0.28457227659951384}","players, expected, arms, rewards, regrets, distributed, player, communication, actions, sum"
Scaling Gaussian Process Regression with Derivatives,"David Eriksson, Kun Dong, Eric Lee, David Bindel, Andrew G. Wilson","Gaussian processes (GPs) with derivatives are useful in many applications, including Bayesian optimization, implicit surface reconstruction, and terrain reconstruction. Fitting a GP to function values and derivatives at $n$ points in $d$ dimensions requires linear solves and log determinants with an ${n(d+1) \times n(d+1)}$ positive definite matrix-- leading to prohibitive $\mathcal{O}(n^3d^3)$ computations for standard direct methods. We propose iterative solvers using fast $\mathcal{O}(nd)$ matrix-vector multiplications (MVMs), together with pivoted Cholesky preconditioning that cuts the iterations to convergence by several orders of magnitude, allowing for fast kernel learning and prediction. Our approaches, together with dimensionality reduction, allows us to scale Bayesian optimization with derivatives to high-dimensional problems and large evaluation budgets.",https://proceedings.neurips.cc/paper_files/paper/2018/file/c2f32522a84d5e6357e6abac087f1b0b-Paper.pdf,2018,"derivatives, bayesian, fast, mathcal, matrix, optimization, reconstruction, together, 3d, allowing","kernel, ski, derivatives, skip, function, matrix, points, active, se, dimensional","{'derivatives': 0.5385550919135659, 'scaling': 0.4982113500402681, 'process': 0.4227474239925258, 'regression': 0.3811978950051023, 'gaussian': 0.3711019321108237}","derivatives, mathcal, reconstruction, together, fast, matrix, budgets, cholesky, cuts, determinants"
Deep Attentive Tracking via Reciprocative Learning,"Shi Pu, Yibing Song, Chao Ma, Honggang Zhang, Ming-Hsuan Yang","Visual attention, derived from cognitive neuroscience, facilitates human perception on the most pertinent subset of the sensory data. Recently, significant efforts have been made to exploit attention schemes to advance computer vision systems. For visual tracking, it is often challenging to track target objects undergoing large appearance changes. Attention maps facilitate visual tracking by selectively paying attention to temporal robust features. Existing tracking-by-detection approaches mainly use additional attention modules to generate feature weights as the classifiers are not equipped with such mechanisms. In this paper, we propose a reciprocative learning algorithm to exploit visual attention for training deep classifiers. The proposed algorithm consists of feed-forward and backward operations to generate attention maps, which serve as regularization terms coupled with the original classification loss function for training. The deep classifier learns to attend to the regions of target objects robust to appearance changes. Extensive experiments on large-scale benchmark datasets show that the proposed attentive tracking method performs favorably against the state-of-the-art approaches.",https://proceedings.neurips.cc/paper_files/paper/2018/file/c32d9bf27a3da7ec8163957080c8628e-Paper.pdf,2018,"attention, tracking, visual, algorithm, appearance, approaches, changes, classifiers, deep, exploit","attention, tracking, learning, classiﬁer, target, reciprocative, maps, input, map, visual","{'reciprocative': 0.5350404858954128, 'tracking': 0.5350404858954128, 'attentive': 0.5050123186960525, 'via': 0.2807242947492294, 'deep': 0.2506961275498691, 'learning': 0.1754173219493608}","attention, tracking, visual, appearance, classifiers, changes, maps, objects, exploit, generate"
Trading robust representations for sample complexity through self-supervised visual experience,"Andrea Tacchetti, Stephen Voinea, Georgios Evangelopoulos","Learning in small sample regimes is among the most remarkable features of the human perceptual system. This ability is related to robustness to transformations, which is acquired through visual experience in the form of weak- or self-supervision during development. We explore the idea of allowing artificial systems to learn representations of visual stimuli through weak supervision prior to downstream supervised tasks. We introduce a novel loss function for representation learning using unlabeled image sets and video sequences, and experimentally demonstrate that these representations support one-shot learning and reduce the sample complexity of multiple recognition tasks. We establish the existence of a trade-off between the sizes of weakly supervised, automatically obtained from video sequences, and fully supervised data sets. Our results suggest that equivalence sets other than class labels, which are abundant in unlabeled visual experience, can be used for self-supervised learning of semantically relevant image embeddings.",https://proceedings.neurips.cc/paper_files/paper/2018/file/c344336196d5ec19bd54fd14befdde87-Paper.pdf,2018,"learning, supervised, sets, visual, experience, image, representations, sample, self, sequences","loss, orbit, set, learning, transformations, supervised, orbits, sets, using, xi","{'experience': 0.4150737184282073, 'trading': 0.4150737184282073, 'self': 0.34309852593264495, 'complexity': 0.32865971762866425, 'sample': 0.31713055185881883, 'supervised': 0.2993111708350265, 'visual': 0.29560322879086254, 'representations': 0.28572988831563767, 'robust': 0.265540968956002}","supervised, sets, visual, experience, sequences, weak, unlabeled, supervision, video, self"
Global Geometry of Multichannel Sparse Blind Deconvolution on the Sphere,"Yanjun Li, Yoram Bresler","Multichannel blind deconvolution is the problem of recovering an unknown signal $f$ and multiple unknown channels $x_i$ from convolutional measurements $y_i=x_i \circledast f$ ($i=1,2,\dots,N$). We consider the case where the $x_i$'s are sparse, and convolution with $f$ is invertible. Our nonconvex optimization formulation solves for a filter $h$ on the unit sphere that produces sparse output $y_i\circledast h$. Under some technical assumptions, we show that all local minima of the objective function correspond to the inverse filter of $f$ up to an inherent sign and shift ambiguity, and all saddle points have strictly negative curvatures. This geometric structure allows successful recovery of $f$ and $x_i$ using a simple manifold gradient descent algorithm with random initialization. Our theoretical findings are complemented by numerical experiments, which demonstrate superior performance of the proposed approach over the previous methods.",https://proceedings.neurips.cc/paper_files/paper/2018/file/c3992e9a68c5ae12bd18488bc579b30d-Paper.pdf,2018,"x_i, circledast, filter, sparse, unknown, y_i, algorithm, allows, ambiguity, approach","xi, gradient, figure, sparse, descent, blind, deconvolution, points, manifold, 64","{'sphere': 0.43329088626556483, 'multichannel': 0.40897322896348787, 'deconvolution': 0.3917195765284491, 'geometry': 0.3917195765284491, 'blind': 0.36740191922637216, 'global': 0.3430842619242952, 'sparse': 0.2922678079566149}","x_i, circledast, y_i, filter, unknown, sparse, complemented, multichannel, curvatures, sign"
Sample Efficient Stochastic Gradient Iterative Hard Thresholding Method for Stochastic Sparse Linear Regression with Limited Attribute Observation,"Tomoya Murata, Taiji Suzuki","We develop new stochastic gradient methods for efficiently solving sparse linear regression in a partial attribute observation setting, where learners are only allowed to observe a fixed number of actively chosen attributes per example at training and prediction times. It is shown that the methods achieve essentially a sample complexity of $O(1/\varepsilon)$ to attain an error of $\varepsilon$ under a variant of restricted eigenvalue condition, and the rate has better dependency on the problem dimension than existing methods. Particularly, if the smallest magnitude of the non-zero components of the optimal solution is not too small, the rate of our proposed {\it Hybrid} algorithm can be boosted to near the minimax optimal sample complexity of {\it full information} algorithms. The core ideas are (i) efficient construction of an unbiased gradient estimator by the iterative usage of the hard thresholding operator for configuring an exploration algorithm; and (ii) an adaptive combination of the exploration and an exploitation algorithms for quickly identifying the support of the optimum and efficiently searching the optimal parameter in its support. Experimental results are presented to validate our theoretical findings and the superiority of our proposed methods.",https://proceedings.neurips.cc/paper_files/paper/2018/file/c3a690be93aa602ee2dc0ccab5b7b67e-Paper.pdf,2018,"methods, optimal, algorithm, algorithms, complexity, efficiently, exploration, gradient, proposed, rate","algorithm, sample, exploration, methods, algorithms, linear, optimal, complexity, attributes, problem","{'stochastic': 0.3764313989895502, 'iterative': 0.3105008972606885, 'thresholding': 0.3105008972606885, 'attribute': 0.2974015690340434, 'observation': 0.2974015690340434, 'hard': 0.28724094068688083, 'limited': 0.271919998964336, 'sample': 0.2513392883242474, 'method': 0.23721668008405358, 'sparse': 0.22189573836150875}","varepsilon, methods, exploration, optimal, support, efficiently, boosted, rate, configuring, searching"
Blockwise Parallel Decoding for Deep Autoregressive Models,"Mitchell Stern, Noam Shazeer, Jakob Uszkoreit","Deep autoregressive sequence-to-sequence models have demonstrated impressive performance across a wide variety of tasks in recent years. While common architecture classes such as recurrent, convolutional, and self-attention networks make different trade-offs between the amount of computation needed per layer and the length of the critical path at training time, generation still remains an inherently sequential process. To overcome this limitation, we propose a novel blockwise parallel decoding scheme in which we make predictions for multiple time steps in parallel then back off to the longest prefix validated by a scoring model. This allows for substantial theoretical improvements in generation speed when applied to architectures that can process output sequences in parallel. We verify our approach empirically through a series of experiments using state-of-the-art self-attention models for machine translation and image super-resolution, achieving iteration reductions of up to 2x over a baseline greedy decoder with no loss in quality, or up to 7x in exchange for a slight decrease in performance. In terms of wall-clock time, our fastest models exhibit real-time speedups of up to 4x over standard greedy decoding.",https://proceedings.neurips.cc/paper_files/paper/2018/file/c4127b9194fe8562c64dc0f5bf2c93bc-Paper.pdf,2018,"time, models, parallel, attention, decoding, generation, greedy, make, performance, process","model, decoding, transformer, al, et, size, block, parallel, models, 2018","{'blockwise': 0.5085156742681437, 'autoregressive': 0.4799761635339894, 'decoding': 0.4440206322657811, 'parallel': 0.4311875476843066, 'models': 0.26680729363676253, 'deep': 0.2382677829026082}","parallel, decoding, time, greedy, self, generation, attention, make, sequence, 7x"
Sublinear Time Low-Rank Approximation of Distance Matrices,"Ainesh Bakshi, David Woodruff","Let $\PP=\{ p_1, p_2, \ldots p_n \}$ and $\QQ = \{ q_1, q_2 \ldots q_m \}$ be two point sets in an arbitrary metric space. Let $\AA$ represent the $m\times n$ pairwise distance matrix with $\AA_{i,j} = d(p_i, q_j)$. Such distance matrices are commonly computed in software packages and have applications to learning image manifolds, handwriting recognition, and multi-dimensional unfolding, among other things. In an attempt to reduce their description size, we study low rank approximation of such matrices. Our main result is to show that for any underlying distance metric $d$, it is possible to achieve an additive error low rank approximation in sublinear time. We note that it is provably impossible to achieve such a guarantee in sublinear time for arbitrary matrices $\AA$, and our proof exploits special properties of distance matrices. We develop a recursive algorithm based on additive projection-cost preserving sampling. We then show that in general, relative error approximation in sublinear time is impossible for distance matrices, even if one allows for bicriteria solutions. Additionally, we show that if $\PP = \QQ$ and $d$ is the squared Euclidean distance, which is not a metric but rather the square of a metric, then a relative error bicriteria solution can be found in sublinear time. Finally, we empirically compare our algorithm with the SVD and input sparsity time algorithms. Our algorithm is several hundred times faster than the SVD, and about $8$-$20$ times faster than input sparsity methods on real-world and and synthetic datasets of size $10^8$. Accuracy-wise, our algorithm is only slightly worse than that of the SVD (optimal) and input-sparsity time algorithms.",https://proceedings.neurips.cc/paper_files/paper/2018/file/c45008212f7bdf6eab6050c2a564435a-Paper.pdf,2018,"distance, time, matrices, algorithm, metric, sublinear, approximation, error, input, show","matrix, rank, time, algorithm, column, distance, let, approximation, row, matrices","{'matrices': 0.44513645788294104, 'sublinear': 0.44513645788294104, 'distance': 0.38982658740884146, 'rank': 0.34941697464327814, 'low': 0.3400756444021242, 'approximation': 0.3358627018094476, 'time': 0.31811142532909925}","distance, matrices, sublinear, svd, metric, time, aa, bicriteria, ldots, pp"
Cooperative Learning of Audio and Video Models from Self-Supervised Synchronization,"Bruno Korbar, Du Tran, Lorenzo Torresani","There is a natural correlation between the visual and auditive elements of a video. In this work we leverage this connection to learn general and effective models for both audio and video analysis from self-supervised temporal synchronization. We demonstrate that a calibrated curriculum learning scheme, a careful choice of negative examples, and the use of a contrastive loss are critical ingredients to obtain powerful multi-sensory representations from models optimized to discern temporal synchronization of audio-video pairs. Without further fine-tuning, the resulting audio features achieve performance superior or comparable to the state-of-the-art on established audio classification benchmarks (DCASE2014 and ESC-50). At the same time, our visual subnet provides a very effective initialization to improve the accuracy of video-based action recognition models: compared to learning from scratch, our self-supervised pretraining yields a remarkable gain of +19.9%  in action recognition accuracy on UCF101 and a boost of +17.7% on HMDB51.",https://proceedings.neurips.cc/paper_files/paper/2018/file/c4616f5a24a66668f11ca4fa80525dc4-Paper.pdf,2018,"audio, video, models, accuracy, action, effective, learning, recognition, self, supervised","audio, video, avts, learning, visual, negatives, supervised, training, hard, self","{'synchronization': 0.4324154629516878, 'audio': 0.41417283586283526, 'cooperative': 0.41417283586283526, 'self': 0.37868622369637356, 'video': 0.3702186699849319, 'supervised': 0.3303570503124445, 'models': 0.2403694353222971, 'learning': 0.15020061822715897}","audio, video, synchronization, self, action, recognition, temporal, supervised, visual, effective"
"Submodular Field Grammars: Representation, Inference, and Application to Image Parsing","Abram L. Friesen, Pedro M. Domingos","Natural scenes contain many layers of part-subpart structure, and distributions over them are thus naturally represented by stochastic image grammars, with one production per decomposition of a part. Unfortunately, in contrast to language grammars, where the number of possible split points for a production $A \rightarrow BC$ is linear in the length of $A$, in an image there are an exponential number of ways to split a region into subregions. This makes parsing intractable and requires image grammars to be severely restricted in practice, for example by allowing only rectangular regions. In this paper, we address this problem by associating with each production a submodular Markov random field whose labels are the subparts and whose labeling segments the current object into these subparts. We call the result a submodular field grammar (SFG). Finding the MAP split of a region into subregions is now tractable, and by exploiting this we develop an efficient approximate algorithm for MAP parsing of images with SFGs. Empirically, we present promising improvements in accuracy when using SFGs for scene understanding, and show exponential improvements in inference time compared to traditional methods, while returning comparable minima.",https://proceedings.neurips.cc/paper_files/paper/2018/file/c5866e93cab1776890fe343c9e7063fb-Paper.pdf,2018,"grammars, image, production, split, exponential, field, improvements, map, number, parsing","parse, sfg, image, grammar, production, region, symbol, sfgs, energy, mrf","{'grammars': 0.43868926345782766, 'field': 0.3966000161931735, 'application': 0.3830503050060202, 'parsing': 0.3830503050060202, 'submodular': 0.3250297884376942, 'representation': 0.3052695062617129, 'image': 0.29308439066583963, 'inference': 0.26846375933243644}","grammars, production, split, sfgs, subparts, subregions, parsing, submodular, field, region"
FD-GAN: Pose-guided Feature Distilling GAN for Robust Person Re-identification,"Yixiao Ge, Zhuowan Li, Haiyu Zhao, Guojun Yin, Shuai Yi, Xiaogang Wang, hongsheng Li","Person re-identification (reID) is an important task that requires to retrieve a person's images from an image dataset, given one image of the person of interest. For learning robust person features, the pose variation of person images is one of the key challenges. Existing works targeting the problem either perform human alignment, or learn human-region-based representations. Extra pose information and computational cost is generally required for inference. To solve this issue, a Feature Distilling Generative Adversarial Network (FD-GAN) is proposed for learning identity-related and pose-unrelated representations. It is a novel framework based on a Siamese structure with multiple novel discriminators on human poses and identities. In addition to the discriminators, a novel same-pose loss is also integrated, which requires appearance of a same person's generated images to be similar. After learning pose-unrelated person features with pose guidance, no auxiliary pose information and additional computational cost is required during testing. Our proposed FD-GAN achieves state-of-the-art performance on three person reID datasets, which demonstrates that the effectiveness and robust feature distilling capability of the proposed FD-GAN.",https://proceedings.neurips.cc/paper_files/paper/2018/file/c5ab0bc60ac7929182aadd08703f1ec6-Paper.pdf,2018,"person, pose, fd, gan, human, images, learning, novel, proposed, based","pose, image, person, gan, identity, images, fd, features, loss, proposed","{'gan': 0.584807157236558, 'fd': 0.3348758866918793, 'distilling': 0.31608159096715643, 'identification': 0.31608159096715643, 'person': 0.3027468259374672, 'pose': 0.27680727059630483, 'guided': 0.26515823448802156, 'feature': 0.24811329492409676, 'robust': 0.21423487800888666}","person, pose, fd, distilling, reid, unrelated, gan, discriminators, human, images"
Estimators for Multivariate Information Measures in General Probability Spaces,"Arman Rahimzamani, Himanshu Asnani, Pramod Viswanath, Sreeram Kannan","Information theoretic quantities play an important role in various settings in machine learning, including causality testing, structure inference in graphical models, time-series problems, feature selection as well as in providing privacy guarantees. A key quantity of interest is the mutual information and generalizations thereof, including conditional mutual information, multivariate mutual information, total correlation and directed information. While the aforementioned information quantities are well defined in arbitrary probability spaces, existing estimators employ a $\Sigma H$ method, which can only work in purely discrete space or purely continuous case since entropy (or differential entropy) is well defined only in that regime.
In this paper, we define a general graph divergence measure ($\mathbb{GDM}$), generalizing the aforementioned information measures and we construct a novel estimator via a coupling trick that directly estimates these multivariate information measures using the Radon-Nikodym derivative. These estimators are proven to be consistent in a general setting which includes several cases where the existing estimators fail, thus providing the only known estimators for the following settings: (1) the data has some discrete and some continuous valued components (2) some (or all) of the components themselves are discrete-continuous \textit{mixtures} (3) the data is real-valued but does not have a joint density on the entire space, rather is supported on a low-dimensional manifold. We show that our proposed estimators significantly outperform known estimators on synthetic and real datasets.",https://proceedings.neurips.cc/paper_files/paper/2018/file/c5ab6cebaca97f7171139e4d414ff5a6-Paper.pdf,2018,"information, estimators, continuous, discrete, mutual, well, aforementioned, components, data, defined","information, estimators, px, estimator, gdm, continuous, xl, distribution, mutual, variables","{'estimators': 0.4328536088467665, 'measures': 0.3913242529000353, 'multivariate': 0.3913242529000353, 'probability': 0.3913242529000353, 'spaces': 0.3913242529000353, 'general': 0.33071499772973295, 'information': 0.30120866513742633}","estimators, information, mutual, aforementioned, quantities, discrete, multivariate, purely, continuous, valued"
Graphical Generative Adversarial Networks,"Chongxuan LI, Max Welling, Jun Zhu, Bo Zhang","We propose Graphical Generative Adversarial Networks (Graphical-GAN) to model structured data. Graphical-GAN conjoins the power of Bayesian networks on compactly representing the dependency structures among random variables and that of generative adversarial networks on learning expressive dependency functions. We introduce a structured recognition model to infer the posterior distribution of latent variables given observations. We generalize the Expectation Propagation (EP) algorithm to learn the generative model and recognition model jointly. Finally, we present two important instances of Graphical-GAN, i.e. Gaussian Mixture GAN (GMGAN) and State Space GAN (SSGAN), which can successfully learn the discrete and temporal structures on visual datasets, respectively.",https://proceedings.neurips.cc/paper_files/paper/2018/file/c5c1cb0bebd56ae38817b251ad72bedb-Paper.pdf,2018,"gan, graphical, model, generative, networks, adversarial, dependency, learn, recognition, structured","gan, graphical, model, models, samples, generative, variables, data, gmgan, latent","{'graphical': 0.6532641449142467, 'generative': 0.4870080033636759, 'adversarial': 0.4598968136460684, 'networks': 0.3529363716413729}","gan, graphical, dependency, generative, structures, recognition, structured, variables, model, conjoins"
"Deep Homogeneous Mixture Models: Representation, Separation, and Approximation","Priyank Jaini, Pascal Poupart, Yaoliang Yu","At their core, many unsupervised learning models provide a compact representation of homogeneous density mixtures, but their similarities and differences are not always clearly understood. In this work, we formally establish the relationships among latent tree graphical models (including special cases such as hidden Markov models and tensorial mixture models), hierarchical tensor formats and sum-product networks. Based on this connection, we then give a unified treatment of exponential separation in \emph{exact} representation size between deep mixture architectures and shallow ones. In contrast, for \emph{approximate} representation, we show that the conditional gradient algorithm can approximate any homogeneous mixture within $\epsilon$ accuracy by combining $O(1/\epsilon^2)$ ``shallow'' architectures, where the hidden constant may decrease (exponentially) with respect to the depth. Our experiments on both synthetic and real datasets confirm the benefits of depth in density estimation.",https://proceedings.neurips.cc/paper_files/paper/2018/file/c5f5c23be1b71adb51ea9dc8e9d444a8-Paper.pdf,2018,"models, mixture, representation, approximate, architectures, density, depth, emph, epsilon, hidden","spn, nodes, mixture, density, models, product, size, homogeneous, shallow, rank","{'separation': 0.49626628899015907, 'homogeneous': 0.46841425256666586, 'mixture': 0.4102120290038016, 'approximation': 0.35342617673087806, 'representation': 0.3453354746369852, 'models': 0.26038030328010003, 'deep': 0.2325282668566068}","mixture, homogeneous, shallow, representation, density, depth, emph, models, hidden, epsilon"
Explanations based on the Missing: Towards Contrastive Explanations with Pertinent Negatives,"Amit Dhurandhar, Pin-Yu Chen, Ronny Luss, Chun-Chen Tu, Paishun Ting, Karthikeyan Shanmugam, Payel Das","In this paper we propose a novel method that provides contrastive explanations justifying the classification of an input by a black box classifier such as a deep neural network. Given an input we find what should be minimally and sufficiently present (viz. important object pixels in an image) to justify its classification and analogously what should be  minimally and necessarily \emph{absent} (viz. certain background pixels). We argue that such explanations are natural for humans and are used commonly in domains such as health care and criminology. What is minimally but critically \emph{absent} is an important part of an explanation, which to the best of our knowledge, has not been explicitly identified by current explanation methods that explain predictions of neural networks. We validate our approach on three real datasets obtained from diverse domains; namely, a handwritten digits dataset MNIST, a large procurement fraud dataset and a brain activity strength dataset. In all three cases, we witness the power of our approach in generating precise explanations that are also easy for human experts to understand and evaluate.",https://proceedings.neurips.cc/paper_files/paper/2018/file/c5ff2543b53f4cc0ad3819a36752467b-Paper.pdf,2018,"dataset, explanations, minimally, absent, approach, classification, domains, emph, explanation, important","x0, pertinent, explanations, method, methods, features, also, classiﬁcation, data, explanation","{'explanations': 0.6403545445283281, 'negatives': 0.35415614223547814, 'pertinent': 0.35415614223547814, 'contrastive': 0.3342797774853793, 'missing': 0.32017727226416404, 'towards': 0.26239825218738994, 'based': 0.23660856509114883}","minimally, explanations, viz, absent, explanation, pixels, dataset, emph, domains, three"
Community Exploration: From Offline Optimization to Online Learning,"Xiaowei Chen, Weiran Huang, Wei Chen, John C. S. Lui","We introduce the community exploration problem that has various real-world applications such as online advertising. In the problem, an explorer allocates limited budget to explore communities so as to maximize the number of members he could meet. We provide a systematic study of the community exploration problem, from offline optimization to online learning. For the offline setting where the sizes of communities are known, we prove that the greedy methods for both of non-adaptive exploration and adaptive exploration are optimal. For the online setting where the sizes of communities are not known and need to be learned from the multi-round explorations, we propose an ``upper confidence'' like algorithm that achieves the logarithmic regret bounds. By combining the feedback from different rounds, we can achieve a constant regret bound.",https://proceedings.neurips.cc/paper_files/paper/2018/file/c60d870eaad6a3946ab3e8734466e532-Paper.pdf,2018,"exploration, communities, online, problem, adaptive, community, known, offline, regret, setting","community, adaptive, exploration, ci, policy, problem, regret, round, ki, communities","{'offline': 0.5553549632523717, 'community': 0.5020724366098452, 'exploration': 0.43163676767450515, 'online': 0.36761067945217124, 'optimization': 0.29138286635187677, 'learning': 0.1820775865624115}","communities, exploration, online, offline, community, sizes, adaptive, regret, explorations, explorer"
Context-aware Synthesis and Placement of Object Instances,"Donghoon Lee, Sifei Liu, Jinwei Gu, Ming-Yu Liu, Ming-Hsuan Yang, Jan Kautz","Learning to insert an object instance into an image in a semantically coherent
manner is a challenging and interesting problem. Solving it requires (a) determining a location to place an object in the scene and (b) determining its appearance at the location. Such an object insertion model can potentially facilitate numerous image editing and scene parsing applications. In this paper, we propose an end-to-end trainable neural network for the task of inserting an object instance mask of a specified class into the semantic label map of an image. Our network consists of two generative modules where one determines where the inserted object mask should be (i.e., location and scale) and the other determines what the object mask shape (and pose) should look like. The two modules are connected together via a spatial transformation network and jointly trained. We devise a learning procedure that leverage both supervised and unsupervised data and show our model can insert an object at diverse locations with various appearances. We conduct extensive experimental validations with comparisons to strong baselines to verify the effectiveness of the proposed network.",https://proceedings.neurips.cc/paper_files/paper/2018/file/c6969ae30d99f73951cb976b88a457af-Paper.pdf,2018,"object, network, image, location, mask, determines, determining, end, insert, instance","object, input, semantic, map, figure, instance, module, network, shape, instances","{'instances': 0.4854634518036628, 'placement': 0.4582177048289404, 'context': 0.4116407927343476, 'aware': 0.3773152110381289, 'synthesis': 0.35470552838066344, 'object': 0.3416589664514241}","object, mask, location, insert, determines, determining, modules, network, scene, image"
Beyond Log-concavity: Provable Guarantees for Sampling Multi-modal Distributions using Simulated Tempering Langevin Monte Carlo,"Holden Lee, Andrej Risteski, Rong Ge","A key task in Bayesian machine learning is sampling from distributions that are only specified up to a partition function (i.e., constant of proportionality). One prevalent example of this is sampling posteriors in parametric 
distributions, such as latent-variable generative models.  However sampling (even very approximately) can be #P-hard.Classical results (going back to Bakry and Emery) on sampling focus on log-concave distributions, and show a natural Markov chain called Langevin diffusion mix in polynomial time.  However, all log-concave distributions are uni-modal, while in practice it is very common for the distribution of interest to have multiple modes.
In this case, Langevin diffusion suffers from torpid mixing.We address this problem by combining Langevin diffusion with simulated tempering. The result is a Markov chain that mixes more rapidly by transitioning between different temperatures of the distribution. We analyze this Markov chain for a mixture of (strongly) log-concave distributions of the same shape. In particular, our technique applies to the canonical multi-modal distribution: a mixture of gaussians (of equal variance). Our algorithm efficiently samples from these distributions given only access to the gradient of the log-pdf. To the best of our knowledge, this is the first result that proves fast mixing for multimodal distributions.",https://proceedings.neurips.cc/paper_files/paper/2018/file/c6ede20e6f597abf4b3f6bb30cee16c7-Paper.pdf,2018,"distributions, log, sampling, chain, concave, diffusion, distribution, langevin, markov, however","chain, distribution, markov, distributions, tempering, simulated, log, langevin, mixture, algorithm","{'concavity': 0.306560670212658, 'simulated': 0.306560670212658, 'tempering': 0.306560670212658, 'log': 0.28935551414584576, 'modal': 0.28935551414584576, 'langevin': 0.26767958099138045, 'beyond': 0.2599431055801662, 'provable': 0.2599431055801662, 'guarantees': 0.2534020088219078, 'distributions': 0.2427379495133539}","distributions, langevin, diffusion, concave, log, chain, sampling, markov, mixing, modal"
M-Walk: Learning to Walk over Graphs using Monte Carlo Tree Search,"Yelong Shen, Jianshu Chen, Po-Sen Huang, Yuqing Guo, Jianfeng Gao","Learning to walk over a graph towards a target node for a given query and a source node is an important problem in applications such as knowledge base completion (KBC). It can be formulated as a reinforcement learning (RL) problem with a known state transition model. To overcome the challenge of sparse rewards, we develop a graph-walking agent called M-Walk, which consists of a deep recurrent neural network (RNN) and Monte Carlo Tree Search (MCTS). The RNN encodes the state (i.e., history of the walked path) and maps it separately to a policy and Q-values. In order to effectively train the agent from sparse rewards, we combine MCTS with the neural policy to generate trajectories yielding more positive rewards. From these trajectories, the network is improved in an off-policy manner using Q-learning, which modifies the RNN policy via parameter sharing. Our proposed RL algorithm repeatedly applies this policy-improvement step to learn the model. At test time, MCTS is combined with the neural policy to predict the target node. Experimental results on several graph-walking benchmarks show that M-Walk is able to learn better policies than other RL-based methods, which are mainly based on policy gradients. M-Walk also outperforms traditional KBC baselines.",https://proceedings.neurips.cc/paper_files/paper/2018/file/c6f798b844366ccd65d99bc7f31e0e02-Paper.pdf,2018,"policy, walk, graph, learning, mcts, neural, node, rewards, rl, rnn","walk, policy, mcts, st, node, nt, model, graph, state, set","{'walk': 0.7169468283667233, 'tree': 0.3007203846478683, 'carlo': 0.295181680015283, 'monte': 0.295181680015283, 'search': 0.28559720479769385, 'graphs': 0.27749364646614355, 'using': 0.2172951028355815, 'learning': 0.12451665826380728}","policy, walk, mcts, rnn, node, kbc, rewards, walking, rl, graph"
Unsupervised Image-to-Image Translation Using Domain-Specific Variational Information Bound,"Hadi Kazemi, Sobhan Soleymani, Fariborz Taherkhani, Seyed Iranmanesh, Nasser Nasrabadi","Unsupervised image-to-image translation is a class of computer vision problems which aims at modeling conditional distribution of images in the target domain, given a set of unpaired images in the source and target domains. An image in the source domain might have multiple representations in the target domain. Therefore, ambiguity in modeling of the conditional distribution arises, specially when the images in the source and target domains come from different modalities. Current approaches mostly rely on simplifying assumptions to map both domains into a shared-latent space. Consequently, they are only able to model the domain-invariant information between the two modalities. These approaches cannot model domain-specific information which has no representation in the target domain. In this work, we propose an unsupervised image-to-image translation framework which maximizes a domain-specific variational information bound and learns the target domain-invariant representation of the two domain. The proposed framework makes it possible to map a single source image into multiple images in the target domain, utilizing several target domain-specific codes sampled randomly from the prior distribution, or extracted from reference images.",https://proceedings.neurips.cc/paper_files/paper/2018/file/c7c46d4baf816bfb07c7f3bf96d88544-Paper.pdf,2018,"domain, target, image, images, source, distribution, domains, information, specific, approaches","domain, image, speciﬁc, information, code, images, vy1, target, invariant, translation","{'image': 0.5321037147820941, 'bound': 0.3758771608880275, 'specific': 0.3758771608880275, 'translation': 0.3153202439296619, 'information': 0.2771130832360731, 'unsupervised': 0.2771130832360731, 'domain': 0.26605185739104703, 'variational': 0.24205656754807944, 'using': 0.22784469669745822}","domain, target, image, images, source, domains, modalities, specific, invariant, translation"
Group Equivariant Capsule Networks,"Jan Eric Lenssen, Matthias Fey, Pascal Libuschewski","We present group equivariant capsule networks, a framework to introduce guaranteed equivariance and invariance properties to the capsule network idea. Our work can be divided into two contributions. First, we present a generic routing by agreement algorithm defined on elements of a group and prove that equivariance of output pose vectors, as well as invariance of output activations, hold under certain conditions. Second, we connect the resulting equivariant capsule networks with work from the field of group convolutional networks. Through this connection, we provide intuitions of how both methods relate and are able to combine the strengths of both approaches in one deep neural network architecture. The resulting framework allows sparse evaluation of the group convolution operator, provides control over specific equivariance and invariance properties, and can use routing by agreement instead of pooling operations. In addition, it is able to provide interpretable and equivariant representation vectors as output capsules, which disentangle evidence of object existence from its pose.",https://proceedings.neurips.cc/paper_files/paper/2018/file/c7d0e7e2922845f3e1185d246d01365d-Paper.pdf,2018,"group, capsule, equivariance, equivariant, invariance, networks, output, able, agreement, framework","group, capsule, pose, output, networks, vectors, equivariant, poses, input, al","{'capsule': 0.5642192849533, 'equivariant': 0.5642192849533, 'group': 0.5404161537204744, 'networks': 0.26695238858557174}","equivariance, equivariant, group, capsule, invariance, routing, agreement, output, pose, vectors"
Fairness Through Computationally-Bounded Awareness,"Michael Kim, Omer Reingold, Guy Rothblum","We study the problem of fair classification within the versatile framework of Dwork et al. [ITCS '12], which assumes the existence of a metric that measures similarity between pairs of individuals.  Unlike earlier work, we do not assume that the entire metric is known to the learning algorithm; instead, the learner can query thisarbitrarymetric a bounded number of times.  We propose a new notion of fairness calledmetric multifairnessand show how to achieve this notion in our setting.
Metric multifairness is parameterized by a similarity metric d on pairs of individuals to classify and a rich collection C of (possibly overlapping) ""comparison sets"" over pairs of individuals.  At a high level, metric multifairness guarantees thatsimilar subpopulations are treated similarly, as long as these subpopulations are identified within the class C.",https://proceedings.neurips.cc/paper_files/paper/2018/file/c8dfece5cc68249206e4690fc4737a8d-Paper.pdf,2018,"metric, individuals, pairs, multifairness, notion, similarity, subpopulations, within, 12, achieve","metric, fairness, x0, learning, multifairness, predictions, hypothesis, class, individuals, algorithm","{'awareness': 0.537209968879108, 'bounded': 0.5070600434212833, 'computationally': 0.5070600434212833, 'fairness': 0.44405593574246977}","metric, individuals, multifairness, pairs, subpopulations, notion, similarity, within, calledmetric, itcs"
Geometry Based Data Generation,"Ofir Lindenbaum, Jay Stanley, Guy Wolf, Smita Krishnaswamy","We propose a new type of generative model for high-dimensional data that learns a manifold geometry of the data, rather than density, and can generate points evenly along this manifold. This is in contrast to existing generative models that represent data density, and are strongly affected by noise and other artifacts of data collection. We demonstrate how this approach corrects sampling biases and artifacts, thus improves several downstream data analysis tasks, such as clustering and classification. Finally, we demonstrate that this approach is especially useful in biology where, despite the advent of single-cell technologies, rare subpopulations and gene-interaction relationships are affected by biased sampling. We show that SUGAR can generate hypothetical populations, and it is able to reveal intrinsic patterns and mutual-information relationships between genes on a single-cell RNA sequencing dataset of hematopoiesis.",https://proceedings.neurips.cc/paper_files/paper/2018/file/c8ed21db4f678f3b13b9d5ee16489088-Paper.pdf,2018,"data, affected, approach, artifacts, cell, demonstrate, density, generate, generative, manifold","data, sugar, points, manifold, al, et, density, new, diffusion, kernel","{'geometry': 0.6251694939769473, 'generation': 0.4924770464733742, 'based': 0.4619954935046186, 'data': 0.391394464037106}","affected, artifacts, cell, data, manifold, relationships, density, generate, corrects, hematopoiesis"
Searching for Efficient Multi-Scale Architectures for Dense Image Prediction,"Liang-Chieh Chen, Maxwell Collins, Yukun Zhu, George Papandreou, Barret Zoph, Florian Schroff, Hartwig Adam, Jon Shlens","The design of neural network architectures is an important component for achieving state-of-the-art performance with machine learning systems across a broad array of tasks. Much work has endeavored to design and build architectures automatically through clever construction of a search space paired with simple learning algorithms. Recent progress has demonstrated that such meta-learning methods may exceed scalable human-invented architectures on image classification tasks. An open question is the degree to which such methods may generalize to new domains. In this work we explore the construction of meta-learning techniques for dense image prediction focused on the tasks of scene parsing, person-part segmentation, and semantic image segmentation. Constructing viable search spaces in this domain is challenging because of the multi-scale representation of visual information and the necessity to operate on high resolution imagery. Based on a survey of techniques in dense image prediction, we construct a recursive search space and demonstrate that even with efficient random search, we can identify architectures that outperform human-invented architectures and achieve state-of-the-art performance on three dense prediction tasks including 82.7% on Cityscapes (street scene parsing), 71.3% on PASCAL-Person-Part (person-part segmentation), and 87.9% on PASCAL VOC 2012 (semantic image segmentation). Additionally, the resulting architecture is more computationally efficient, requiring half the parameters and half the computational cost as previous state of the art systems.",https://proceedings.neurips.cc/paper_files/paper/2018/file/c90070e1f03e982448983975a0f52d57-Paper.pdf,2018,"architectures, image, learning, search, segmentation, tasks, art, dense, part, person","search, image, architecture, dense, network, architectures, dpc, prediction, space, proxy","{'searching': 0.45256221541366604, 'dense': 0.42716299006380926, 'architectures': 0.4091419528865057, 'scale': 0.3403224650094886, 'prediction': 0.30832322543730717, 'image': 0.3023527863376754, 'multi': 0.2769535609878186, 'efficient': 0.25752477473759355}","architectures, segmentation, person, image, search, dense, part, invented, half, pascal"
Adversarial Scene Editing: Automatic Object Removal from Weak Supervision,"Rakshith R. Shetty, Mario Fritz, Bernt Schiele","While great progress has been made recently in automatic image manipulation, it has been limited to object centric images like faces or structured scene datasets.
In this work, we take a step towards general scene-level image editing by developing an automatic interaction-free object removal model. Our model learns to find and remove objects from general scene images using image-level labels and unpaired data in a generative adversarial network (GAN) framework. We achieve this with two key contributions: a two-stage editor architecture consisting of a mask generator and image in-painter that co-operate to remove objects, and a novel GAN based prior for the mask generator that allows us to flexibly incorporate knowledge about object shapes. We experimentally show on two datasets that our method effectively removes a wide variety of objects using weak supervision only.",https://proceedings.neurips.cc/paper_files/paper/2018/file/c911241d00294e8bb714eee2e83fa475-Paper.pdf,2018,"image, object, objects, scene, two, automatic, datasets, gan, general, generator","image, mask, object, removal, masks, model, images, generator, loss, painter","{'editing': 0.4121087301672485, 'weak': 0.4121087301672485, 'removal': 0.38897988257533533, 'supervision': 0.372569704052926, 'automatic': 0.3494408564610128, 'scene': 0.32631200886909956, 'object': 0.29003345626005084, 'adversarial': 0.23981536766213551}","scene, mask, objects, image, object, remove, automatic, generator, gan, two"
Variational Inverse Control with Events: A General Framework for Data-Driven Reward Definition,"Justin Fu, Avi Singh, Dibya Ghosh, Larry Yang, Sergey Levine","The design of a reward function often poses a major practical challenge to real-world applications of reinforcement learning. Approaches such as inverse reinforcement learning attempt to overcome this challenge, but require expert demonstrations, which can be difficult or expensive to obtain in practice. We propose inverse event-based control, which generalizes inverse reinforcement learning methods to cases where full demonstrations are not needed, such as when only samples of desired goal states are available. Our method is grounded in an alternative perspective on control and reinforcement learning, where an agent's goal is to maximize the probability that one or more events will happen at some point in the future, rather than maximizing cumulative rewards. We demonstrate the effectiveness of our methods on continuous control tasks, with a focus on high-dimensional observations like images where rewards are hard or even impossible to specify.",https://proceedings.neurips.cc/paper_files/paper/2018/file/c9319967c038f9b923068dabdf60cfe3-Paper.pdf,2018,"learning, reinforcement, control, inverse, challenge, demonstrations, goal, methods, rewards, agent","event, learning, et, st, reward, query, goal, inference, al, control","{'definition': 0.38889141686868206, 'events': 0.38889141686868206, 'reward': 0.3515799338506775, 'driven': 0.32975411543885064, 'inverse': 0.31426845083267296, 'control': 0.2971263757959531, 'general': 0.2971263757959531, 'framework': 0.29244263242084606, 'variational': 0.23638211648064533, 'data': 0.22011061176436936}","inverse, reinforcement, control, demonstrations, rewards, goal, challenge, learning, happen, cumulative"
MULAN: A Blind and Off-Grid Method for Multichannel Echo Retrieval,"Helena Peic Tukuljac, Antoine Deleforge, Remi Gribonval","This paper addresses the general problem of blind echo retrieval, i.e., given M sensors measuring in the discrete-time domain M mixtures of K delayed and attenuated copies of an unknown source signal, can the echo location and weights be recovered? This problem has broad applications in fields such as sonars, seismology, ultrasounds or room acoustics. It belongs to the broader class of blind channel identification problems, which have been intensively studied in signal processing. All existing methods proceed in two steps: (i) blind estimation of sparse discrete-time filters and (ii) echo information retrieval by peak picking. The precision of these methods is fundamentally limited by the rate at which the signals are sampled: estimated echo locations are necessary on-grid, and since true locations never match the sampling grid, the weight estimation precision is also strongly limited. This is the so-called basis-mismatch problem in compressed sensing. We propose a radically different approach to the problem, building on top of the framework of finite-rate-of-innovation sampling. The approach operates directly in the parameter-space of echo locations and weights, and enables near-exact blind and off-grid echo retrieval from discrete-time measurements. It is shown to outperform conventional methods by several orders of magnitudes in precision.",https://proceedings.neurips.cc/paper_files/paper/2018/file/c9f95a0a5af052bffce5c89917335f67-Paper.pdf,2018,"echo, blind, problem, discrete, grid, locations, methods, precision, retrieval, time","ﬁlters, time, hm, discrete, echo, grid, methods, signal, locations, xm","{'echo': 0.41348880996723486, 'grid': 0.41348880996723486, 'mulan': 0.41348880996723486, 'multichannel': 0.3902825079245373, 'blind': 0.35061107255203333, 'retrieval': 0.35061107255203333, 'method': 0.29816828756861213}","echo, blind, grid, locations, retrieval, precision, discrete, signal, weights, problem"
Diminishing Returns Shape Constraints for Interpretability and Regularization,"Maya Gupta, Dara Bahri, Andrew Cotter, Kevin Canini","We investigate machine learning models that can provide diminishing returns and accelerating returns guarantees to capture prior knowledge or policies about how outputs should depend on inputs.  We show that one can build flexible, nonlinear, multi-dimensional models using lattice functions with any combination of concavity/convexity and monotonicity constraints on any subsets of features, and compare to new shape-constrained neural networks.  We demonstrate on real-world examples that these shape constrained models can provide tuning-free regularization and improve model understandability.",https://proceedings.neurips.cc/paper_files/paper/2018/file/caa202034f268232c26fac9435f54e15-Paper.pdf,2018,"models, constrained, provide, returns, shape, accelerating, build, capture, combination, compare","constraints, models, shape, model, convex, lattice, linear, returns, concave, constrained","{'diminishing': 0.4578623418555532, 'returns': 0.4578623418555532, 'shape': 0.4321656566177622, 'interpretability': 0.3882368830534541, 'constraints': 0.3700047947269371, 'regularization': 0.326076021162629}","returns, shape, constrained, understandability, concavity, lattice, monotonicity, models, diminishing, accelerating"
Breaking the Activation Function Bottleneck through Adaptive Parameterization,"Sebastian Flennerhag, Hujun Yin, John Keane, Mark Elliot","Standard neural network architectures are non-linear only by virtue of a simple element-wise activation function, making them both brittle and excessively large. In this paper, we consider methods for making the feed-forward layer more flexible while preserving its basic structure. We develop simple drop-in replacements that learn to adapt their parameterization conditional on the input, thereby increasing statistical efficiency significantly. We present an adaptive LSTM that advances the state of the art for the Penn Treebank and Wikitext-2 word-modeling tasks while using fewer parameters and converging in half as many iterations.",https://proceedings.neurips.cc/paper_files/paper/2018/file/cac8e13055d2e4f62b6322254203b293-Paper.pdf,2018,"making, simple, activation, adapt, adaptive, advances, architectures, art, basic, brittle","adaptation, al, et, lstm, model, adaptive, layer, policy, alstm, feed","{'bottleneck': 0.43964028232093805, 'parameterization': 0.43964028232093805, 'activation': 0.4210928564983911, 'breaking': 0.4210928564983911, 'function': 0.3949517280221715, 'adaptive': 0.32063622575417616}","making, replacements, brittle, excessively, treebank, virtue, wikitext, parameterization, penn, converging"
Sketching Method for Large Scale Combinatorial Inference,"Wei Sun, Junwei Lu, Han Liu","We present computationally efficient algorithms to test various combinatorial structures of large-scale graphical models. In order to test the hypotheses on their topological structures, we propose two adjacency matrix sketching frameworks: neighborhood sketching and subgraph sketching. The neighborhood sketching algorithm is proposed to test the connectivity of graphical models. This algorithm randomly subsamples vertices and conducts neighborhood regression and screening. The global sketching algorithm is proposed to test the topological properties requiring exponential computation complexity, especially testing the chromatic number and the maximum clique. This algorithm infers the corresponding property based on the sampled subgraph. Our algorithms are shown to substantially accelerate the computation of existing methods. We validate our theory and method through both synthetic simulations and a real application in neuroscience.",https://proceedings.neurips.cc/paper_files/paper/2018/file/cb463625fc9dde2d82207e15bde1b674-Paper.pdf,2018,"sketching, algorithm, test, neighborhood, algorithms, computation, graphical, models, proposed, structures","test, algorithm, testing, method, sketching, graph, fast, clique, set, step","{'combinatorial': 0.47857855181650133, 'sketching': 0.47857855181650133, 'scale': 0.398079520581559, 'method': 0.3817290392585296, 'large': 0.3644080354564949, 'inference': 0.3239561066834436}","sketching, neighborhood, test, subgraph, topological, graphical, structures, algorithm, computation, adjacency"
Symbolic Graph Reasoning Meets Convolutions,"Xiaodan Liang, Zhiting Hu, Hao Zhang, Liang Lin, Eric P. Xing","Beyond local convolution networks, we explore how to harness various external human knowledge for endowing the networks with the capability of semantic global reasoning. Rather than using separate graphical models (e.g. CRF) or constraints for modeling broader dependencies, we propose a new Symbolic Graph Reasoning (SGR) layer, which performs reasoning over a group of symbolic nodes whose outputs explicitly represent different properties of each semantic in a prior knowledge graph. To cooperate with local convolutions, each SGR is constituted by three modules: a) a primal local-to-semantic voting module where the features of all symbolic nodes are generated by voting from local representations; b) a graph reasoning module propagates information over knowledge graph to achieve global semantic coherency; c) a dual semantic-to-local mapping module learns new associations of the evolved symbolic nodes with local representations, and accordingly enhances local features. The SGR layer can be injected between any convolution layers and instantiated with distinct prior graphs. Extensive experiments show incorporating SGR significantly improves plain ConvNets on three semantic segmentation tasks and one image classification task. More analyses show the SGR layer learns shared symbolic representations for domains/datasets with the different label set given a universal knowledge graph, demonstrating its superior generalization capability.",https://proceedings.neurips.cc/paper_files/paper/2018/file/cbb6a3b884f4f88b3a8e3d44c636cbd8-Paper.pdf,2018,"local, semantic, graph, sgr, symbolic, knowledge, reasoning, layer, module, nodes","sgr, graph, local, reasoning, layer, symbolic, semantic, knowledge, nodes, features","{'meets': 0.48354021293066435, 'symbolic': 0.48354021293066435, 'convolutions': 0.4535223994375794, 'reasoning': 0.4535223994375794, 'graph': 0.34786864097629877}","sgr, symbolic, semantic, local, graph, reasoning, module, nodes, knowledge, voting"
Topkapi: Parallel and Fast Sketches for Finding Top-K Frequent Elements,"Ankush Mandal, He Jiang, Anshumali Shrivastava, Vivek Sarkar","Identifying the top-K frequent items is one of the most common and important operations in large data processing systems. As a result, several solutions have been proposed to solve this problem approximately. In this paper, we identify that in modern distributed settings with both multi-node as well as multi-core parallelism, existing algorithms, although theoretically sound, are suboptimal from the performance perspective. In particular, for identifying top-K frequent items, Count-Min Sketch (CMS) has fantastic update time but lack the important property of reducibility which is needed for exploiting available massive data parallelism. On the other end, popular Frequent algorithm (FA) leads to reducible summaries but the update costs are significant. In this paper, we present Topkapi, a fast and parallel algorithm for finding top-K frequent items, which gives the best of both worlds, i.e., it is reducible as well as efficient update time similar to CMS. Topkapi possesses strong theoretical guarantees and leads to significant performance gains due to increased parallelism, relative to past work.",https://proceedings.neurips.cc/paper_files/paper/2018/file/cc06a6150b92e17dd3076a0f0f9d2af4-Paper.pdf,2018,"frequent, items, parallelism, top, update, algorithm, cms, data, identifying, important","cms, data, topkapi, fa, frequent, threads, node, algorithms, number, parallelism","{'elements': 0.38186825449008455, 'frequent': 0.38186825449008455, 'sketches': 0.38186825449008455, 'topkapi': 0.38186825449008455, 'top': 0.3604365982019329, 'finding': 0.3452305960731259, 'parallel': 0.3237989397849743, 'fast': 0.250523622951057}","frequent, parallelism, items, update, top, cms, reducible, topkapi, identifying, leads"
The Price of Fair PCA: One Extra dimension,"Samira Samadi, Uthaipon Tantipongpipat, Jamie H. Morgenstern, Mohit Singh, Santosh Vempala","We investigate whether the standard dimensionality reduction technique of PCA inadvertently produces data representations with different fidelity for two different populations. We show on several real-world data sets, PCA has higher reconstruction error on population A than on B (for example, women versus men or lower- versus higher-educated individuals). This can happen even when the data set has a similar number of samples from A and B. This motivates our study of dimensionality reduction techniques which maintain similar fidelity for A and B. We define the notion of Fair PCA and give a polynomial-time algorithm for finding a low dimensional representation of the data which is nearly-optimal with respect to this measure. Finally, we show on real-world data sets that our algorithm can be used to efficiently generate a fair low dimensional representation of the data.",https://proceedings.neurips.cc/paper_files/paper/2018/file/cc4af25fa9d2d5c953496579b75f6f6c-Paper.pdf,2018,"data, pca, algorithm, different, dimensional, dimensionality, fair, fidelity, higher, low","data, pca, error, loss, fair, reconstruction, two, algorithm, al, et","{'dimension': 0.445315298925409, 'extra': 0.445315298925409, 'fair': 0.4203227934887659, 'pca': 0.3888359604078232, 'price': 0.3888359604078232, 'one': 0.35260531780901866}","pca, versus, data, fair, fidelity, dimensionality, reduction, higher, similar, sets"
Orthogonally Decoupled Variational Gaussian Processes,"Hugh Salimbeni, Ching-An Cheng, Byron Boots, Marc Deisenroth","Gaussian processes (GPs) provide a powerful non-parametric framework for reasoning over functions. Despite appealing theory, its superlinear computational and memory complexities have presented a long-standing challenge. State-of-the-art sparse variational inference methods trade modeling accuracy against complexity. However, the complexities of these methods still  scale superlinearly in the number of basis functions, implying that that sparse GP methods are able to learn from large datasets only when a small model is used. Recently, a decoupled approach was proposed that removes the unnecessary coupling between the complexities of modeling the mean and the covariance functions of a GP. It achieves a linear complexity in the number of mean parameters, so an expressive posterior mean function can be modeled. While promising, this approach suffers from optimization difficulties due to ill-conditioning and non-convexity. In this work, we propose an alternative decoupled parametrization. It adopts an orthogonal basis in the mean function to model the residues that cannot be learned by the standard coupled approach. Therefore, our method extends, rather than replaces, the coupled approach to achieve strictly better performance. This construction admits a straightforward natural gradient update rule, so the structure of the information manifold that is lost during decoupling can be leveraged to speed up learning. Empirically, our algorithm demonstrates significantly faster convergence in multiple experiments.",https://proceedings.neurips.cc/paper_files/paper/2018/file/cc638784cf213986ec75983a4aa08cdb-Paper.pdf,2018,"approach, mean, complexities, functions, methods, basis, complexity, coupled, decoupled, function","decoupled, basis, natural, parameters, gp, coupled, variational, mean, function, posterior","{'decoupled': 0.5541354600883837, 'orthogonally': 0.5541354600883837, 'processes': 0.37752341349899926, 'gaussian': 0.36040793776664265, 'variational': 0.3368233578600662}","complexities, mean, decoupled, coupled, gp, basis, functions, approach, modeling, sparse"
Algorithmic Assurance: An Active Approach to Algorithmic Testing using Bayesian Optimisation,"Shivapratap Gopakumar, Sunil Gupta, Santu Rana, Vu Nguyen, Svetha Venkatesh","We introduce algorithmic assurance, the problem of testing whether
machine learning algorithms are conforming to their intended design
goal. We address this problem by proposing an efficient framework
for algorithmic testing. To provide assurance, we need to efficiently
discover scenarios where an algorithm decision deviates maximally
from its intended gold standard. We mathematically formulate this
task as an optimisation problem of an expensive, black-box function.
We use an active learning approach based on Bayesian optimisation
to solve this optimisation problem. We extend this framework to algorithms
with vector-valued outputs by making appropriate modification in Bayesian
optimisation via the EXP3 algorithm. We theoretically analyse our
methods for convergence. Using two real-world applications, we demonstrate
the efficiency of our methods. The significance of our problem formulation
and initial solutions is that it will serve as the foundation in assuring
humans about machines making complex decisions.",https://proceedings.neurips.cc/paper_files/paper/2018/file/cc70903297fe1e25537ae50aea186306-Paper.pdf,2018,"problem, optimisation, algorithm, algorithmic, algorithms, assurance, bayesian, framework, intended, learning","algorithm, function, task, error, assurance, bo, optimisation, algorithmic, ht, phase","{'algorithmic': 0.6589061709983731, 'assurance': 0.3644163528000243, 'optimisation': 0.34396415252634405, 'active': 0.28854868495182595, 'testing': 0.28323416016515246, 'approach': 0.25358541765098813, 'bayesian': 0.20850005310780081, 'using': 0.20850005310780081}","optimisation, assurance, intended, problem, algorithmic, testing, making, assuring, exp3, gold"
Domain-Invariant Projection Learning for Zero-Shot Recognition,"An Zhao, Mingyu Ding, Jiechao Guan, Zhiwu Lu, Tao Xiang, Ji-Rong Wen","Zero-shot learning (ZSL) aims to recognize unseen object classes without any training samples, which can be regarded as a form of transfer learning from seen classes to unseen ones. This is made possible by learning a projection between a feature space and a semantic space (e.g. attribute space). Key to ZSL is thus to learn a projection function that is robust against the often large domain gap between the seen and unseen classes. In this paper, we propose a novel ZSL model termed domain-invariant projection learning (DIPL). Our model has two novel components: (1) A domain-invariant feature self-reconstruction task is introduced to the seen/unseen class data, resulting in a simple linear formulation that casts ZSL into a min-min optimization problem. Solving the problem is non-trivial, and a novel iterative algorithm is formulated as the solver, with rigorous theoretic algorithm analysis provided. (2) To further align the two domains via the learned projection, shared semantic structure among seen and unseen classes is explored via forming superclasses in the semantic space. Extensive experiments show that our model outperforms the state-of-the-art alternatives by significant margins.",https://proceedings.neurips.cc/paper_files/paper/2018/file/ccb1d45fb76f7c5a0bf619f979c6cf36-Paper.pdf,2018,"unseen, classes, learning, projection, seen, space, zsl, domain, model, novel","zsl, model, unseen, class, seen, classes, semantic, space, dipl, projection","{'projection': 0.47794611792961034, 'invariant': 0.4092003900520346, 'zero': 0.4092003900520346, 'recognition': 0.38688016104800454, 'shot': 0.3807815863711023, 'domain': 0.3382978952687506, 'learning': 0.16601580781193914}","zsl, unseen, projection, seen, classes, semantic, space, min, domain, invariant"
High Dimensional Linear Regression using Lattice Basis Reduction,"Ilias Zadik, David Gamarnik","We consider a high dimensional linear regression problem where the goal is to efficiently recover an unknown vector \beta^* from n noisy linear observations Y=X \beta^+W  in R^n, for known X in R^{n \times p} and unknown W in R^n. Unlike most of the literature on this model we make no sparsity assumption on \beta^. Instead we adopt a regularization based on assuming that the underlying vectors \beta^* have rational entries with the same denominator Q. We call this Q-rationality assumption.  We propose a new polynomial-time algorithm for this task which is based on the seminal Lenstra-Lenstra-Lovasz (LLL) lattice basis reduction algorithm.  We establish that under the Q-rationality assumption, our algorithm recovers exactly the vector \beta^* for a large class of distributions for the iid entries of X and non-zero noise W. We prove that it is successful under small noise, even when the learner has access to only one observation (n=1). Furthermore, we prove that in the case of the Gaussian white noise for W, n=o(p/\log p) and Q sufficiently large, our algorithm tolerates a nearly optimal information-theoretic level of the noise.",https://proceedings.neurips.cc/paper_files/paper/2018/file/ccc0aa1b81bf81e16c676ddb977c5881-Paper.pdf,2018,"beta, algorithm, noise, assumption, based, entries, large, lenstra, linear, prove","algorithm, log, vector, 2n, entries, noise, assumption, iid, regression, lbr","{'basis': 0.4461508360583986, 'lattice': 0.4461508360583986, 'dimensional': 0.3605405673767293, 'high': 0.3467603920269868, 'reduction': 0.34087453522053257, 'regression': 0.29806940087969797, 'linear': 0.2854223720935702, 'using': 0.25526426653886325}","beta, noise, lenstra, rationality, assumption, entries, vector, algorithm, unknown, denominator"
A Retrieve-and-Edit Framework for Predicting Structured Outputs,"Tatsunori B. Hashimoto, Kelvin Guu, Yonatan Oren, Percy S. Liang","For the task of generating complex outputs such as source code, editing existing outputs can be easier than generating complex outputs from scratch.
With this motivation, we propose an approach that first retrieves a training example based on the input (e.g., natural language description) and then edits it to the desired output (e.g., code).
Our contribution is a computationally efficient method for learning a retrieval model that embeds the input in a task-dependent way without relying on a hand-crafted metric or incurring the expense of jointly training the retriever with the editor.
Our retrieve-and-edit framework can be applied on top of any base model.
We show that on a new autocomplete task for GitHub Python code and the Hearthstone cards benchmark, retrieve-and-edit significantly boosts the performance of a vanilla sequence-to-sequence model on both tasks.",https://proceedings.neurips.cc/paper_files/paper/2018/file/cd17d3ce3b64f227987cd92cd701cc58-Paper.pdf,2018,"code, model, outputs, task, complex, edit, generating, input, retrieve, sequence","edit, sequence, pret, retrieve, pθ, retriever, editor, code, based, model","{'predicting': 0.45442334837681686, 'retrieve': 0.45442334837681686, 'edit': 0.42891967034857215, 'outputs': 0.42891967034857215, 'framework': 0.3417220192280195, 'structured': 0.3198135945373081}","outputs, retrieve, code, edit, task, generating, sequence, complex, autocomplete, cards"
Efficient inference for time-varying behavior during learning,"Nicholas A. Roy, Ji Hyun Bak, Athena Akrami, Carlos Brody, Jonathan W. Pillow","The process of learning new behaviors over time is a problem of great interest in both neuroscience and artificial intelligence. However, most standard analyses of animal training data either treat behavior as fixed or track only coarse performance statistics (e.g., accuracy, bias), providing limited insight into the evolution of the policies governing behavior. To overcome these limitations, we propose a dynamic psychophysical model that efficiently tracks trial-to-trial changes in behavior over the course of training. Our model consists of a dynamic logistic regression model, parametrized by a set of time-varying weights that express dependence on sensory stimuli as well as task-irrelevant covariates, such as stimulus, choice, and answer history. Our implementation scales to large behavioral datasets, allowing us to infer 500K parameters (e.g. 10 weights over 50K trials) in minutes on a desktop computer. We optimize hyperparameters governing how rapidly each weight evolves over time using the decoupled Laplace approximation, an efficient method for maximizing marginal likelihood in non-conjugate models. To illustrate performance, we apply our method to psychophysical data from both rats and human subjects learning a delayed sensory discrimination task. The model successfully tracks the psychophysical weights of rats over the course of training, capturing day-to-day and trial-to-trial fluctuations that underlie changes in performance, choice bias, and dependencies on task history. Finally, we investigate why rats frequently make mistakes on easy trials, and suggest that apparent lapses can be explained by sub-optimal weighting of known task covariates.",https://proceedings.neurips.cc/paper_files/paper/2018/file/cdcb2f5c7b071143529ef7f2705dfbc4-Paper.pdf,2018,"model, task, trial, behavior, performance, psychophysical, rats, time, training, weights","model, weights, behavior, trial, trials, choice, method, weight, training, animal","{'varying': 0.5477839798812849, 'behavior': 0.5246742168723282, 'time': 0.3914672445416882, 'inference': 0.35515886771356053, 'efficient': 0.3302438397172886, 'learning': 0.19027416797603086}","trial, rats, psychophysical, tracks, day, trials, weights, governing, task, behavior"
Re-evaluating evaluation,"David Balduzzi, Karl Tuyls, Julien Perolat, Thore Graepel","Progress in machine learning is measured by careful evaluation on problems of outstanding common interest. However, the proliferation of benchmark suites and environments, adversarial attacks, and other complications has diluted the basic evaluation model by overwhelming researchers with choices. Deliberate or accidental cherry picking is increasingly likely, and designing well-balanced evaluation suites requires increasing effort. In this paper we take a step back and propose Nash averaging. The approach builds on a detailed analysis of the algebraic structure of evaluation in two basic scenarios: agent-vs-agent and agent-vs-task. The key strength of Nash averaging is that it automatically adapts to redundancies in evaluation data, so that results are not biased by the incorporation of easy tasks or weak agents. Nash averaging thus encourages maximally inclusive evaluation -- since there is no harm (computational cost aside) from including all available tasks and agents.",https://proceedings.neurips.cc/paper_files/paper/2018/file/cdf1035c34ec380218a8cc9a43d438f9-Paper.pdf,2018,"evaluation, agent, averaging, nash, agents, basic, suites, tasks, vs, accidental","agents, nash, elo, tasks, evaluation, environments, agent, pij, averaging, antisymmetric","{'evaluating': 0.7627164975871511, 'evaluation': 0.6467329775946093}","evaluation, averaging, nash, suites, vs, agent, basic, agents, cherry, deliberate"
Learning Abstract Options,"Matthew Riemer, Miao Liu, Gerald Tesauro","Building systems that autonomously create temporal abstractions from data is a key challenge in scaling learning and planning in reinforcement learning. One popular approach for addressing this challenge is the options framework (Sutton et al., 1999). However, only recently in (Bacon et al., 2017) was a policy gradient theorem derived for online learning of general purpose options in an end to end fashion. In this work, we extend previous work on this topic that only focuses on learning a two-level hierarchy including options and primitive actions to enable learning simultaneously at multiple resolutions in time. We achieve this by considering an arbitrarily deep hierarchy of options where high level temporally extended options are composed of lower level options with finer resolutions in time. We extend results from (Bacon et al., 2017) and derive policy gradient theorems for a deep hierarchy of options. Our proposed hierarchical option-critic architecture is capable of learning internal policies, termination conditions, and hierarchical compositions over options without the need for any intrinsic rewards or subgoals.  Our empirical results in both discrete and continuous environments demonstrate the efficiency of our framework.",https://proceedings.neurips.cc/paper_files/paper/2018/file/cdf28f8b7d14ab02d12a2329d71e4079-Paper.pdf,2018,"options, learning, al, et, hierarchy, level, 2017, bacon, challenge, deep","options, option, o1, level, learning, policy, critic, function, state, gradient","{'abstract': 0.6888380790237318, 'options': 0.6888380790237318, 'learning': 0.22584109850465653}","options, hierarchy, bacon, al, et, resolutions, level, 2017, learning, hierarchical"
Optimal Algorithms for Continuous Non-monotone Submodular and DR-Submodular Maximization,"Rad Niazadeh, Tim Roughgarden, Joshua Wang","In this paper we study the fundamental problems of maximizing a continuous non monotone submodular function over a hypercube, with and without coordinate-wise concavity. This family of optimization problems has several applications in machine learning, economics, and communication systems. Our main result is the first 1/2 approximation algorithm for continuous submodular function maximization; this approximation factor of is the best possible for algorithms that use only polynomially many queries.  For the special case of DR-submodular maximization, we provide a faster 1/2-approximation algorithm that runs in (almost) linear time. Both of these results improve upon prior work [Bian et al., 2017, Soma and Yoshida, 2017, Buchbinder et al., 2012].Our first algorithm is a single-pass algorithm that uses novel ideas such as reducing the guaranteed approximation problem to analyzing a zero-sum game for each coordinate, and incorporates the geometry of this zero-sum game to fix the value at this coordinate. Our second algorithm is a faster single-pass algorithm that
exploits coordinate-wise concavity to identify a monotone equilibrium condition sufficient for getting the required approximation guarantee, and hunts for the equilibrium point using binary search. We further run experiments to verify the performance of our proposed algorithms in related machine learning applications.",https://proceedings.neurips.cc/paper_files/paper/2018/file/cdfa4c42f465a5a66871587c69fcfa34-Paper.pdf,2018,"algorithm, approximation, coordinate, submodular, 2017, al, algorithms, applications, concavity, continuous","algorithm, submodular, al, et, function, dr, zi, continuous, point, coordinate","{'submodular': 0.5828863437447691, 'dr': 0.3933577627547757, 'monotone': 0.35561776426573466, 'maximization': 0.3178777657766937, 'continuous': 0.29580128188333155, 'algorithms': 0.2558386356583948, 'optimal': 0.2558386356583948, 'non': 0.24412639997540345}","coordinate, approximation, submodular, concavity, algorithm, monotone, 2017, pass, equilibrium, maximization"
Sequential Context Encoding for Duplicate Removal,"Lu Qi, Shu Liu, Jianping Shi, Jiaya Jia","Duplicate removal is a critical step to accomplish a reasonable amount of predictions in prevalent proposal-based object detection frameworks. Albeit simple and effective, most previous algorithms utilized a greedy process without making sufficient use of properties of input data. In this work, we design a new two-stage framework to effectively select the appropriate proposal candidate for each object. The first stage suppresses most of easy negative object proposals, while the second stage selects true positives in the reduced proposal set. These two stages share the same network structure, an encoder and a decoder formed as recurrent neural networks (RNN) with global attention and context gate. The encoder scans proposal candidates in a sequential manner to capture the global context information, which is then fed to the decoder to extract optimal proposals. In our extensive experiments, the proposed method outperforms other alternatives by a large margin.",https://proceedings.neurips.cc/paper_files/paper/2018/file/ce5140df15d046a66883807d18d0264b-Paper.pdf,2018,"proposal, object, stage, context, decoder, encoder, global, proposals, two, accomplish","stage, object, proposal, proposals, feature, detection, nms, network, context, model","{'duplicate': 0.5014814138219937, 'removal': 0.473336688070227, 'encoding': 0.43787852705866664, 'context': 0.42522296160559164, 'sequential': 0.38976480059403135}","proposal, stage, proposals, object, decoder, encoder, context, global, scans, suppresses"
PG-TS: Improved Thompson Sampling for Logistic Contextual Bandits,"Bianca Dumitrascu, Karen Feng, Barbara Engelhardt","We address the problem of regret minimization in logistic contextual bandits, where a learner decides among sequential actions or arms given their respective contexts to maximize binary rewards. Using a fast inference procedure with Polya-Gamma distributed augmentation variables, we propose an improved version of Thompson Sampling, a Bayesian formulation of contextual bandits with near-optimal performance. Our approach, Polya-Gamma augmented Thompson Sampling (PG-TS), achieves state-of-the-art performance on simulated and real data. PG-TS explores the action space efficiently and exploits high-reward arms, quickly converging to solutions of low regret. Its explicit estimation of the posterior distribution of the context feature covariance leads to substantial empirical gains over approximate approaches. PG-TS is the first approach to demonstrate the benefits of Polya-Gamma augmentation in bandits and to propose an efficient Gibbs sampler for approximating the analytically unsolvable integral of logistic contextual bandits.",https://proceedings.neurips.cc/paper_files/paper/2018/file/ce6c92303f38d297e263c7180f03d402-Paper.pdf,2018,"bandits, contextual, gamma, pg, polya, ts, approach, arms, augmentation, logistic","ts, pg, reward, arm, laplace, logistic, rewards, data, arms, distribution","{'pg': 0.41200956037841124, 'ts': 0.41200956037841124, 'logistic': 0.38888627850924024, 'thompson': 0.37248004892996095, 'contextual': 0.3262334851916189, 'bandits': 0.3147894300528076, 'improved': 0.301036191847072, 'sampling': 0.27272487769713166}","bandits, pg, polya, ts, gamma, contextual, thompson, arms, logistic, augmentation"
Variance-Reduced Stochastic Gradient Descent on Streaming Data,"Ellango Jothimurugesan, Ashraf Tahmasbi, Phillip Gibbons, Srikanta Tirthapura","We present an algorithm STRSAGA for efficiently maintaining a machine learning model over data points that arrive over time, quickly updating the model as new training data is observed. We present a competitive analysis comparing the sub-optimality of the model maintained by STRSAGA with that of an offline algorithm that is given the entire data beforehand, and analyze the risk-competitiveness of STRSAGA under different arrival patterns. Our theoretical and experimental results show that the risk of STRSAGA is comparable to that of offline algorithms on a variety of input arrival patterns, and its experimental performance is significantly better than prior algorithms suited for streaming data, such as SGD and SSVRG.",https://proceedings.neurips.cc/paper_files/paper/2018/file/cebd648f9146a6345d604ab093b02c73-Paper.pdf,2018,"data, strsaga, model, algorithm, algorithms, arrival, experimental, offline, patterns, present","data, time, algorithm, points, strsaga, risk, dynasaga, streaming, ni, step","{'reduced': 0.5185224018005246, 'streaming': 0.4286088562641328, 'variance': 0.4105714686182263, 'descent': 0.3401752021880209, 'gradient': 0.2983215144590683, 'stochastic': 0.2966715063204666, 'data': 0.29348110583881293}","strsaga, arrival, offline, patterns, risk, data, competitiveness, ssvrg, experimental, beforehand"
Reinforced Continual Learning,"Ju Xu, Zhanxing Zhu","Most artificial intelligence models are limited in their ability to solve new tasks faster, without forgetting previously acquired knowledge. The recently emerging paradigm of continual learning aims to solve this issue, in which the model learns various tasks in a sequential fashion. In this work, a novel approach for continual learning is proposed,  which  searches for the best neural architecture for each coming task via sophisticatedly designed reinforcement learning strategies.  We name it as Reinforced Continual Learning. Our method not only has good performance on preventing catastrophic forgetting but also fits new tasks well. The experiments on sequential classification tasks for variants of MNIST and CIFAR-100 datasets demonstrate that the proposed approach outperforms existing continual learning alternatives for deep networks.",https://proceedings.neurips.cc/paper_files/paper/2018/file/cee631121c2ec9232f3a2f028ad5c89b-Paper.pdf,2018,"learning, continual, tasks, approach, forgetting, new, proposed, sequential, solve, 100","network, task, rcl, accuracy, learning, tasks, parameters, model, number, controller","{'continual': 0.7207856159083227, 'reinforced': 0.6516311447602321, 'learning': 0.23631535514673951}","continual, forgetting, tasks, learning, sequential, solve, coming, sophisticatedly, reinforced, searches"
GradiVeQ: Vector Quantization for Bandwidth-Efficient Gradient Aggregation in Distributed CNN Training,"Mingchao Yu, Zhifeng Lin, Krishna Narra, Songze Li, Youjie Li, Nam Sung Kim, Alexander Schwing, Murali Annavaram, Salman Avestimehr","Data parallelism can boost the training speed of convolutional neural networks (CNN), but could suffer from significant communication costs caused by gradient aggregation. To alleviate this problem, several scalar quantization techniques have been developed to compress the gradients. But these techniques could perform poorly when used together with decentralized aggregation protocols like ring all-reduce (RAR), mainly due to their inability to directly aggregate compressed gradients. In this paper, we empirically demonstrate the strong linear correlations between CNN gradients, and propose a gradient vector quantization technique, named GradiVeQ, to exploit these correlations through principal component analysis (PCA) for substantial gradient dimension reduction. GradiveQ enables direct aggregation of compressed gradients, hence allows us to build a distributed learning system that parallelizes GradiveQ gradient compression and RAR communications. Extensive experiments on popular CNNs demonstrate that applying GradiveQ slashes the wall-clock gradient aggregation time of the original RAR by more than 5x without noticeable accuracy loss, and reduce the end-to-end training time by almost 50%. The results also show that \GradiveQ is compatible with scalar quantization techniques such as QSGD (Quantized SGD), and achieves a much higher speed-up gain under the same compression ratio.",https://proceedings.neurips.cc/paper_files/paper/2018/file/cf05968255451bdefe3c5bc64d550517-Paper.pdf,2018,"gradient, gradiveq, aggregation, gradients, quantization, rar, techniques, cnn, compressed, compression","gradient, compression, gradiveq, aggregation, time, node, gradients, rar, training, iterations","{'bandwidth': 0.37222916073787493, 'gradiveq': 0.37222916073787493, 'quantization': 0.37222916073787493, 'aggregation': 0.35133848092995595, 'cnn': 0.35133848092995595, 'vector': 0.35133848092995595, 'distributed': 0.2562364028447936, 'training': 0.24007590099064594, 'gradient': 0.2141546181448673, 'efficient': 0.2118122713407817}","gradiveq, aggregation, rar, quantization, gradients, gradient, scalar, compressed, correlations, techniques"
Theoretical Linear Convergence of Unfolded ISTA and Its Practical Weights and Thresholds,"Xiaohan Chen, Jialin Liu, Zhangyang Wang, Wotao Yin","In recent years, unfolding iterative algorithms as neural networks has become an empirical success in solving sparse recovery problems. However, its theoretical understanding is still immature, which prevents us from fully utilizing the power of neural networks. In this work, we study unfolded ISTA (Iterative Shrinkage Thresholding Algorithm) for sparse signal recovery. We introduce a weight structure that is necessary for asymptotic convergence to the true sparse signal. With this structure, unfolded ISTA can attain a linear convergence, which is better than the sublinear convergence of ISTA/FISTA in general cases. Furthermore, we propose to incorporate thresholding in the network to perform support selection, which is easy to implement and able to boost the convergence rate both theoretically and empirically. Extensive simulations, including sparse vector recovery and a compressive sensing experiment on real image data, corroborate our theoretical results and demonstrate their practical usefulness. We have made our codes publicly available: https://github.com/xchen-tamu/linear-lista-cpss.",https://proceedings.neurips.cc/paper_files/paper/2018/file/cf8c9be2a4508a24ae92c9d3d379131d-Paper.pdf,2018,"convergence, sparse, ista, recovery, iterative, linear, networks, neural, signal, structure","lista, ista, 10, θk, xk, convergence, support, cpss, 16, cp","{'ista': 0.39960228261730985, 'thresholds': 0.39960228261730985, 'unfolded': 0.39960228261730985, 'theoretical': 0.36126316497392347, 'weights': 0.36126316497392347, 'practical': 0.32292404733053715, 'convergence': 0.3004971011984192, 'linear': 0.2556432089341834}","ista, recovery, sparse, unfolded, convergence, thresholding, iterative, signal, compressive, cpss"
Optimization for Approximate Submodularity,"Yaron Singer, Avinatan Hassidim","We consider the problem of maximizing a submodular function when given access to its approximate version. Submodular functions are heavily studied in a wide variety of disciplines, since they are used to model many real world phenomena, and are amenable to optimization. However, there are many cases in which the phenomena we observe is only approximately submodular and the approximation guarantees cease to hold. We describe a technique which we call the sampled
mean approximation that yields strong guarantees for maximization of submodular functions from approximate surrogates under cardinality and intersection of matroid constraints. In particular, we show tight guarantees for maximization under a cardinality constraint and 1/(1+P) approximation
under intersection of P matroids.",https://proceedings.neurips.cc/paper_files/paper/2018/file/cfa0860e83a4c3a763a7e62d825349f7-Paper.pdf,2018,"submodular, approximation, guarantees, approximate, cardinality, functions, intersection, many, maximization, phenomena","algorithm, approximation, bundle, submodular, fs, value, function, bundles, mean, every","{'submodularity': 0.7250467941494373, 'approximate': 0.5740996455643782, 'optimization': 0.38041653914686596}","submodular, intersection, cardinality, phenomena, guarantees, maximization, approximation, cease, approximate, matroids"
Efficient Neural Network Robustness Certification with General Activation Functions,"Huan Zhang, Tsui-Wei Weng, Pin-Yu Chen, Cho-Jui Hsieh, Luca Daniel","Finding minimum distortion of adversarial examples and thus certifying robustness in neural networks classifiers is known to be a challenging problem. Nevertheless, recently it has been shown to be possible to give a non-trivial certified lower bound of minimum distortion, and some recent progress has been made towards this direction by exploiting the piece-wise linear nature of ReLU activations. However, a generic robustness certification for \textit{general} activation functions still remains largely unexplored. To address this issue, in this paper we introduce CROWN, a general framework to certify robustness of neural networks with general activation functions. The novelty in our algorithm consists of bounding a given activation function with linear and quadratic functions, hence allowing it to tackle general activation functions including but not limited to the four popular choices: ReLU, tanh, sigmoid and arctan. In addition, we facilitate the search for a tighter certified lower bound by \textit{adaptively} selecting appropriate surrogates for each neuron activation. Experimental results show that CROWN on ReLU networks can notably improve the certified lower bounds compared to the current state-of-the-art algorithm Fast-Lin, while having comparable computational efficiency. Furthermore, CROWN also demonstrates its effectiveness and flexibility on networks with general activation functions, including tanh, sigmoid and arctan.",https://proceedings.neurips.cc/paper_files/paper/2018/file/d04863f100d59b3eb688a11f95b0ae60-Paper.pdf,2018,"activation, functions, general, networks, certified, crown, lower, relu, robustness, algorithm","activation, bounds, lower, functions, crown, bound, relu, linear, general, networks","{'activation': 0.44055239269029595, 'certification': 0.44055239269029595, 'general': 0.37231856310633105, 'robustness': 0.37231856310633105, 'functions': 0.35139819987000637, 'network': 0.29424683005106145, 'efficient': 0.2772953369539881, 'neural': 0.21824380394224127}","activation, crown, certified, functions, general, relu, arctan, robustness, sigmoid, tanh"
Neural Edit Operations for Biological Sequences,"Satoshi Koide, Keisuke Kawano, Takuro Kutsuna","The evolution of biological sequences, such as proteins or DNAs, is driven by the three basic edit operations: substitution, insertion, and deletion. Motivated by the recent progress of neural network models for biological tasks, we implement two neural network architectures that can treat such edit operations. The first proposal is the edit invariant neural networks, based on differentiable Needleman-Wunsch algorithms. The second is the use of deep CNNs with concatenations. Our analysis shows that CNNs can recognize star-free regular expressions, and that deeper CNNs can recognize more complex regular expressions including the insertion/deletion of characters. The experimental results for the protein secondary structure prediction task suggest the importance of insertion/deletion. The test accuracy on the widely-used CB513 dataset is 71.5%, which is 1.2-points better than the current best result on non-ensemble models.",https://proceedings.neurips.cc/paper_files/paper/2018/file/d0921d442ee91b896ad95059d13df618-Paper.pdf,2018,"cnns, deletion, edit, insertion, neural, biological, expressions, models, network, operations","regular, cnns, expressions, accuracy, used, cnn, algorithm, fi, einns, neural","{'biological': 0.4943787354941342, 'operations': 0.4943787354941342, 'sequences': 0.4943787354941342, 'edit': 0.4666326345530401, 'neural': 0.22141134127174106}","deletion, insertion, edit, cnns, expressions, recognize, regular, biological, operations, cb513"
Adapted Deep Embeddings: A Synthesis of Methods for k-Shot Inductive Transfer Learning,"Tyler Scott, Karl Ridgeway, Michael C. Mozer","The focus in machine learning has branched beyond training classifiers on a single task to investigating how previously acquired knowledge in a source domain can be leveraged to facilitate learning in a related target domain, known as inductive transfer learning. Three active lines of research have independently explored transfer learning using neural networks. In weight transfer, a model trained on the source domain is used as an initialization point for a network to be trained on the target domain. In deep metric learning, the source domain is used to construct an embedding that captures class structure in both the source and target domains. In few-shot learning, the focus is on generalizing well in the target domain based on a limited number of labeled examples. We compare state-of-the-art methods from these three paradigms and also explore hybrid adapted-embedding methods that use limited target-domain data to fine tune embeddings constructed from source-domain data. We conduct a systematic comparison of methods in a variety of domains, varying the number of labeled instances available in the target domain (k), as well as the number of target-domain classes. We reach three principal conclusions: (1) Deep embeddings are far superior, compared to weight transfer, as a starting point for inter-domain transfer or model re-use (2) Our hybrid methods robustly outperform every few-shot learning and every deep metric learning method previously proposed, with a mean error reduction of 34% over state-of-the-art. (3) Among loss functions for discovering embeddings, the histogram loss (Ustinova & Lempitsky, 2016) is most robust. We hope our results will motivate a unification of research in weight transfer, deep metric learning, and few-shot learning.",https://proceedings.neurips.cc/paper_files/paper/2018/file/d09bf41544a3365a46c9077ebb5e35c3-Paper.pdf,2018,"domain, learning, target, transfer, source, deep, methods, embeddings, metric, number","learning, domain, data, target, methods, adapted, source, 10, embedding, transfer","{'adapted': 0.43591001617829866, 'inductive': 0.43591001617829866, 'embeddings': 0.3603217388862893, 'methods': 0.3388009522382504, 'shot': 0.327800170176657, 'transfer': 0.3229706130018491, 'synthesis': 0.31849914147086034, 'deep': 0.20424800720120084, 'learning': 0.14291660101371645}","domain, target, transfer, source, learning, shot, embeddings, weight, metric, three"
The Importance of Sampling inMeta-Reinforcement Learning,"Bradly Stadie, Ge Yang, Rein Houthooft, Peter Chen, Yan Duan, Yuhuai Wu, Pieter Abbeel, Ilya Sutskever",We interpret meta-reinforcement learning as the problem of learning how to quickly find a good sampling distribution in a new environment. This interpretation leads to the development of two new meta-reinforcement learning algorithms: E-MAML and E-$\text{RL}^2$. Results are presented on a new environment we call `Krazy World': a difficult high-dimensional gridworld which is designed to highlight the importance of correctly differentiating through sampling distributions in  meta-reinforcement learning. Further results are presented on a set of maze environments. We show E-MAML and E-$\text{RL}^2$ deliver better performance than baseline algorithms on both tasks.,https://proceedings.neurips.cc/paper_files/paper/2018/file/d0f5722f11a0cc839fa2ca6ea49d8585-Paper.pdf,2018,"learning, meta, new, reinforcement, algorithms, environment, maml, presented, results, rl","learning, meta, maml, agent, policy, update, gradient, sampling, rl2, rl","{'inmeta': 0.6230979152915531, 'importance': 0.51505062054468, 'sampling': 0.41245232898273027, 'reinforcement': 0.36690799321295225, 'learning': 0.20428765765220952}","meta, maml, reinforcement, presented, text, rl, environment, differentiating, krazy, new"
KONG: Kernels for ordered-neighborhood graphs,"Moez Draief, Konstantin Kutzkov, Kevin Scaman, Milan Vojnovic","We present novel graph kernels for graphs with node and edge labels that have ordered neighborhoods, i.e. when neighbor nodes follow an order. Graphs with ordered neighborhoods are a natural data representation for evolving graphs where edges are created over time, which induces an order. Combining convolutional subgraph kernels and string kernels, we design new scalable algorithms for generation of explicit graph feature maps using sketching techniques. We obtain precise bounds for the approximation accuracy and computational complexity of the proposed approaches and demonstrate their applicability on real datasets.  In particular, our experiments demonstrate that neighborhood ordering results in more informative features. For the special case of general graphs, i.e. graphs without ordered neighborhoods, the new graph kernels yield efficient and simple algorithms for the comparison of label distributions between graphs.",https://proceedings.neurips.cc/paper_files/paper/2018/file/d0fb963ff976f9c37fc81fe03c21ea7b-Paper.pdf,2018,"graphs, kernels, graph, neighborhoods, ordered, algorithms, demonstrate, new, order, accuracy","kernel, graph, graphs, node, kernels, feature, string, strings, explicit, al","{'kong': 0.47939866266323244, 'neighborhood': 0.47939866266323244, 'ordered': 0.47939866266323244, 'kernels': 0.43340362578420233, 'graphs': 0.3502742694906663}","graphs, kernels, neighborhoods, ordered, graph, created, string, subgraph, order, evolving"
Glow: Generative Flow with Invertible 1x1 Convolutions,"Durk P. Kingma, Prafulla Dhariwal","Flow-based generative models are conceptually attractive due to tractability of the exact log-likelihood, tractability of exact latent-variable inference, and parallelizability of both training and synthesis. In this paper we propose Glow, a simple type of generative flow using invertible 1x1 convolution. Using our method we demonstrate a significant improvement in log-likelihood and qualitative sample quality. Perhaps most strikingly, we demonstrate that a generative model optimized towards the plain log-likelihood objective is capable of efficient synthesis of large and subjectively realistic-looking images.",https://proceedings.neurips.cc/paper_files/paper/2018/file/d139db6a236200b21cc7f752979132d0-Paper.pdf,2018,"generative, likelihood, log, demonstrate, exact, flow, synthesis, tractability, using, 1x1","al, et, log, models, model, ﬂow, generative, 2016, dinh, data","{'1x1': 0.4476780361401371, 'flow': 0.4476780361401371, 'glow': 0.4476780361401371, 'invertible': 0.42255292640531034, 'convolutions': 0.37960126761717955, 'generative': 0.275871400987614}","likelihood, tractability, log, synthesis, generative, flow, exact, 1x1, glow, parallelizability"
Efficient Projection onto the Perfect Phylogeny Model,"Bei Jia, Surjyendu Ray, Sam Safavi, José Bento","Several algorithms build on the perfect phylogeny model to infer evolutionary trees. This problem is particularly hard when evolutionary trees are inferred from the fraction of genomes that have mutations in different positions, across different samples. Existing algorithms might do extensive searches over the space of possible trees. At the center of these algorithms is a projection problem that assigns a fitness cost to phylogenetic trees. In order to perform a wide search over the space of the trees, it is critical to solve this projection problem fast. In this paper, we use Moreau's decomposition for proximal operators, and a tree reduction scheme, to develop a new algorithm to compute this projection. Our algorithm terminates with an exact solution in a finite number of steps, and is extremely fast. In particular, it can search over all evolutionary trees with fewer than 11 nodes, a size relevant for several biological problems (more than 2 billion trees) in about 2 hours.",https://proceedings.neurips.cc/paper_files/paper/2018/file/d198bd736a97e7cecfdf8f4f2027ef80-Paper.pdf,2018,"trees, algorithms, evolutionary, problem, projection, algorithm, different, fast, search, several","ti, problem, algorithm, lemma, trees, solve, let, nodes, 10, compute","{'phylogeny': 0.47741865127913274, 'onto': 0.45062440399761916, 'perfect': 0.45062440399761916, 'projection': 0.45062440399761916, 'model': 0.2984632608199155, 'efficient': 0.2716690135384019}","trees, evolutionary, projection, fast, search, algorithms, genomes, moreau, phylogenetic, phylogeny"
"Do Less, Get More: Streaming Submodular Maximization with Subsampling","Moran Feldman, Amin Karbasi, Ehsan Kazemi","In this paper, we develop the first one-pass streaming algorithm for submodular maximization that does not evaluate the entire stream even once. By carefully subsampling each element of the data stream, our algorithm enjoys the tightest approximation guarantees in various settings while having the smallest memory footprint and requiring the lowest number of function evaluations. More specifically, for a monotone submodular function and a $p$-matchoid constraint, our randomized algorithm achieves a $4p$ approximation ratio (in expectation) with $O(k)$ memory and $O(km/p)$ queries per element ($k$ is the size of the largest feasible solution and $m$ is the number of matroids used to define the constraint). For the non-monotone case, our approximation ratio increases only slightly to $4p+2-o(1)$.  To the best or our knowledge, our algorithm is the first that combines the benefits of streaming and subsampling in a novel way in order to truly scale submodular maximization to massive machine learning problems. To showcase its practicality, we empirically evaluated the performance of our algorithm on a video summarization application and observed that it outperforms the state-of-the-art algorithm by up to fifty-fold while maintaining practically the same utility. We also evaluated the scalability of our algorithm on a large dataset of Uber pick up locations.",https://proceedings.neurips.cc/paper_files/paper/2018/file/d1f255a373a3cef72e03aa9d980c7eca-Paper.pdf,2018,"algorithm, approximation, submodular, 4p, constraint, element, evaluated, first, function, maximization","algorithm, set, submodular, monotone, constraint, function, streaming, approximation, elements, non","{'get': 0.46113916666267823, 'less': 0.46113916666267823, 'subsampling': 0.41689600407397753, 'streaming': 0.38117606899059514, 'maximization': 0.37265284148527683, 'submodular': 0.34166317315197675}","submodular, algorithm, 4p, subsampling, monotone, element, stream, streaming, ratio, maximization"
Temporal alignment and latent Gaussian process factor inference in population spike trains,"Lea Duncker, Maneesh Sahani","We introduce a novel scalable approach to identifying common latent structure in neural population spike-trains, which allows for variability both in the trajectory and in the rate of progression of the underlying computation. Our approach is based on shared latent Gaussian processes (GPs) which are combined linearly, as in the Gaussian Process Factor Analysis (GPFA) algorithm. We extend GPFA to handle unbinned spike-train data by incorporating a continuous time point-process likelihood model, achieving scalability with a sparse variational approximation. Shared variability is separated into terms that express condition dependence, as well as trial-to-trial variation in trajectories. Finally, we introduce a nested GP formulation to capture variability in the rate of evolution along the trajectory. We show that the new method learns to recover latent trajectories in synthetic data, and can accurately identify the trial-to-trial timing of movement-related parameters from motor cortical data without any supervision.",https://proceedings.neurips.cc/paper_files/paper/2018/file/d1ff1ec86b62cd5f3903ff19c3a326b2-Paper.pdf,2018,"trial, data, latent, variability, approach, gaussian, gpfa, introduce, process, rate","time, latent, model, trial, process, gp, inducing, warping, points, trials","{'trains': 0.38680851652149767, 'factor': 0.3650995970762606, 'spike': 0.3650995970762606, 'alignment': 0.32798795416158855, 'population': 0.31973460603402093, 'temporal': 0.30063795019049316, 'latent': 0.2865907619892479, 'process': 0.2865907619892479, 'gaussian': 0.25157902677416066, 'inference': 0.23671440615766726}","trial, variability, gpfa, spike, trajectory, latent, trajectories, shared, gaussian, movement"
Dimensionality Reduction for Stationary Time Series via Stochastic Nonconvex Optimization,"Minshuo Chen, Lin Yang, Mengdi Wang, Tuo Zhao","Stochastic optimization naturally arises in machine learning. Efficient algorithms with provable guarantees, however, are still largely missing, when the objective function is nonconvex and the data points are dependent. This paper studies this fundamental challenge through a streaming PCA problem for stationary time series data. Specifically, our goal is to estimate the principle component of time series data with respect to the covariance matrix of the stationary distribution. Computationally, we propose a variant of Oja's algorithm combined with downsampling to control the bias of the stochastic gradient caused by the data dependency. Theoretically, we quantify the uncertainty of our proposed stochastic algorithm based on diffusion approximations. This allows us to prove the asymptotic rate of convergence and further implies near optimal asymptotic sample complexity. Numerical experiments are provided to support our analysis.",https://proceedings.neurips.cc/paper_files/paper/2018/file/d25414405eb37dae1c14b18d6a2cac34-Paper.pdf,2018,"data, stochastic, algorithm, asymptotic, series, stationary, time, algorithms, allows, analysis","algorithm, γ2, zk, λr, data, stationary, saddle, ij, time, distribution","{'dimensionality': 0.4058656971966972, 'stationary': 0.4058656971966972, 'series': 0.37562627810112137, 'reduction': 0.3551373813381383, 'nonconvex': 0.3396213298605356, 'time': 0.31353423484726445, 'stochastic': 0.26594501437064794, 'optimization': 0.24388039739610506, 'via': 0.24388039739610506}","stationary, stochastic, asymptotic, series, data, downsampling, oja, arises, caused, dependency"
"Nonlocal Neural Networks, Nonlocal Diffusion and Nonlocal Modeling","Yunzhe Tao, Qi Sun, Qiang Du, Wei Liu","Nonlocal neural networks have been proposed and shown to be effective in several computer vision tasks, where the nonlocal operations can directly capture long-range dependencies in the feature space. In this paper, we study the nature of diffusion and damping effect of nonlocal networks by doing spectrum analysis on the weight matrices of the well-trained networks, and then propose a new formulation of the nonlocal block. The new block not only learns the nonlocal interactions but also has stable dynamics, thus allowing deeper nonlocal structures. Moreover, we interpret our formulation from the general nonlocal modeling perspective, where we make connections between the proposed nonlocal network and other nonlocal models, such as nonlocal diffusion process and Markov jump process.",https://proceedings.neurips.cc/paper_files/paper/2018/file/d296c101daa88a51f6ca8cfc1ac79b50-Paper.pdf,2018,"nonlocal, networks, block, diffusion, formulation, new, process, proposed, allowing, also","nonlocal, blocks, block, proposed, network, 20, 10, weight, xi, networks","{'nonlocal': 0.912825325525061, 'diffusion': 0.2750819840230567, 'modeling': 0.23247661510239973, 'neural': 0.13627196125881608, 'networks': 0.1358837854610025}","nonlocal, diffusion, block, formulation, damping, jump, networks, process, interpret, spectrum"
Partially-Supervised Image Captioning,"Peter Anderson, Stephen Gould, Mark Johnson","Image captioning models are becoming increasingly successful at describing the content of images in restricted domains. However, if these models are to function in the wild --- for example, as assistants for people with impaired vision --- a much larger number and variety of visual concepts must be understood. To address this problem, we teach image captioning models new visual concepts from labeled images and object detection datasets. Since image labels and object classes can be interpreted as partial captions, we formulate this problem as learning from partially-specified sequence data. We then propose a novel algorithm for training sequence models, such as recurrent neural networks, on partially-specified sequences which we represent using finite state automata. In the context of image captioning, our method lifts the restriction that previously required image captioning models to be trained on paired image-sentence corpora only, or otherwise required specialized model architectures to take advantage of alternative data modalities. Applying our approach to an existing neural captioning model, we achieve state of the art results on the novel object captioning task using the COCO dataset. We further show that we can train a captioning model to describe new visual concepts from the Open Images dataset while maintaining competitive COCO evaluation scores.",https://proceedings.neurips.cc/paper_files/paper/2018/file/d2ed45a52bc0edfa11c2064e9edee8bf-Paper.pdf,2018,"captioning, image, models, concepts, images, model, object, visual, coco, data","image, captioning, model, training, object, sequence, beam, images, models, labels","{'partially': 0.5771507789428599, 'captioning': 0.5528020973244329, 'supervised': 0.44093203239217954, 'image': 0.40851653867360166}","captioning, image, concepts, object, visual, coco, models, partially, images, specified"
SLANG: Fast Structured Covariance Approximations for Bayesian Deep Learning with Natural Gradient,"Aaron Mishkin, Frederik Kunstner, Didrik Nielsen, Mark Schmidt, Mohammad Emtiyaz Khan","Uncertainty estimation in large deep-learning models is a computationally challenging
task, where it is difficult to form even a Gaussian approximation to the
posterior distribution. In such situations, existing methods usually resort to a diagonal
approximation of the covariance matrix despite the fact that these matrices
are known to give poor uncertainty estimates. To address this issue, we propose
a new stochastic, low-rank, approximate natural-gradient (SLANG) method for
variational inference in large deep models. Our method estimates a “diagonal
plus low-rank” structure based solely on back-propagated gradients of the network
log-likelihood. This requires strictly less gradient computations than methods that
compute the gradient of the whole variational objective. Empirical evaluations
on standard benchmarks confirm that SLANG enables faster and more accurate
estimation of uncertainty than mean-field methods, and performs comparably to
state-of-the-art methods.",https://proceedings.neurips.cc/paper_files/paper/2018/file/d3157f2f0212a80a5d042c127522a2d5-Paper.pdf,2018,"methods, gradient, uncertainty, approximation, deep, diagonal, estimates, estimation, large, low","slang, method, methods, gaussian, gradient, mean, approximation, rank, covariance, full","{'slang': 0.4411536708073544, 'covariance': 0.4163947291256884, 'approximations': 0.3646561259935764, 'natural': 0.3565022900739139, 'structured': 0.31047467456983807, 'fast': 0.2894176580255275, 'gradient': 0.2538089593187063, 'bayesian': 0.25240514890534993, 'deep': 0.20670495007630538, 'learning': 0.14463577531267993}","slang, uncertainty, diagonal, methods, estimates, rank, gradient, estimation, variational, approximation"
On Learning Markov Chains,"Yi Hao, Alon Orlitsky, Venkatadheeraj Pichapati","The problem of estimating an unknown discrete distribution from its samples is a fundamental tenet of statistical learning. Over the past decade, it attracted significant research effort and has been solved for a variety of divergence measures.  Surprisingly, an equally important problem, estimating an unknown Markov chain from its samples, is still far from understood. We consider two problems related to the min-max risk (expected loss) of estimating an unknown k-state Markov chain from its n sequential samples: predicting the conditional distribution of the next sample with respect to the KL-divergence, and estimating the transition matrix with respect to a natural loss induced by KL or a more general f-divergence measure.For the first measure, we determine the min-max prediction risk to within a linear factor in the alphabet size, showing it is \Omega(k\log\log n/n) and O(k^2\log\log n/n). For the second, if the transition probabilities can be arbitrarily small, then only trivial uniform risk upper bounds can be derived. We therefore consider transition probabilities that are bounded away from zero, and resolve the problem for essentially all sufficiently smooth f-divergences, including KL-, L_2-, Chi-squared, Hellinger, and Alpha-divergences.",https://proceedings.neurips.cc/paper_files/paper/2018/file/d34ab169b70c9dcd35e62896010cd9ff-Paper.pdf,2018,"estimating, log, divergence, kl, problem, risk, samples, transition, unknown, chain","xn, mk, markov, risk, estimation, ρkl, let, distribution, kn, estimator","{'chains': 0.7403563131751798, 'markov': 0.6210787926399354, 'learning': 0.2571646610142531}","kl, estimating, divergence, log, transition, risk, max, divergences, unknown, min"
Is Q-Learning Provably Efficient?,"Chi Jin, Zeyuan Allen-Zhu, Sebastien Bubeck, Michael I. Jordan","Model-free reinforcement learning (RL) algorithms directly parameterize and update value functions or policies, bypassing the modeling of the environment. They are typically simpler, more flexible to use, and thus more prevalent in modern deep RL than model-based approaches. However, empirical work has suggested that they require large numbers of samples to learn.  The theoretical question of whether not model-free algorithms are in fact \emph{sample efficient} is one of the most fundamental questions in RL. The problem is unsolved even in the basic scenario with finitely many states and actions. We prove that, in an episodic MDP setting, Q-learning with UCB exploration achieves regret $\tlO(\sqrt{H^3 SAT})$ where $S$ and $A$ are the numbers of states and actions, $H$ is the number of steps per episode, and $T$ is the total number of steps. Our regret matches the optimal regret up to a single $\sqrt{H}$ factor.  Thus we establish the sample efficiency of a classical model-free approach. Moreover, to the best of our knowledge, this is the first model-free analysis to establish $\sqrt{T}$ regret \emph{without} requiring access to a ``simulator.''",https://proceedings.neurips.cc/paper_files/paper/2018/file/d3b1fb02964aa64e257f9f26a31f72cf-Paper.pdf,2018,"model, free, regret, rl, sqrt, actions, algorithms, emph, establish, learning","learning, model, regret, xk, algorithm, setting, αi, episode, state, free","{'provably': 0.8090619726696048, 'efficient': 0.5092450206842745, 'learning': 0.2934079639141391}","free, regret, rl, sqrt, numbers, model, states, emph, steps, actions"
Preference Based Adaptation for Learning Objectives,"Yao-Xiang Ding, Zhi-Hua Zhou","In many real-world learning tasks, it is hard to directly optimize the true performance measures, meanwhile choosing the right surrogate objectives is also difficult. Under this situation, it is desirable to incorporate an optimization of objective process into the learning loop based on weak modeling of the relationship between the true measure and the objective. In this work, we discuss the task of objective adaptation, in which the learner iteratively adapts the learning objective to the underlying true objective based on the preference feedback from an oracle. We show that when the objective can be linearly parameterized, this preference based learning problem can be solved by utilizing the dueling bandit model. A novel sampling based algorithm DL^2M is proposed to learn the optimal parameter, which enjoys strong theoretical guarantees and efficient empirical performance. To avoid learning a hypothesis from scratch after each objective function update, a boosting based hypothesis adaptation approach is proposed to efficiently adapt any pre-learned element hypothesis to the current objective. We apply the overall approach to multi-label learning, and show that the proposed approach achieves significant performance under various multi-label performance measures.",https://proceedings.neurips.cc/paper_files/paper/2018/file/d403137434343677b98efc88cbd5493d-Paper.pdf,2018,"objective, learning, based, performance, approach, hypothesis, proposed, true, adaptation, label","objective, adapt, adaptation, dl2m, wt, learning, hypothesis, limo, based, lw","{'preference': 0.5613197198772647, 'objectives': 0.537638914714929, 'adaptation': 0.4472056689614671, 'based': 0.39731106223837953, 'learning': 0.194975841920992}","objective, hypothesis, true, preference, based, learning, adaptation, performance, label, measures"
Learning Gaussian Processes by Minimizing PAC-Bayesian Generalization Bounds,"David Reeb, Andreas Doerr, Sebastian Gerwinn, Barbara Rakitsch","Gaussian Processes (GPs) are a generic modelling tool for supervised learning. While they have been successfully applied on large datasets, their use in safety-critical applications is hindered by the lack of good performance guarantees. To this end, we propose a method to learn GPs and their sparse approximations by directly optimizing a PAC-Bayesian bound on their generalization performance, instead of maximizing the marginal likelihood. Besides its theoretical appeal, we find in our evaluation that our learning method is robust and yields significantly better generalization guarantees than other common GP approaches on several regression benchmark datasets.",https://proceedings.neurips.cc/paper_files/paper/2018/file/d43ab110ab2489d6b9b2caa394bf920f-Paper.pdf,2018,"datasets, generalization, gps, guarantees, learning, method, performance, appeal, applications, applied","pac, kl, gp, bound, training, sgp, bayesian, ln, distribution, gps","{'minimizing': 0.48248838734933697, 'pac': 0.40475552074722526, 'generalization': 0.37873696845665694, 'processes': 0.34825670595050945, 'bounds': 0.3383677792247114, 'gaussian': 0.33246807142826357, 'bayesian': 0.2924690077362905, 'learning': 0.16759357672505845}","gps, generalization, guarantees, hindered, appeal, datasets, modelling, safety, pac, besides"
Learning Invariances using the Marginal Likelihood,"Mark van der Wilk, Matthias Bauer, ST John, James Hensman","In many supervised learning tasks, learning what changes do not affect the predic-tion target is as crucial to generalisation as learning what does. Data augmentationis a common way to enforce a model to exhibit an invariance: training data is modi-fied according to an invariance designed by a human and added to the training data.We argue that invariances should be incorporated the model structure, and learnedusing themarginal likelihood, which can correctly reward the reduced complexityof invariant models. We incorporate invariances in a Gaussian process, due to goodmarginal likelihood approximations being available for these models. Our maincontribution is a derivation for a variational inference scheme for invariant Gaussianprocesses where the invariance is described by a probability distribution that canbe sampled from, much like how data augmentation is implemented in practice",https://proceedings.neurips.cc/paper_files/paper/2018/file/d465f14a648b3d0a1faa6f447e526c60-Paper.pdf,2018,"data, invariance, learning, invariances, invariant, likelihood, model, models, training, according","likelihood, marginal, invariant, xa, al, et, invariance, invariances, xn, gaussian","{'invariances': 0.5759515715418285, 'marginal': 0.520692940601068, 'likelihood': 0.502903634816225, 'using': 0.32952948552199157, 'learning': 0.1888303500683918}","invariance, invariances, invariant, likelihood, data, augmentationis, canbe, complexityof, fied, gaussianprocesses"
NEON2: Finding Local Minima via First-Order Oracles,"Zeyuan Allen-Zhu, Yuanzhi Li","We propose a reduction for non-convex optimization that can (1) turn an stationary-point finding algorithm into an local-minimum finding one, and (2) replace the Hessian-vector product computations with only gradient computations. It works both in the stochastic and the deterministic settings, without hurting the algorithm's performance.As applications, our reduction turns Natasha2 into a first-order method without hurting its theoretical performance. It also converts SGD, GD, SCSG, and SVRG into algorithms finding approximate local minima, outperforming some best known results.",https://proceedings.neurips.cc/paper_files/paper/2018/file/d4b2aeb2453bdadaa45cbe9882ffefcf-Paper.pdf,2018,"finding, algorithm, computations, hurting, local, performance, reduction, without, algorithms, also","x0, needed, neon2, xt, fi, algorithm, 2f, approximate, local, method","{'neon2': 0.4174120018621948, 'oracles': 0.4174120018621948, 'first': 0.3939855178607808, 'finding': 0.37736416294511327, 'minima': 0.34503134307763744, 'order': 0.31388984002661774, 'local': 0.2972684851109503, 'via': 0.2190071460602217}","hurting, finding, computations, reduction, gd, natasha2, local, converts, scsg, without"
Genetic-Gated Networks for Deep Reinforcement Learning,"Simyung Chang, John Yang, Jaeseok Choi, Nojun Kwak","We introduce the Genetic-Gated Networks (G2Ns), simple neural networks that combine a gate vector composed of binary genetic genes in the hidden layer(s) of networks. Our method can take both advantages of gradient-free optimization and gradient-based optimization methods, of which the former is effective for problems with multiple local minima, while the latter can quickly find local minima. In addition, multiple chromosomes can define different models, making it easy to construct multiple models and can be effectively applied to problems that require multiple models. We show that this G2N can be applied to typical reinforcement learning algorithms to achieve a large improvement in sample efficiency and performance.",https://proceedings.neurips.cc/paper_files/paper/2018/file/d516b13671a4179d9b7b458a6ebdeb92-Paper.pdf,2018,"multiple, models, networks, applied, genetic, gradient, local, minima, optimization, problems","genetic, actor, elite, g2ppo, model, generation, policy, g2ac, method, models","{'genetic': 0.6033441893978562, 'gated': 0.528376416311807, 'reinforcement': 0.37640092611673476, 'deep': 0.29950962830431965, 'networks': 0.28546378472744766, 'learning': 0.20957314900981472}","genetic, multiple, minima, applied, chromosomes, g2n, g2ns, networks, local, models"
Lipschitz regularity of deep neural networks: analysis and efficient estimation,"Aladin Virmaux, Kevin Scaman","Deep neural networks are notorious for being sensitive to small well-chosen perturbations, and estimating the regularity of such architectures is of utmost importance for safe and robust practical applications.  In this paper, we investigate one of the key characteristics to assess the regularity of such methods: the Lipschitz constant of deep learning architectures.  First, we show that, even for two layer neural networks, the exact computation of this quantity is NP-hard and state-of-art methods may significantly overestimate it. Then, we both extend and improve previous estimation methods by providing AutoLip, the first generic algorithm for upper bounding the Lipschitz constant of any automatically differentiable function.  We provide a power method algorithm working with automatic differentiation, allowing efficient computations even on large convolutions. Second, for sequential neural networks, we propose an improved algorithm named SeqLip that takes advantage of the linear computation graph to split the computation per pair of consecutive layers. Third we propose heuristics on SeqLip in order to tackle very large networks.  Our experiments show that SeqLip can significantly improve on the existing upper bounds.  Finally, we provide an implementation of AutoLip in the PyTorch environment that may be used to better estimate the robustness of a given neural network to small perturbations or regularize it using more precise Lipschitz estimations.  These results also hint at the difficulty to estimate the Lipschitz constant of deep networks.",https://proceedings.neurips.cc/paper_files/paper/2018/file/d54e99a6c03704e95e6965532dec148b-Paper.pdf,2018,"networks, lipschitz, neural, algorithm, computation, constant, deep, methods, seqlip, architectures","lipschitz, constant, autolip, neural, seqlip, layers, upper, function, may, algorithm","{'regularity': 0.5231255994091325, 'lipschitz': 0.4567775806288652, 'analysis': 0.36402599002708513, 'estimation': 0.35639690328813484, 'efficient': 0.2976779712468178, 'deep': 0.24511334273068322, 'neural': 0.23428584666569627, 'networks': 0.23361847463548704}","seqlip, lipschitz, autolip, regularity, constant, networks, computation, perturbations, neural, upper"
Accelerated Stochastic Matrix Inversion:  General Theory and  Speeding up BFGS Rules for Faster Second-Order Optimization,"Robert Gower, Filip Hanzely, Peter Richtarik, Sebastian U. Stich","We present the first accelerated randomized algorithm for solving linear systems in Euclidean spaces. One essential problem of this type is the matrix inversion problem. In particular, our algorithm can be specialized to invert positive definite matrices in such a way that all iterates (approximate solutions) generated by the algorithm are positive definite matrices themselves. This opens the way for many applications in the field of optimization and machine learning.  As an application of our general theory, we develop the first  accelerated (deterministic and stochastic) quasi-Newton updates. Our updates lead to provably more aggressive approximations of the inverse Hessian, and lead to speed-ups over classical non-accelerated rules in numerical experiments. Experiments with empirical risk minimization show that our rules can accelerate training of machine learning models.",https://proceedings.neurips.cc/paper_files/paper/2018/file/d554f7bb7be44a7267068a7df88ddd20-Paper.pdf,2018,"accelerated, algorithm, definite, experiments, first, lead, learning, machine, matrices, positive","accelerated, algorithm, bfgs, matrix, method, sketch, project, linear, stochastic, xk","{'bfgs': 0.3362237543469422, 'speeding': 0.3362237543469422, 'accelerated': 0.3173538120190635, 'rules': 0.3039653748702828, 'second': 0.3039653748702828, 'inversion': 0.29358049622120824, 'theory': 0.2717069953936235, 'faster': 0.26132211674454897, 'general': 0.2568864758960457, 'order': 0.2528370530657448}","accelerated, definite, rules, updates, positive, matrices, lead, way, ups, aggressive"
Blind Deconvolutional Phase Retrieval via Convex Programming,"Ali Ahmed, Alireza Aghasi, Paul Hand","We consider the task of recovering two real or complex $m$-vectors from phaseless Fourier measurements of their circular convolution.  Our method is a novel convex relaxation that is based on a lifted matrix recovery formulation that allows a nontrivial convex relaxation of the bilinear measurements from convolution.    We prove that if  the two signals belong to known random subspaces of dimensions $k$ and $n$, then they can be recovered up to the inherent scaling ambiguity with $m  >> (k+n) \log^2 m$  phaseless measurements.  Our method provides the first theoretical recovery guarantee for this problem by a computationally efficient algorithm and does not require a solution estimate to be computed for initialization. Our proof is based Rademacher complexity estimates.  Additionally, we provide an ADMM implementation of the method and provide numerical experiments that verify the theory.",https://proceedings.neurips.cc/paper_files/paper/2018/file/d5b3d8dadd770c460b1cde910a711987-Paper.pdf,2018,"measurements, method, based, convex, convolution, phaseless, provide, recovery, relaxation, two","convex, set, bℓb, cℓc, δm, program, uℓ, vℓ, problem, phase","{'deconvolutional': 0.4659838282440521, 'phase': 0.4068831001922077, 'blind': 0.39512336458511627, 'retrieval': 0.39512336458511627, 'programming': 0.38518064979977346, 'convex': 0.31131964635868586, 'via': 0.24449174407696744}","measurements, phaseless, recovery, relaxation, convolution, method, convex, admm, circular, lifted"
Graph Convolutional Policy Network for Goal-Directed Molecular Graph Generation,"Jiaxuan You, Bowen Liu, Zhitao Ying, Vijay Pande, Jure Leskovec","Generating novel graph structures that optimize given objectives while obeying some given underlying rules is fundamental for chemistry, biology and social science research. This is especially important in the task of molecular graph generation, whose goal is to discover novel molecules with desired properties such as drug-likeness and synthetic accessibility, while obeying physical laws such as chemical valency. However, designing models that finds molecules that optimize desired properties while incorporating highly complex and non-differentiable rules remains to be a challenging task. Here we propose Graph Convolutional Policy Network (GCPN), a general graph convolutional network based model for goal-directed graph generation through reinforcement learning. The model is trained to optimize domain-specific rewards and adversarial loss through policy gradient, and acts in an environment that incorporates domain-specific rules. Experimental results show that GCPN can achieve 61% improvement on chemical property optimization over state-of-the-art baselines while resembling known molecules, and achieve 184% improvement on the constrained property optimization task.",https://proceedings.neurips.cc/paper_files/paper/2018/file/d60678e8f2ba9c540798ebbde31177e8-Paper.pdf,2018,"graph, molecules, optimize, rules, task, achieve, chemical, convolutional, desired, domain","molecules, graph, molecule, generation, property, molecular, state, st, policy, gcpn","{'graph': 0.5211449200818948, 'molecular': 0.400636126179451, 'directed': 0.3781511574711724, 'goal': 0.3781511574711724, 'generation': 0.2853212025019915, 'convolutional': 0.26766142848926766, 'policy': 0.26519636683427955, 'network': 0.2419134741989048}","molecules, graph, rules, gcpn, obeying, optimize, chemical, desired, task, improvement"
Spectral Filtering for General Linear Dynamical Systems,"Elad Hazan, Holden Lee, Karan Singh, Cyril Zhang, Yi Zhang","We give a polynomial-time algorithm for learning latent-state linear dynamical systems without system identification, and without assumptions on the spectral radius of the system's transition matrix. The algorithm extends the recently introduced technique of spectral filtering, previously applied only to systems with a symmetric transition matrix, using a novel convex relaxation to allow for the efficient identification of phases.",https://proceedings.neurips.cc/paper_files/paper/2018/file/d6288499d0083cc34e60a077b7c4b3e1-Paper.pdf,2018,"algorithm, identification, matrix, spectral, system, systems, transition, without, allow, applied","𝑦𝑡, lds, algorithm, time, system, linear, regret, pseudo, theorem, ldss","{'filtering': 0.4718141711873713, 'dynamical': 0.4217428083450645, 'spectral': 0.4056234298968268, 'systems': 0.4056234298968268, 'general': 0.39873844106704803, 'linear': 0.333873199476808}","identification, spectral, transition, matrix, system, systems, radius, phases, without, filtering"
Dialog-to-Action: Conversational Question Answering Over a Large-Scale Knowledge Base,"Daya Guo, Duyu Tang, Nan Duan, Ming Zhou, Jian Yin","We present an approach to map utterances in conversation to logical forms, which will be executed on a large-scale knowledge base. To handle enormous ellipsis phenomena in conversation, we introduce dialog memory management to manipulate historical entities, predicates, and logical forms when inferring the logical form of current utterances. Dialog memory management is embodied in a generative model, in which a logical form is interpreted in a top-down manner following a small and flexible grammar. We learn the model from denotations without explicit annotation of logical forms, and evaluate it on a large-scale dataset consisting of 200K dialogs over 12.8M entities. Results verify the benefits of modeling dialog memory, and show that our semantic parsing-based approach outperforms a memory network based encoder-decoder model by a huge margin.",https://proceedings.neurips.cc/paper_files/paper/2018/file/d63fbf8c3173730f82b150c5ef38b8ff-Paper.pdf,2018,"logical, memory, dialog, forms, model, approach, based, conversation, entities, form","action, logical, question, memory, model, dialog, set, form, previous, entities","{'base': 0.3997557718051029, 'conversational': 0.36140192791944076, 'dialog': 0.36140192791944076, 'action': 0.3304367632785224, 'answering': 0.3304367632785224, 'question': 0.3304367632785224, 'scale': 0.3006125236021528, 'knowledge': 0.2920829193928603, 'large': 0.2751852670025407}","logical, dialog, forms, memory, conversation, utterances, management, entities, form, 200k"
Text-Adaptive Generative Adversarial Networks: Manipulating Images with Natural Language,"Seonghyeon Nam, Yunji Kim, Seon Joo Kim","This paper addresses the problem of manipulating images using natural language description. Our task aims to semantically modify visual attributes of an object in an image according to the text describing the new visual appearance. Although existing methods synthesize images having new attributes, they do not fully preserve text-irrelevant contents of the original image. In this paper, we propose the text-adaptive generative adversarial network (TAGAN) to generate semantically manipulated images while preserving text-irrelevant contents. The key to our method is the text-adaptive discriminator that creates word level local discriminators according to input text to classify fine-grained attributes independently. With this discriminator, the generator learns to generate images where only regions that correspond to the given text is modified. Experimental results show that our method outperforms existing methods on CUB and Oxford-102 datasets, and our results were mostly preferred on a user study. Extensive analysis shows that our method is able to effectively disentangle visual attributes and produce pleasing outputs.",https://proceedings.neurips.cc/paper_files/paper/2018/file/d645920e395fedad7bbbed0eca3fe2e0-Paper.pdf,2018,"text, attributes, images, method, visual, according, adaptive, contents, discriminator, existing","text, image, images, method, discriminator, visual, attributes, methods, generator, original","{'manipulating': 0.450168821034515, 'images': 0.36378755576253735, 'natural': 0.36378755576253735, 'text': 0.36378755576253735, 'language': 0.34394441350061605, 'adaptive': 0.3098887769730524, 'generative': 0.27740629049055965, 'adversarial': 0.26196339321083856, 'networks': 0.2010372908864054}","text, attributes, images, contents, irrelevant, visual, semantically, discriminator, according, adaptive"
Exploration in Structured Reinforcement Learning,"Jungseul Ok, Alexandre Proutiere, Damianos Tranos","We address reinforcement learning problems with finite state and action spaces where the underlying MDP has some known structure that could be potentially exploited to minimize the exploration rates of suboptimal (state, action) pairs. For any arbitrary structure, we derive problem-specific regret lower bounds satisfied by any learning algorithm. These lower bounds are made explicit for unstructured MDPs and for those whose transition probabilities and average reward functions are Lipschitz continuous w.r.t. the state and action. For Lipschitz MDPs, the bounds are shown not to scale with the sizes S and A of the state and action spaces, i.e., they are smaller than c log T where T is the time horizon and the constant c only depends on the Lipschitz structure, the span of the bias function, and the minimal action sub-optimality gap. This contrasts with unstructured MDPs where the regret lower bound typically scales as SA log T. We devise DEL (Directed Exploration Learning), an algorithm that matches our regret lower bounds. We further simplify the algorithm for Lipschitz MDPs, and show that the simplified version is still able to efficiently exploit the structure.",https://proceedings.neurips.cc/paper_files/paper/2018/file/d693d554e0ede0d75f7d2873b015f228-Paper.pdf,2018,"action, bounds, lipschitz, lower, mdps, state, structure, algorithm, learning, regret","regret, state, action, lower, optimal, structure, log, mdps, bounds, reward","{'exploration': 0.623556775916074, 'structured': 0.5646307314171316, 'reinforcement': 0.4724209010892936, 'learning': 0.26303531428780835}","mdps, action, lipschitz, lower, bounds, structure, regret, unstructured, state, exploration"
Tree-to-tree Neural Networks for Program Translation,"Xinyun Chen, Chang Liu, Dawn Song","Program translation is an important tool to migrate legacy code in one language into an ecosystem built in a different language. In this work, we are the first to employ deep neural networks toward tackling this problem. We observe that program translation is a modular procedure, in which a sub-tree of the source tree is translated into the corresponding target sub-tree at each step. To capture this intuition, we design a tree-to-tree neural network to translate a source tree into a target one. Meanwhile, we develop an attention mechanism for the tree-to-tree model, so that when the decoder expands one non-terminal in the target tree, the attention mechanism locates the corresponding sub-tree in the source tree to guide the expansion of the decoder. We evaluate the program translation capability of our tree-to-tree model against several state-of-the-art approaches. Compared against other neural translation models, we observe that our approach is consistently better than the baselines with a margin of up to 15 points. Further, our approach can improve the previous state-of-the-art program translation approaches by a margin of 20 points on the translation of real-world projects.",https://proceedings.neurips.cc/paper_files/paper/2018/file/d759175de8ea5b1d9a2660e45554894f-Paper.pdf,2018,"tree, translation, program, neural, one, source, sub, target, approach, approaches","tree, translation, program, model, source, attention, neural, node, target, decoder","{'tree': 0.7784094608390243, 'translation': 0.38920473041951215, 'program': 0.3820363103407639, 'neural': 0.22013897926084047, 'networks': 0.2195119051135612}","tree, translation, program, sub, source, target, margin, observe, decoder, corresponding"
Virtual Class Enhanced Discriminative Embedding Learning,"Binghui Chen, Weihong Deng, Haifeng Shen","Recently, learning discriminative features to improve the recognition performances gradually becomes the primary goal of deep learning, and numerous remarkable works have emerged. In this paper, we propose a novel yet extremely simple method Virtual Softmax to enhance the discriminative property of learned features by injecting a dynamic virtual negative class into the original softmax. Injecting virtual class aims to enlarge inter-class margin and compress intra-class distribution by strengthening the decision boundary constraint. Although it seems weird to optimize with this additional virtual class, we show that our method derives from an intuitive and clear motivation, and it indeed encourages the features to be more compact and separable. This paper empirically and experimentally demonstrates the superiority of Virtual Softmax, improving the performances on a variety of object classification and face verification tasks.",https://proceedings.neurips.cc/paper_files/paper/2018/file/d79aac075930c83c2f1e369a511148fe-Paper.pdf,2018,"class, virtual, features, softmax, discriminative, injecting, learning, method, paper, performances","softmax, virtual, class, xi, ew, features, original, feature, classes, wyi","{'enhanced': 0.4777528931013127, 'virtual': 0.4777528931013127, 'discriminative': 0.45093988710978367, 'class': 0.4319157566037195, 'embedding': 0.35926561411459734, 'learning': 0.15663512438902344}","virtual, class, softmax, injecting, performances, features, discriminative, enlarge, strengthening, weird"
Neural Networks Trained to Solve Differential Equations Learn General Representations,"Martin Magill, Faisal Qureshi, Hendrick de Haan","We introduce a technique based on the singular vector canonical correlation analysis (SVCCA) for measuring the generality of neural network layers across a continuously-parametrized set of tasks. We illustrate this method by studying generality in neural networks trained to solve parametrized boundary value problems based on the Poisson partial differential equation. We find that the first hidden layers are general, and that they learn generalized coordinates over the input domain. Deeper layers are successively more specific. Next, we validate our method against an existing technique that measures layer generality using transfer learning experiments. We find excellent agreement between the two methods, and note that our method is much faster, particularly for continuously-parametrized problems. Finally, we also apply our method to networks trained on MNIST, and show it is consistent with, and complimentary to, another study of intrinsic dimensionality.",https://proceedings.neurips.cc/paper_files/paper/2018/file/d7a84628c025d30f7b2c52c958767e76-Paper.pdf,2018,"method, generality, layers, parametrized, based, continuously, find, networks, neural, problems","layers, layer, networks, width, svcca, network, ﬁrst, trained, generality, transfer","{'trained': 0.42683129769044764, 'solve': 0.40287617296466366, 'equations': 0.38587974148598747, 'learn': 0.3619246167602035, 'differential': 0.3317447288099527, 'general': 0.32611374552879585, 'representations': 0.29382361157565007, 'neural': 0.19115969869525487, 'networks': 0.19061517311665913}","generality, parametrized, layers, continuously, method, technique, complimentary, find, svcca, trained"
Deep Generative Models with Learnable Knowledge Constraints,"Zhiting Hu, Zichao Yang, Russ R. Salakhutdinov, LIANHUI Qin, Xiaodan Liang, Haoye Dong, Eric P. Xing","The broad set of deep generative models (DGMs) has achieved remarkable advances. However, it is often difficult to incorporate rich structured domain knowledge with the end-to-end DGMs. Posterior regularization (PR) offers a principled framework to impose structured constraints on probabilistic models, but has limited applicability to the diverse DGMs that can lack a Bayesian formulation or even explicit density evaluation. PR also requires constraints to be fully specified {\it a priori}, which is impractical or suboptimal for complex knowledge with learnable uncertain parts. In this paper, we establish mathematical correspondence between PR and reinforcement learning (RL), and, based on the connection, expand PR to learn constraints as the extrinsic reward in RL. The resulting algorithm is model-agnostic to apply to any DGMs, and is flexible to adapt arbitrary constraints with the model jointly. Experiments on human image generation and templated sentence generation show models with learned knowledge constraints by our algorithm greatly improve over base generative models.",https://proceedings.neurips.cc/paper_files/paper/2018/file/d7e77c835af3d2a803c1cf28d60575bc-Paper.pdf,2018,"constraints, dgms, models, pr, knowledge, algorithm, end, generation, generative, model","model, constraint, generative, models, pr, learning, pθ, knowledge, constraints, image","{'learnable': 0.5493254089216628, 'constraints': 0.47031278875110144, 'knowledge': 0.4252318436652779, 'generative': 0.3586371332142988, 'models': 0.3053568839776724, 'deep': 0.27269384861143114}","dgms, pr, constraints, knowledge, rl, models, generation, structured, end, templated"
How to Start Training: The Effect of Initialization and Architecture,"Boris Hanin, David Rolnick","We identify and study two common failure modes for early training in deep ReLU nets. For each, we give a rigorous proof of when it occurs and how to avoid it, for fully connected, convolutional, and residual architectures. We show that the first failure mode, exploding or vanishing mean activation length, can be avoided by initializing weights from a symmetric distribution with variance 2/fan-in and, for ResNets, by correctly scaling the residual modules. We prove that the second failure mode, exponentially large variance of activation length, never occurs in residual nets once the first failure mode is avoided. In contrast, for fully connected nets, we prove that this failure mode can happen and is avoided by keeping constant the sum of the reciprocals of layer widths. We demonstrate empirically the effectiveness of our theoretical results in predicting when networks are able to start training. In particular, we note that many popular initializations fail our criteria, whereas correct initialization and architecture allows much deeper networks to be trained.",https://proceedings.neurips.cc/paper_files/paper/2018/file/d81f9c1be2e08964bf9f24b15f0e4900-Paper.pdf,2018,"failure, mode, avoided, nets, residual, activation, connected, first, fully, length","networks, variance, training, connected, fully, residual, layer, mean, weights, depth","{'initialization': 0.5002671951548119, 'start': 0.5002671951548119, 'architecture': 0.45226996470221376, 'effect': 0.43681830774273916, 'training': 0.3226563372272467}","failure, avoided, mode, nets, residual, occurs, length, activation, connected, variance"
Video-to-Video Synthesis,"Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Guilin Liu, Andrew Tao, Jan Kautz, Bryan Catanzaro","We study the problem of video-to-video synthesis, whose goal is to learn a mapping function from an input source video (e.g., a sequence of semantic segmentation masks) to an output photorealistic video that precisely depicts the content of the source video. While its image counterpart, the image-to-image translation problem, is a popular topic, the video-to-video synthesis problem is less explored in the literature. Without modeling temporal dynamics, directly applying existing image synthesis approaches to an input video often results in temporally incoherent videos of low visual quality. In this paper, we propose a video-to-video synthesis approach under the generative adversarial learning framework. Through carefully-designed generators and discriminators, coupled with a spatio-temporal adversarial objective, we achieve high-resolution, photorealistic, temporally coherent video results on a diverse set of input formats including segmentation masks, sketches, and poses. Experiments on multiple benchmarks show the advantage of our method compared to strong baselines.  In particular, our model is capable of synthesizing 2K resolution videos of street scenes up to 30 seconds long, which significantly advances the state-of-the-art of video synthesis. Finally, we apply our method to future video prediction, outperforming several competing systems. Code, models, and more results are available at our website: https://github.com/NVIDIA/vid2vid. (Please use Adobe Reader to see the embedded videos in the paper.)",https://proceedings.neurips.cc/paper_files/paper/2018/file/d86ea612dec96096c5e0fcc8dd42ab6d-Paper.pdf,2018,"video, synthesis, image, input, problem, results, videos, adversarial, masks, method","video, videos, xt, image, synthesis, st, model, frames, results, network","{'video': 0.9112131188613646, 'synthesis': 0.4119352522119766}","video, synthesis, videos, photorealistic, masks, image, temporally, resolution, input, segmentation"
Near-Optimal Policies for Dynamic Multinomial Logit Assortment Selection Models,"Yining Wang, Xi Chen, Yuan Zhou","In this paper we consider the dynamic assortment selection problem under an uncapacitated multinomial-logit (MNL) model. By carefully analyzing a revenue  potential function, we show that a trisection based algorithm achieves an item-independent regret bound of O(sqrt(T log log T), which matches information theoretical lower bounds up to iterated logarithmic terms. Our proof technique draws tools from the unimodal/convex bandit literature as well as adaptive confidence parameters in minimax multi-armed bandit problems.",https://proceedings.neurips.cc/paper_files/paper/2018/file/d88518acbcc3d08d1f18da62f9bb26ec-Paper.pdf,2018,"bandit, log, achieves, adaptive, algorithm, analyzing, armed, assortment, based, bound","regret, assortment, algorithm, log, revenue, conﬁdence, yτ, trisection, function, parameters","{'assortment': 0.3976443217223153, 'logit': 0.3976443217223153, 'multinomial': 0.3976443217223153, 'near': 0.33717599792926534, 'dynamic': 0.3213417924630591, 'policies': 0.3148589387658435, 'selection': 0.3148589387658435, 'optimal': 0.2586265999539114, 'models': 0.20863546725761842}","bandit, log, assortment, logit, mnl, trisection, revenue, uncapacitated, draws, iterated"
Occam's razor is insufficient to infer the preferences of irrational agents,"Stuart Armstrong, Sören Mindermann","Inverse reinforcement learning (IRL) attempts to infer human rewards or preferences from observed behavior. Since human planning systematically deviates from rationality, several approaches have been tried to account for specific human shortcomings. 
However, the general problem of inferring the reward function of an agent of unknown rationality has received little attention.
Unlike the well-known ambiguity problems in IRL, this one is practically relevant but cannot be resolved by observing the agent's policy in enough environments.
This paper shows (1) that a No Free Lunch result implies it is impossible to uniquely decompose a policy into a planning algorithm and reward function, and (2) that even with a reasonable simplicity prior/Occam's razor on the set of decompositions, we cannot distinguish between the true decomposition and others that lead to high regret.
To address this, we need simple `normative' assumptions, which cannot be deduced exclusively from observations.",https://proceedings.neurips.cc/paper_files/paper/2018/file/d89a66c7c80a29b1bdbab0f2a1a94af8-Paper.pdf,2018,"cannot, human, agent, function, irl, planning, policy, rationality, reward, account","reward, human, function, complexity, policy, planner, al, et, reasonable, pg","{'insufficient': 0.39312087978273413, 'irrational': 0.39312087978273413, 'occam': 0.39312087978273413, 'razor': 0.39312087978273413, 'preferences': 0.3710576905122626, 'infer': 0.3554036086016388, 'agents': 0.3432613593459587}","irl, rationality, cannot, human, planning, reward, agent, lunch, normative, occam"
Bandit Learning with Implicit Feedback,"Yi Qi, Qingyun Wu, Hongning Wang, Jie Tang, Maosong Sun","Implicit feedback, such as user clicks, although abundant in online information service systems, does not provide substantial evidence on users' evaluation of system's output. Without proper modeling, such incomplete supervision inevitably misleads model estimation, especially in a bandit learning setting where the feedback is acquired on the fly. In this work, we perform contextual bandit learning with implicit feedback by modeling the feedback as a composition of user result examination and relevance judgment. Since users' examination behavior is unobserved, we introduce latent variables to model it. We perform Thompson sampling on top of variational Bayesian inference for arm selection and model update. Our upper regret bound analysis of the proposed algorithm proves its feasibility of learning from implicit feedback in a bandit setting; and extensive empirical evaluations on click logs collected from a major MOOC platform further demonstrate its learning effectiveness in practice.",https://proceedings.neurips.cc/paper_files/paper/2018/file/d8c9d05ec6e86d5bbad7a2f88a1701d0-Paper.pdf,2018,"feedback, learning, bandit, implicit, model, examination, modeling, perform, setting, user","bandit, xt, model, click, examination, regret, arm, feedback, log, user","{'feedback': 0.5718689463125145, 'implicit': 0.5718689463125145, 'bandit': 0.5450132672272113, 'learning': 0.22111615733172701}","feedback, implicit, bandit, examination, users, user, modeling, perform, inevitably, misleads"
Adversarial Regularizers in Inverse Problems,"Sebastian Lunz, Ozan Öktem, Carola-Bibiane Schönlieb","Inverse Problems in medical imaging and computer vision are traditionally solved using purely model-based methods. Among those variational regularization models are one of the most popular approaches. We propose a new framework for applying data-driven approaches to inverse problems, using a neural network as a regularization functional. The network learns to discriminate between the distribution of ground truth images and the distribution of unregularized reconstructions. Once trained, the network is applied to the inverse problem by solving the corresponding variational problem. Unlike other data-based approaches for inverse problems, the algorithm can be applied even if only unsupervised training data is available. Experiments demonstrate the potential of the framework for denoising on the BSDS dataset and for computer tomography reconstruction on the LIDC dataset.",https://proceedings.neurips.cc/paper_files/paper/2018/file/d903e9608cfbf08910611e4346a0ba44-Paper.pdf,2018,"inverse, approaches, data, network, problems, applied, based, computer, dataset, distribution","regularization, functional, pn, data, inverse, pr, ex, ψθ, distribution, algorithm","{'regularizers': 0.5726626530440043, 'problems': 0.5235965248213419, 'inverse': 0.5118887271260032, 'adversarial': 0.3686110362495087}","inverse, approaches, computer, problems, regularization, applied, network, bsds, lidc, tomography"
The emergence of multiple retinal cell types through efficient coding of natural movies,"Samuel Ocko, Jack Lindsey, Surya Ganguli, Stephane Deny","One of the most striking aspects of early visual processing in the retina is the immediate parcellation of visual information into multiple parallel pathways, formed by different retinal ganglion cell types each tiling the entire visual field. Existing theories of efficient coding have been unable to account for the functional advantages of such cell-type diversity in encoding natural scenes. Here we go beyond previous theories to analyze how a simple linear retinal encoding model with different convolutional cell types efficiently encodes naturalistic spatiotemporal movies given a fixed firing rate budget. We find that optimizing the receptive fields and cell densities of two cell types makes them match the properties of the two main cell types in the primate retina, midget and parasol cells, in terms of spatial and temporal sensitivity, cell spacing, and their relative ratio. Moreover, our theory gives a precise account of how the ratio of midget to parasol cells decreases with retinal eccentricity.  Also, we train a nonlinear encoding model with a rectifying nonlinearity to efficiently encode naturalistic movies, and again find emergent receptive fields resembling those of midget and parasol cells that are now further subdivided into ON and OFF types. Thus our work provides a theoretical justification, based on the efficient coding of natural movies, for the existence of the four most dominant cell types in the primate retina that together comprise 70% of all ganglion cells.",https://proceedings.neurips.cc/paper_files/paper/2018/file/d94fd74dcde1aa553be72c1006578b23-Paper.pdf,2018,"cell, types, cells, encoding, midget, movies, parasol, retina, retinal, visual","cell, type, cells, types, spatial, two, ganglion, optimal, rate, ﬁring","{'cell': 0.3699958300781311, 'emergence': 0.3699958300781311, 'movies': 0.3699958300781311, 'retinal': 0.3699958300781311, 'types': 0.3699958300781311, 'coding': 0.3230692596515035, 'natural': 0.2989986697815628, 'multiple': 0.27823333156597213, 'efficient': 0.21054142292375383}","cell, types, cells, midget, parasol, retina, retinal, movies, encoding, ganglion"
Multiple Instance Learning for Efficient Sequential Data Classification on Resource-constrained Devices,"Don Dennis, Chirag Pabbaraju, Harsha Vardhan Simhadri, Prateek Jain","We study the problem of fast and efficient classification of sequential data (such as
time-series) on tiny devices, which is critical for various IoT related applications
like audio keyword detection or gesture detection. Such tasks are cast as a standard classification task by sliding windows over the data stream to construct data points. Deploying such classification modules on tiny devices is challenging as predictions over sliding windows of data need to be invoked continuously at a high frequency. Each such predictor instance in itself is expensive as it evaluates large models over long windows of data. In this paper, we address this challenge by exploiting the following two observations about classification tasks arising in typical IoT related applications: (a) the ""signature"" of a particular class (e.g. an audio keyword) typically occupies a small fraction of the overall data, and (b) class signatures tend to be discernible early on in the data. We propose a method, EMI-RNN, that exploits these observations by using a multiple instance learning formulation along with an early prediction technique to learn a model that achieves better accuracy compared to baseline models, while simultaneously reducing computation by a large fraction. For instance, on a gesture detection benchmark [ 25 ], EMI-RNN improves standard LSTM model’s accuracy by up to 1% while requiring 72x less computation. This enables us to deploy such models for continuous real-time prediction on a small device such as Raspberry Pi0 and Arduino variants, a task that the baseline LSTM could not achieve. Finally, we also provide an analysis of our multiple instance learning algorithm in a simple setting and show that the proposed algorithm converges to the global optima at a linear rate, one of the first such result in this domain. The code for EMI-RNN is available at: https://github.com/Microsoft/EdgeML/tree/master/tf/examples/EMI-RNN",https://proceedings.neurips.cc/paper_files/paper/2018/file/d9fbed9da256e344c1fa46bb46c34c5f-Paper.pdf,2018,"data, classification, emi, instance, rnn, detection, models, windows, accuracy, algorithm","data, rnn, lstm, accuracy, early, point, sequential, time, instances, algorithm","{'resource': 0.42006716598920135, 'devices': 0.37976458215634745, 'instance': 0.37976458215634745, 'constrained': 0.3264874642911447, 'sequential': 0.3264874642911447, 'multiple': 0.31588649809910335, 'classification': 0.2991594144906397, 'efficient': 0.23903388000951029, 'data': 0.23775593103210507, 'learning': 0.1377223952938158}","emi, rnn, windows, instance, keyword, data, iot, sliding, gesture, classification"
Dropping Symmetry for Fast Symmetric Nonnegative Matrix Factorization,"Zhihui Zhu, Xiao Li, Kai Liu, Qiuwei Li","Symmetric nonnegative matrix factorization (NMF)---a special but important class of the general NMF---is demonstrated to be useful for data analysis and in particular for various clustering tasks. Unfortunately, designing fast algorithms for Symmetric NMF is not as easy as for the nonsymmetric counterpart, the latter admitting the splitting property that allows efficient alternating-type algorithms. To overcome this issue, we transfer the symmetric NMF to a nonsymmetric one, then we can adopt the idea from the state-of-the-art algorithms for nonsymmetric NMF to design fast algorithms solving symmetric NMF.  We rigorously establish that solving nonsymmetric reformulation returns a solution for symmetric NMF and then apply fast alternating based algorithms for the corresponding reformulated problem. Furthermore, we show these fast algorithms admit strong convergence guarantee in the sense that the generated sequence is convergent at least at a sublinear rate and it converges globally to a critical point of the symmetric NMF.  We conduct experiments on both synthetic data and image clustering to support our result.",https://proceedings.neurips.cc/paper_files/paper/2018/file/d9ff90f4000eacd3a6c9cb27f78994cf-Paper.pdf,2018,"nmf, algorithms, symmetric, fast, nonsymmetric, alternating, clustering, data, solving, admit","nmf, algorithms, convergence, symmetric, algorithm, 10, solving, point, symhals, alternating","{'dropping': 0.43149844426590356, 'nonnegative': 0.43149844426590356, 'symmetry': 0.4072813844877148, 'symmetric': 0.3900991071317566, 'factorization': 0.35667514852658094, 'matrix': 0.31970202996413977, 'fast': 0.2830833730852739}","nmf, symmetric, nonsymmetric, fast, algorithms, alternating, clustering, solving, admitting, convergent"
On the Local Minima of the Empirical Risk,"Chi Jin, Lydia T. Liu, Rong Ge, Michael I. Jordan","Population risk is always of primary interest in machine learning; however, learning algorithms only have access to the empirical risk. Even for applications with nonconvex non-smooth losses (such as modern deep networks), the population risk is generally significantly more well behaved from an optimization point of view than the empirical risk.  In particular, sampling can create many spurious local minima. We consider a general framework which aims to optimize a smooth nonconvex function $F$ (population risk) given only access to an approximation $f$ (empirical risk) that is pointwise close to $F$ (i.e., $\norm{F-f}_{\infty} \le \nu$). Our objective is to find the $\epsilon$-approximate local minima of the underlying function $F$ while avoiding the shallow local minima---arising because of the tolerance $\nu$---which exist only in $f$. We propose a simple algorithm based on stochastic gradient descent (SGD) on a smoothed version of $f$ that is guaranteed 
to achieve our goal as long as $\nu \le O(\epsilon^{1.5}/d)$. We also provide an almost matching lower bound showing that our algorithm achieves optimal error tolerance $\nu$ among all algorithms making a polynomial number of queries of $f$. As a concrete example, we show that our results can be directly used to give sample complexities for learning a ReLU unit.",https://proceedings.neurips.cc/paper_files/paper/2018/file/da4902cb0bc38210839714ebdcf0efc3-Paper.pdf,2018,"risk, nu, empirical, learning, local, minima, population, access, algorithm, algorithms","function, gradient, algorithm, al, et, risk, local, fσ, minima, order","{'empirical': 0.520010091184588, 'minima': 0.520010091184588, 'risk': 0.50838248737419, 'local': 0.44802483933775866}","nu, risk, minima, population, tolerance, le, local, empirical, access, nonconvex"
GIANT: Globally Improved Approximate Newton Method for Distributed Optimization,"Shusen Wang, Fred Roosta, Peng Xu, Michael W. Mahoney","For distributed computing environment, we consider the empirical risk minimization problem and propose a distributed and communication-efficient Newton-type optimization method. At every iteration, each worker locally finds an Approximate NewTon (ANT) direction, which is sent to the main driver. The main driver, then, averages all the ANT directions received from workers to form a Globally Improved ANT (GIANT) direction. GIANT is highly communication efficient and naturally exploits the trade-offs between local computations and global communications in that more local computations result in fewer overall rounds of communications. Theoretically, we show that GIANT enjoys an improved convergence rate as compared with first-order methods and existing distributed Newton-type methods. Further, and in sharp contrast with many existing distributed Newton-type methods, as well as popular first-order methods, a highly advantageous practical feature of GIANT is that it only involves one tuning parameter. We conduct large-scale experiments on a computer cluster and, empirically, demonstrate the superior performance of GIANT.",https://proceedings.neurips.cc/paper_files/paper/2018/file/dabd8d2ce74e782c65a973ef76fd540b-Paper.pdf,2018,"giant, distributed, methods, newton, ant, type, communication, communications, computations, direction","giant, number, local, methods, ht, wt, convergence, hessian, newton, distributed","{'giant': 0.43351947264498925, 'globally': 0.43351947264498925, 'newton': 0.40918898635108497, 'approximate': 0.3432652590136794, 'improved': 0.31675248267718775, 'method': 0.3126124714147573, 'distributed': 0.2984276406865106, 'optimization': 0.22745839133024354}","giant, newton, ant, distributed, driver, type, communications, direction, computations, methods"
Stochastic Cubic Regularization for Fast Nonconvex Optimization,"Nilesh Tripuraneni, Mitchell Stern, Chi Jin, Jeffrey Regier, Michael I. Jordan","This paper proposes a stochastic variant of a classic algorithm---the cubic-regularized Newton method [Nesterov and Polyak]. The proposed algorithm efficiently escapes saddle points and finds approximate local minima for general smooth, nonconvex functions in only $\mathcal{\tilde{O}}(\epsilon^{-3.5})$ stochastic gradient and stochastic Hessian-vector product evaluations. The latter can be computed as efficiently as stochastic gradients. This improves upon the $\mathcal{\tilde{O}}(\epsilon^{-4})$ rate of stochastic gradient descent. Our rate matches the best-known result for finding local minima without requiring any delicate acceleration or variance-reduction techniques.",https://proceedings.neurips.cc/paper_files/paper/2018/file/db1915052d15f7815c8b88e879465a1e-Paper.pdf,2018,"stochastic, algorithm, efficiently, epsilon, gradient, local, mathcal, minima, rate, tilde","stochastic, cubic, gradient, xt, algorithm, hessian, order, descent, point, vector","{'cubic': 0.548156018868244, 'nonconvex': 0.4243266208587173, 'regularization': 0.41359263715625316, 'fast': 0.38099913403456387, 'stochastic': 0.33227462282319115, 'optimization': 0.3047068479570059}","stochastic, mathcal, tilde, minima, epsilon, efficiently, delicate, rate, local, escapes"
Derivative Estimation in Random Design,"Yu Liu, Kris De Brabanter","We propose a nonparametric derivative estimation method for random design without
having to estimate the regression function. The method is based on a variance-reducing linear combination of symmetric difference quotients. First, we discuss
the special case of uniform random design and establish the estimator’s asymptotic
properties. Secondly, we generalize these results for any distribution of the dependent variable and compare the proposed estimator with popular estimators for
derivative estimation such as local polynomial regression and smoothing splines.",https://proceedings.neurips.cc/paper_files/paper/2018/file/db29450c3f5e97f97846693611f98c15-Paper.pdf,2018,"derivative, design, estimation, estimator, method, random, regression, asymptotic, based, case","order, derivative, estimator, design, regression, op, bias, variance, ﬁrst, random","{'derivative': 0.6049961890267169, 'design': 0.5129966219433081, 'random': 0.4482484521617857, 'estimation': 0.4121739951433943}","derivative, estimator, estimation, design, regression, quotients, random, splines, smoothing, secondly"
Approximating Real-Time Recurrent Learning with Random Kronecker Factors,"Asier Mujika, Florian Meier, Angelika Steger","Despite all the impressive advances of recurrent neural networks, sequential data is still in need of better modelling. Truncated backpropagation through time (TBPTT), the learning algorithm most widely used in practice, suffers from the truncation bias, which drastically limits its ability to learn long-term dependencies.The Real Time Recurrent Learning algorithm (RTRL) addresses this issue,  but its high computational requirements  make it infeasible in practice. The Unbiased Online Recurrent Optimization algorithm (UORO) approximates RTRL with a smaller runtime and memory cost, but with the disadvantage  of obtaining noisy gradients that also limit its practical applicability. In this paper we propose the Kronecker Factored RTRL (KF-RTRL) algorithm that uses a Kronecker product decomposition to approximate the gradients for a large class of RNNs. We show that KF-RTRL is an unbiased and memory efficient online learning algorithm. Our theoretical analysis shows that, under reasonable assumptions, the noise introduced by our algorithm is not only stable over time but also asymptotically much smaller than the one of the UORO algorithm. We also confirm these theoretical results experimentally. Further, we show empirically that the KF-RTRL algorithm captures long-term dependencies and almost matches the performance of TBPTT on real world tasks by training Recurrent Highway Networks on a synthetic string memorization task and on the Penn TreeBank task, respectively. These results indicate that RTRL based approaches might be a promising future alternative to TBPTT.",https://proceedings.neurips.cc/paper_files/paper/2018/file/dba132f6ab6a3e3d17a8d59e82105f4c-Paper.pdf,2018,"algorithm, rtrl, recurrent, also, kf, learning, tbptt, time, dependencies, gradients","rtrl, ht, time, kf, algorithm, uoro, learning, gt, variance, rnn","{'factors': 0.43791720435966736, 'approximating': 0.4133399034289132, 'kronecker': 0.4133399034289132, 'real': 0.4133399034289132, 'random': 0.32445776120511854, 'time': 0.29538839943714895, 'recurrent': 0.2872943830727122, 'learning': 0.14357467378522806}","rtrl, kf, tbptt, algorithm, recurrent, uoro, kronecker, dependencies, unbiased, smaller"
Hyperbolic Neural Networks,"Octavian Ganea, Gary Becigneul, Thomas Hofmann","Hyperbolic spaces have recently gained momentum in the context of machine learning due to their high capacity and tree-likeliness properties. However, the representational power of hyperbolic geometry is not yet on par with Euclidean geometry, firstly because of the absence of corresponding hyperbolic neural network layers. Here, we bridge this gap in a principled manner by combining the formalism of Möbius gyrovector spaces with the Riemannian geometry of the Poincaré model of hyperbolic spaces. As a result, we derive hyperbolic versions of important deep learning tools: multinomial logistic regression, feed-forward and recurrent neural networks. This allows to embed sequential data and perform classification in the hyperbolic space. Empirically, we show that, even if hyperbolic optimization tools are limited, hyperbolic sentence embeddings either outperform or are on par with their Euclidean variants on textual entailment and noisy-prefix recognition tasks.",https://proceedings.neurips.cc/paper_files/paper/2018/file/dbab2adc8f9d078009ee3fa810bea142-Paper.pdf,2018,"hyperbolic, geometry, spaces, euclidean, learning, neural, par, tools, absence, allows","hyperbolic, euclidean, dn, poincaré, one, mlr, ht, möbius, space, rn","{'hyperbolic': 0.8451506231642099, 'neural': 0.3785072447835001, 'networks': 0.37742905268613236}","hyperbolic, geometry, spaces, par, euclidean, tools, gyrovector, likeliness, möbius, poincaré"
Mean Field for the Stochastic Blockmodel: Optimization Landscape and Convergence Issues,"Soumendu Sundar Mukherjee, Purnamrita Sarkar, Y. X. Rachel Wang, Bowei Yan","Variational approximation has been widely used in large-scale Bayesian inference recently, the simplest kind of which involves imposing a mean field assumption to approximate complicated latent structures. Despite the computational scalability of mean field, theoretical studies of its loss function surface and the convergence behavior of iterative updates for optimizing the loss are far from complete. In this paper, we focus on the problem of community detection for a simple two-class Stochastic Blockmodel (SBM). Using batch co-ordinate ascent (BCAVI) for updates, we give a complete characterization of all the critical points and show different convergence behaviors with respect to initializations. When the parameters are known, we show a significant proportion of random initializations will converge to ground truth. On the other hand, when the parameters themselves need to be estimated, a random initialization will converge to an uninformative local optimum.",https://proceedings.neurips.cc/paper_files/paper/2018/file/dbb240d23ce3d732b67bcfbae5956b18-Paper.pdf,2018,"complete, converge, convergence, field, initializations, loss, mean, parameters, random, show","log, mean, ﬁeld, likelihood, variational, initialization, one, truth, random, updates","{'blockmodel': 0.4227529025252876, 'issues': 0.4227529025252876, 'landscape': 0.3990266702574632, 'field': 0.38219264056221447, 'mean': 0.3584664082943901, 'convergence': 0.317906146331317, 'stochastic': 0.24187718786694776, 'optimization': 0.22180940235950575}","initializations, field, complete, updates, converge, mean, bcavi, ordinate, sbm, uninformative"
Fast Greedy MAP Inference for Determinantal Point Process to Improve Recommendation Diversity,"Laming Chen, Guoxin Zhang, Eric Zhou","The determinantal point process (DPP) is an elegant probabilistic model of repulsion with applications in various machine learning tasks including summarization and search. However, the maximum a posteriori (MAP) inference for DPP which plays an important role in many applications is NP-hard, and even the popular greedy algorithm can still be too computationally expensive to be used in large-scale real-time scenarios. To overcome the computational challenge, in this paper, we propose a novel algorithm to greatly accelerate the greedy MAP inference for DPP. In addition, our algorithm also adapts to scenarios where the repulsion is only required among nearby few items in the result sequence. We apply the proposed algorithm to generate relevant and diverse recommendations. Experimental results show that our proposed algorithm is significantly faster than state-of-the-art competitors, and provides a better relevance-diversity trade-off on several public datasets, which is also confirmed in an online A/B test.",https://proceedings.neurips.cc/paper_files/paper/2018/file/dbbf603ff0e99629dda5d75b6f75f966-Paper.pdf,2018,"algorithm, dpp, also, applications, greedy, inference, map, proposed, repulsion, scenarios","algorithm, dpp, items, diversity, yg, ci, log, item, inference, greedy","{'improve': 0.38141193605846513, 'diversity': 0.3600058898064174, 'determinantal': 0.34481805828739803, 'map': 0.34481805828739803, 'recommendation': 0.33303746093201736, 'greedy': 0.3152738109517526, 'point': 0.28259237508467505, 'process': 0.28259237508467505, 'fast': 0.250224256493216, 'inference': 0.23341187199663924}","dpp, repulsion, algorithm, scenarios, greedy, map, nearby, competitors, confirmed, applications"
Gather-Excite: Exploiting Feature Context in Convolutional Neural Networks,"Jie Hu, Li Shen, Samuel Albanie, Gang Sun, Andrea Vedaldi","While the use of bottom-up local operators in convolutional neural networks (CNNs) matches well some of the statistics of natural images, it may also prevent such models from capturing contextual long-range feature interactions. In this work, we propose a simple, lightweight approach for better context exploitation in CNNs. We do so by introducing a pair of operators: gather, which efficiently aggregates feature responses from a large spatial extent, and excite, which redistributes the pooled information to local features. The operators are cheap, both in terms of number of added parameters and computational complexity, and can be integrated directly in existing architectures to improve their performance. Experiments on several datasets show that gather-excite can bring benefits comparable to increasing the depth of a CNN at a fraction of the cost. For example, we find ResNet-50 with gather-excite operators is able to outperform its 101-layer counterpart on ImageNet with no additional learnable parameters. We also propose a parametric gather-excite operator pair which yields further performance gains, relate it to the recently-introduced Squeeze-and-Excitation Networks, and analyse the effects of these changes to the CNN feature activation statistics.",https://proceedings.neurips.cc/paper_files/paper/2018/file/dc363817786ff182b7bc59565d864523-Paper.pdf,2018,"excite, gather, operators, feature, also, cnn, cnns, local, networks, pair","ge, feature, resnet, operator, 50, gather, excite, performance, model, context","{'excite': 0.45655149307787035, 'gather': 0.45655149307787035, 'context': 0.3871253702753131, 'exploiting': 0.37738391358656664, 'feature': 0.3382641144129095, 'convolutional': 0.3050179872231367, 'neural': 0.20447011811895915, 'networks': 0.2038876773109112}","excite, gather, operators, feature, pair, cnn, cnns, statistics, excitation, pooled"
Active Learning for Non-Parametric Regression Using Purely Random Trees,"Jack Goetz, Ambuj Tewari, Paul Zimmerman","Active learning is the task of using labelled data to select additional points to label, with the goal of fitting the most accurate model with a fixed budget of labelled points. In binary classification active learning is known to produce faster rates than passive learning for a broad range of settings. However in regression restrictive structure and tailored methods were previously needed to obtain theoretically superior performance. In this paper we propose an intuitive tree based active learning algorithm for non-parametric regression with provable improvement over random sampling. When implemented with Mondrian Trees our algorithm is tuning parameter free, consistent and minimax optimal for Lipschitz functions.",https://proceedings.neurips.cc/paper_files/paper/2018/file/dc4c44f624d600aa568390f1f1104aa0-Paper.pdf,2018,"learning, active, algorithm, labelled, points, regression, accurate, additional, based, binary","algorithm, sampling, active, random, trees, σ2, learning, tree, nk, points","{'purely': 0.4406314027479216, 'parametric': 0.4159017724044159, 'trees': 0.398355820444751, 'active': 0.3488965597577395, 'random': 0.326468741188897, 'regression': 0.2943819166320298, 'non': 0.2734654511852676, 'using': 0.25210633432885915, 'learning': 0.1444645455333632}","active, labelled, points, regression, mondrian, passive, restrictive, learning, budget, tailored"
DeepProbLog:  Neural Probabilistic Logic Programming,"Robin Manhaeve, Sebastijan Dumancic, Angelika Kimmig, Thomas Demeester, Luc De Raedt","We introduce DeepProbLog, a probabilistic logic programming language that incorporates deep learning by means of neural predicates. We show how existing inference and learning techniques can be adapted for the new language. Our experiments demonstrate that DeepProbLog supports (i) both symbolic and subsymbolic representations and inference, (ii) program induction, (iii) probabilistic (logic) programming, and (iv) (deep) learning from examples. To the best of our knowledge, this work is the first to propose a framework where general-purpose neural networks and expressive probabilistic-logical modeling and reasoning are integrated in a way that exploits the full expressiveness and strengths of both worlds and can be trained end-to-end based on examples.",https://proceedings.neurips.cc/paper_files/paper/2018/file/dc5d637ed5e62c36ecb73b654b05ba2a-Paper.pdf,2018,"learning, probabilistic, deep, deepproblog, end, examples, inference, language, logic, neural","neural, deepproblog, probabilistic, program, logic, network, learning, al, problog, et","{'deepproblog': 0.5517451865840137, 'logic': 0.49880899342949453, 'programming': 0.4560706973312014, 'probabilistic': 0.421552239369498, 'neural': 0.24710335018695997}","deepproblog, probabilistic, logic, programming, language, examples, end, subsymbolic, iv, predicates"
Image-to-image translation for cross-domain disentanglement,"Abel Gonzalez-Garcia, Joost van de Weijer, Yoshua Bengio","Deep image translation methods have recently shown excellent results, outputting high-quality images covering multiple modes of the data distribution. There has also been increased interest in disentangling the internal representations learned by deep methods to further improve their performance and achieve a finer control. In this paper, we bridge these two objectives and introduce the concept of cross-domain disentanglement. We aim to separate the internal representation into three parts. The shared part contains information for both domains. The exclusive parts, on the other hand, contain only factors of variation that are particular to each domain. We achieve this through bidirectional image translation based on Generative Adversarial Networks and cross-domain autoencoders, a novel network component. Our model offers multiple advantages. We can output diverse samples covering multiple modes of the distributions of both domains, perform domain- specific image transfer and interpolation, and cross-domain retrieval without the need of labeled data, only paired images. We compare our model to the state-of-the-art in multi-modal image translation and achieve better results for translation on challenging datasets as well as for cross-domain retrieval on realistic datasets.",https://proceedings.neurips.cc/paper_files/paper/2018/file/dc6a70712a252123c40d2adba6a11d84-Paper.pdf,2018,"domain, cross, image, translation, achieve, multiple, covering, data, datasets, deep","image, domain, representation, images, shared, translation, model, domains, exclusive, input","{'image': 0.6368085999405556, 'disentanglement': 0.44984051402978587, 'cross': 0.38513737615357263, 'translation': 0.3773674896293349, 'domain': 0.3184042999702778}","domain, cross, translation, image, covering, modes, parts, retrieval, internal, multiple"
Expanding Holographic Embeddings for Knowledge Completion,"Yexiang Xue, Yang Yuan, Zhitian Xu, Ashish Sabharwal","Neural models operating over structured spaces such as knowledge graphs require a continuous embedding of the discrete elements of this space (such as entities) as well as the relationships between them. Relational embeddings with high expressivity, however, have high model complexity, making them computationally difficult to train. We propose a new family of embeddings for knowledge graphs that interpolate between a method with high model complexity and one, namely Holographic embeddings (HolE), with low dimensionality and high training efficiency. This interpolation, termed HolEx, is achieved by concatenating several linearly perturbed copies of original HolE. We formally characterize the number of perturbed copies needed to provably recover the full entity-entity or entity-relation interaction matrix, leveraging ideas from Haar wavelets and compressed sensing. In practice, using just a handful of Haar-based or random perturbation vectors results in a much stronger knowledge completion system. On the Freebase FB15K dataset, HolEx outperforms originally reported HolE by 14.7\% on the HITS@10 metric, and the current path-based state-of-the-art method, PTransE, by 4\% (absolute).",https://proceedings.neurips.cc/paper_files/paper/2018/file/dd28e50635038e9cf3a648c2dd17ad0a-Paper.pdf,2018,"high, embeddings, entity, hole, knowledge, based, complexity, copies, graphs, haar","vectors, product, matrix, holex, full, hole, tensor, diagonal, embedding, random","{'expanding': 0.5040417456067381, 'holographic': 0.5040417456067381, 'completion': 0.42739395306049804, 'embeddings': 0.4166391950351843, 'knowledge': 0.36827982217218896}","hole, entity, haar, holex, embeddings, copies, perturbed, high, knowledge, graphs"
Breaking the Curse of Horizon: Infinite-Horizon Off-Policy Estimation,"Qiang Liu, Lihong Li, Ziyang Tang, Dengyong Zhou","We consider the off-policy estimation problem of estimating the expected reward of a target policy using samples collected by a different behavior policy. Importance sampling (IS) has been a key technique to derive (nearly) unbiased estimators, but is known to suffer from an excessively high variance in long-horizon problems.  In the extreme case of in infinite-horizon problems, the variance of an IS-based estimator may even be unbounded. In this paper, we propose a new off-policy estimation method that applies IS directly on the stationary state-visitation distributions to avoid the exploding variance issue faced by existing estimators.Our key contribution is a novel approach to estimating the density ratio of two stationary distributions, with trajectories sampled from only the behavior distribution. We develop a mini-max loss function for the estimation problem, and derive a closed-form solution for the case of RKHS. We support our method with both theoretical  and empirical analyses.",https://proceedings.neurips.cc/paper_files/paper/2018/file/dda04f9d634145a9c68d5dfe53b21272-Paper.pdf,2018,"policy, estimation, variance, behavior, case, derive, distributions, estimating, estimators, horizon","policy, s0, reward, average, state, ratio, case, horizon, trajectory, behavior","{'horizon': 0.6893465420850549, 'curse': 0.38125178324147835, 'infinite': 0.35985472527292073, 'breaking': 0.34467327104252743, 'estimation': 0.2597405958986033, 'policy': 0.2523651292480877}","policy, variance, horizon, estimation, stationary, estimators, estimating, behavior, derive, key"
TopRank: A practical algorithm for online stochastic ranking,"Tor Lattimore, Branislav Kveton, Shuai Li, Csaba Szepesvari","Online learning to rank is a sequential decision-making problem where in each round the learning agent chooses a list of items and receives feedback in the form of clicks from the user. Many sample-efficient algorithms have been proposed for this problem that assume a specific click model connecting rankings and user behavior. We propose a generalized click model that encompasses many existing models, including the position-based and cascade models. Our generalization motivates a novel online learning algorithm based on topological sort, which we call TopRank. TopRank is (a) more natural than existing algorithms, (b) has stronger regret guarantees than existing algorithms with comparable generality, (c) has a more insightful proof that leaves the door open to many generalizations, (d) outperforms existing algorithms empirically.",https://proceedings.neurips.cc/paper_files/paper/2018/file/de03beffeed9da5f3639a621bcab5dd4-Paper.pdf,2018,"algorithms, existing, learning, many, based, click, model, models, online, problem","items, position, toprank, algorithm, model, regret, et, item, user, td","{'toprank': 0.508003380418895, 'ranking': 0.47949262123862496, 'practical': 0.41052445093154727, 'algorithm': 0.38201369175127714, 'online': 0.3362668566895036, 'stochastic': 0.2906530702654979}","toprank, click, existing, algorithms, user, many, online, door, list, sort"
rho-POMDPs have Lipschitz-Continuous epsilon-Optimal Value Functions,"Mathieu Fehr, Olivier Buffet, Vincent Thomas, Jilles Dibangoye","Many state-of-the-art algorithms for solving Partially Observable Markov Decision Processes (POMDPs) rely on turning the problem into a “fully observable” problem—a belief MDP—and exploiting the piece-wise linearity and convexity (PWLC) of the optimal value function in this new state space (the belief simplex ∆). This approach has been extended to solving ρ-POMDPs—i.e., for information-oriented criteria—when the reward ρ is convex in ∆. General ρ-POMDPs can also be turned into “fully observable” problems, but with no means to exploit the PWLC property. In this paper, we focus on POMDPs and ρ-POMDPs with λ ρ -Lipschitz reward function, and demonstrate that, for finite horizons, the optimal value function is Lipschitz-continuous. Then, value function approximators are proposed for both upper- and lower-bounding the optimal value function, which are shown to provide uniformly improvable bounds. This allows proposing two algorithms derived from HSVI which are empirically evaluated on various benchmark problems.",https://proceedings.neurips.cc/paper_files/paper/2018/file/de7f47e09c8e05e6021ababdf6bc58e7-Paper.pdf,2018,"function, pomdps, value, observable, optimal, algorithms, belief, fully, lipschitz, problem","lc, 600, hsvi, lipschitz, 10, pomdps, function, value, upper, bounds","{'rho': 0.4213677396868767, 'epsilon': 0.3977192471460573, 'pomdps': 0.3809403746649052, 'lipschitz': 0.367925670062066, 'value': 0.34051300964293385, 'continuous': 0.31686451710211444, 'functions': 0.3038498124992752, 'optimal': 0.2740562354152839}","pomdps, observable, value, pwlc, function, belief, lipschitz, optimal, solving, reward"
Minimax Estimation of Neural Net Distance,"Kaiyi Ji, Yingbin Liang","An important class of distance metrics proposed for training generative adversarial networks (GANs) is the integral probability metric (IPM), in which the neural net distance captures the practical GAN training via two neural networks. This paper investigates the minimax estimation problem of the neural net distance based on samples drawn from the distributions. We develop the first known minimax lower bound on the estimation error of the neural net distance, and an upper bound tighter than an existing bound on the estimator error for the empirical neural net distance. Our lower and upper bounds match not only in the order of the sample size but also in terms of the norm of the parameter matrices of neural networks, which justifies the empirical neural net distance as a good approximation of the true neural net distance for training GANs in practice.",https://proceedings.neurips.cc/paper_files/paper/2018/file/dea9ddb25cbf2352cf4dec30222a02a5-Paper.pdf,2018,"neural, distance, net, bound, networks, training, empirical, error, estimation, gans","neural, bound, dfnn, networks, distance, upper, set, µn, νm, estimation","{'minimax': 0.5141329310584902, 'distance': 0.5011955106135251, 'net': 0.4899886072711522, 'estimation': 0.4130869778954995, 'neural': 0.27155239417043175}","net, distance, neural, gans, minimax, bound, upper, estimation, training, ipm"
Using Large Ensembles of Control Variates for Variational Inference,"Tomas Geffner, Justin Domke","Variational inference is increasingly being addressed with stochastic optimization. In this setting, the gradient's variance plays a crucial role in the optimization procedure, since high variance gradients lead to poor convergence. A popular approach used to reduce gradient's variance involves the use of control variates. Despite the good results obtained, control variates developed for variational inference are typically looked at in isolation. In this paper we clarify the large number of control variates that are available by giving a systematic view of how they are derived. We also present a Bayesian risk minimization framework in which the quality of a procedure for combining control variates is quantified by its effect on optimization convergence rates, which leads to a very simple combination rule. Results show that combining a large number of control variates this way significantly improves the convergence of inference over using the typical gradient estimators or a reduced number of control variates.",https://proceedings.neurips.cc/paper_files/paper/2018/file/dead35fa1512ad67301d09326177c42f-Paper.pdf,2018,"control, variates, convergence, gradient, inference, number, optimization, variance, combining, large","control, term, gradient, variates, qw, using, use, variance, data, learning","{'ensembles': 0.49213084127072815, 'variates': 0.49213084127072815, 'control': 0.37600483564686404, 'large': 0.33877473824517157, 'inference': 0.30116829094376774, 'variational': 0.2991347322130717, 'using': 0.28157163023153625}","variates, control, variance, convergence, inference, combining, gradient, optimization, procedure, number"
Deep Generative Markov State Models,"Hao Wu, Andreas Mardt, Luca Pasquali, Frank Noe","We propose a deep generative Markov State Model (DeepGenMSM) learning framework for inference of metastable dynamical systems and prediction of trajectories. After unsupervised training on time series data, the model contains (i) a probabilistic encoder that maps from high-dimensional configuration space to a small-sized vector indicating the membership to metastable (long-lived) states, (ii) a Markov chain that governs the transitions between metastable states and facilitates analysis of the long-time dynamics, and (iii) a generative part that samples the conditional distribution of configurations in the next time step. The model can be operated in a recursive fashion to generate trajectories to predict the system evolution from a defined starting state and propose new configurations. The DeepGenMSM is demonstrated to provide accurate estimates of the long-time kinetics and generate valid distributions for molecular dynamics (MD) benchmark systems. Remarkably, we show that DeepGenMSMs are able to make long time-steps in molecular configuration space and generate physically realistic structures in regions that were not seen in training data.",https://proceedings.neurips.cc/paper_files/paper/2018/file/deb54ffb41e085fd7f69a75b6359c989-Paper.pdf,2018,"time, long, generate, metastable, model, configuration, configurations, data, deepgenmsm, dynamics","xt, time, distribution, model, space, transition, states, conﬁgurations, data, fig","{'state': 0.5690287577790696, 'markov': 0.5313657908685125, 'generative': 0.41353592309144654, 'models': 0.3520997386865123, 'deep': 0.31443677177595536}","metastable, long, deepgenmsm, time, configuration, configurations, molecular, generate, trajectories, states"
Zero-Shot Transfer with Deictic Object-Oriented Representation in Reinforcement Learning,"Ofir Marom, Benjamin Rosman","Object-oriented representations in reinforcement learning have shown promise in transfer learning, with previous research introducing a propositional object-oriented framework that has provably efficient learning bounds with respect to sample complexity. However, this framework has limitations in terms of the classes of tasks it can efficiently learn. In this paper we introduce a novel deictic object-oriented framework that has provably efficient learning bounds and can solve a broader range of tasks. Additionally, we show that this framework is capable of zero-shot transfer of transition dynamics across tasks and demonstrate this empirically for the Taxi and Sokoban domains.",https://proceedings.neurips.cc/paper_files/paper/2018/file/df0aab058ce179e4f7ab135ed4e641a9-Paper.pdf,2018,"framework, learning, object, oriented, tasks, bounds, efficient, provably, transfer, across","taxi, dynamics, transition, deictic, mdps, object, box, one, passenger, set","{'deictic': 0.4463853977990407, 'oriented': 0.4035577588566051, 'zero': 0.36073011991416953, 'shot': 0.3356775571384196, 'transfer': 0.33073194056468147, 'object': 0.3141566539857848, 'representation': 0.3106249943626697, 'reinforcement': 0.262851738846496, 'learning': 0.14635103903990224}","oriented, object, framework, provably, transfer, tasks, deictic, propositional, bounds, sokoban"
Practical Methods for Graph Two-Sample Testing,"Debarghya Ghoshdastidar, Ulrike von Luxburg","Hypothesis testing for graphs has been an important tool in applied research fields for more than two decades, and still remains a challenging problem as one often needs to draw inference from few replicates of large graphs. Recent studies in statistics and learning theory have provided some theoretical insights about such high-dimensional graph testing problems, but the practicality of the developed theoretical methods remains an open question.In this paper, we consider the problem of two-sample testing of large graphs. We demonstrate the practical merits and limitations of existing theoretical tests and their bootstrapped variants. We also propose two new tests based on asymptotic distributions. We show that these tests are computationally less expensive and, in some cases, more reliable than the existing methods.",https://proceedings.neurips.cc/paper_files/paper/2018/file/dfa92d8f817e5b08fcaafb50d03763cf-Paper.pdf,2018,"graphs, testing, tests, theoretical, two, existing, large, methods, problem, remains","tests, graphs, test, al, et, two, based, testing, ij, asymp","{'two': 0.45827008990385765, 'practical': 0.4241262304861105, 'methods': 0.4079157555621695, 'testing': 0.4079157555621695, 'sample': 0.40099185715410496, 'graph': 0.3413509462965404}","tests, testing, graphs, theoretical, remains, two, replicates, bootstrapped, practicality, merits"
Point process latent variable models of larval zebrafish behavior,"Anuj Sharma, Robert Johnson, Florian Engert, Scott Linderman","A fundamental goal of systems neuroscience is to understand how neural activity gives rise to natural behavior.  In order to achieve this goal, we must first build comprehensive models that offer quantitative descriptions of behavior.  We develop a new class of probabilistic models to tackle this challenge in the study of larval zebrafish, an important model organism for neuroscience.  Larval zebrafish locomote via sequences of punctate swim bouts--brief flicks of the tail--which are naturally modeled as a marked point process.  However, these sequences of swim bouts belie a set of discrete and continuous internal states, latent variables that are not captured by standard point process models.  We incorporate these variables as latent marks of a point process and explore various models for their dynamics.  To infer the latent variables and fit the parameters of this model, we develop an amortized variational inference algorithm that targets the collapsed posterior distribution, analytically marginalizing out the discrete latent variables.  With a dataset of over 120,000 swim bouts, we show that our models reveal interpretable discrete classes of swim bouts and continuous internal states like hunger that modulate their dynamics.  These models are a major step toward understanding the natural behavioral program of the larval zebrafish and, ultimately, its neural underpinnings.",https://proceedings.neurips.cc/paper_files/paper/2018/file/e02af5824e1eb6ad58d6bc03ac9e827f-Paper.pdf,2018,"models, bouts, latent, swim, variables, discrete, larval, point, process, zebrafish","latent, models, states, discrete, continuous, model, time, process, point, variables","{'larval': 0.42632855916086454, 'zebrafish': 0.42632855916086454, 'behavior': 0.385425237294565, 'variable': 0.37225730881158264, 'latent': 0.3158716041892004, 'point': 0.3158716041892004, 'process': 0.3158716041892004, 'models': 0.22368547288827684}","bouts, swim, larval, zebrafish, variables, latent, models, discrete, point, neuroscience"
Learning to Navigate in Cities Without a Map,"Piotr Mirowski, Matt Grimes, Mateusz Malinowski, Karl Moritz Hermann, Keith Anderson, Denis Teplyashin, Karen Simonyan, koray kavukcuoglu, Andrew Zisserman, Raia Hadsell","Navigating through unstructured environments is a basic capability of intelligent creatures, and thus is of fundamental interest in the study and development of artificial intelligence. Long-range navigation is a complex cognitive task that relies on developing an internal representation of space, grounded by recognisable landmarks and robust visual processing, that can simultaneously support continuous self-localisation (""I am here"") and a representation of the goal (""I am going there""). Building upon recent research that applies deep reinforcement learning to maze navigation problems, we present an end-to-end deep reinforcement learning approach that can be applied on a city scale. Recognising that successful navigation relies on integration of general policies with locale-specific knowledge, we propose a dual pathway architecture that allows locale-specific features to be encapsulated, while still enabling transfer to multiple cities. A key contribution of this paper is an interactive navigation environment that uses Google Street View for its photographic content and worldwide coverage. Our baselines demonstrate that deep reinforcement learning agents can learn to navigate in multiple cities and to traverse to target destinations that may be kilometres away. A video summarizing our research and showing the trained agent in diverse city environments as well as on the transfer task is available at: https://sites.google.com/view/learn-navigate-cities-nips18",https://proceedings.neurips.cc/paper_files/paper/2018/file/e034fb6b66aacc1d48f445ddfb08da98-Paper.pdf,2018,"navigation, cities, deep, learning, reinforcement, city, end, environments, google, learn","agent, goal, city, learning, new, navigation, lstm, task, training, environment","{'cities': 0.5286746624789538, 'navigate': 0.5286746624789538, 'map': 0.47795192899730027, 'without': 0.4272291955156468, 'learning': 0.17333023559766358}","navigation, cities, locale, navigate, city, google, relies, reinforcement, environments, view"
Fast greedy algorithms for dictionary selection with generalized sparsity constraints,"Kaito Fujii, Tasuku Soma","In dictionary selection, several atoms are selected from finite candidates that successfully approximate given data points in the sparse representation. We propose a novel efficient greedy algorithm for dictionary selection. Not only does our algorithm work much faster than the known methods, but it can also handle more complex sparsity constraints, such as average sparsity. Using numerical experiments, we show that our algorithm outperforms the known methods for dictionary selection, achieving competitive performances with dictionary learning algorithms in a smaller running time.",https://proceedings.neurips.cc/paper_files/paper/2018/file/e069ea4c9c233d36ff9c7f329bc08ff1-Paper.pdf,2018,"dictionary, algorithm, selection, known, methods, sparsity, achieving, algorithms, also, approximate","dictionary, zt, replacement, sparsity, selection, constraint, greedy, omp, time, average","{'dictionary': 0.44196645833658127, 'sparsity': 0.37475822864877845, 'greedy': 0.3653279734047547, 'constraints': 0.35715911474678264, 'selection': 0.34995367075587497, 'generalized': 0.33767752717788724, 'fast': 0.28995088505897976, 'algorithms': 0.2874535763975614}","dictionary, selection, sparsity, atoms, known, algorithm, candidates, performances, selected, successfully"
Acceleration through Optimistic No-Regret Dynamics,"Jun-Kun Wang, Jacob D. Abernethy","We consider the problem of minimizing a smooth convex function by reducing the optimization to computing the Nash equilibrium of a particular zero-sum convex-concave game. Zero-sum games can be solved using online learning dynamics, where a classical technique involves simulating two no-regret algorithms that play against each other and, after $T$ rounds, the average iterate is guaranteed to solve the original optimization problem with error decaying as $O(\log T/T)$.
In this paper we show that the technique can be enhanced to a rate of $O(1/T^2)$ by extending recent work \cite{RS13,SALS15} that leverages \textit{optimistic learning} to speed up equilibrium computation. The resulting optimization algorithm derived from this analysis coincides \textit{exactly} with the well-known \NA \cite{N83a} method, and indeed the same story allows us to recover several variants of the Nesterov's algorithm via small tweaks. We are also able to establish the accelerated linear rate for a function which is both strongly-convex and smooth. This methodology unifies a number of different iterative optimization methods: we show that the \HB algorithm is precisely the non-optimistic variant of \NA, and recent prior work already established a similar perspective on \FW \cite{AW17,ALLW18}.",https://proceedings.neurips.cc/paper_files/paper/2018/file/e06f967fb0d355592be4e7674fa31d26-Paper.pdf,2018,"optimization, algorithm, cite, convex, equilibrium, function, learning, na, optimistic, problem","xt, algorithm, yt, player, pt, game, regret, convex, αt, function","{'optimistic': 0.5545008316547544, 'acceleration': 0.49565433226226385, 'regret': 0.476709990822049, 'dynamics': 0.46861838979523307}","cite, na, optimistic, optimization, equilibrium, textit, convex, sum, smooth, zero"
Hybrid Retrieval-Generation Reinforced Agent for Medical Image Report Generation,"Yuan Li, Xiaodan Liang, Zhiting Hu, Eric P. Xing","Generating long and coherent reports to describe medical images poses challenges to bridging visual patterns with informative human linguistic descriptions. We propose a novel Hybrid Retrieval-Generation Reinforced Agent (HRGR-Agent) which reconciles traditional retrieval-based approaches populated with human prior knowledge, with modern learning-based approaches to achieve structured, robust, and diverse report generation. HRGR-Agent employs a hierarchical decision-making procedure. For each sentence, a high-level retrieval policy module chooses to either retrieve a template sentence from an off-the-shelf template database, or invoke a low-level generation module to generate a new sentence. HRGR-Agent is updated via reinforcement learning, guided by sentence-level and word-level rewards. Experiments show that our approach achieves the state-of-the-art results on two medical report datasets, generating well-balanced structured sentences with robust coverage of heterogeneous medical report contents. In addition, our model achieves the highest detection precision of medical abnormality terminologies, and improved human evaluation performance.",https://proceedings.neurips.cc/paper_files/paper/2018/file/e07413354875be01a996dc560274708e-Paper.pdf,2018,"agent, level, medical, sentence, generation, hrgr, human, report, retrieval, achieves","generation, retrieval, module, agent, report, sentence, medical, template, hrgr, sentences","{'generation': 0.5294027215569017, 'report': 0.3716826048914735, 'medical': 0.3508225995292688, 'reinforced': 0.3360221901871326, 'retrieval': 0.3151621848249279, 'agent': 0.30723157885296765, 'hybrid': 0.30036177548279175, 'image': 0.24831783872955823}","sentence, medical, hrgr, report, agent, retrieval, level, template, generation, human"
Dimensionally Tight Bounds for Second-Order Hamiltonian Monte Carlo,"Oren Mangoubi, Nisheeth Vishnoi","Hamiltonian Monte Carlo (HMC) is a widely deployed method to sample from high-dimensional  distributions in  Statistics and Machine learning. HMC is known to run very efficiently in practice and its popular second-order ``leapfrog"" implementation has long been conjectured to run in $d^{1/4}$ gradient evaluations. Here we show that this conjecture is true when sampling from strongly log-concave target distributions that satisfy a weak third-order regularity property associated with the input data.  Our regularity condition is weaker than the Lipschitz Hessian property and allows us to show faster convergence bounds for a much larger class of distributions than would be possible with the usual Lipschitz Hessian constant alone.  Important distributions that satisfy our regularity condition include posterior distributions used in Bayesian logistic regression for which the data satisfies an ``incoherence"" property. Our result compares favorably with the best available bounds for the class of strongly log-concave distributions, which grow like $d^{{1}/{2}}$ gradient evaluations with the dimension. Moreover, our simulations on synthetic data suggest that, when our regularity condition is satisfied, leapfrog HMC performs better than its competitors -- both in terms of accuracy and in terms of the number of gradient evaluations it requires.",https://proceedings.neurips.cc/paper_files/paper/2018/file/e07bceab69529b0f0b43625953fbf2a0-Paper.pdf,2018,"distributions, regularity, condition, data, evaluations, gradient, hmc, property, bounds, class","hmc, gradient, bound, show, pt, evaluations, bounds, error, start, step","{'dimensionally': 0.4243240392650246, 'second': 0.38361303743153813, 'hamiltonian': 0.3705070221703279, 'tight': 0.3705070221703279, 'carlo': 0.32979602033684147, 'monte': 0.32979602033684147, 'order': 0.31908762615867187, 'bounds': 0.28087630201158437}","regularity, hmc, distributions, leapfrog, condition, evaluations, property, concave, hessian, satisfy"
Invertibility of Convolutional Generative Networks from Partial Measurements,"Fangchang Ma, Ulas Ayaz, Sertac Karaman","In this work, we present new theoretical results on convolutional generative neural networks, in particular their invertibility (i.e., the recovery of input latent code given the network output). The study of network inversion problem is motivated by image inpainting and the mode collapse problem in training GAN. Network inversion is highly non-convex, and thus is typically computationally intractable and without optimality guarantees. However, we rigorously prove that, under some mild technical assumptions, the input of a two-layer convolutional generative network can be deduced from the network output efficiently using simple gradient descent. This new theoretical finding implies that the mapping from the low- dimensional latent space to the high-dimensional image space is bijective (i.e., one-to-one). In addition, the same conclusion holds even when the network output is only partially observed (i.e., with missing pixels). Our theorems hold for 2-layer convolutional generative network with ReLU as the activation function, but we demonstrate empirically that the same conclusion extends to multi-layer networks and networks with other activation functions, including the leaky ReLU, sigmoid and tanh.",https://proceedings.neurips.cc/paper_files/paper/2018/file/e0ae4561193dbf6e4cf7e8f4006948e3-Paper.pdf,2018,"network, convolutional, generative, layer, networks, output, activation, conclusion, dimensional, image","network, layer, generative, networks, matrix, latent, convolutional, problem, gaussian, relu","{'invertibility': 0.5175210258835699, 'measurements': 0.48847610628573956, 'partial': 0.4678684078746838, 'convolutional': 0.34575118919550996, 'generative': 0.3189105538480258, 'networks': 0.23111557300057706}","network, conclusion, inversion, layer, convolutional, output, generative, relu, activation, bijective"
Towards Robust Detection of Adversarial Examples,"Tianyu Pang, Chao Du, Yinpeng Dong, Jun Zhu","Although the recent progress is substantial, deep learning methods can be vulnerable to the maliciously generated adversarial examples. In this paper, we present a novel training procedure and a thresholding test strategy, towards robust detection of adversarial examples. In training, we propose to minimize the reverse cross-entropy (RCE), which encourages a deep network to learn latent representations that better distinguish adversarial examples from normal ones. In testing, we propose to use a thresholding strategy as the detector to filter out adversarial examples for reliable predictions. Our method is simple to implement using standard algorithms, with little extra training cost compared to the common cross-entropy minimization. We apply our method to defend various attacking methods on the widely used MNIST and CIFAR-10 datasets, and achieve significant improvements on robust predictions under all the threat models in the adversarial setting.",https://proceedings.neurips.cc/paper_files/paper/2018/file/e0f7a4d0ef9b84b83b693bbf3feb8e6e-Paper.pdf,2018,"adversarial, examples, training, cross, deep, entropy, method, methods, predictions, propose","adversarial, examples, ce, rce, training, method, attack, non, 10, normal","{'examples': 0.5373864269321257, 'towards': 0.46955988359646184, 'detection': 0.4362699121887787, 'robust': 0.4054442322848099, 'adversarial': 0.3687992436160581}","adversarial, examples, thresholding, cross, entropy, predictions, strategy, training, robust, maliciously"
Bayesian Model-Agnostic Meta-Learning,"Jaesik Yoon, Taesup Kim, Ousmane Dia, Sungwoong Kim, Yoshua Bengio, Sungjin Ahn","Due to the inherent model uncertainty, learning to infer Bayesian posterior from a few-shot dataset is an important step towards robust meta-learning. In this paper, we propose a novel Bayesian model-agnostic meta-learning method. The proposed method combines efficient gradient-based meta-learning with nonparametric variational inference in a principled probabilistic framework. Unlike previous methods, during fast adaptation, the method is capable of learning complex uncertainty structure beyond a simple Gaussian approximation, and during meta-update, a novel Bayesian mechanism prevents meta-level overfitting. Remaining a gradient-based method, it is also the first Bayesian model-agnostic meta-learning method applicable to various tasks including reinforcement learning. Experiment results show the accuracy and robustness of the proposed method in sinusoidal regression, image classification, active learning, and reinforcement learning.",https://proceedings.neurips.cc/paper_files/paper/2018/file/e1021d43911ca2c1845910d84f40aeae-Paper.pdf,2018,"learning, meta, method, bayesian, model, agnostic, based, gradient, novel, proposed","task, learning, meta, θ0, al, et, model, particles, bayesian, method","{'agnostic': 0.5719746658688767, 'meta': 0.5451140220613289, 'model': 0.42170274197795843, 'bayesian': 0.38594306331175987, 'learning': 0.2211570343582002}","meta, learning, bayesian, method, agnostic, uncertainty, reinforcement, sinusoidal, prevents, remaining"
Learning Signed Determinantal Point Processes through the Principal Minor Assignment Problem,Victor-Emmanuel Brunel,"Symmetric determinantal point processes (DPP) are a class of probabilistic models that encode the random selection of items that have a repulsive behavior. They have attracted a lot of attention in machine learning, where returning diverse sets of items is sought for. Sampling and learning these symmetric DPP's is pretty well understood. In this work, we consider a new class of DPP's, which we call signed DPP's, where we break the symmetry and allow attractive behaviors. We set the ground for learning signed DPP's through a method of moments, by solving the so called principal assignment problem for a class of matrices $K$ that satisfy $K_{i,j}=\pm K_{j,i}$, $i\neq j$, in polynomial time.",https://proceedings.neurips.cc/paper_files/paper/2018/file/e1228be46de6a0234ac22ded31417bc7-Paper.pdf,2018,"dpp, class, learning, items, k_, signed, symmetric, allow, assignment, attention","dpp, principal, symmetric, cycles, minors, matrix, graph, set, det, cycle","{'minor': 0.3980865484137263, 'signed': 0.3980865484137263, 'principal': 0.37574467008730694, 'determinantal': 0.35989285514471203, 'assignment': 0.347597232233906, 'problem': 0.3375509768182927, 'point': 0.29494678212758213, 'processes': 0.27120984569587747, 'learning': 0.13051587322393904}","dpp, k_, signed, items, symmetric, class, neq, pm, pretty, repulsive"
Direct Estimation of Differences in Causal Graphs,"Yuhao Wang, Chandler Squires, Anastasiya Belyaeva, Caroline Uhler","We consider the problem of estimating the differences between two causal directed acyclic graph (DAG) models with a shared topological order given i.i.d. samples from each model. This is of interest for example in genomics, where changes in the structure or edge weights of the underlying causal graphs reflect alterations in the gene regulatory networks. We here provide the first provably consistent method for directly estimating the differences in a pair of causal DAGs without separately learning two possibly large and dense DAG models and computing their difference. Our two-step algorithm first uses invariance tests between regression coefficients of the two data sets to estimate the skeleton of the difference graph and then orients some of the edges using invariance tests between regression residual variances. We demonstrate the properties of our method through a simulation study and apply it to the analysis of gene expression data from ovarian cancer and during T-cell activation.",https://proceedings.neurips.cc/paper_files/paper/2018/file/e1314fc026da60d837353d20aefaf054-Paper.pdf,2018,"two, causal, dag, data, difference, differences, estimating, first, gene, graph","algorithm, dag, dci, two, causal, data, ij, dags, difference, edge","{'differences': 0.5435898536547283, 'direct': 0.5130818688503584, 'graphs': 0.39717578233046535, 'causal': 0.38256710548061507, 'estimation': 0.3703388645484312}","dag, causal, gene, tests, differences, invariance, difference, two, estimating, graph"
A^2-Nets: Double Attention Networks,"Yunpeng Chen, Yannis Kalantidis, Jianshu Li, Shuicheng Yan, Jiashi Feng","Learning to capture long-range relations is fundamental to image/video recognition. Existing CNN models generally rely on increasing depth to model such relations which is highly inefficient. In this work, we propose the “double attention block”, a novel component that aggregates and propagates informative global features from the entire spatio-temporal space of input images/videos, enabling subsequent convolution layers to access features from the entire space efficiently. The component is designed with a double attention mechanism in two steps, where the first step gathers features from the entire space into a compact set through second-order attention pooling and the second step adaptively selects and distributes features to each location via another attention. The proposed double attention block is easy to adopt and can be plugged into existing deep neural networks conveniently. We conduct extensive ablation studies and experiments on both image and video recognition tasks for evaluating its performance. On the image recognition task, a ResNet-50 equipped with our double attention blocks outperforms a much larger ResNet-152 architecture on ImageNet-1k dataset with over 40% less the number of parameters and less FLOPs. On the action recognition task, our proposed model achieves the state-of-the-art results on the Kinetics and UCF-101 datasets with significantly higher efficiency than recent works.",https://proceedings.neurips.cc/paper_files/paper/2018/file/e165421110ba03099a1c0393373c5b43-Paper.pdf,2018,"attention, double, features, recognition, entire, image, space, block, component, existing","attention, features, resnet, feature, input, double, global, proposed, video, block","{'double': 0.5982325787366318, 'nets': 0.5469756017725517, 'attention': 0.5055768571780224, 'networks': 0.29551229126665457}","double, attention, recognition, entire, features, relations, resnet, block, image, video"
Sparse Attentive Backtracking: Temporal Credit Assignment Through Reminding,"Nan Rosemary Ke, Anirudh Goyal ALIAS PARTH GOYAL, Olexa Bilaniuk, Jonathan Binas, Michael C. Mozer, Chris Pal, Yoshua Bengio","Learning long-term dependencies in extended temporal sequences requires credit assignment to events far back in the past. The most common method for training recurrent neural networks, back-propagation through time (BPTT), requires credit information to be propagated backwards through every single step of the forward computation, potentially over thousands or millions of time steps.
This becomes computationally expensive or even infeasible when used with long sequences. Importantly, biological brains are unlikely to perform such detailed reverse replay over very long sequences of internal states (consider days, months, or years.) However, humans are often reminded of past memories or mental states which are associated with the current mental state.
We consider the hypothesis that such memory associations between past and present could be used for credit assignment through arbitrarily long sequences, propagating the credit assigned to the current state to the associated past state. Based on this principle, we study a novel algorithm which only back-propagates through a few of these temporal skip connections, realized by a learned attention mechanism that associates current states with relevant past states. We demonstrate in experiments that our method matches or outperforms regular BPTT and truncated BPTT in tasks involving particularly long-term dependencies, but without requiring the biologically implausible backward replay through the whole history of states. Additionally, we demonstrate that the proposed method transfers to longer sequences significantly better than LSTMs trained with BPTT and LSTMs trained with full self-attention.",https://proceedings.neurips.cc/paper_files/paper/2018/file/e16e74a63567ecb44ade5c87002bb1d9-Paper.pdf,2018,"long, past, sequences, states, bptt, credit, back, current, method, state","sab, attention, lstm, past, bptt, al, et, 10, time, state","{'backtracking': 0.4224524706365519, 'reminding': 0.4224524706365519, 'attentive': 0.39874309955816045, 'credit': 0.39874309955816045, 'assignment': 0.3688728245874525, 'temporal': 0.32834138701810067, 'sparse': 0.28495696879976995}","bptt, sequences, past, credit, states, long, back, mental, lstms, replay"
Ridge Regression and Provable Deterministic Ridge Leverage Score Sampling,Shannon McCurdy,"Ridge leverage scores provide a balance between low-rank approximation and regularization, and are ubiquitous in randomized linear algebra and machine learning.  Deterministic algorithms are also of interest in the moderately big data regime, because deterministic algorithms provide interpretability to the practitioner by having no failure probability and always returning the same results. We provide provable guarantees for deterministic column sampling using ridge leverage scores.   The matrix sketch returned by our algorithm is a column subset of the original matrix, yielding additional interpretability.  Like the randomized counterparts, the deterministic algorithm provides $(1+\epsilon)$  error column subset selection, $(1+\epsilon)$ error projection-cost preservation, and an additive-multiplicative spectral bound.  We also show that under the assumption of power-law decay of ridge leverage scores, this deterministic algorithm is provably as accurate as randomized algorithms. Lastly, ridge regression is frequently used to regularize ill-posed linear least-squares problems.  While ridge regression provides shrinkage for the regression coefficients, many of the coefficients remain small but non-zero. Performing ridge regression with the matrix sketch returned by our algorithm and a particular regularization parameter forces coefficients to zero and has a provable $(1+\epsilon)$ bound on the statistical risk.  As such, it is an interesting alternative to elastic net regularization.",https://proceedings.neurips.cc/paper_files/paper/2018/file/e1d5be1c7f2f456670de3d53c7b54f4a-Paper.pdf,2018,"ridge, deterministic, algorithm, regression, algorithms, coefficients, column, epsilon, leverage, matrix","ridge, leverage, scores, regression, matrix, rank, algorithm, theorem, column, data","{'ridge': 0.6983185289484037, 'deterministic': 0.32956334033532003, 'score': 0.32956334033532003, 'leverage': 0.3156598108943168, 'provable': 0.296063886755435, 'regression': 0.2332701955528751, 'sampling': 0.23112186429142093}","ridge, deterministic, column, coefficients, randomized, scores, regression, leverage, returned, epsilon"
Dynamic Network Model from Partial Observations,"Elahe Ghalebi, Baharan Mirzasoleiman, Radu Grosu, Jure Leskovec","Can evolving networks be inferred and modeled without directly observing their nodes and edges? In many applications, the edges of a dynamic network might not be observed, but one can observe the dynamics of stochastic cascading processes (e.g., information diffusion, virus propagation) occurring over the unobserved network. While there have been efforts to infer networks based on such data, providing a generative probabilistic model that is able to identify the underlying time-varying network remains an open question. Here we consider the problem of inferring generative dynamic network models based on network cascade diffusion data. We propose a novel framework for providing a non-parametric dynamic network model---based on a mixture of coupled hierarchical Dirichlet processes---based on data capturing cascade node infection times. Our approach allows us to infer the evolving community structure in networks and to obtain an explicit predictive distribution over the edges of the underlying network---including those that were not involved in transmission of any cascade, or are likely to appear in the future. We show the effectiveness of our approach using extensive experiments on synthetic as well as real-world networks.",https://proceedings.neurips.cc/paper_files/paper/2018/file/e1dc4bf1f94e87fdfeb2d91ae3dc10ef-Paper.pdf,2018,"network, based, networks, cascade, data, dynamic, edges, approach, diffusion, evolving","network, model, dyference, edges, time, euv, infopath, dynamic, edge, distribution","{'observations': 0.5182468323220452, 'partial': 0.5182468323220452, 'dynamic': 0.46324779505750313, 'model': 0.35837064813022534, 'network': 0.34613927906179026}","cascade, network, edges, evolving, dynamic, diffusion, infer, networks, providing, processes"
Overfitting or perfect fitting? Risk bounds for classification and regression rules that interpolate,"Mikhail Belkin, Daniel J. Hsu, Partha Mitra","Many modern machine learning models are trained to achieve zero or near-zero training error in order to obtain near-optimal (but non-zero) test error. This phenomenon of strong generalization performance for ``overfitted'' / interpolated classifiers appears to be  ubiquitous in high-dimensional data, having been observed in deep networks, kernel machines, boosting and random forests. Their performance is consistently robust  even when the data contain large amounts of label noise. 

Very little theory is available to explain these observations. The vast majority of theoretical analyses of generalization allows for interpolation only when there is little or no label noise. This paper takes a step toward a theoretical foundation for interpolated classifiers by analyzing local interpolating schemes, including  geometric simplicial interpolation algorithm and singularly weighted $k$-nearest neighbor schemes. Consistency or near-consistency is proved for these schemes in  classification and regression problems. Moreover, the nearest neighbor schemes exhibit optimal rates under some standard statistical assumptions.

Finally, this paper suggests a way to explain the phenomenon of adversarial examples, which are seemingly ubiquitous in modern machine learning, and also discusses some connections to kernel machines and random forests in the interpolated regime.",https://proceedings.neurips.cc/paper_files/paper/2018/file/e22312179bf43e61576081a2f250f845-Paper.pdf,2018,"schemes, interpolated, near, zero, classifiers, consistency, data, error, explain, forests","data, risk, nearest, neighbor, training, interpolation, rd, interpolating, function, based","{'fitting': 0.38481568711196895, 'interpolate': 0.38481568711196895, 'overfitting': 0.38481568711196895, 'perfect': 0.363218611566935, 'rules': 0.347895242607563, 'risk': 0.31097479810315704, 'classification': 0.274054353598751, 'regression': 0.2570919340192833, 'bounds': 0.2547242134555298}","interpolated, schemes, near, zero, forests, interpolation, neighbor, nearest, phenomenon, machines"
Actor-Critic Policy Optimization in Partially Observable Multiagent Environments,"Sriram Srinivasan, Marc Lanctot, Vinicius Zambaldi, Julien Perolat, Karl Tuyls, Remi Munos, Michael Bowling","Optimization of parameterized policies for reinforcement learning (RL) is an important and challenging problem in artificial intelligence. Among the most common approaches are algorithms based on gradient ascent of a score function representing discounted return. In this paper, we examine the role of these policy gradient and actor-critic algorithms in partially-observable multiagent environments. We show several candidate policy update rules and relate them to a foundation of regret minimization and multiagent learning techniques for the one-shot and tabular cases, leading to previously unknown convergence guarantees. We apply our method to model-free multiagent reinforcement learning in adversarial sequential decision problems (zero-sum imperfect information games), using RL-style function approximation. We evaluate on commonly used benchmark Poker domains, showing performance against fixed policies and empirical convergence to approximate Nash equilibria in self-play with rates similar to or better than a baseline model-free algorithm for zero-sum games, without any domain-specific state space reductions.",https://proceedings.neurips.cc/paper_files/paper/2018/file/e22dd5dabde45eda5a1a67772c8e25dd-Paper.pdf,2018,"learning, multiagent, algorithms, convergence, free, function, games, gradient, model, policies","st, policy, player, ηπ, learning, gradient, regret, actor, using, algorithms","{'actor': 0.4047172998525673, 'observable': 0.4047172998525673, 'critic': 0.38200328274765966, 'multiagent': 0.38200328274765966, 'partially': 0.38200328274765966, 'environments': 0.3431734140348699, 'policy': 0.26789785169749275, 'optimization': 0.21234650754285742}","multiagent, games, rl, sum, free, zero, policies, policy, reinforcement, reductions"
End-to-end Symmetry Preserving Inter-atomic Potential Energy Model for Finite and Extended Systems,"Linfeng Zhang, Jiequn Han, Han Wang, Wissam Saidi, Roberto Car, Weinan E","Machine learning models are changing the paradigm of molecular modeling, which is a fundamental tool for material science, chemistry, and computational biology. Of particular interest is the inter-atomic potential energy surface (PES). Here we develop Deep Potential - Smooth Edition (DeepPot-SE), an end-to-end machine learning-based PES model, which is able to efficiently represent the PES for a wide variety of systems with the accuracy of ab initio quantum mechanics models. By construction, DeepPot-SE is extensive and continuously differentiable, scales linearly with system size, and preserves all the natural symmetries of the system. Further, we show that DeepPot-SE describes finite and extended systems including organic molecules, metals, semiconductors, and insulators with high fidelity.",https://proceedings.neurips.cc/paper_files/paper/2018/file/e2ad76f2326fbc6b56a45a56c59fafdb-Paper.pdf,2018,"deeppot, pes, se, end, learning, machine, models, potential, system, systems","model, systems, system, deeppot, ri, se, atomic, different, rji, energy","{'end': 0.51099931425102, 'extended': 0.3013203445141766, 'inter': 0.3013203445141766, 'potential': 0.3013203445141766, 'energy': 0.2844092921281162, 'finite': 0.2844092921281162, 'symmetry': 0.2844092921281162, 'atomic': 0.27241070951157037, 'preserving': 0.24350107450896416, 'systems': 0.23419425078868614}","deeppot, pes, se, potential, system, systems, end, ab, edition, initio"
Relational recurrent neural networks,"Adam Santoro, Ryan Faulkner, David Raposo, Jack Rae, Mike Chrzanowski, Theophane Weber, Daan Wierstra, Oriol Vinyals, Razvan Pascanu, Timothy Lillicrap","Memory-based neural networks model temporal data by leveraging an ability to remember information for long periods. It is unclear, however, whether they also have an ability to perform complex relational reasoning with the information they remember. Here, we first confirm our intuitions that standard memory architectures may struggle at tasks that heavily involve an understanding of the ways in which entities are connected -- i.e., tasks involving relational reasoning. We then improve upon these deficits by using a new memory module -- a Relational Memory Core (RMC) -- which employs multi-head dot product attention to allow memories to interact. Finally, we test the RMC on a suite of tasks that may profit from more capable relational reasoning across sequential information, and show large gains in RL domains (BoxWorld & Mini PacMan), program evaluation, and language modeling, achieving state-of-the-art results on the WikiText-103, Project Gutenberg, and GigaWord datasets.",https://proceedings.neurips.cc/paper_files/paper/2018/file/e2eabaf96372e20a9e3d4b5f83723a61-Paper.pdf,2018,"memory, relational, information, reasoning, tasks, ability, may, remember, rmc, 103","memory, model, relational, attention, reasoning, information, memories, vector, tasks, rmc","{'relational': 0.6918521422231608, 'recurrent': 0.5198159612631914, 'neural': 0.3548579697914927, 'networks': 0.35384714354182345}","relational, memory, remember, rmc, reasoning, information, ability, tasks, 103, boxworld"
Improved Expressivity Through Dendritic Neural Networks,"Xundong Wu, Xiangwen Liu, Wei Li, Qing Wu","A typical biological neuron, such as a pyramidal neuron of the neocortex, receives thousands of afferent synaptic inputs on its dendrite tree and sends the efferent axonal output downstream. In typical artificial neural networks, dendrite trees are modeled as linear structures that funnel weighted synaptic inputs to the cell bodies. However, numerous experimental and theoretical studies have shown that dendritic arbors are far more than simple linear accumulators. That is, synaptic inputs can actively modulate their neighboring synaptic activities; therefore, the dendritic structures are highly nonlinear. In this study, we model such local nonlinearity of dendritic trees with our dendritic neural network (DENN) structure and apply this structure to typical machine learning tasks. Equipped with localized nonlinearities, DENNs can attain greater model expressivity than regular neural networks while maintaining efficient network inference. Such strength is evidenced by the increased fitting power when we train DENNs with supervised machine learning tasks. We also empirically show that the locality structure can improve the generalization performance of DENNs, as exemplified by DENNs outranking naive deep neural network architectures when tested on 121 classification tasks from the UCI machine learning repository.",https://proceedings.neurips.cc/paper_files/paper/2018/file/e32c51ad39723ee92b285b362c916ca7-Paper.pdf,2018,"dendritic, denns, neural, synaptic, inputs, learning, machine, network, structure, tasks","layer, network, neural, denn, denns, model, 10, networks, learning, function","{'expressivity': 0.5949883473857452, 'dendritic': 0.5615957162709801, 'improved': 0.4347302672439948, 'neural': 0.26647013428695, 'networks': 0.26571108410513106}","denns, dendritic, synaptic, dendrite, typical, inputs, neuron, trees, structure, machine"
DAGs with NO TEARS: Continuous Optimization for Structure Learning,"Xun Zheng, Bryon Aragam, Pradeep K. Ravikumar, Eric P. Xing","Estimating the structure of directed acyclic graphs (DAGs, also known as Bayesian networks) is a challenging problem since the search space of DAGs is combinatorial and scales superexponentially with the number of nodes. Existing approaches rely on various local heuristics for enforcing the acyclicity constraint. In this paper, we introduce a fundamentally different strategy: we formulate the structure learning problem as a purely continuous optimization problem over real matrices that avoids this combinatorial constraint entirely. 
This is achieved by a novel characterization of acyclicity that is not only smooth but also exact. The resulting problem can be efficiently solved by standard numerical algorithms, which also makes implementation effortless. The proposed method outperforms existing ones, without imposing any structural assumptions on the graph such as bounded treewidth or in-degree.",https://proceedings.neurips.cc/paper_files/paper/2018/file/e347c51419ffb23ca3fd5050202f9c3d-Paper.pdf,2018,"problem, also, acyclicity, combinatorial, constraint, dags, existing, structure, achieved, acyclic","10, problem, learning, 15, graph, program, method, notears, wecp, optimization","{'dags': 0.537956780925352, 'tears': 0.537956780925352, 'continuous': 0.4045383629425215, 'structure': 0.38311691232500367, 'optimization': 0.28225441235180726, 'learning': 0.17637345270516713}","acyclicity, dags, combinatorial, constraint, problem, also, superexponentially, treewidth, acyclic, enforcing"
Kalman Normalization: Normalizing Internal Representations Across Network Layers,"Guangrun Wang, jiefeng peng, Ping Luo, Xinjiang Wang, Liang Lin","As an indispensable component, Batch Normalization (BN) has successfully improved the training of deep neural networks (DNNs) with mini-batches, by normalizing the distribution of the internal representation for each hidden layer. However, the effectiveness of BN would diminish with the scenario of micro-batch (e.g. less than 4 samples in a mini-batch), since the estimated statistics in a mini-batch are not reliable with insufficient samples. This limits BN's room in training larger models on segmentation, detection, and video-related problems, which require small batches constrained by memory consumption. In this paper, we present a novel normalization method, called Kalman Normalization (KN), for improving and accelerating the training of DNNs, particularly under the context of micro-batches. Specifically, unlike the existing solutions treating each hidden layer as an isolated system, KN treats all the layers in a network as a whole system, and estimates the statistics of a certain layer by considering the distributions of all its preceding layers, mimicking the merits of Kalman Filtering. On ResNet50 trained in ImageNet, KN has 3.4% lower error than its BN counterpart when using a batch size of 4; Even when using typical batch sizes, KN still maintains an advantage over BN while other BN variants suffer a performance degradation. Moreover, KN can be naturally generalized to many existing normalization variants to obtain gains, e.g. equipping Group Normalization with Group Kalman Normalization (GKN). KN can outperform BN and its variants for large scale object detection and segmentation task in COCO 2017.",https://proceedings.neurips.cc/paper_files/paper/2018/file/e369853df766fa44e1ed0ff613f563bd-Paper.pdf,2018,"bn, batch, kn, normalization, batches, kalman, layer, mini, training, variants","bn, kn, batch, training, layer, xk, normalization, statistics, using, gn","{'internal': 0.40729517997622305, 'normalizing': 0.40729517997622305, 'kalman': 0.3844364840714607, 'across': 0.36821798122146243, 'layers': 0.35563793307280755, 'normalization': 0.3453592853167, 'representations': 0.2803752709923326, 'network': 0.24593436680840697}","bn, kn, normalization, batch, kalman, batches, mini, variants, micro, layer"
Connectionist Temporal Classification with Maximum Entropy Regularization,"Hu Liu, Sheng Jin, Changshui Zhang","Connectionist Temporal Classification (CTC) is an objective function for end-to-end sequence learning, which adopts dynamic programming algorithms to directly learn the mapping between sequences. CTC has shown promising results in many sequence learning applications including speech recognition and scene text recognition. However, CTC tends to produce highly peaky and overconfident distributions, which is a symptom of overfitting. To remedy this, we propose a regularization method based on maximum conditional entropy which penalizes peaky distributions and encourages exploration. We also introduce an entropy-based pruning method to dramatically reduce the number of CTC feasible paths by ruling out unreasonable alignments. Experiments on scene text recognition show that our proposed methods consistently improve over the CTC baseline without the need to adjust training settings. Code has been made publicly available at: https://github.com/liuhu-bigeye/enctc.crnn.",https://proceedings.neurips.cc/paper_files/paper/2018/file/e44fea3bec53bcea3b7513ccef5857ac-Paper.pdf,2018,"ctc, recognition, based, distributions, end, entropy, learning, method, peaky, scene","ctc, sequence, feasible, entropy, paths, training, model, esctc, path, enctc","{'connectionist': 0.5049993164727206, 'maximum': 0.4282059096123411, 'temporal': 0.3924990088565463, 'entropy': 0.38583679190235326, 'classification': 0.359645580673725, 'regularization': 0.359645580673725}","ctc, peaky, recognition, text, scene, entropy, sequence, end, bigeye, crnn"
Are GANs Created Equal? A Large-Scale Study,"Mario Lucic, Karol Kurach, Marcin Michalski, Sylvain Gelly, Olivier Bousquet","Generative adversarial networks (GAN) are a powerful subclass of generative models. Despite a very rich research activity leading to numerous interesting GAN algorithms, it is still very hard to assess which algorithm(s) perform better than others.  We conduct a neutral, multi-faceted large-scale empirical study on state-of-the art models and evaluation measures. We find that most models can reach similar scores with enough hyperparameter optimization and random restarts. This suggests that improvements can arise from a higher computational budget and tuning more than fundamental algorithmic changes.  To overcome some limitations of the current metrics, we also propose several data sets on which precision and recall can be computed.  Our experimental results suggest that future GAN research should be based on more systematic and objective evaluation procedures. Finally, we did not find evidence that any of the tested algorithms consistently outperforms the non-saturating GAN introduced in \cite{goodfellow2014generative}.",https://proceedings.neurips.cc/paper_files/paper/2018/file/e46de7e1bcaaced9a54f1e9d0d2f800d-Paper.pdf,2018,"gan, models, algorithms, evaluation, find, generative, research, activity, adversarial, algorithm","data, fid, gan, model, set, models, budget, samples, precision, recall","{'created': 0.4737881476908778, 'equal': 0.4737881476908778, 'study': 0.4136975978184036, 'gans': 0.38287465533849036, 'scale': 0.3562841634206172, 'large': 0.3261479311135967}","gan, evaluation, research, faceted, goodfellow2014generative, neutral, restarts, saturating, find, generative"
Recurrent Transformer Networks for Semantic Correspondence,"Seungryong Kim, Stephen Lin, SANG RYUL JEON, Dongbo Min, Kwanghoon Sohn","We present recurrent transformer networks (RTNs) for obtaining dense correspondences between semantically similar images. Our networks accomplish this through an iterative process of estimating spatial transformations between the input images and using these transformations to generate aligned convolutional activations. By directly estimating the transformations between an image pair, rather than employing spatial transformer networks to independently normalize each individual image, we show that greater accuracy can be achieved. This process is conducted in a recursive manner to refine both the transformation estimates and the feature representations. In addition, a technique is presented for weakly-supervised training of RTNs that is based on a proposed classification loss. With RTNs, state-of-the-art performance is attained on several benchmarks for semantic correspondence.",https://proceedings.neurips.cc/paper_files/paper/2018/file/e4a93f0332b2519177ed55741ea4e5e7-Paper.pdf,2018,"networks, rtns, transformations, estimating, image, images, process, spatial, transformer, accomplish","geometric, matching, feature, rtns, networks, methods, target, source, ﬁelds, image","{'correspondence': 0.5541210318153684, 'transformer': 0.5541210318153684, 'semantic': 0.4387588367150323, 'recurrent': 0.36352958595401813, 'networks': 0.24746047672368726}","rtns, transformations, transformer, spatial, estimating, images, networks, correspondences, process, accomplish"
Generative Neural Machine Translation,"Harshil Shah, David Barber","We introduce Generative Neural Machine Translation (GNMT), a latent variable architecture which is designed to model the semantics of the source and target sentences. We modify an encoder-decoder translation model by adding a latent variable as a language agnostic representation which is encouraged to learn the meaning of the sentence. GNMT achieves competitive BLEU scores on pure translation tasks, and is superior when there are missing words in the source sentence. We augment the model to facilitate multilingual translation and semi-supervised learning without adding parameters. This framework significantly reduces overfitting when there is limited paired data available, and is effective for translating between pairs of languages not seen during training.",https://proceedings.neurips.cc/paper_files/paper/2018/file/e4bb4c5173c2ce17fd8fcd40041c068f-Paper.pdf,2018,"translation, model, adding, gnmt, latent, sentence, source, variable, achieves, agnostic","gnmt, sentence, source, translation, multi, data, model, paired, target, latent","{'translation': 0.5883274423603276, 'machine': 0.5774915559461297, 'generative': 0.45786638157274895, 'neural': 0.33276523256216445}","translation, gnmt, sentence, adding, source, variable, latent, bleu, encouraged, multilingual"
FRAGE: Frequency-Agnostic Word Representation,"Chengyue Gong, Di He, Xu Tan, Tao Qin, Liwei Wang, Tie-Yan Liu","Continuous word representation (aka word embedding) is a basic building block in many neural network-based models used in natural language processing tasks. Although it is widely accepted that words with similar semantics should be close to each other in the embedding space, we find that word embeddings learned in several tasks are biased towards word frequency: the embeddings of high-frequency and low-frequency words lie in different subregions of the embedding space, and the embedding of a rare word and a popular word can be far from each other even if they are semantically similar. This makes learned word embeddings ineffective, especially for rare words, and consequently limits the performance of these neural network models. In order to mitigate the issue, in this paper, we propose a neat, simple yet effective adversarial training method to blur the boundary between the embeddings of high-frequency words and low-frequency words. We conducted comprehensive studies on ten datasets across four natural language processing tasks, including word similarity, language modeling, machine translation and text classification. Results show that we achieve higher performance than the baselines in all tasks.",https://proceedings.neurips.cc/paper_files/paper/2018/file/e555ebe0ce426f7f9b2bef0706315e0c-Paper.pdf,2018,"word, frequency, words, embedding, embeddings, tasks, language, high, learned, low","words, word, rare, embeddings, popular, tasks, model, embedding, method, task","{'frage': 0.5092140765230593, 'frequency': 0.4806353692416607, 'word': 0.4446304561640312, 'agnostic': 0.4317797464518818, 'representation': 0.3543454163807042}","word, frequency, words, embeddings, embedding, language, rare, tasks, processing, similar"
Variational Memory Encoder-Decoder,"Hung Le, Truyen Tran, Thin Nguyen, Svetha Venkatesh","Introducing variability while maintaining coherence is a core task in learning to generate utterances in conversation. Standard neural encoder-decoder models and their extensions using conditional variational autoencoder often result in either trivial or digressive responses. To overcome this, we explore a novel approach that injects variability into neural encoder-decoder via the use of external memory as a mixture model, namely Variational Memory Encoder-Decoder (VMED). By associating each memory read with a mode in the latent mixture distribution at each timestep, our model can capture the variability observed in sequential data such as natural conversations. We empirically compare the proposed model against other recent approaches on various conversational datasets. The results show that VMED consistently achieves significant improvement over others in both metric-based and qualitative evaluations.",https://proceedings.neurips.cc/paper_files/paper/2018/file/e57c6b956a6521b28495f2886ca0977a-Paper.pdf,2018,"decoder, encoder, memory, model, variability, mixture, neural, variational, vmed, achieves","memory, model, vmed, mog, kl, latent, models, cvae, decoder, read","{'decoder': 0.5798454617030656, 'encoder': 0.5364086137656849, 'memory': 0.48642756584027885, 'variational': 0.37340763625163914}","variability, decoder, vmed, encoder, memory, mixture, digressive, timestep, variational, associating"
Joint Active Feature Acquisition and Classification with Variable-Size Set Encoding,"Hajin Shim, Sung Ju Hwang, Eunho Yang","We consider the problem of active feature acquisition where the goal is to sequentially select the subset of features in order to achieve the maximum prediction performance in the most cost-effective way at test time. In this work, we formulate this active feature acquisition as a jointly learning problem of training both the classifier (environment) and the RL agent that decides either tostop and predict' orcollect a new feature' at test time, in a cost-sensitive manner. We also introduce a novel encoding scheme to represent acquired subsets of features by proposing an order-invariant set encoding at the feature level, which also significantly reduces the search space for our agent. We evaluate our model on a carefully designed synthetic dataset for the active feature acquisition as well as several medical datasets. Our framework shows meaningful feature acquisition process for diagnosis that complies with human knowledge, and outperforms all baselines in terms of prediction performance as well as feature acquisition cost.",https://proceedings.neurips.cc/paper_files/paper/2018/file/e5841df2166dd424a57127423d276bbe-Paper.pdf,2018,"feature, acquisition, active, cost, agent, also, encoding, features, order, performance","features, feature, acquisition, cost, learning, set, model, classiﬁer, agent, data","{'set': 0.38983251734614166, 'acquisition': 0.3679538813938977, 'encoding': 0.3403900599109585, 'size': 0.3403900599109585, 'variable': 0.3403900599109585, 'joint': 0.3305521061941687, 'active': 0.3086734702419247, 'feature': 0.28883127806780906, 'classification': 0.27762719174695477}","acquisition, feature, active, cost, encoding, agent, test, complies, orcollect, tostop"
Data-Efficient Hierarchical Reinforcement Learning,"Ofir Nachum, Shixiang (Shane) Gu, Honglak Lee, Sergey Levine","Hierarchical reinforcement learning (HRL) is a promising approach to extend traditional reinforcement learning (RL) methods to solve more complex tasks. Yet, the majority of current HRL methods require careful task-specific design and on-policy training, making them difficult to apply in real-world scenarios. In this paper, we study how we can develop HRL algorithms that are general, in that they do not make onerous additional assumptions beyond standard RL algorithms, and efficient, in the sense that they can be used with modest numbers of interaction samples, making them suitable for real-world problems such as robotic control. For generality, we develop a scheme where lower-level controllers are supervised with goals that are learned and proposed automatically by the higher-level controllers. To address efficiency, we propose to use off-policy experience for both higher- and lower-level training. This poses a considerable challenge, since changes to the lower-level behaviors change the action space for the higher-level policy, and we introduce an off-policy correction to remedy this challenge. This allows us to take advantage of recent advances in off-policy model-free RL to learn both higher and lower-level policies using substantially fewer environment interactions than on-policy algorithms. We find that our resulting HRL agent is generally applicable and highly sample-efficient. Our experiments show that our method can be used to learn highly complex behaviors for simulated robots, such as pushing objects and utilizing them to reach target locations, learning from only a few million samples, equivalent to a few days of real-time interaction. In comparisons with a  number of prior HRL methods, we find that our approach substantially outperforms previous state-of-the-art techniques.",https://proceedings.neurips.cc/paper_files/paper/2018/file/e6384711491713d29bc63fc5eeb5ba4f-Paper.pdf,2018,"level, policy, hrl, higher, lower, algorithms, learning, methods, real, rl","policy, level, st, gt, lower, hrl, goal, higher, training, ant","{'hierarchical': 0.6106269431438942, 'reinforcement': 0.4449430333959435, 'efficient': 0.42997604497377984, 'data': 0.42767726018661445, 'learning': 0.247736140292725}","hrl, level, policy, higher, lower, rl, controllers, behaviors, interaction, substantially"
Verifiable Reinforcement Learning via Policy Extraction,"Osbert Bastani, Yewen Pu, Armando Solar-Lezama","While deep reinforcement learning has successfully solved many challenging control tasks, its real-world applicability has been limited by the inability to ensure the safety of learned policies. We propose an approach to verifiable reinforcement learning by training decision tree policies, which can represent complex policies (since they are nonparametric), yet can be efficiently verified using existing techniques (since they are highly structured). The challenge is that decision tree policies are difficult to train. We propose VIPER, an algorithm that combines ideas from model compression and imitation learning to learn decision tree policies guided by a DNN policy (called the oracle) and its Q-function, and show that it substantially outperforms two baselines. We use VIPER to (i) learn a provably robust decision tree policy for a variant of Atari Pong with a symbolic state space, (ii) learn a decision tree policy for a toy game based on Pong that provably never loses, and (iii) learn a provably stable decision tree policy for cart-pole. In each case, the decision tree policy achieves performance equal to that of the original DNN policy.",https://proceedings.neurips.cc/paper_files/paper/2018/file/e6d8545daa42d5ced125a4bf747b3688-Paper.pdf,2018,"decision, tree, policy, policies, learn, learning, provably, dnn, pong, propose","decision, tree, policy, s0, policies, cart, pole, learning, st, use","{'extraction': 0.5618616858283714, 'verifiable': 0.5618616858283714, 'policy': 0.3719177278544433, 'reinforcement': 0.3308493553763116, 'via': 0.2947968044639048, 'learning': 0.18421086783558305}","tree, decision, policy, policies, pong, viper, provably, learn, dnn, since"
Stochastic Spectral and Conjugate Descent Methods,"Dmitry Kovalev, Peter Richtarik, Eduard Gorbunov, Elnur Gasanov","The state-of-the-art methods for solving optimization problems in big dimensions are variants of randomized coordinate descent (RCD). In this paper we introduce a fundamentally new type of acceleration strategy for RCD based on the augmentation of the set of coordinate directions by a few spectral or conjugate directions. As we increase the number of extra directions to be sampled from, the rate of the method improves, and interpolates between the linear rate of RCD and a linear rate independent of the condition number. We develop and analyze also inexact variants of these methods where the spectral and conjugate directions are allowed to be approximate only. We motivate the above development by proving several negative results which highlight the limitations of RCD with importance sampling.",https://proceedings.neurips.cc/paper_files/paper/2018/file/e721a54a8cf18c8543d44782d9ef681f-Paper.pdf,2018,"directions, rcd, rate, conjugate, coordinate, linear, methods, number, spectral, variants","10, rcd, rate, 100, descent, probabilities, iterations, number, sscd, stochastic","{'conjugate': 0.5806583866815307, 'methods': 0.4513032668016558, 'spectral': 0.4513032668016558, 'descent': 0.38093934496497944, 'stochastic': 0.3322224799473406}","rcd, directions, conjugate, spectral, rate, coordinate, variants, inexact, interpolates, motivate"
A Simple Proximal Stochastic Gradient Method for Nonsmooth Nonconvex Optimization,"Zhize Li, Jian Li","We analyze stochastic gradient algorithms for optimizing nonconvex, nonsmooth finite-sum problems. In particular, the objective function is given by the summation of a differentiable (possibly nonconvex) component, together with a possibly non-differentiable but convex component.
We propose a proximal stochastic gradient algorithm based on variance reduction, called ProxSVRG+.
Our main contribution lies in the analysis of ProxSVRG+.
It recovers several existing convergence results and improves/generalizes them (in terms of the number of stochastic gradient oracle calls and proximal oracle calls).
In particular, ProxSVRG+ generalizes the best results given by the SCSG algorithm, recently proposed by [Lei et al., NIPS'17] for the smooth nonconvex case.
ProxSVRG+ is also more straightforward than SCSG and yields simpler analysis.
Moreover, ProxSVRG+ outperforms the deterministic proximal gradient descent (ProxGD) for a wide range of minibatch sizes, which partially solves an open problem proposed in [Reddi et al., NIPS'16].
Also, ProxSVRG+ uses much less proximal oracle calls than ProxSVRG [Reddi et al., NIPS'16].
Moreover, for nonconvex functions satisfied Polyak-\L{}ojasiewicz condition, we prove that ProxSVRG+ achieves a global linear convergence rate without restart unlike ProxSVRG.
Thus, it can \emph{automatically} switch to the faster linear convergence in some regions as long as the objective function satisfies the PL condition locally in these regions.
Finally, we conduct several experiments and the experimental results are consistent with the theoretical results.",https://proceedings.neurips.cc/paper_files/paper/2018/file/e727fa59ddefcefb5d39501167623132-Paper.pdf,2018,"proxsvrg, gradient, nonconvex, proximal, results, al, calls, convergence, et, nips","proxsvrg, convergence, size, proxgd, condition, nonconvex, minibatch, sfo, gradient, pl","{'nonsmooth': 0.461189980710524, 'proximal': 0.4038853999570878, 'simple': 0.3868884634330825, 'nonconvex': 0.3570063619712463, 'method': 0.35234022534990667, 'gradient': 0.2811134264310527, 'stochastic': 0.2795585957567633, 'optimization': 0.2563645029781816}","proxsvrg, proximal, nips, nonconvex, calls, oracle, reddi, al, et, scsg"
Hierarchical Graph Representation Learning with Differentiable Pooling,"Zhitao Ying, Jiaxuan You, Christopher Morris, Xiang Ren, Will Hamilton, Jure Leskovec","Recently, graph neural networks (GNNs) have revolutionized the field of graph representation learning through effectively learned node embeddings, and achieved state-of-the-art results in tasks such as node classification and link prediction. However, current GNN methods are inherently flat and do not learn hierarchical representations of graphs---a limitation that is especially problematic for the task of graph classification, where the goal is to predict the label associated with an entire graph. Here we propose DiffPool, a differentiable graph pooling module that can generate hierarchical representations of graphs and can be combined with various graph neural network architectures in an end-to-end fashion. DiffPool learns a differentiable soft cluster assignment for nodes at each layer of a deep GNN, mapping nodes to a set of clusters, which then form the coarsened input for the next GNN layer. Our experimental results show that combining existing GNN methods with DiffPool yields an average improvement of 5-10% accuracy on graph classification benchmarks, compared to all existing pooling approaches, achieving a new state-of-the-art on four out of five benchmark datasets.",https://proceedings.neurips.cc/paper_files/paper/2018/file/e77dbaf6759253c7c6d0efc5690369c7-Paper.pdf,2018,"graph, gnn, classification, diffpool, art, differentiable, end, existing, graphs, hierarchical","graph, diffpool, gnn, layer, nodes, pooling, node, matrix, cluster, assignment","{'pooling': 0.5266782030631502, 'differentiable': 0.45092306023727563, 'hierarchical': 0.45092306023727563, 'representation': 0.38829020730935543, 'graph': 0.3629179292742817, 'learning': 0.18294302235832094}","graph, gnn, diffpool, pooling, node, classification, hierarchical, nodes, differentiable, graphs"
Robust Detection of Adversarial Attacks by Modeling the Intrinsic Properties of Deep Neural Networks,"Zhihao Zheng, Pengyu Hong",It has been shown that deep neural network (DNN) based classifiers are vulnerable to human-imperceptive adversarial perturbations which can cause DNN classifiers to output wrong predictions with high confidence. We propose an unsupervised learning approach to detect adversarial inputs without any knowledge of attackers. Our approach tries to capture the intrinsic properties of a DNN classifier and uses them to detect adversarial inputs. The intrinsic properties used in this study are the output distributions of the hidden neurons in a DNN classifier presented with natural images. Our approach can be easily applied to any DNN classifiers or combined with other defense strategy to improve robustness. Experimental results show that our approach demonstrates state-of-the-art robustness in defending black-box and gray-box attacks.,https://proceedings.neurips.cc/paper_files/paper/2018/file/e7a425c6ece20cbc9056f98699b53c6f-Paper.pdf,2018,"dnn, approach, adversarial, classifiers, box, classifier, detect, inputs, intrinsic, output","dnn, defense, adversarial, defender, attack, gan, classiﬁer, hidden, input, methods","{'intrinsic': 0.43261301991476087, 'properties': 0.4143620583459581, 'attacks': 0.3629154871883972, 'modeling': 0.3501846516530042, 'detection': 0.31551113829993777, 'robust': 0.2932179544620337, 'adversarial': 0.26671623668401917, 'deep': 0.21475596694416374, 'neural': 0.20526946016686734, 'networks': 0.20468474240298556}","dnn, classifiers, detect, intrinsic, adversarial, approach, box, inputs, classifier, robustness"
Removing the Feature Correlation Effect of Multiplicative Noise,"Zijun Zhang, Yining Zhang, Zongpeng Li","Multiplicative noise, including dropout, is widely used to regularize deep neural networks (DNNs), and is shown to be effective in a wide range of architectures and tasks. From an information perspective, we consider injecting multiplicative noise into a DNN as training the network to solve the task with noisy information pathways, which leads to the observation that multiplicative noise tends to increase the correlation between features, so as to increase the signal-to-noise ratio of information pathways. However, high feature correlation is undesirable, as it increases redundancy in representations. In this work, we propose non-correlating multiplicative noise (NCMN), which exploits batch normalization to remove the correlation effect in a simple yet effective way. We show that NCMN significantly improves the performance of standard multiplicative noise on image classification tasks, providing a better alternative to dropout for batch-normalized networks. Additionally, we present a unified view of NCMN and shake-shake regularization, which explains the performance gain of the latter.",https://proceedings.neurips.cc/paper_files/paper/2018/file/e7b24b112a44fdd9ee93bdf998c6ca0e-Paper.pdf,2018,"noise, multiplicative, correlation, information, ncmn, batch, dropout, effective, increase, networks","ncmn, noise, shake, zs, feature, correlation, multiplicative, 10, eq, batch","{'correlation': 0.44130493710205815, 'removing': 0.44130493710205815, 'effect': 0.4082463090486171, 'multiplicative': 0.4082463090486171, 'noise': 0.39644717397833573, 'feature': 0.34640936118940324}","multiplicative, noise, ncmn, correlation, shake, pathways, dropout, increase, batch, information"
The Effect of Network Width on the Performance of  Large-batch Training,"Lingjiao Chen, Hongyi Wang, Jinman Zhao, Dimitris Papailiopoulos, Paraschos Koutris","Distributed implementations of mini-batch stochastic gradient descent (SGD)  suffer from communication overheads, attributed to the high frequency of gradient updates inherent in small-batch training. Training with large batches can reduce these overheads; however it besets the convergence of the algorithm and the generalization performance.In this work, we take a first step towards analyzing how the structure (width and depth) of a neural network affects the performance of large-batch training. We present new theoretical results which suggest that--for a fixed number of parameters--wider networks are more amenable to fast large-batch training compared to deeper ones. We provide extensive experiments on residual and fully-connected neural networks which suggest that wider networks can be trained using larger batches without incurring a convergence slow-down, unlike their deeper variants.",https://proceedings.neurips.cc/paper_files/paper/2018/file/e7c573c14a09b84f6b7782ce3965f335-Paper.pdf,2018,"batch, training, large, networks, batches, convergence, deeper, gradient, neural, overheads","batch, size, gradient, network, networks, 10, diversity, width, neural, number","{'width': 0.47337725437131917, 'effect': 0.4133388180977848, 'performance': 0.4133388180977848, 'batch': 0.39129203072618585, 'large': 0.32586507894277816, 'training': 0.3053131856364764, 'network': 0.2858362706922014}","batch, overheads, batches, wider, deeper, training, suggest, large, attributed, besets"
Efficient Loss-Based Decoding on Graphs for Extreme Classification,"Itay Evron, Edward Moroshko, Koby Crammer","In extreme classification problems, learning algorithms are required to map instances to labels from an extremely large label set.
  We build on a recent extreme classification framework with logarithmic time and space (LTLS), and on a general approach for error correcting output coding (ECOC) with loss-based decoding, and introduce a flexible and efficient approach accompanied by theoretical bounds.
  Our framework employs output codes induced by graphs, for which we show how to perform efficient loss-based decoding to potentially improve accuracy.
  In addition, our framework offers a tradeoff between accuracy, model size and prediction time.
  We show how to find the sweet spot of this tradeoff using only the training data.
Our experimental study demonstrates the validity of our assumptions and claims,  and shows that our method is competitive with state-of-the-art algorithms.",https://proceedings.neurips.cc/paper_files/paper/2018/file/e7e69cdf28f8ce6b69b4e1853ee21bab-Paper.pdf,2018,"framework, accuracy, algorithms, approach, based, classification, decoding, efficient, extreme, loss","ltls, loss, decoding, accuracy, model, time, based, binary, 20, classiﬁcation","{'extreme': 0.45546393139994557, 'decoding': 0.43990315360202253, 'loss': 0.3732711528479078, 'graphs': 0.3681032791587644, 'classification': 0.3587915499268872, 'based': 0.33658437557804244, 'efficient': 0.2866810541118143}","tradeoff, decoding, extreme, framework, ecoc, ltls, spot, sweet, output, accompanied"
Scalable methods for 8-bit training of neural networks,"Ron Banner, Itay Hubara, Elad Hoffer, Daniel Soudry","Quantized Neural Networks (QNNs) are often used to improve network efficiency during the inference phase, i.e. after the network has been trained. Extensive research in the field suggests many different quantization schemes. Still, the number of bits required, as well as the best quantization scheme, are yet unknown. Our theoretical analysis suggests that most of the training process is robust to substantial precision reduction, and points to only a few specific operations that require higher precision.  Armed with this knowledge, we quantize the model parameters,  activations and layer gradients to 8-bit, leaving at higher precision only the final step in the computation of the weight gradients. Additionally, as QNNs require batch-normalization to be trained at high precision, we introduce Range Batch-Normalization (BN) which has significantly higher tolerance to quantization noise and improved computational complexity. Our simulations show that Range BN is equivalent to the traditional batch norm if a precise scale adjustment, which can be approximated analytically, is applied. To the best of the authors' knowledge, this work is the first to quantize the weights, activations, as well as a substantial volume of the gradients stream, in all layers (including batch normalization) to 8-bit while showing state-of-the-art results over the ImageNet-1K dataset.",https://proceedings.neurips.cc/paper_files/paper/2018/file/e82c4b19b8151ddc25d4d93baf7b908f-Paper.pdf,2018,"batch, precision, gradients, higher, normalization, quantization, activations, best, bit, bn","quantization, gradients, precision, batch, range, bit, layer, gl, bn, angle","{'bit': 0.5528297519419608, 'methods': 0.45522264269558854, 'scalable': 0.45522264269558854, 'training': 0.377758509231903, 'neural': 0.262310793992443, 'networks': 0.2615635918476104}","precision, batch, quantization, normalization, qnns, quantize, bn, higher, gradients, bit"
Solving Large Sequential Games with the Excessive Gap Technique,"Christian Kroer, Gabriele Farina, Tuomas Sandholm","There has been tremendous recent progress on equilibrium-finding algorithms for zero-sum imperfect-information extensive-form games, but there has been a puzzling gap between theory and practice. First-order methods have significantly better theoretical convergence rates than any counterfactual-regret minimization (CFR) variant. Despite this, CFR variants have been favored in practice. Experiments with first-order methods have only been conducted on small- and medium-sized games because those methods are complicated to implement in this setting, and because CFR variants have been enhanced extensively for over a decade they perform well in practice. In this paper we show that a particular first-order method, a state-of-the-art variant of the excessive gap technique---instantiated with the dilated entropy distance function---can efficiently solve large real-world problems competitively with CFR and its variants. We show this on large endgames encountered by the Libratus poker AI, which recently beat top human poker specialist professionals at no-limit Texas hold'em. We show experimental results on our variant of the excessive gap technique as well as a prior version. We introduce a numerically friendly implementation of the smoothed best response computation associated with first-order methods for extensive-form game solving. We present, to our knowledge, the first GPU implementation of a first-order method for extensive-form games. We present comparisons of several excessive gap technique and CFR variants.",https://proceedings.neurips.cc/paper_files/paper/2018/file/e836d813fd184325132fca8edcdfb40e-Paper.pdf,2018,"first, cfr, order, gap, methods, variants, excessive, extensive, form, games","cfr, egt, rm, libratus, gradient, al, et, game, simplex, dgf","{'excessive': 0.4311028421586058, 'gap': 0.4311028421586058, 'technique': 0.4311028421586058, 'solving': 0.3655466030179532, 'sequential': 0.33506468769969766, 'games': 0.32937735195419465, 'large': 0.2967640721965824}","cfr, excessive, variants, gap, order, first, games, variant, poker, practice"
Step Size Matters in Deep Learning,"Kamil Nar, Shankar Sastry","Training a neural network with the gradient descent algorithm gives rise to a discrete-time nonlinear dynamical system. Consequently, behaviors that are typically observed in these systems emerge during training, such as convergence to an orbit but not to a fixed point or dependence of convergence on the initialization. Step size of the algorithm plays a critical role in these behaviors: it determines the subset of the local optima that the algorithm can converge to, and it specifies the magnitude of the oscillations if the algorithm converges to an orbit. To elucidate the effects of the step size on training of neural networks, we study the gradient descent algorithm as a discrete-time dynamical system, and by analyzing the Lyapunov stability of different solutions, we show the relationship between the step size of the algorithm and the solutions that can be obtained with this algorithm. The results provide an explanation for several phenomena observed in practice, including the deterioration in the training error with increased depth, the hardness of estimating linear mappings with large singular values, and the distinct performance of deep residual networks.",https://proceedings.neurips.cc/paper_files/paper/2018/file/e8fd4a8a5bab2b3785d794ab51fef55c-Paper.pdf,2018,"algorithm, training, size, step, behaviors, convergence, descent, discrete, dynamical, gradient","algorithm, step, size, gradient, training, descent, function, networks, local, convergence","{'matters': 0.5699267303961983, 'size': 0.5272329053019088, 'step': 0.5272329053019088, 'deep': 0.28292067145292393, 'learning': 0.19796550906241872}","algorithm, orbit, step, size, behaviors, training, dynamical, observed, solutions, discrete"
A Reduction for Efficient LDA Topic Reconstruction,"Matteo Almanza, Flavio Chierichetti, Alessandro Panconesi, Andrea Vattani","We present a novel approach for LDA (Latent Dirichlet Allocation) topic reconstruction. The main technical idea is to show that the distribution over the documents generated by LDA can be transformed into a distribution for a much simpler generative model in which documents are generated from {\em the same set of topics} but have a much simpler structure: documents are single topic and topics are chosen uniformly at random. Furthermore, this reduction is approximation preserving, in the sense that approximate distributions-- the only ones we can hope to compute in practice-- are mapped into approximate distribution in the simplified world. This opens up the possibility of efficiently reconstructing LDA topics in a roundabout way. Compute an approximate document distribution from the given corpus, transform it into an approximate distribution for the single-topic world, and run a reconstruction algorithm in the uniform, single topic world-- a much simpler task than direct LDA reconstruction. Indeed, we show the viability of the approach by giving very simple algorithms for a generalization of two notable cases that have been studied in the literature, $p$-separability and Gibbs sampling for matrix-like topics.",https://proceedings.neurips.cc/paper_files/paper/2018/file/e9257036daf20f062a498aab563d7712-Paper.pdf,2018,"distribution, approximate, lda, topic, topics, documents, much, reconstruction, simpler, single","topics, topic, lda, documents, algorithm, sta, words, set, distribution, reconstruction","{'lda': 0.5219973387130806, 'reconstruction': 0.5219973387130806, 'topic': 0.45579241723475233, 'reduction': 0.39882386367843886, 'efficient': 0.29703594884260437}","lda, topics, topic, documents, simpler, distribution, reconstruction, approximate, much, single"
Convex Elicitation of Continuous Properties,"Jessica Finocchiaro, Rafael Frongillo","A property or statistic of a distribution is said to be elicitable if it can be expressed as the minimizer of some loss function in expectation. Recent work shows that continuous real-valued properties are elicitable if and only if they are identifiable, meaning the set of distributions with the same property value can be described by linear constraints. From a practical standpoint, one may ask for which such properties do there exist convex loss functions. In this paper, in a finite-outcome setting, we show that in fact every elicitable real-valued property can be elicited by a convex loss function. Our proof is constructive, and leads to convex loss functions for new properties.",https://proceedings.neurips.cc/paper_files/paper/2018/file/e9510081ac30ffa83f10b68cde1cac07-Paper.pdf,2018,"loss, convex, elicitable, properties, property, function, functions, real, valued, ask","function, convex, loss, property, elicitable, properties, one, continuous, increasing, functions","{'elicitation': 0.5945271616968428, 'properties': 0.5374863293842986, 'continuous': 0.4470787491589926, 'convex': 0.3971983028414396}","elicitable, property, loss, valued, properties, convex, constructive, said, identifiable, standpoint"
The Nearest Neighbor Information Estimator is Adaptively Near Minimax Rate-Optimal,"Jiantao Jiao, Weihao Gao, Yanjun Han","We analyze the Kozachenko–Leonenko (KL) fixed k-nearest neighbor estimator for the differential entropy. We obtain the first uniform upper bound on its performance for any fixed k over H\""{o}lder balls on a torus without assuming any conditions on how close the density could be from zero. Accompanying a recent minimax lower bound over the H\""{o}lder ball, we show that the KL estimator for any fixed k is achieving the minimax rates up to logarithmic factors without cognizance of the smoothness parameter s of the H\""{o}lder ball for $s \in (0,2]$ and arbitrary dimension d, rendering it the first estimator that provably satisfies this property.",https://proceedings.neurips.cc/paper_files/paper/2018/file/e9fd7c2c6623306db59b6aef5c0d5cac-Paper.pdf,2018,"estimator, fixed, lder, ball, bound, first, kl, minimax, without, accompanying","ln, frk, estimator, density, rk, dx, entropy, vdrk, bound, kl","{'adaptively': 0.3674680045446534, 'estimator': 0.3674680045446534, 'neighbor': 0.3674680045446534, 'rate': 0.3519653633388234, 'nearest': 0.33994058061970306, 'minimax': 0.33011561775366016, 'near': 0.33011561775366016, 'information': 0.27091348537750365, 'optimal': 0.2532110243185963}","lder, estimator, ball, fixed, kl, minimax, cognizance, kozachenko, leonenko, torus"
Bandit Learning with Positive Externalities,"Virag Shah, Jose Blanchet, Ramesh Johari","In many platforms, user arrivals exhibit a self-reinforcing behavior: future user arrivals are likely to have preferences similar to users who were satisfied in the past. In other words, arrivals exhibit {\em positive externalities}. We study multiarmed bandit (MAB) problems with positive externalities. We show that the self-reinforcing preferences may lead standard benchmark algorithms such as UCB to exhibit linear regret. We develop a new algorithm, Balanced Exploration (BE), which explores arms carefully to avoid suboptimal convergence of arrivals before sufficient evidence is gathered. We also introduce an adaptive variant of BE which successively eliminates suboptimal arms. We analyze their asymptotic regret, and establish optimality by showing that no algorithm can perform better.",https://proceedings.neurips.cc/paper_files/paper/2018/file/ea159dc9788ffac311592613b7f71fbb-Paper.pdf,2018,"arrivals, exhibit, algorithm, arms, externalities, positive, preferences, regret, reinforcing, self","arm, algorithm, ln, regret, time, positive, µa, exploration, arms, lower","{'externalities': 0.6228315005025141, 'positive': 0.5630750596399354, 'bandit': 0.5033186187773567, 'learning': 0.2042003114231803}","arrivals, exhibit, externalities, reinforcing, suboptimal, arms, preferences, positive, self, user"
Minimax Statistical Learning with Wasserstein distances,"Jaeho Lee, Maxim Raginsky","As opposed to standard empirical risk minimization (ERM), distributionally robust optimization aims to minimize the worst-case risk over a larger ambiguity set containing the original empirical distribution of the training data. In this work, we describe a minimax framework for statistical learning with ambiguity sets given by balls in Wasserstein space. In particular, we prove generalization bounds that involve the covering number properties of the original ERM problem. As an illustrative example, we provide generalization guarantees for transport-based domain adaptation problems where the Wasserstein distance between the source and target domain distributions can be reliably estimated from unlabeled samples.",https://proceedings.neurips.cc/paper_files/paper/2018/file/ea8fcd92d59581717e06eb187f10666d-Paper.pdf,2018,"ambiguity, domain, empirical, erm, generalization, original, risk, wasserstein, adaptation, aims","risk, z0, pn, domain, hypothesis, space, minimax, erm, metric, wp","{'distances': 0.5515977920317512, 'minimax': 0.4955289810707573, 'statistical': 0.4627308291985526, 'wasserstein': 0.4464985374070901, 'learning': 0.19159890539150645}","erm, ambiguity, wasserstein, risk, original, generalization, domain, empirical, balls, illustrative"
Dirichlet belief networks for topic structure learning,"He Zhao, Lan Du, Wray Buntine, Mingyuan Zhou","Recently, considerable research effort has been devoted to developing deep architectures for topic models to learn topic structures. Although several deep models have been proposed to learn better topic proportions of documents, how to leverage the benefits of deep structures for learning word distributions of topics has not yet been rigorously studied. Here we propose a new multi-layer generative process on word distributions of topics, where each layer consists of a set of topics and each topic is drawn from a mixture of the topics of the layer above. As the topics in all layers can be directly interpreted by words, the proposed model is able to discover interpretable topic hierarchies. As a self-contained module, our model can be flexibly adapted to different kinds of topic models to improve their modelling accuracy and interpretability. Extensive experiments on text corpora demonstrate the advantages of the proposed model.",https://proceedings.neurips.cc/paper_files/paper/2018/file/eaae5e04a259d09af85c108fe4d7dd0c-Paper.pdf,2018,"topic, topics, deep, layer, model, models, proposed, distributions, learn, structures","dirbn, topic, layer, al, et, topics, gbn, model, pfa, models","{'belief': 0.5370260725187249, 'dirichlet': 0.5068864679072708, 'topic': 0.46891505676034845, 'structure': 0.3824540893182771, 'networks': 0.2398261756698897, 'learning': 0.17606831247651125}","topic, topics, layer, word, structures, proposed, deep, devoted, hierarchies, proportions"
Efficient Stochastic Gradient Hard Thresholding,"Pan Zhou, Xiaotong Yuan, Jiashi Feng","Stochastic gradient hard thresholding methods have recently been shown to work favorably in solving large-scale empirical risk minimization problems under sparsity or rank constraint. Despite the improved iteration complexity over full gradient methods, the gradient evaluation and hard thresholding complexity of the existing stochastic algorithms usually scales linearly with data size, which could still be expensive when data is huge and the hard thresholding step could be as expensive as singular value decomposition in rank-constrained problems. To address these deficiencies, we propose an efficient hybrid stochastic gradient hard thresholding (HSG-HT) method that can be provably shown to have sample-size-independent gradient evaluation and hard thresholding complexity bounds. Specifically, we prove that the stochastic gradient evaluation complexity of HSG-HT scales linearly with inverse of sub-optimality and its hard thresholding complexity scales logarithmically. By applying the heavy ball acceleration technique, we further propose an accelerated variant of HSG-HT which can be shown to have improved factor dependence on restricted condition number. Numerical results confirm our theoretical affirmation and demonstrate the computational efficiency of the proposed methods.",https://proceedings.neurips.cc/paper_files/paper/2018/file/ec5aa0b7846082a2415f0902f0da88f2-Paper.pdf,2018,"gradient, hard, thresholding, complexity, stochastic, evaluation, hsg, ht, methods, scales","ht, hard, thresholding, hsg, complexity, gradient, xt, ahsg, log, fg","{'thresholding': 0.5814127977753353, 'hard': 0.5378585390050096, 'gradient': 0.3543939603841433, 'stochastic': 0.3524338170804817, 'efficient': 0.35051772569126594}","thresholding, hard, hsg, ht, gradient, complexity, scales, evaluation, stochastic, shown"
Learning long-range spatial dependencies with horizontal gated recurrent units,"Drew Linsley, Junkyung Kim, Vijay Veerabadran, Charles Windolf, Thomas Serre","Progress in deep learning has spawned great successes in many engineering applications. As a prime example, convolutional neural networks, a type of feedforward neural networks, are now approaching -- and sometimes even surpassing -- human accuracy on a variety of visual recognition tasks. Here, however, we show that these neural networks and their recent extensions struggle in recognition tasks where co-dependent visual features must be detected over long spatial ranges. We introduce a visual challenge, Pathfinder, and describe a novel recurrent neural network architecture called the horizontal gated recurrent unit (hGRU) to learn intrinsic horizontal connections -- both within and across feature columns. We demonstrate that a single hGRU layer matches or outperforms all tested feedforward hierarchical baselines including state-of-the-art architectures with orders of magnitude more parameters.",https://proceedings.neurips.cc/paper_files/paper/2018/file/ec8956637a99787bd197eacd77acce5e-Paper.pdf,2018,"neural, networks, visual, feedforward, hgru, horizontal, recognition, recurrent, tasks, accuracy","hgru, model, pathﬁnder, challenge, horizontal, xyk, models, fig, feedforward, layer","{'horizontal': 0.39105282959868204, 'range': 0.39105282959868204, 'dependencies': 0.3691057058565966, 'spatial': 0.3691057058565966, 'units': 0.3535339737488993, 'long': 0.33158685000681376, 'gated': 0.32324294080860744, 'recurrent': 0.25654913830724824, 'learning': 0.1282098119997776}","hgru, horizontal, feedforward, visual, neural, recognition, recurrent, columns, pathfinder, prime"
Tight Bounds for Collaborative PAC Learning via Multiplicative Weights,"Jiecao Chen, Qin Zhang, Yuan Zhou","We study the collaborative PAC learning problem recently proposed in Blum  et al.~\cite{BHPQ17}, in which we have $k$ players and they want to learn a target function collaboratively, such that the learned function approximates the target function well on all players' distributions simultaneously. The quality of the collaborative learning algorithm is measured by the ratio between the sample complexity of the algorithm and that of the learning algorithm for a single distribution (called the overhead).  We obtain a collaborative learning algorithm with overhead $O(\ln k)$, improving the one with overhead $O(\ln^2 k)$ in \cite{BHPQ17}.  We also show that an $\Omega(\ln k)$ overhead is inevitable when $k$ is polynomial bounded by the VC dimension of the hypothesis class.  Finally, our experimental study has demonstrated the superiority of our algorithm compared with the one in Blum  et al.~\cite{BHPQ17} on real-world datasets.",https://proceedings.neurips.cc/paper_files/paper/2018/file/ed519dacc89b2bead3f453b0b05a4a8b-Paper.pdf,2018,"algorithm, learning, overhead, bhpq17, cite, collaborative, function, ln, al, blum","algorithm, ln, let, learning, set, sample, player, errdi, samples, complexity","{'collaborative': 0.4211803098097698, 'weights': 0.4211803098097698, 'multiplicative': 0.40679082084701984, 'tight': 0.40679082084701984, 'pac': 0.3688871947854675, 'bounds': 0.308382553136177, 'via': 0.24443629439612152, 'learning': 0.15274189285432657}","overhead, bhpq17, ln, collaborative, cite, blum, players, algorithm, al, et"
(Probably) Concave Graph Matching,"Haggai Maron, Yaron Lipman","In this paper we address the graph matching problem. Following the recent works of \cite{zaslavskiy2009path,Vestner2017} we analyze and generalize the idea of concave relaxations. We introduce the concepts of \emph{conditionally concave} and \emph{probably conditionally concave} energies on polytopes and show that they encapsulate many instances of the graph matching problem, including matching Euclidean graphs and graphs on surfaces. We further prove that local minima of probably conditionally concave energies on general matching polytopes (\eg, doubly stochastic) are with high probability extreme points of the matching polytope (\eg, permutations).",https://proceedings.neurips.cc/paper_files/paper/2018/file/eda80a3d5b344bc40f3bc04f65b7a357-Paper.pdf,2018,"matching, concave, conditionally, eg, emph, energies, graph, graphs, polytopes, probably","concave, conditionally, et, al, probability, energy, pr, ds, deﬁnite, positive","{'probably': 0.5878913709356299, 'concave': 0.531487197555524, 'matching': 0.4750830241754182, 'graph': 0.38236267463540435}","matching, concave, conditionally, energies, eg, polytopes, probably, emph, graphs, graph"
HOUDINI: Lifelong Learning as Program Synthesis,"Lazar Valkov, Dipak Chaudhari, Akash Srivastava, Charles Sutton, Swarat Chaudhuri","We present a neurosymbolic framework for the lifelong learning of algorithmic tasks that mix perception and procedural reasoning. Reusing high-level concepts across domains and learning complex procedures are key challenges in lifelong learning. We show that a program synthesis approach that combines gradient descent with combinatorial search over programs can be a more effective response to these challenges than purely neural methods. Our framework, called HOUDINI, represents neural networks as strongly typed, differentiable functional programs that use symbolic higher-order combinators to compose a library of neural functions. Our learning algorithm consists of: (1) a symbolic program synthesizer that performs a type-directed search over parameterized programs, and decides on the library functions to reuse, and the architectures to combine them, while learning a sequence of tasks; and (2) a neural module that trains these programs using stochastic gradient descent. We evaluate HOUDINI on three benchmarks that combine perception with the algorithmic tasks of counting, summing, and shortest-path computation. Our experiments show that HOUDINI transfers high-level concepts more effectively than traditional transfer learning and progressive neural networks, and that the typed representation of networks signiﬁcantly accelerates the search.",https://proceedings.neurips.cc/paper_files/paper/2018/file/edc27f139c3b4e4bb29d1cdbc45663f9-Paper.pdf,2018,"learning, neural, programs, houdini, networks, search, tasks, algorithmic, challenges, combine","task, programs, transfer, houdini, tasks, neural, level, program, learning, nn","{'houdini': 0.5646627041197044, 'lifelong': 0.5329720441836191, 'program': 0.43887099343670777, 'synthesis': 0.41257273246315845, 'learning': 0.18512920418647535}","houdini, programs, typed, lifelong, library, search, perception, neural, symbolic, concepts"
Video Prediction via Selective Sampling,"Jingwei Xu, Bingbing Ni, Xiaokang Yang","Most adversarial learning based video prediction methods suffer from image blur, since the commonly used adversarial and regression loss pair work rather in a competitive way than collaboration, yielding compromised blur effect. 
  In the meantime, as often relying on a single-pass architecture, the predictor is inadequate to explicitly capture the forthcoming uncertainty.
  Our work involves two key insights:
  (1) Video prediction can be approached as a stochastic process: we sample a collection of proposals conforming to possible frame distribution at following time stamp, and one can select the final prediction from it.
  (2) De-coupling combined loss functions into dedicatedly designed sub-networks encourages them to work in a collaborative way.
  Combining above two insights we propose a two-stage network called VPSS (\textbf{V}ideo \textbf{P}rediction via \textbf{S}elective \textbf{S}ampling).Specifically a \emph{Sampling} module produces a collection of high quality proposals, facilitated by a multiple choice adversarial learning scheme, yielding diverse frame proposal set. 
  Subsequently a \emph{Selection} module selects high possibility candidates from proposals and combines them to produce final prediction.Extensive experiments on diverse challenging datasets 
  demonstrate the effectiveness of proposed video prediction approach, i.e., yielding more diverse proposals and accurate prediction results.",https://proceedings.neurips.cc/paper_files/paper/2018/file/ede7e2b6d13a41ddf9f4bdef84fdc737-Paper.pdf,2018,"prediction, proposals, textbf, adversarial, diverse, two, video, work, yielding, blur","prediction, proposals, motion, loss, quality, 10, video, image, adversarial, sampling","{'selective': 0.5943705848654701, 'video': 0.4803189652016813, 'prediction': 0.4049349450511961, 'sampling': 0.39343661082846504, 'via': 0.3118535281994753}","textbf, proposals, prediction, yielding, video, blur, diverse, frame, final, adversarial"
Manifold-tiling Localized Receptive Fields are Optimal in Similarity-preserving Neural Networks,"Anirvan Sengupta, Cengiz Pehlevan, Mariano Tepper, Alexander Genkin, Dmitri Chklovskii","Many neurons in the brain, such as place cells in the rodent hippocampus, have localized receptive fields, i.e., they respond to a small neighborhood of stimulus space. What is the functional significance of such representations and how can they arise? Here, we propose that localized receptive fields emerge in similarity-preserving networks of rectifying neurons that learn low-dimensional manifolds populated by sensory inputs.  Numerical simulations of such networks on standard datasets yield manifold-tiling localized receptive fields. More generally, we show analytically that, for data lying on symmetric manifolds, optimal solutions of objectives, from which similarity-preserving networks are derived, have localized receptive fields. Therefore, nonnegative similarity-preserving mapping (NSM) implemented by neural networks can model representations of continuous manifolds in the brain.",https://proceedings.neurips.cc/paper_files/paper/2018/file/ee14c41e92ec5c97b54cf9b74e25bd99-Paper.pdf,2018,"fields, localized, networks, receptive, manifolds, preserving, similarity, brain, neurons, representations","receptive, manifold, nsm, yt, ﬁelds, input, localized, optimization, similarity, solution","{'fields': 0.38313374116102383, 'localized': 0.38313374116102383, 'receptive': 0.38313374116102383, 'tiling': 0.38313374116102383, 'similarity': 0.31669704920173924, 'manifold': 0.3096155946714025, 'preserving': 0.3096155946714025, 'optimal': 0.2491889645875379, 'neural': 0.17158940995335117, 'networks': 0.17110063107697981}","localized, receptive, fields, manifolds, similarity, preserving, neurons, brain, networks, hippocampus"
Snap ML: A Hierarchical Framework for Machine Learning,"Celestine Dünner, Thomas Parnell, Dimitrios Sarigiannis, Nikolas Ioannou, Andreea Anghel, Gummadi Ravi, Madhusudanan Kandasamy, Haralampos Pozidis","We describe a new software framework for fast training of generalized linear models. The framework, named Snap Machine Learning (Snap ML), combines recent advances in machine learning systems and algorithms in a nested manner to reflect the hierarchical architecture of modern computing systems. We prove theoretically that such a hierarchical system can accelerate training in distributed environments where intra-node communication is cheaper than inter-node communication. Additionally, we provide a review of the implementation of Snap ML in terms of GPU acceleration, pipelining, communication patterns and software architecture, highlighting aspects that were critical for achieving high performance. We evaluate the performance of Snap ML in both single-node and multi-node environments, quantifying the benefit of the hierarchical scheme and the data streaming functionality, and comparing with other widely-used machine learning software frameworks. Finally, we present a logistic regression benchmark on the Criteo Terabyte Click Logs dataset and show that Snap ML achieves the same test loss an order of magnitude faster than any of the previously reported results, including those obtained using TensorFlow and scikit-learn.",https://proceedings.neurips.cc/paper_files/paper/2018/file/eecca5b6365d9607ee5a9d336962c534-Paper.pdf,2018,"snap, ml, node, communication, hierarchical, learning, machine, software, architecture, environments","training, data, ml, snap, using, gpu, node, tensorflow, performance, machine","{'snap': 0.5204004446143597, 'ml': 0.45439805721154164, 'hierarchical': 0.4205426873610169, 'machine': 0.40446917858487025, 'framework': 0.3913361656613561, 'learning': 0.17061746679362888}","snap, ml, node, software, hierarchical, communication, machine, environments, systems, architecture"
Embedding Logical Queries on Knowledge Graphs,"Will Hamilton, Payal Bajaj, Marinka Zitnik, Dan Jurafsky, Jure Leskovec","Learning low-dimensional embeddings of knowledge graphs is a powerful approach used to predict unobserved or missing edges between entities. However, an open challenge in this area is developing techniques that can go beyond simple edge prediction and handle more complex logical queries, which might involve multiple unobserved edges, entities, and variables. For instance, given an incomplete biological knowledge graph, we might want to predict ""em what drugs are likely to target proteins involved with both diseases X and Y?"" -- a query that requires reasoning about all possible proteins that might interact with diseases X and Y. Here we introduce a framework to efficiently make predictions about conjunctive logical queries -- a flexible but tractable subset of first-order logic -- on incomplete knowledge graphs. In our approach, we embed graph nodes in a low-dimensional space and represent logical operators as learned geometric operations (e.g., translation, rotation) in this embedding space. By performing logical operations within a low-dimensional embedding space, our approach achieves a time complexity that is linear in the number of query variables, compared to the exponential complexity required by a naive enumeration-based approach. We demonstrate the utility of this framework in two application studies on real-world datasets with millions of relations: predicting logical relationships in a network of drug-gene-disease interactions and in a graph-based representation of social interactions derived from a popular web forum.",https://proceedings.neurips.cc/paper_files/paper/2018/file/ef50c335cca9f340bde656363ebd02fd-Paper.pdf,2018,"logical, approach, dimensional, graph, knowledge, low, might, space, based, complexity","query, queries, nodes, edges, embedding, graph, node, edge, using, dag","{'logical': 0.5326915024873274, 'queries': 0.5027951676789468, 'embedding': 0.40057892382904636, 'graphs': 0.3892128648441983, 'knowledge': 0.3892128648441983}","logical, might, diseases, proteins, incomplete, unobserved, graph, edges, entities, dimensional"
Parsimonious Bayesian deep networks,Mingyuan Zhou,"Combining Bayesian nonparametrics and a forward model selection strategy, we construct parsimonious Bayesian deep networks (PBDNs) that infer capacity-regularized network architectures from the data and require neither cross-validation nor fine-tuning when training the model. One of the two essential components of a PBDN is the development of a special infinite-wide single-hidden-layer neural network, whose number of active hidden units can be inferred from the data. The other one is the construction of a greedy layer-wise learning algorithm that uses a forward model selection criterion to determine when to stop adding another hidden layer. We develop both Gibbs sampling and stochastic gradient descent based maximum a posteriori inference for PBDNs, providing state-of-the-art classification accuracy and interpretable data subtypes near the decision boundaries, while maintaining low computational complexity for out-of-sample prediction.",https://proceedings.neurips.cc/paper_files/paper/2018/file/efb76cff97aaf057654ef2f38cd77d73-Paper.pdf,2018,"data, hidden, layer, model, bayesian, forward, network, one, pbdns, selection","pbdn, hidden, data, ishm, layer, hyperplanes, support, network, yi, ln","{'parsimonious': 0.7376657739836694, 'bayesian': 0.4471493669376559, 'deep': 0.36618899404528205, 'networks': 0.349016146016817}","pbdns, hidden, layer, forward, selection, nonparametrics, pbdn, subtypes, bayesian, boundaries"
Sample-Efficient Reinforcement Learning with Stochastic Ensemble Value Expansion,"Jacob Buckman, Danijar Hafner, George Tucker, Eugene Brevdo, Honglak Lee","There is growing interest in combining model-free and model-based approaches in reinforcement learning with the goal of achieving the high performance of model-free algorithms with low sample complexity. This is difficult because an imperfect dynamics model can degrade the performance of the learning algorithm, and in sufficiently complex environments, the dynamics model will always be imperfect. As a result, a key challenge is to combine model-based approaches with model-free learning in such a way that errors in the model do not degrade performance. We propose stochastic ensemble value expansion (STEVE), a novel model-based technique that addresses this issue. By dynamically interpolating between model rollouts of various horizon lengths, STEVE ensures that the model is only utilized when doing so does not introduce significant errors. Our approach outperforms model-free baselines on challenging continuous control benchmarks with an order-of-magnitude increase in sample efficiency.",https://proceedings.neurips.cc/paper_files/paper/2018/file/f02208a057804ee16ac72ff4d3cec53b-Paper.pdf,2018,"model, free, based, learning, performance, approaches, degrade, dynamics, errors, imperfect","model, steve, mve, function, learning, policy, environment, use, targets, value","{'expansion': 0.490319774075337, 'ensemble': 0.4432769309313307, 'value': 0.39623408778732444, 'sample': 0.3746211182163731, 'reinforcement': 0.28872226968442377, 'stochastic': 0.2805354319283606, 'efficient': 0.2790102334387811, 'learning': 0.16075527728180963}","model, free, steve, degrade, imperfect, errors, dynamics, performance, rollouts, interpolating"
Learning Versatile Filters for Efficient Convolutional Neural Networks,"Yunhe Wang, Chang Xu, Chunjing XU, Chao Xu, Dacheng Tao","This paper introduces versatile filters to construct efficient convolutional neural network. Considering the demands of efficient deep learning techniques running on cost-effective hardware, a number of methods have been developed to learn compact neural networks. Most of these works aim to slim down filters in different ways, e.g., investigating small, sparse or binarized filters. In contrast, we treat filters from an additive perspective. A series of secondary filters can be derived from a primary filter. These secondary filters all inherit in the primary filter without occupying more storage, but once been unfolded in computation they could significantly enhance the capability of the filter by integrating information extracted from different receptive fields. Besides spatial versatile filters, we additionally investigate versatile filters from the channel perspective. The new techniques are general to upgrade filters in existing CNNs. Experimental results on benchmark datasets and neural networks demonstrate that CNNs constructed with our versatile filters are able to achieve comparable accuracy as that of original filters, but require less memory and FLOPs.",https://proceedings.neurips.cc/paper_files/paper/2018/file/f0adc8838f4bdedde4ec2cfad0515589-Paper.pdf,2018,"filters, versatile, filter, neural, cnns, different, efficient, networks, perspective, primary","ﬁlters, versatile, convolution, proposed, model, ﬁlter, 50, feature, memory, network","{'filters': 0.5396261737852595, 'versatile': 0.5396261737852595, 'convolutional': 0.3819560238595098, 'efficient': 0.32532572362085876, 'neural': 0.25604586151068104, 'networks': 0.2553165052612321, 'learning': 0.1874405331410592}","filters, versatile, filter, secondary, primary, cnns, perspective, inherit, occupying, slim"
Neural Nearest Neighbors Networks,"Tobias Plötz, Stefan Roth","Non-local methods exploiting the self-similarity of natural signals have been well studied, for example in image analysis and restoration. Existing approaches, however, rely on k-nearest neighbors (KNN) matching in a fixed feature space. The main hurdle in optimizing this feature space w.r.t. application performance is the non-differentiability of the KNN selection rule. To overcome this, we propose a continuous deterministic relaxation of KNN selection that maintains differentiability w.r.t. pairwise distances, but retains the original KNN as the limit of a temperature parameter approaching zero. To exploit our relaxation, we propose the neural nearest neighbors block (N3 block), a novel non-local processing layer that leverages the principle of self-similarity and can be used as building block in modern neural network architectures. We show its effectiveness for the set reasoning task of correspondence classification as well as for image restoration, including image denoising and single image super-resolution, where we outperform strong convolutional neural network (CNN) baselines and recent non-local models that rely on KNN selection in hand-chosen features spaces.",https://proceedings.neurips.cc/paper_files/paper/2018/file/f0e52b27a7a5d6a1a87373dffa53dbe5-Paper.pdf,2018,"knn, image, non, block, local, neural, selection, differentiability, feature, nearest","image, neighbors, local, block, nearest, non, network, knn, n3, n3net","{'neighbors': 0.6586955793870383, 'nearest': 0.6093519842249336, 'neural': 0.3125428033158136, 'networks': 0.3116525134065226}","knn, block, restoration, selection, neighbors, image, differentiability, nearest, non, relaxation"
Learning latent variable structured prediction models with Gaussian perturbations,"Kevin Bello, Jean Honorio","The standard margin-based structured prediction commonly uses a maximum loss over all possible structured outputs. The large-margin formulation including latent variables not only results in a non-convex formulation but also increases the search space by a factor of the size of the latent space. Recent work has proposed the use of the maximum loss over random structured outputs sampled independently from some proposal distribution, with theoretical guarantees. We extend this work by including latent variables. We study a new family of loss functions under Gaussian perturbations and analyze the effect of the latent space on the generalization bounds. We show that the non-convexity of learning with latent variables originates naturally, as it relates to a tight upper bound of the Gibbs decoder distortion with respect to the latent space. Finally, we provide a formulation using random samples and relaxations that produces a tighter upper bound of the Gibbs decoder distortion up to a statistical accuracy, which enables a polynomial time evaluation of the objective function. We illustrate the method with synthetic experiments and a computer vision application.",https://proceedings.neurips.cc/paper_files/paper/2018/file/f18a6d1cde4b205199de8729a6637b42-Paper.pdf,2018,"latent, space, formulation, loss, structured, variables, bound, decoder, distortion, gibbs","latent, hx, variables, structured, distortion, yx, let, formulation, margin, distribution","{'perturbations': 0.4952788442929779, 'variable': 0.43246262941124464, 'latent': 0.3669576426588059, 'structured': 0.34856683323471127, 'prediction': 0.33742536509304333, 'gaussian': 0.3221277823704108, 'models': 0.25986221217573036, 'learning': 0.16238114829506872}","latent, formulation, space, structured, variables, distortion, gibbs, margin, loss, decoder"
Differentially Private Change-Point Detection,"Rachel Cummings, Sara Krehbiel, Yajun Mei, Rui Tuo, Wanrong Zhang","The change-point detection problem seeks to identify distributional changes at an unknown change-point k* in a stream of data. This problem appears in many important practical settings involving personal data, including biosurveillance, fault detection, finance, signal detection, and security systems. The field of differential privacy offers data analysis tools that provide powerful worst-case privacy guarantees. We study the statistical problem of change-point problem through the lens of differential privacy. We give private algorithms for both online and offline change-point detection, analyze these algorithms theoretically, and then provide empirical validation of these results.",https://proceedings.neurips.cc/paper_files/paper/2018/file/f19ec2b84181033bf4753a5a51d5d608-Paper.pdf,2018,"change, detection, point, problem, data, privacy, algorithms, differential, provide, analysis","change, p1, p0, point, privacy, log, data, algorithm, private, accuracy","{'change': 0.5576875634912453, 'differentially': 0.4415828538509428, 'private': 0.41937572304615567, 'point': 0.4131969616651402, 'detection': 0.3839029024405337}","change, detection, privacy, point, differential, problem, biosurveillance, fault, personal, finance"
Proximal Graphical Event Models,"Debarun Bhattacharjya, Dharmashankar Subramanian, Tian Gao","Event datasets include events that occur irregularly over the timeline and are prevalent in numerous domains. We introduce proximal graphical event models (PGEM) as a representation of such datasets. PGEMs belong to a broader family of models that characterize relationships between various types of events, where the rate of occurrence of an event type depends only on whether or not its parents have occurred in the most recent history. The main advantage over the state of the art models is that they are entirely data driven and do not require additional inputs from the user, which can require knowledge of the domain such as choice of basis functions or hyperparameters in graphical event models. We theoretically justify our learning of  optimal windows for parental history and the choices of parental sets, and the algorithm are sound and complete in terms of parent structure learning.  We present additional efficient heuristics for learning PGEMs from data, demonstrating their effectiveness on synthetic and real datasets.",https://proceedings.neurips.cc/paper_files/paper/2018/file/f1ababf130ee6a25f12da7478af8f1ac-Paper.pdf,2018,"event, models, datasets, learning, additional, data, events, graphical, history, parental","event, parent, set, pgem, windows, time, learning, optimal, search, theorem","{'event': 0.5631319061431288, 'graphical': 0.5330954110129674, 'proximal': 0.5330954110129674, 'models': 0.33837999615437814}","event, parental, pgems, history, events, graphical, models, additional, datasets, require"
Low-rank Interaction with Sparse Additive Effects Model for Large Data Frames,"Geneviève Robin, Hoi-To Wai, Julie Josse, Olga Klopp, Eric Moulines","Many applications of machine learning involve the analysis of large data frames -- matrices collecting heterogeneous measurements (binary, numerical, counts, etc.) across samples -- with missing values. Low-rank models, as studied by Udell et al. (2016), are popular in this framework for tasks such as visualization, clustering and missing value imputation. Yet, available methods with statistical guarantees and efficient optimization do not allow explicit modeling of main additive effects such as row and column, or covariate effects. In this paper, we introduce a low-rank interaction and sparse additive effects (LORIS) model which combines matrix regression on a dictionary and low-rank design, to estimate main effects and interactions simultaneously. We provide statistical guarantees in the form of upper bounds on the estimation error of both components. Then, we introduce a mixed coordinate gradient descent (MCGD) method which provably converges sub-linearly to an optimal solution and is computationally efficient for large scale data sets. We show on simulated and survey data that the method has a clear advantage over current practices.",https://proceedings.neurips.cc/paper_files/paper/2018/file/f21e255f89e0f258accbe4e984eef486-Paper.pdf,2018,"effects, data, low, rank, additive, efficient, guarantees, introduce, large, main","data, method, rank, low, mcgd, loris, matrix, ub, gradient, effects","{'frames': 0.4028809121688108, 'additive': 0.36422723234809956, 'effects': 0.36422723234809956, 'interaction': 0.36422723234809956, 'rank': 0.2984989799286534, 'low': 0.2905188938123226, 'large': 0.2773365619832837, 'sparse': 0.2717553606585003, 'model': 0.2518652139916114, 'data': 0.22802859667023287}","effects, rank, additive, missing, low, main, statistical, loris, mcgd, udell"
Generalized Cross Entropy Loss for Training Deep Neural Networks with Noisy Labels,"Zhilu Zhang, Mert Sabuncu","Deep neural networks (DNNs) have achieved tremendous success in a variety of applications across many disciplines. Yet, their superior performance comes with the expensive cost of requiring correctly annotated large-scale datasets. Moreover, due to DNNs' rich capacity, errors in training labels can hamper performance. To combat this problem, mean absolute error (MAE) has recently been proposed as a noise-robust alternative to the commonly-used categorical cross entropy (CCE) loss. However, as we show in this paper, MAE can perform poorly with DNNs and large-scale datasets. Here, we present a theoretically grounded set of noise-robust loss functions that can be seen as a generalization of MAE and CCE. Proposed loss functions can be readily applied with any existing DNN architecture and algorithm, while yielding good performance in a wide range of noisy label scenarios. We report results from experiments conducted with CIFAR-10, CIFAR-100 and FASHION-MNIST datasets and synthetically generated noisy labels.",https://proceedings.neurips.cc/paper_files/paper/2018/file/f2925f97bc13ad2852a7a551802feea0-Paper.pdf,2018,"datasets, dnns, loss, mae, performance, cce, cifar, functions, labels, large","loss, lq, noise, labels, 10, xi, cifar, training, mae, noisy","{'labels': 0.3890181384736215, 'cross': 0.37074936141801196, 'noisy': 0.36326974337646706, 'entropy': 0.35052648076803106, 'generalized': 0.35052648076803106, 'loss': 0.3399179857619621, 'training': 0.2959008136091981, 'deep': 0.2149655987534984, 'neural': 0.20546983182102083, 'networks': 0.20488454329095948}","mae, dnns, cce, loss, cifar, noisy, labels, datasets, noise, robust"
Inference Aided Reinforcement Learning for Incentive Mechanism Design in Crowdsourcing,"Zehong Hu, Yitao Liang, Jie Zhang, Zhao Li, Yang Liu","Incentive mechanisms for crowdsourcing are designed to incentivize financially self-interested workers to generate and report high-quality labels. Existing mechanisms are often developed as one-shot static solutions, assuming a certain level of knowledge about worker models (expertise levels, costs for exerting efforts, etc.). In this paper, we propose a novel inference aided reinforcement mechanism that acquires data sequentially and requires no such prior assumptions. Specifically, we first design a Gibbs sampling augmented Bayesian inference algorithm to estimate workers' labeling strategies from the collected labels at each step. Then we propose a reinforcement incentive learning (RIL) method, building on top of the above estimates, to uncover how workers respond to different payments. RIL dynamically determines the payment without accessing any ground-truth labels. We theoretically prove that RIL is able to incentivize rational workers to provide high-quality labels both at each step and in the long run. Empirical results show that our mechanism performs consistently well under both rational and non-fully rational (adaptive learning) worker models. Besides, the payments offered by RIL are more robust and have lower variances compared to existing one-shot mechanisms.",https://proceedings.neurips.cc/paper_files/paper/2018/file/f2e43fa3400d826df4195a9ac70dca62-Paper.pdf,2018,"labels, ril, workers, mechanisms, rational, existing, high, incentive, incentivize, inference","workers, inference, ril, algorithm, labels, bayesian, worker, label, data, mechanism","{'aided': 0.4288005070682531, 'crowdsourcing': 0.4288005070682531, 'incentive': 0.4288005070682531, 'mechanism': 0.40473486407327997, 'design': 0.3635943756397403, 'inference': 0.26241215758011077, 'reinforcement': 0.2524969666500855, 'learning': 0.14058569133242074}","ril, workers, rational, labels, mechanisms, incentive, payments, incentivize, worker, shot"
Fast and Effective Robustness Certification,"Gagandeep Singh, Timon Gehr, Matthew Mirman, Markus Püschel, Martin Vechev","We present a new method and system, called DeepZ, for certifying neural network
robustness based on abstract interpretation. Compared to state-of-the-art automated
verifiers for neural networks, DeepZ: (i) handles ReLU, Tanh and Sigmoid activation functions, (ii) supports feedforward and convolutional architectures, (iii)
is significantly more scalable and precise, and (iv) and is sound with respect to
floating point arithmetic. These benefits are due to carefully designed approximations tailored to the setting of neural networks. As an example, DeepZ achieves a
verification accuracy of 97% on a large network with 88,500 hidden units under
$L_{\infty}$ attack with $\epsilon = 0.1$ with an average runtime of 133 seconds.",https://proceedings.neurips.cc/paper_files/paper/2018/file/f2f446980d8e971ef3da97af089481c3-Paper.pdf,2018,"deepz, neural, network, networks, 133, 500, 88, 97, abstract, accuracy","ux, robustness, networks, deepz, lx, relu, zonotope, abstract, point, network","{'certification': 0.5554851632523162, 'effective': 0.5554851632523162, 'robustness': 0.46945026571306975, 'fast': 0.4030991377254774}","deepz, verifiers, neural, 133, 88, 97, iv, l_, 500, certifying"
Online Structured Laplace Approximations for Overcoming Catastrophic Forgetting,"Hippolyt Ritter, Aleksandar Botev, David Barber","We introduce the Kronecker factored online Laplace approximation for overcoming catastrophic forgetting in neural networks. The method is grounded in a Bayesian online learning framework, where we recursively approximate the posterior after every task with a Gaussian, leading to a quadratic penalty on changes to the weights. The Laplace approximation requires calculating the Hessian around a mode, which is typically intractable for modern architectures. In order to make our method scalable, we leverage recent block-diagonal Kronecker factored approximations to the curvature. Our algorithm achieves over 90% test accuracy across a sequence of 50 instantiations of the permuted MNIST dataset, substantially outperforming related methods for overcoming catastrophic forgetting.",https://proceedings.neurips.cc/paper_files/paper/2018/file/f31b20466ae89669f9741e047487eb37-Paper.pdf,2018,"approximation, catastrophic, factored, forgetting, kronecker, laplace, method, online, overcoming, 50","approximation, posterior, task, laplace, kronecker, hessian, approximate, diagonal, online, factored","{'catastrophic': 0.43038983091432553, 'laplace': 0.43038983091432553, 'forgetting': 0.40623499002976854, 'overcoming': 0.40623499002976854, 'approximations': 0.35575877249536403, 'structured': 0.3028993104528536, 'online': 0.28489148137822934}","overcoming, kronecker, laplace, catastrophic, factored, forgetting, online, instantiations, permuted, approximation"
Optimization over Continuous and Multi-dimensional Decisions with Observational Data,"Dimitris Bertsimas, Christopher McCord","We consider the optimization of an uncertain objective over continuous and multi-dimensional decision spaces in problems in which we are only provided with observational data. We propose a novel algorithmic framework that is tractable, asymptotically consistent, and superior to comparable methods on example problems. Our approach leverages predictive machine learning methods and incorporates information on the uncertainty of the predicted outcomes for the purpose of prescribing decisions. We demonstrate the efficacy of our method on examples involving both synthetic and real data sets.",https://proceedings.neurips.cc/paper_files/paper/2018/file/f337d999d9ad116a7b4f3d409fcc6480-Paper.pdf,2018,"data, methods, problems, algorithmic, approach, asymptotically, comparable, consider, consistent, continuous","decision, data, cost, training, predicted, set, approach, method, optimal, auxiliary","{'decisions': 0.4736891512919648, 'observational': 0.4736891512919648, 'dimensional': 0.4055557273103245, 'continuous': 0.37739004400136134, 'multi': 0.3071190629881352, 'data': 0.28404734552991473, 'optimization': 0.2633124935845991}","prescribing, uncertain, asymptotically, observational, problems, incorporates, involving, efficacy, outcomes, leverages"
Neural Architecture Search with Bayesian Optimisation and Optimal Transport,"Kirthevasan Kandasamy, Willie Neiswanger, Jeff Schneider, Barnabas Poczos, Eric P. Xing","Bayesian Optimisation (BO) refers to a class of methods for global optimisation
of a function f which is only accessible via point evaluations. It is
typically used in settings where f is expensive to evaluate. A common use case
for BO in machine learning is model selection, where it is not possible to
analytically model the generalisation performance of a statistical model, and
we resort to noisy and expensive training and validation procedures to choose
the best model. Conventional BO methods have focused on Euclidean and
categorical domains, which, in the context of model selection, only permits
tuning scalar hyper-parameters of machine learning algorithms. However, with
the surge of interest in deep learning, there is an increasing demand to tune
neural network architectures. In this work, we develop NASBOT, a Gaussian
process based BO framework for neural architecture search. To accomplish this,
we develop a distance metric in the space of neural network architectures which
can be computed efficiently via an optimal transport program. This distance
might be of independent interest to the deep learning community as it may find
applications outside of BO. We demonstrate that NASBOT outperforms other
alternatives for architecture search in several cross validation based model
selection tasks on multi-layer perceptrons and convolutional neural networks.",https://proceedings.neurips.cc/paper_files/paper/2018/file/f33ba15effa5c10e873bf3842afb46a6-Paper.pdf,2018,"model, bo, learning, neural, selection, architecture, architectures, based, deep, develop","layers, layer, neural, bo, network, architecture, 16, networks, number, nasbot","{'optimisation': 0.4727022417591653, 'architecture': 0.4527600068963928, 'transport': 0.4372916078039642, 'search': 0.37660376922391975, 'optimal': 0.32572473617629016, 'bayesian': 0.2865369597008046, 'neural': 0.2242912938789736}","bo, nasbot, optimisation, selection, model, validation, neural, expensive, interest, distance"
An Information-Theoretic Analysis for Thompson Sampling with Many Actions,"Shi Dong, Benjamin Van Roy","Information-theoretic Bayesian regret bounds of Russo and Van Roy capture the dependence of regret on prior uncertainty. However, this dependence is through entropy, which can become arbitrarily large as the number of actions increases.  We establish new bounds that depend instead on a notion of rate-distortion.  Among other things, this allows us to recover through information-theoretic arguments a near-optimal bound for the linear bandit.  We also offer a bound for the logistic bandit that dramatically improves on the best previously available, though this bound depends on an information-theoretic statistic that we have only been able to quantify via computation.",https://proceedings.neurips.cc/paper_files/paper/2018/file/f3e52c300b822a8123e7ace55fe15c08-Paper.pdf,2018,"bound, information, theoretic, bandit, bounds, dependence, regret, able, actions, allows","bound, θt, information, sampling, thompson, yα, action, bandit, regret, ht","{'actions': 0.45280527304064183, 'many': 0.4273924065266969, 'thompson': 0.4093616907894456, 'theoretic': 0.3953759811202356, 'analysis': 0.3150923755103594, 'information': 0.3150923755103594, 'sampling': 0.2997291193854931}","theoretic, dependence, bound, bandit, information, regret, bounds, roy, russo, things"
"BML: A High-performance, Low-cost Gradient Synchronization Algorithm for DML Training","Songtao Wang, Dan Li, Yang Cheng, Jinkun Geng, Yanshu Wang, Shuai Wang, Shu-Tao Xia, Jianping Wu","In distributed machine learning (DML), the network performance between machines significantly impacts the speed of iterative training. In this paper we propose BML, a new gradient synchronization algorithm with higher network performance and lower network cost than the current practice. BML runs on BCube network, instead of using the traditional Fat-Tree topology. BML algorithm is designed in such a way that, compared to the parameter server (PS) algorithm on a Fat-Tree network connecting the same number of server machines, BML achieves theoretically 1/k of the gradient synchronization time, with k/5 of switches (the typical number of k is 2∼4). Experiments of LeNet-5 and VGG-19 benchmarks on a testbed with 9 dual-GPU servers show that, BML reduces the job completion time of DML training by up to 56.4%.",https://proceedings.neurips.cc/paper_files/paper/2018/file/f410588e48dc83f2822a880a68f78923-Paper.pdf,2018,"bml, network, algorithm, dml, fat, gradient, machines, number, performance, server","gradient, server, network, algorithm, fat, synchronization, tree, bcube, servers, bml","{'bml': 0.3784843590177534, 'dml': 0.3784843590177534, 'cost': 0.3572426176644656, 'synchronization': 0.3572426176644656, 'performance': 0.3304811884818231, 'high': 0.2941681917904105, 'algorithm': 0.2846166242816402, 'low': 0.2729264504371226, 'training': 0.2441103037761238, 'gradient': 0.2177534216249395}","bml, dml, fat, synchronization, server, network, machines, tree, bcube, lenet"
Proximal SCOPE for Distributed Sparse Learning,"Shenyi Zhao, Gong-Duo Zhang, Ming-Wei Li, Wu-Jun Li","Distributed sparse learning with a cluster of multiple machines has attracted much attention in machine learning, especially for large-scale applications with high-dimensional data. One popular way to implement sparse learning is to use L1 regularization. In this paper, we propose a novel method, called proximal SCOPE (pSCOPE), for distributed sparse learning with L1 regularization. pSCOPE is based on a cooperative autonomous local learning (CALL) framework. In the CALL framework of pSCOPE, we find that the data partition affects the convergence of the learning procedure, and subsequently we define a metric to measure the goodness of a data partition. Based on the defined metric, we theoretically prove that pSCOPE is convergent with a linear convergence rate if the data partition is good enough. We also prove that better data partition implies faster convergence rate. Furthermore, pSCOPE is also communication efficient. Experimental results on real data sets show that pSCOPE can outperform other state-of-the-art distributed methods for sparse learning.",https://proceedings.neurips.cc/paper_files/paper/2018/file/f4334c131c781e2a6f0a5e34814c8147-Paper.pdf,2018,"learning, data, pscope, partition, sparse, convergence, distributed, also, based, call","10, partition, pscope, learning, data, proximal, sparse, wt, distributed, worker","{'scope': 0.6063819014593415, 'proximal': 0.5012332202837827, 'distributed': 0.417423279981938, 'sparse': 0.4090229329574767, 'learning': 0.1988071782166996}","pscope, partition, sparse, l1, distributed, data, learning, convergence, call, metric"
BinGAN: Learning Compact Binary Descriptors with a Regularized GAN,"Maciej Zieba, Piotr Semberecki, Tarek El-Gaaly, Tomasz Trzcinski","In this paper, we propose a novel regularization method for Generative Adversarial Networks that allows the model to learn discriminative yet compact binary representations of image patches (image descriptors). We exploit the dimensionality reduction that takes place in the intermediate layers of the discriminator network and train the binarized penultimate layer's low-dimensional representation to mimic the distribution of the higher-dimensional preceding layers. To achieve this, we introduce two loss terms that aim at: (i) reducing the correlation between the dimensions of the binarized penultimate layer's low-dimensional representation (i.e. maximizing joint entropy)  and (ii) propagating the relations between the dimensions in the high-dimensional space to the low-dimensional space. We evaluate the resulting binary image descriptors on two challenging applications, image matching and retrieval, where they achieve state-of-the-art results.",https://proceedings.neurips.cc/paper_files/paper/2018/file/f442d33fa06832082290ad8544a8da27-Paper.pdf,2018,"dimensional, image, low, achieve, binarized, binary, descriptors, dimensions, layer, layers","binary, dimensional, image, discriminator, descriptors, layers, high, vectors, low, space","{'bingan': 0.447017769522483, 'descriptors': 0.447017769522483, 'binary': 0.3903225066623012, 'gan': 0.3903225066623012, 'compact': 0.3790414053393966, 'regularized': 0.36124114811704633, 'learning': 0.14655836719006476}","dimensional, descriptors, penultimate, binarized, image, low, dimensions, binary, layers, layer"
Incorporating Context into Language Encoding Models for fMRI,"Shailee Jain, Alexander Huth","Language encoding models help explain language processing in the human brain by learning functions that predict brain responses from the language stimuli that elicited them. Current word embedding-based approaches treat each stimulus word independently and thus ignore the influence of context on language understanding. In this work we instead build encoding models using rich contextual representations derived from an LSTM language model. Our models show a significant improvement in encoding performance relative to state-of-the-art embeddings in nearly every brain area. By varying the amount of context used in the models and providing the models with distorted context, we show that this improvement is due to a combination of better word embeddings learned by the LSTM language model and contextual information. We are also able to use our models to map context sensitivity across the cortex. These results suggest that LSTM language models learn high-level representations that are related to representations in the human brain.",https://proceedings.neurips.cc/paper_files/paper/2018/file/f471223d1a1614b58a7dc45c9d01df19-Paper.pdf,2018,"language, models, brain, context, encoding, lstm, representations, word, contextual, embeddings","context, layer, lstm, representations, models, model, word, encoding, performance, contextual","{'fmri': 0.47999029244546826, 'incorporating': 0.47999029244546826, 'encoding': 0.41911312456553984, 'context': 0.4069999167866361, 'language': 0.36672904010046514, 'models': 0.25184063614873764}","language, brain, lstm, context, models, encoding, word, representations, contextual, embeddings"
Communication Efficient Parallel Algorithms for Optimization on Manifolds,"Bayan Saparbayeva, Michael Zhang, Lizhen Lin","The last decade has witnessed an explosion in the development of models, theory and computational algorithms for ``big data'' analysis. In particular, distributed inference has served as a natural and dominating paradigm for statistical inference. However, the existing literature on parallel inference almost exclusively focuses on Euclidean data and parameters. While this assumption is valid for many applications, it is increasingly more common to encounter problems where the data or the parameters lie on a non-Euclidean space, like a manifold for example. Our work aims to fill a critical gap in the literature by generalizing parallel inference algorithms to optimization on manifolds. We show that our proposed algorithm is both communication efficient and carries theoretical convergence guarantees. In addition, we demonstrate the performance of our algorithm to the estimation of Fr\'echet means on simulated spherical data and the low-rank matrix completion problem over Grassmann manifolds applied to the Netflix prize data set.",https://proceedings.neurips.cc/paper_files/paper/2018/file/f4a4da9aa7eadfd23c7bdb7cf57b3112-Paper.pdf,2018,"data, inference, algorithm, algorithms, euclidean, literature, manifolds, parallel, parameters, addition","ln, data, dr, algorithm, function, parallel, inference, loss, ls, manifold","{'manifolds': 0.5083707313216063, 'parallel': 0.4768114578835906, 'communication': 0.43705147553937557, 'algorithms': 0.36573221975714476, 'efficient': 0.319981994374208, 'optimization': 0.2950381460135801}","inference, manifolds, data, euclidean, parallel, literature, echet, encounter, fr, grassmann"
Memory Augmented Policy Optimization for Program Synthesis and Semantic Parsing,"Chen Liang, Mohammad Norouzi, Jonathan Berant, Quoc V. Le, Ni Lao","We present Memory Augmented Policy Optimization (MAPO), a simple and novel way to leverage a memory buffer of promising trajectories to reduce the variance of policy gradient estimate. MAPO is applicable to deterministic environments with discrete actions, such as structured prediction and combinatorial optimization tasks. We express the expected return objective as a weighted sum of two terms: an
expectation over the high-reward trajectories inside the memory buffer, and a separate expectation over trajectories outside the buffer. To make an efficient algorithm of MAPO, we propose: (1) memory weight clipping to accelerate and stabilize training; (2) systematic exploration to discover high-reward trajectories; (3) distributed sampling from inside and outside of the memory buffer to scale up training. MAPO improves the sample efficiency and robustness of policy gradient, especially on tasks with sparse rewards. We evaluate MAPO on weakly supervised program synthesis from natural language (semantic parsing). On the WikiTableQuestions benchmark, we improve the state-of-the-art by 2.6%, achieving an accuracy of 46.3%. On the WikiSQL benchmark, MAPO achieves an accuracy of 74.9% with only weak supervision, outperforming several strong baselines with full supervision. Our source code is available at https://goo.gl/TXBp4e",https://proceedings.neurips.cc/paper_files/paper/2018/file/f4e369c0a468d3aeeda0593ba90b5e55-Paper.pdf,2018,"mapo, memory, buffer, trajectories, policy, accuracy, benchmark, expectation, gradient, high","memory, policy, mapo, buffer, trajectories, program, training, gradient, samples, reward","{'augmented': 0.4326869279330083, 'parsing': 0.40027388422664395, 'memory': 0.3629774880141114, 'semantic': 0.3629774880141114, 'program': 0.3562921244770101, 'synthesis': 0.3349421983883814, 'policy': 0.3034421526880215, 'optimization': 0.24052033622631283}","mapo, buffer, trajectories, memory, inside, outside, policy, expectation, supervision, reward"
Context-dependent upper-confidence bounds for directed exploration,"Raksha Kumaraswamy, Matthew Schlegel, Adam White, Martha White","Directed exploration strategies for reinforcement learning are critical for learning an optimal policy in a minimal number of interactions with the environment. Many algorithms use optimism to direct exploration, either through visitation estimates or upper confidence bounds, as opposed to data-inefficient strategies like e-greedy that use random, undirected exploration. Most data-efficient exploration methods require significant computation, typically relying on a learned model to guide exploration. Least-squares methods have the potential to provide some of the data-efficiency benefits of model-based approaches—because they summarize past interactions—with the computation closer to that of model-free approaches. In this work, we provide a novel, computationally efficient, incremental exploration strategy, leveraging this property of least-squares temporal difference learning (LSTD). We derive upper confidence bounds on the action-values learned by LSTD, with context-dependent (or state-dependent) noise variance. Such context-dependent noise focuses exploration on a subset of variable states, and allows for reduced exploration in other states. We empirically demonstrate that our algorithm can converge more quickly than other incremental exploration strategies using confidence estimates on action-values.",https://proceedings.neurips.cc/paper_files/paper/2018/file/f516dfb84b9051ed85b89cdc3a8ab7f5-Paper.pdf,2018,"exploration, confidence, data, dependent, learning, model, strategies, action, approaches, bounds","conﬁdence, exploration, upper, variance, ucls, action, policy, value, state, estimates","{'directed': 0.4210858551315165, 'upper': 0.4210858551315165, 'confidence': 0.38954186028121945, 'context': 0.3782833212004916, 'dependent': 0.3782833212004916, 'exploration': 0.3467393263501946, 'bounds': 0.2953063522348121}","exploration, confidence, lstd, strategies, dependent, incremental, squares, interactions, states, estimates"
Adaptive Negative Curvature Descent with Applications in Non-convex Optimization,"Mingrui Liu, Zhe Li, Xiaoyu Wang, Jinfeng Yi, Tianbao Yang","Negative curvature descent (NCD) method has been utilized to design deterministic or stochastic algorithms for non-convex optimization aiming at finding second-order stationary points or local minima. In existing studies, NCD needs to approximate the smallest eigen-value of the Hessian matrix with a sufficient precision (e.g., $\epsilon_2\ll 1$) in order to achieve a sufficiently accurate second-order stationary solution (i.e., $\lambda_{\min}(\nabla^2 f(\x))\geq -\epsilon_2)$.  One issue  with this approach is that the target precision $\epsilon_2$ is usually set to be very small in order to find a high quality solution, which increases the complexity for computing a negative curvature. To address this issue, we propose an adaptive NCD to allow for an adaptive error dependent on the current gradient's magnitude in approximating the smallest eigen-value of the Hessian, and to encourage competition between  a noisy NCD step and gradient descent step. We consider the applications of the proposed adaptive NCD for both deterministic and stochastic non-convex optimization, and demonstrate that it can help reduce the the overall complexity in computing the negative curvatures during the course of optimization without sacrificing the iteration complexity.",https://proceedings.neurips.cc/paper_files/paper/2018/file/f52854cc99ae1c1966b0a21d0127975b-Paper.pdf,2018,"ncd, order, adaptive, complexity, epsilon_2, negative, optimization, computing, convex, curvature","ϵ2, adancg, negative, algorithm, curvature, complexity, stochastic, objective, time, ϵ1","{'curvature': 0.45770063720924536, 'negative': 0.45770063720924536, 'applications': 0.3996504662606256, 'adaptive': 0.31507355475799387, 'convex': 0.3057857201849069, 'descent': 0.30027325003431227, 'non': 0.28405899008023244, 'optimization': 0.2401457309754957}","ncd, epsilon_2, eigen, negative, order, adaptive, curvature, smallest, hessian, deterministic"
LF-Net: Learning Local Features from Images,"Yuki Ono, Eduard Trulls, Pascal Fua, Kwang Moo Yi","We present a novel deep architecture and a training strategy to learn a local feature pipeline from scratch, using collections of images without the need for human supervision. To do so we exploit depth and relative camera pose cues to create a virtual target that the network should achieve on one image, provided the outputs of the network for the other image. While this process is inherently non-differentiable, we show that we can optimize the network in a two-branch setup by confining it to one branch, while preserving differentiability in the other. We train our method on both indoor and outdoor datasets, with depth data from 3D sensors for the former, and depth estimates from an off-the-shelf Structure-from-Motion solution for the latter. Our models outperform the state of the art on sparse feature matching on both datasets, while running at 60+ fps for QVGA images.",https://proceedings.neurips.cc/paper_files/paper/2018/file/f5496252609c43eb8a3d147ab9b9c006-Paper.pdf,2018,"depth, network, branch, datasets, feature, image, images, one, 3d, 60","images, image, training, keypoints, network, scale, results, use, keypoint, lf","{'lf': 0.5380212157103562, 'images': 0.4347822724858433, 'net': 0.4347822724858433, 'features': 0.39310703800567526, 'local': 0.38316280087358684, 'learning': 0.17639457816711587}","depth, branch, feature, network, 60, confining, outdoor, qvga, images, fps"
Why so gloomy? A Bayesian explanation of human pessimism bias in the multi-armed bandit task,"Dalin Guo, Angela J. Yu","How humans make repeated choices among options with imperfectly known reward outcomes is an important problem in psychology and neuroscience. This is often studied using multi-armed bandits, which is also frequently studied in machine learning. We present data from a human stationary bandit experiment, in which we vary the average abundance and variability of reward availability (mean and variance of reward rate distributions). Surprisingly, we find subjects significantly underestimate prior mean of reward rates -- based on their self-report, at the end of a game, on their reward expectation of non-chosen arms. Previously, human learning in the bandit task was found to be well captured by a Bayesian ideal learning model, the Dynamic Belief Model (DBM), albeit under an incorrect generative assumption of the temporal structure - humans assume reward rates can change over time even though they are actually fixed. We find that the ""pessimism bias"" in the bandit task is well captured by the prior mean of DBM when fitted to human choices; but it is poorly captured by the prior mean of the Fixed Belief Model (FBM), an alternative Bayesian model that (correctly) assumes reward rates to be constants. This pessimism bias is also incompletely captured by a simple reinforcement learning model (RL) commonly used in neuroscience and psychology, in terms of fitted initial Q-values. While it seems sub-optimal, and thus mysterious, that humans have an underestimated prior reward expectation, our simulations show that an underestimated prior mean helps to maximize long-term gain, if the observer assumes volatility when reward rates are stable and utilizes a softmax decision policy instead of the optimal one (obtainable by dynamic programming). This raises the intriguing possibility that the brain underestimates reward rates to compensate for the incorrect non-stationarity assumption in the generative model and a simplified decision policy.",https://proceedings.neurips.cc/paper_files/paper/2018/file/f55cadb97eaff2ba1980e001b0bd9842-Paper.pdf,2018,"reward, model, mean, prior, rates, captured, learning, bandit, human, humans","reward, prior, dbm, human, mean, rates, model, fbm, policy, learning","{'explanation': 0.3717197026808181, 'gloomy': 0.3717197026808181, 'pessimism': 0.3717197026808181, 'armed': 0.3360557286962332, 'bias': 0.31519364128855615, 'human': 0.31519364128855615, 'bandit': 0.3003917547116483, 'task': 0.29433155388087906, 'multi': 0.22748053602460996, 'bayesian': 0.2126786494477021}","reward, captured, mean, rates, prior, humans, bandit, pessimism, underestimated, dbm"
CapProNet: Deep Feature Learning via Orthogonal Projections onto Capsule Subspaces,"Liheng Zhang, Marzieh Edraki, Guo-Jun Qi","In this paper, we formalize the idea behind capsule nets of using a capsule vector rather than a neuron activation to predict the label of samples. To this end, we propose to learn a group of capsule subspaces onto which an input feature vector is projected. Then the lengths of resultant capsules are used to score the probability of belonging to different classes.  We train such a Capsule Projection Network (CapProNet) by learning an orthogonal projection matrix for each capsule subspace, and show that each capsule subspace is updated until it contains input feature vectors corresponding to the associated class.  With low dimensionality of capsule subspace as well as an iterative method to estimate the matrix inverse, only a small negligible computing overhead is incurred to train the network. Experiment results on image datasets show the presented network can greatly improve the performance of state-of-the-art Resnet backbones by $10-20\%$ with almost the same computing cost.",https://proceedings.neurips.cc/paper_files/paper/2018/file/f5b1b89d98b7286673128a5fb112cb9a-Paper.pdf,2018,"capsule, network, subspace, computing, feature, input, matrix, projection, show, train","capsule, cappronet, projection, subspaces, feature, input, resnet, subspace, training, wl","{'cappronet': 0.3859052414433801, 'projections': 0.3859052414433801, 'capsule': 0.36424701665730946, 'onto': 0.36424701665730946, 'orthogonal': 0.36424701665730946, 'subspaces': 0.36424701665730946, 'feature': 0.2859215153675561, 'via': 0.20247622301501947, 'deep': 0.18081799822894878, 'learning': 0.1265221338660563}","capsule, subspace, projection, vector, computing, feature, matrix, network, backbones, belonging"
Robust Subspace Approximation in a Stream,"Roie Levin, Anish Prasad Sevekari, David Woodruff","We study robust subspace estimation in the streaming and distributed settings. Given a set of n data points {ai}{i=1}^n in R^d and an integer k, we wish to find a linear subspace S of dimension k for which sumi M(dist(S, ai)) is minimized, where dist(S,x) := min{y in S} |x-y|2, and M() is some loss function. When M is the identity function, S gives a subspace that is more robust to outliers than that provided by the truncated SVD. Though the problem is NP-hard, it is approximable within a (1+epsilon) factor in polynomial time when k and epsilon are constant.
    We give the first sublinear approximation algorithm for this problem in the turnstile streaming and arbitrary partition distributed models, achieving the same time guarantees as in the offline case. Our algorithm is the first based entirely on oblivious dimensionality reduction, and significantly simplifies prior methods for this problem, which held in neither the streaming nor distributed models.",https://proceedings.neurips.cc/paper_files/paper/2018/file/f5e536083a438cec5b64a4954abc17f1-Paper.pdf,2018,"distributed, problem, streaming, subspace, ai, algorithm, dist, epsilon, first, function","log, poly, matrix, algorithm, rank, time, approximation, probability, problem, subspace","{'stream': 0.596830123642494, 'subspace': 0.5633341262090308, 'approximation': 0.4250447661597396, 'robust': 0.3818185596869346}","streaming, subspace, dist, distributed, ai, epsilon, robust, approximable, minimized, simplifies"
PointCNN: Convolution On X-Transformed Points,"Yangyan Li, Rui Bu, Mingchao Sun, Wei Wu, Xinhan Di, Baoquan Chen","We present a simple and general framework for feature learning from point cloud. The key to the success of CNNs is the convolution operator that is capable of leveraging spatially-local correlation in data represented densely in grids (e.g. images). However, point cloud are irregular and unordered, thus a direct convolving of kernels against the features associated with the points will result in deserting the shape information while being variant to the orders. To address these problems, we propose to learn a X-transformation from the input points, which is used for simultaneously weighting the input features associated with the points and permuting them into latent potentially canonical order. Then element-wise product and sum operations of typical convolution operator are applied on the X-transformed features. The proposed method is a generalization of typical CNNs into learning features from point cloud, thus we call it PointCNN. Experiments show that PointCNN achieves on par or better performance than state-of-the-art methods on multiple challenging benchmark datasets and tasks.",https://proceedings.neurips.cc/paper_files/paper/2018/file/f5f8590cd58a54e94377e6ae2eded4d9-Paper.pdf,2018,"features, cloud, point, points, associated, cnns, convolution, input, learning, operator","pointcnn, conv, point, points, features, input, figure, local, representative, convolution","{'pointcnn': 0.5393220075667275, 'transformed': 0.5393220075667275, 'convolution': 0.45730927407411576, 'points': 0.45730927407411576}","cloud, pointcnn, features, points, point, operator, convolution, cnns, typical, associated"
Learning convex bounds for linear quadratic control policy synthesis,"Jack Umenberger, Thomas B. Schön","Learning to make decisions from observed data in dynamic environments remains a problem of fundamental importance in a numbers of fields, from artificial intelligence and robotics, to medicine and finance.
This paper concerns the problem of learning control policies for unknown linear dynamical systems so as to maximize a quadratic reward function.
We present a method to optimize the expected value of the reward over the posterior distribution of the unknown system parameters, given data.
The algorithm involves sequential convex programing, and enjoys reliable local convergence and robust stability guarantees.
Numerical simulations and stabilization of a real-world inverted pendulum are used to demonstrate the approach, with strong performance and robustness properties observed in both.",https://proceedings.neurips.cc/paper_files/paper/2018/file/f610a13de080fb8df6cf972fc01ad93f-Paper.pdf,2018,"data, learning, observed, problem, reward, unknown, algorithm, approach, artificial, concerns","control, jc, cost, problem, method, tr, cf, data, number, robust","{'quadratic': 0.4332365904402618, 'control': 0.40044636927039845, 'synthesis': 0.3829507554428508, 'convex': 0.35016053427298743, 'bounds': 0.3469356867071713, 'policy': 0.3469356867071713, 'linear': 0.3353032884649761, 'learning': 0.17183726169803348}","observed, reward, unknown, programing, stabilization, inverted, pendulum, finance, robotics, medicine"
Manifold Structured Prediction,"Alessandro Rudi, Carlo Ciliberto, GianMaria Marconi, Lorenzo Rosasco","Structured prediction provides a general framework to deal with supervised problems where the outputs have semantically rich structure. While classical approaches consider finite, albeit potentially huge, output spaces, in this paper we discuss how structured prediction can be extended to a continuous scenario. Specifically, we study a structured prediction approach to manifold-valued regression. We characterize a class of problems for which the considered approach is statistically consistent and study how geometric optimization can be used to compute the corresponding estimator. Promising experimental results on both simulated and real data complete our study.",https://proceedings.neurips.cc/paper_files/paper/2018/file/f6185f0ef02dcaec414a3171cd01c697-Paper.pdf,2018,"prediction, structured, study, approach, problems, albeit, approaches, characterize, class, classical","manifold, loss, structured, space, estimator, function, learning, prediction, eq, problem","{'manifold': 0.6363888878696737, 'structured': 0.5542254239663739, 'prediction': 0.5365103566803655}","structured, prediction, study, albeit, huge, semantically, deal, statistically, problems, considered"
Efficient Gradient Computation for Structured Output Learning with Rational and Tropical Losses,"Corinna Cortes, Vitaly Kuznetsov, Mehryar Mohri, Dmitry Storcheus, Scott Yang","Many structured prediction problems admit a natural loss function for evaluation such as the edit-distance or $n$-gram loss. However, existing learning algorithms are typically designed to optimize alternative objectives such as the cross-entropy. This is because a na\""{i}ve implementation of the natural loss functions often results in intractable gradient computations. In this paper, we design efficient gradient computation algorithms for two broad families of structured prediction loss functions: rational and tropical losses. These families include as special cases the $n$-gram loss, the edit-distance loss, and many other loss functions commonly used in natural language processing and computational biology tasks that are based on sequence similarity measures. Our algorithms make use of weighted automata and graph operations over appropriate semirings to design efficient solutions. They facilitate efficient gradient computation and hence enable one to train learning models such as neural networks with complex structured losses.",https://proceedings.neurips.cc/paper_files/paper/2018/file/f6d9e459b9fbf6dd26c4f7d621adec1d-Paper.pdf,2018,"loss, algorithms, efficient, functions, gradient, natural, structured, computation, design, distance","loss, yi, al, et, structured, gradient, efﬁcient, computation, function, prediction","{'rational': 0.4306478561555162, 'tropical': 0.4306478561555162, 'losses': 0.37602878937659984, 'output': 0.36516080507930515, 'computation': 0.34801239816024565, 'structured': 0.3030809031904656, 'gradient': 0.24776464854895033, 'efficient': 0.24505468722417775, 'learning': 0.14119135957274273}","loss, gram, structured, edit, families, natural, losses, functions, gradient, distance"
Mean-field theory of graph neural networks in graph partitioning,"Tatsuro Kawamoto, Masashi Tsubaki, Tomoyuki Obuchi","A theoretical performance analysis of the graph neural network (GNN) is presented. For classification tasks, the neural network approach has the advantage in terms of flexibility that it can be employed in a data-driven manner, whereas Bayesian inference requires the assumption of a specific model. A fundamental question is then whether GNN has a high accuracy in addition to this flexibility. Moreover, whether the achieved performance is predominately a result of the backpropagation or the architecture itself is a matter of considerable interest. To gain a better insight into these questions, a mean-field theory of a minimal GNN architecture is developed for the graph partitioning problem. This demonstrates a good agreement with numerical experiments.",https://proceedings.neurips.cc/paper_files/paper/2018/file/f6e794a75c5d51de081dbefa224304f9-Paper.pdf,2018,"gnn, architecture, flexibility, graph, network, neural, performance, whether, accuracy, achieved","gnn, xt, graph, model, group, matrix, algorithm, limit, mean, ﬁeld","{'graph': 0.5660209118963833, 'partitioning': 0.4351350588682549, 'field': 0.39338681332910586, 'mean': 0.3689656552178044, 'theory': 0.35163856778995684, 'neural': 0.19487860237775426, 'networks': 0.1943234833623971}","gnn, flexibility, whether, graph, architecture, predominately, matter, partitioning, considerable, agreement"
Adversarially Robust Generalization Requires More Data,"Ludwig Schmidt, Shibani Santurkar, Dimitris Tsipras, Kunal Talwar, Aleksander Madry","Machine learning models are often susceptible to adversarial perturbations of their inputs. Even small perturbations can cause state-of-the-art classifiers with high ""standard"" accuracy to produce an incorrect prediction with high confidence. To better understand this phenomenon, we study adversarially robust learning from the viewpoint of generalization. We show that already in a simple natural data model, the sample complexity of robust learning can be significantly larger than that of ""standard"" learning. This gap is information theoretic and holds irrespective of the training algorithm or the model family. We complement our theoretical results with experiments on popular image classification datasets and show that a similar gap exists here as well. We postulate that the difficulty of training robust classifiers stems, at least partially, from this inherently larger sample complexity.",https://proceedings.neurips.cc/paper_files/paper/2018/file/f708f064faaf32a43e4d3c784e6af9ea-Paper.pdf,2018,"learning, robust, classifiers, complexity, gap, high, larger, model, perturbations, sample","robust, model, generalization, adversarial, standard, test, sample, robustness, training, accuracy","{'requires': 0.5734451980681471, 'adversarially': 0.500715144685454, 'generalization': 0.4248719696022817, 'robust': 0.36685819115410945, 'data': 0.32456713592817943}","robust, perturbations, classifiers, gap, larger, postulate, standard, learning, incorrect, irrespective"
Assessing Generative Models via Precision and Recall,"Mehdi S. M. Sajjadi, Olivier Bachem, Mario Lucic, Olivier Bousquet, Sylvain Gelly","Recent advances in generative modeling have led to an increased interest in the study of statistical divergences as means of model comparison. Commonly used evaluation methods, such as the Frechet Inception Distance (FID), correlate well with the perceived quality of samples and are sensitive to mode dropping. However, these metrics are unable to distinguish between different failure cases since they only yield one-dimensional scores. We propose a novel definition of precision and recall for distributions which disentangles the divergence into two separate dimensions. The proposed notion is intuitive, retains desirable properties, and naturally leads to an efficient algorithm that can be used to evaluate generative models. We relate this notion to total variation as well as to recent evaluation metrics such as Inception Score and FID. To demonstrate the practical utility of the proposed approach we perform an empirical study on several variants of Generative Adversarial Networks and Variational Autoencoders. In an extensive set of experiments we show that the proposed metric is able to disentangle the quality of generated samples from the coverage of the target distribution.",https://proceedings.neurips.cc/paper_files/paper/2018/file/f7696a9b362ac5a51c3dc8f098b73923-Paper.pdf,2018,"generative, proposed, evaluation, fid, inception, metrics, notion, quality, recent, samples","recall, precision, prd, samples, data, models, set, figure, high, model","{'assessing': 0.4972581236278906, 'precision': 0.4972581236278906, 'recall': 0.4972581236278906, 'generative': 0.324644054378449, 'models': 0.2764139227815154, 'via': 0.2764139227815154}","fid, inception, generative, metrics, notion, evaluation, quality, proposed, frechet, dropping"
Improved Network Robustness with Adversary Critic,"Alexander Matyasko, Lap-Pui Chau","Ideally, what confuses neural network should be confusing to humans. However, recent experiments have shown that small, imperceptible perturbations can change the network prediction. To address this gap in perception, we propose a novel approach for learning robust classifier. Our main idea is: adversarial examples for the robust classifier should be indistinguishable from the regular data of the adversarial target. We formulate a problem of learning robust classifier in the framework of Generative Adversarial Networks (GAN), where the adversarial attack on classifier acts as a generator, and the critic network learns to distinguish between regular and adversarial images. The classifier cost is augmented with the objective that its adversarial examples should confuse the adversary critic. To improve the stability of the adversarial mapping, we introduce adversarial cycle-consistency constraint which ensures that the adversarial mapping of the adversarial examples is close to the original. In the experiments, we show the effectiveness of our defense. Our method surpasses in terms of robustness networks trained with adversarial training. Additionally, we verify in the experiments with human annotators on MTurk that adversarial examples are indeed visually confusing.",https://proceedings.neurips.cc/paper_files/paper/2018/file/f77ecc17109b1b806350eb7e7bbfd861-Paper.pdf,2018,"adversarial, classifier, examples, experiments, network, robust, confusing, critic, learning, mapping","adversarial, attack, classiﬁer, examples, robust, conﬁdence, training, images, network, input","{'adversary': 0.5444836120813762, 'critic': 0.5139254667224712, 'robustness': 0.4160041474833005, 'improved': 0.39782880997609127, 'network': 0.3287719544891367}","adversarial, classifier, examples, confusing, regular, robust, critic, mapping, network, experiments"
Fast deep reinforcement learning using online adjustments from the past,"Steven Hansen, Alexander Pritzel, Pablo Sprechmann, Andre Barreto, Charles Blundell","We propose Ephemeral Value Adjusments (EVA): a means of allowing deep reinforcement learning agents to rapidly adapt to experience in their replay buffer.
EVA shifts the value predicted by a neural network with an estimate of the value function found by prioritised sweeping over experience tuples from the replay buffer near the current state. EVA combines a number of recent ideas around combining episodic memory-like structures into reinforcement learning agents: slot-based storage, content-based retrieval, and memory-based planning.
We show that EVA is performant on a demonstration task and Atari games.",https://proceedings.neurips.cc/paper_files/paper/2018/file/f7bdb0e100275600f9e183e25d81822d-Paper.pdf,2018,"eva, based, value, agents, buffer, experience, learning, memory, reinforcement, replay","value, buffer, replay, eva, et, st, al, learning, trajectory, function","{'adjustments': 0.5083491395202382, 'past': 0.5083491395202382, 'online': 0.3364957278558423, 'fast': 0.33350106131948515, 'reinforcement': 0.2993387685234487, 'using': 0.290850895611256, 'deep': 0.23818975214137988, 'learning': 0.16666634959533386}","eva, buffer, replay, value, experience, agents, memory, adjusments, ephemeral, performant"
A Practical Algorithm for Distributed Clustering and Outlier Detection,"Jiecao Chen, Erfan Sadeqi Azer, Qin Zhang","We study the classic k-means/median clustering, which are fundamental problems in unsupervised learning, in the setting where data are partitioned across multiple sites, and where we are allowed to discard a small portion of the data by labeling them as outliers.  We propose a simple approach based on constructing small summary for the original dataset. The proposed method is time and communication efficient, has good approximation guarantees, and can identify the global outliers effectively.To the best of our knowledge, this is the first practical algorithm with theoretical guarantees for distributed clustering with outliers. Our experiments on both real and synthetic data have demonstrated the clear superiority of our algorithm against all the baseline algorithms in almost all metrics.",https://proceedings.neurips.cc/paper_files/paper/2018/file/f7f580e11d00a75814d2ded41fe8e8fe-Paper.pdf,2018,"data, outliers, algorithm, clustering, guarantees, small, across, algorithms, allowed, almost","means, algorithm, clustering, summary, outliers, points, time, running, median, set","{'outlier': 0.519865740598013, 'practical': 0.4201105857625185, 'algorithm': 0.39093407334631997, 'clustering': 0.379841678133991, 'detection': 0.3578669845989991, 'distributed': 0.3578669845989991}","outliers, clustering, discard, portion, guarantees, small, partitioned, summary, sites, data"
How Much Restricted Isometry is Needed In Nonconvex Matrix Recovery?,"Richard Zhang, Cedric Josz, Somayeh Sojoudi, Javad Lavaei","When the linear measurements of an instance of low-rank matrix recovery
satisfy a restricted isometry property (RIP) --- i.e. they
are approximately norm-preserving --- the problem is known
to contain no spurious local minima, so exact recovery is guaranteed.
In this paper, we show that moderate RIP is not enough to eliminate
spurious local minima, so existing results can only hold for near-perfect
RIP. In fact, counterexamples are ubiquitous: every $x$ is the spurious
local minimum of a rank-1 instance of matrix recovery that satisfies
RIP. One specific counterexample has RIP constant $\delta=1/2$, but
causes randomly initialized stochastic gradient descent (SGD) to fail
12\% of the time. SGD is frequently able to avoid and escape spurious
local minima, but this empirical result shows that it can occasionally
be defeated by their existence. Hence, while exact recovery guarantees
will likely require a proof of no spurious local minima, arguments
based solely on norm preservation will only be applicable to a narrow
set of nearly-isotropic instances.",https://proceedings.neurips.cc/paper_files/paper/2018/file/f8da71e562ff44a2bc7edf3578c593da-Paper.pdf,2018,"local, rip, spurious, minima, recovery, exact, instance, matrix, norm, rank","local, spurious, rip, minima, sgd, minimum, recovery, instance, problem, matrix","{'isometry': 0.4129337720947208, 'much': 0.4129337720947208, 'restricted': 0.4129337720947208, 'needed': 0.3897586205359159, 'recovery': 0.3897586205359159, 'matrix': 0.3059472564357132, 'nonconvex': 0.3017114702927535}","rip, spurious, recovery, minima, local, sgd, norm, instance, rank, exact"
Limited Memory Kelley's Method Converges for Composite Convex and Submodular Objectives,"Song Zhou, Swati Gupta, Madeleine Udell","The original simplicial method (OSM), a variant of the classic Kelley’s cutting plane method, has been shown to converge to the minimizer of a composite convex and submodular objective, though no rate of convergence for this method was known. Moreover, OSM is required to solve subproblems in each iteration whose size grows linearly in the number of iterations.  We propose a limited memory version of Kelley’s method (L-KM) and of OSM that requires limited memory (at most n+ 1 constraints for an n-dimensional problem) independent of the iteration. We prove convergence for L-KM when the convex part of the objective g is strongly convex and show it converges linearly when g is also smooth. Our analysis relies on duality between minimization of the composite convex and submodular objective and minimization of a convex function over the submodular base polytope.  We introduce a limited memory version, L-FCFW, of the Fully-Corrective Frank-Wolfe (FCFW) method with approximate correction, to solve the dual problem. We show that L-FCFW and L-KM are dual algorithms that produce the same sequence of iterates; hence both converge linearly (when g is smooth and strongly convex) and with limited memory.  We propose L-KM to minimize composite convex and submodular objectives; however, our results on L-FCFW hold for general polytopes and may be of independent interest.",https://proceedings.neurips.cc/paper_files/paper/2018/file/f8e59f4b2fe7c5705bf878bbd494ccdf-Paper.pdf,2018,"convex, method, fcfw, km, limited, memory, submodular, composite, linearly, objective","km, fcfw, convex, function, memory, osm, method, set, submodular, dual","{'converges': 0.39121841601553076, 'kelley': 0.39121841601553076, 'composite': 0.369261999039116, 'objectives': 0.3536836732767289, 'limited': 0.32337981397839133, 'memory': 0.30977083932389937, 'submodular': 0.28985810591346234, 'method': 0.2821090253395226, 'convex': 0.261369540187503}","fcfw, km, convex, osm, submodular, composite, limited, memory, linearly, kelley"
Towards Understanding Acceleration Tradeoff between Momentum and Asynchrony in Nonconvex Stochastic Optimization,"Tianyi Liu, Shiyang Li, Jianping Shi, Enlu Zhou, Tuo Zhao","Asynchronous momentum stochastic gradient descent algorithms (Async-MSGD) have been widely used in distributed machine learning, e.g., training large collaborative filtering systems and deep neural networks. Due to current technical limit, however, establishing convergence properties of Async-MSGD for these highly complicated nonoconvex problems is generally infeasible. Therefore, we propose to analyze the algorithm through a simpler but nontrivial nonconvex problems --- streaming PCA. This allows us to make progress toward understanding Aync-MSGD and gaining new insights for more general problems. Specifically, by exploiting the diffusion approximation of stochastic optimization, we establish the asymptotic rate of convergence of Async-MSGD for streaming PCA. Our results indicate a fundamental tradeoff between asynchrony and momentum: To ensure convergence and acceleration through asynchrony, we have to reduce the momentum (compared with Sync-MSGD). To the best of our knowledge, this is the first theoretical attempt on understanding Async-MSGD for distributed nonconvex stochastic optimization. Numerical experiments on both streaming PCA and training deep neural networks are provided to support our findings for Async-MSGD.",https://proceedings.neurips.cc/paper_files/paper/2018/file/f9d1152547c0bde01830b7e8bd60024c-Paper.pdf,2018,"msgd, async, convergence, momentum, pca, problems, stochastic, streaming, asynchrony, deep","msgd, async, λ1, momentum, al, et, optimization, λ2, delay, convergence","{'asynchrony': 0.4102161055953055, 'momentum': 0.4102161055953055, 'tradeoff': 0.4102161055953055, 'acceleration': 0.331501222243691, 'understanding': 0.3188309560253579, 'towards': 0.303933706889534, 'nonconvex': 0.2997257979871329, 'stochastic': 0.23470428575753727, 'optimization': 0.21523161325875448}","msgd, async, momentum, pca, streaming, asynchrony, convergence, nonconvex, stochastic, problems"
Metric on Nonlinear Dynamical Systems with Perron-Frobenius Operators,"Isao Ishikawa, Keisuke Fujii, Masahiro Ikeda, Yuka Hashimoto, Yoshinobu Kawahara","The development of a metric for structural data is a long-term problem in pattern recognition and machine learning. In this paper, we develop a general metric for comparing nonlinear dynamical systems that is defined with Perron-Frobenius operators in reproducing kernel Hilbert spaces. Our metric includes the existing fundamental metrics for dynamical systems, which are basically defined with principal angles between some appropriately-chosen subspaces, as its special cases. We also describe the estimation of our metric from finite data. We empirically illustrate our metric with an example of rotation dynamics in a unit disk in a complex plane, and evaluate the performance with real-world time-series data.",https://proceedings.neurips.cc/paper_files/paper/2018/file/fa1e9c965314ccd7810fb5ea838303e5-Paper.pdf,2018,"metric, data, defined, dynamical, systems, also, angles, appropriately, basically, cases","metric, dynamical, systems, operator, data, d1, d2, deﬁned, let, hk","{'frobenius': 0.41784732134110775, 'operators': 0.41784732134110775, 'perron': 0.41784732134110775, 'nonlinear': 0.37775771648463974, 'dynamical': 0.3376681116281718, 'metric': 0.3376681116281718, 'systems': 0.3247621415119419}","metric, dynamical, defined, systems, basically, disk, frobenius, perron, angles, plane"
A Mathematical Model For Optimal Decisions In A Representative Democracy,"Malik Magdon-Ismail, Lirong Xia","Direct democracy, where each voter casts one vote, fails when the average voter competence falls below 50%. This happens in noisy settings when voters have limited information. Representative democracy, where voters choose representatives to vote, can be an elixir in both these situations. We introduce a mathematical model for studying representative democracy, in particular understanding the parameters of a representative democracy that gives maximum decision making capability. Our main result states that under general and natural conditions,for fixed voting cost, the optimal number of representatives is linear;for polynomial cost, the optimal number of representatives is logarithmic.",https://proceedings.neurips.cc/paper_files/paper/2018/file/fa2431bf9d65058fe34e9713e32d60e6-Paper.pdf,2018,"democracy, representative, representatives, cost, number, optimal, vote, voter, voters, 50","group, representative, optimal, representatives, democracy, competence, cost, size, ln, voters","{'democracy': 0.46103293513531085, 'mathematical': 0.46103293513531085, 'representative': 0.46103293513531085, 'decisions': 0.43515830615749346, 'optimal': 0.2998543521616853, 'model': 0.2882195590750181}","democracy, representatives, representative, voter, voters, vote, elixir, happens, cost, competence"
Loss Functions for Multiset Prediction,"Sean Welleck, Zixin Yao, Yu Gai, Jialin Mao, Zheng Zhang, Kyunghyun Cho","We study the problem of multiset prediction. The goal of multiset prediction is to train a predictor that maps an input to a multiset consisting of multiple items. Unlike existing problems in supervised learning, such as classification, ranking and sequence generation, there is no known order among items in a target multiset, and each item in the multiset may appear more than once, making this problem extremely challenging. In this paper, we propose a novel multiset loss function by viewing this problem from the perspective of sequential decision making. The proposed multiset loss function is empirically evaluated on two families of datasets, one synthetic and the other real, with varying levels of difficulty, against various baseline loss functions including reinforcement learning, sequence, and aggregated distribution matching loss functions. The experiments reveal the effectiveness of the proposed loss function over the others.",https://proceedings.neurips.cc/paper_files/paper/2018/file/fb3f76858cb38e5b7fd113e0bc1c0721-Paper.pdf,2018,"multiset, loss, function, problem, functions, items, learning, making, prediction, proposed","multiset, loss, policy, prediction, function, oracle, set, multi, yt, step","{'multiset': 0.6283113803137068, 'loss': 0.4655229385070007, 'functions': 0.45307762583189154, 'prediction': 0.42805825311822704}","multiset, loss, items, function, sequence, making, prediction, viewing, ranking, functions"
SEGA: Variance Reduction via Gradient Sketching,"Filip Hanzely, Konstantin Mishchenko, Peter Richtarik","We propose a novel randomized first order optimization method---SEGA (SkEtched GrAdient method)---which progressively throughout its iterations builds a variance-reduced estimate of the gradient from random linear measurements (sketches) of the gradient provided  at each iteration by an oracle. In each iteration, SEGA updates the current estimate of the gradient  through a sketch-and-project operation using the information provided by the latest sketch, and this is subsequently used to compute an unbiased estimate of the true gradient through a random relaxation procedure. This unbiased estimate is then used to perform a gradient step. Unlike standard subspace descent methods, such as coordinate descent, SEGA can be used for optimization problems with  a non-separable proximal term. We provide a general convergence analysis and prove linear convergence for strongly convex objectives. In the special case of  coordinate sketches, SEGA can be enhanced with various techniques such as importance sampling, minibatching and acceleration, and its rate is up to a small constant factor identical to the best-known rate of coordinate descent.",https://proceedings.neurips.cc/paper_files/paper/2018/file/fc2c7c47b918d0c2d792a719dfb602ef-Paper.pdf,2018,"gradient, estimate, sega, coordinate, descent, used, convergence, iteration, linear, method","sega, gradient, hk, coordinate, cd, method, methods, sketches, xk, sampling","{'sega': 0.5245516112142871, 'sketching': 0.4742244564226511, 'variance': 0.4153454597804589, 'reduction': 0.40077541544368395, 'gradient': 0.3017902997556319, 'via': 0.2752210066332949}","sega, gradient, estimate, coordinate, sketches, sketch, descent, unbiased, iteration, provided"
Sharp Bounds for Generalized Uniformity Testing,"Ilias Diakonikolas, Daniel M. Kane, Alistair Stewart","We study the problem of generalized uniformity testing of a discrete probability distribution: Given samples from a probability distribution p over an unknown size discrete domain Ω, we want to distinguish, with probability at least 2/3, between the case that p is uniform on some subset of Ω versus ε-far, in total variation distance, from any such uniform distribution. We establish tight bounds on the sample complexity of generalized uniformity testing. In more detail, we present a computationally efficient tester whose sample complexity is optimal, within constant factors, and a matching worst-case information-theoretic lower bound. Specifically, we show that the sample complexity of generalized uniformity testing is Θ(1/(ε^(4/3) ||p||3) + 1/(ε^2 ||p||2 )).",https://proceedings.neurips.cc/paper_files/paper/2018/file/fc325d4b598aaede18b53dca4ecfcb9c-Paper.pdf,2018,"complexity, distribution, generalized, probability, sample, testing, uniformity, case, discrete, uniform","samples, probability, algorithm, distribution, sample, uniform, f2, pi, large, domain","{'sharp': 0.5251538719482687, 'uniformity': 0.5251538719482687, 'testing': 0.40816366975817003, 'generalized': 0.4012355632932123, 'bounds': 0.3476194226360273}","uniformity, generalized, testing, probability, uniform, complexity, sample, distribution, discrete, case"
Non-Local Recurrent Network for Image Restoration,"Ding Liu, Bihan Wen, Yuchen Fan, Chen Change Loy, Thomas S. Huang","Many classic methods have shown non-local self-similarity in natural images to be an effective prior for image restoration. However, it remains unclear and challenging to make use of this intrinsic property via deep networks. In this paper, we propose a non-local recurrent network (NLRN) as the first attempt to incorporate non-local operations into a recurrent neural network (RNN) for image restoration. The main contributions of this work are: (1) Unlike existing methods that measure self-similarity in an isolated manner, the proposed non-local module can be flexibly integrated into existing deep networks for end-to-end training to capture deep feature correlation between each location and its neighborhood. (2) We fully employ the RNN structure for its parameter efficiency and allow deep feature correlation to be propagated along adjacent recurrent states. This new design boosts robustness against inaccurate correlation estimation due to severely degraded images. (3) We show that it is essential to maintain a confined neighborhood for computing deep feature correlation given degraded images. This is in contrast to existing practice that deploys the whole image. Extensive experiments on both image denoising and super-resolution tasks are conducted. Thanks to the recurrent non-local operations and correlation propagation, the proposed NLRN achieves superior results to state-of-the-art methods with many fewer parameters.",https://proceedings.neurips.cc/paper_files/paper/2018/file/fc49306d97602c8ed1be1dfbf0835ead-Paper.pdf,2018,"correlation, deep, local, non, image, recurrent, existing, feature, images, methods","image, local, non, 30, nlrn, 27, 25, correlation, restoration, module","{'restoration': 0.5648992190995018, 'local': 0.4023045201213629, 'image': 0.37740413821023944, 'recurrent': 0.37060058621527764, 'non': 0.35058876617022794, 'network': 0.3410993760542657}","correlation, local, recurrent, degraded, nlrn, non, restoration, image, deep, feature"
Practical exact algorithm for trembling-hand equilibrium refinements in games,"Gabriele Farina, Nicola Gatti, Tuomas Sandholm","Nash equilibrium strategies have the known weakness that they do not prescribe rational play in situations that are reached with zero probability according to the strategies themselves, for example, if players have made mistakes. Trembling-hand refinements---such as extensive-form perfect equilibria and quasi-perfect equilibria---remedy this problem in sound ways. Despite their appeal, they have not received attention in practice since no known algorithm for computing them scales beyond toy instances. In this paper, we design an exact polynomial-time algorithm for finding trembling-hand equilibria in zero-sum extensive-form games. It is several orders of magnitude faster than the best prior ones, numerically stable, and quickly solves game instances with tens of thousands of nodes in the game tree. It enables, for the first time, the use of trembling-hand refinements in practice.",https://proceedings.neurips.cc/paper_files/paper/2018/file/fc79250f8c5b804390e8da280b4cf06e-Paper.pdf,2018,"equilibria, hand, trembling, algorithm, extensive, form, game, instances, known, perfect","algorithm, lp, player, polynomial, basis, qpe, tlp, solution, limit, matrix","{'equilibrium': 0.39191561453937734, 'refinements': 0.39191561453937734, 'trembling': 0.39191561453937734, 'hand': 0.3699200685729196, 'exact': 0.35431398035027784, 'practical': 0.31671234616117827, 'games': 0.2994369665022727, 'algorithm': 0.29471680019472046}","trembling, equilibria, refinements, hand, perfect, game, strategies, instances, zero, practice"
Thermostat-assisted continuously-tempered Hamiltonian Monte Carlo for Bayesian learning,"Rui Luo, Jianhong Wang, Yaodong Yang, Jun WANG, Zhanxing Zhu","In this paper, we propose a novel sampling method, the thermostat-assisted continuously-tempered Hamiltonian Monte Carlo, for the purpose of multimodal Bayesian learning. It simulates a noisy dynamical system by incorporating both a continuously-varying tempering variable and the Nos\'e-Hoover thermostats. A significant benefit is that it is not only able to efficiently generate i.i.d. samples when the underlying posterior distributions are multimodal, but also capable of adaptively neutralising the noise arising from the use of mini-batches. While the properties of the approach have been studied using synthetic datasets, our experiments on three real datasets have also shown its performance gains over several strong baselines for Bayesian learning with various types of neural networks plunged in.",https://proceedings.neurips.cc/paper_files/paper/2018/file/fcf1d8d2f36c0cde8eca4b86a8fe1df8-Paper.pdf,2018,"also, bayesian, continuously, datasets, learning, multimodal, able, adaptively, approach, arising","system, dynamics, tempering, 10, hamiltonian, hmc, sampling, noise, eq, distribution","{'assisted': 0.398525426857694, 'continuously': 0.398525426857694, 'thermostat': 0.398525426857694, 'tempered': 0.3761589172825317, 'hamiltonian': 0.34798044772565817, 'carlo': 0.30974464705879773, 'monte': 0.30974464705879773, 'bayesian': 0.22801548839998337, 'learning': 0.1306597630478526}","continuously, multimodal, hoover, neutralising, nos, plunged, simulates, thermostat, thermostats, bayesian"
Flexible neural representation for physics prediction,"Damian Mrowca, Chengxu Zhuang, Elias Wang, Nick Haber, Li F. Fei-Fei, Josh Tenenbaum, Daniel L. Yamins","Humans have a remarkable capacity to understand the physical dynamics of objects in their environment, flexibly capturing complex structures and interactions at multiple levels of detail.Inspired by this ability, we propose a hierarchical particle-based object representation that covers a wide variety of types of three-dimensional objects, including both arbitrary rigid geometrical shapes and deformable materials.We then describe the Hierarchical Relation Network (HRN), an end-to-end differentiable neural network based on hierarchical graph convolution, that learns to predict physical dynamics in this representation. 
Compared to other neural network baselines, the HRN accurately handles complex collisions and nonrigid deformations, generating plausible dynamics predictions at long time scales in novel settings, and scaling to large scene configurations.
These results demonstrate an architecture with the potential to form the basis of next-generation physics predictors for use in computer vision, robotics, and quantitative cognitive science.",https://proceedings.neurips.cc/paper_files/paper/2018/file/fd9dd764a6f1d73f4340d570804eacc4-Paper.pdf,2018,"dynamics, hierarchical, network, based, complex, end, hrn, neural, objects, physical","object, objects, graph, particle, particles, figure, hierarchical, pi, prediction, effects","{'physics': 0.547290418350498, 'flexible': 0.5285924183470547, 'representation': 0.4212583685580256, 'prediction': 0.41242983235104014, 'neural': 0.27112040416477856}","hrn, hierarchical, dynamics, physical, objects, network, end, complex, collisions, deformable"
A Stein variational Newton method,"Gianluca Detommaso, Tiangang Cui, Youssef Marzouk, Alessio Spantini, Robert Scheichl","Stein variational gradient descent (SVGD) was recently proposed as a general purpose nonparametric variational inference algorithm: it minimizes the Kullback–Leibler divergence between the target distribution and its approximation by implementing a form of functional gradient descent on a reproducing kernel Hilbert space [Liu & Wang, NIPS 2016]. In this paper, we accelerate and generalize the SVGD algorithm by including second-order information, thereby approximating a Newton-like iteration in function space. We also show how second-order information can lead to more effective choices of kernel. We observe significant computational gains over the original SVGD algorithm in multiple test cases.",https://proceedings.neurips.cc/paper_files/paper/2018/file/fdaa09fc5ed18d3226b3a1a00f1bc48c-Paper.pdf,2018,"algorithm, svgd, descent, gradient, information, kernel, order, second, space, variational","svn, svgd, algorithm, kernel, particles, target, distribution, newton, posterior, density","{'newton': 0.5856292401577717, 'stein': 0.5609228715011081, 'method': 0.44740941277776414, 'variational': 0.37713226149063245}","svgd, kernel, second, liu, wang, variational, descent, nips, order, algorithm"
Dual Swap Disentangling,"Zunlei Feng, Xinchao Wang, Chenglong Ke, An-Xiang Zeng, Dacheng Tao, Mingli Song","Learning interpretable disentangled representations is a crucial yet challenging task. In this paper, we propose a weakly semi-supervised method, termed as Dual Swap Disentangling (DSD), for disentangling using both labeled and unlabeled data. Unlike conventional weakly supervised methods that rely on full annotations on the group of samples, we require only limited annotations on paired samples that indicate their shared attribute like the color. Our model takes the form of a dual autoencoder structure. To achieve disentangling using the labeled pairs, we follow aencoding-swap-decoding'' process, where we first swap the parts of their encodings corresponding to the shared attribute, and then decode the obtained hybrid codes to reconstruct the original input pairs. For unlabeled pairs, we follow theencoding-swap-decoding'' process twice on designated encoding parts and enforce the final outputs to approximate the input pairs. By isolating parts of the encoding and swapping them back and forth, we impose the dimension-wise modularity and portability of the encodings of the unlabeled samples, which implicitly encourages disentangling under the guidance of labeled pairs. This dual swap mechanism, tailored for semi-supervised setting, turns out to be very effective. Experiments on image datasets from a wide domain show that our model yields state-of-the-art disentangling performances.",https://proceedings.neurips.cc/paper_files/paper/2018/file/fdf1bc5669e8ff5ba45d02fded729feb-Paper.pdf,2018,"disentangling, pairs, swap, dual, labeled, parts, samples, supervised, unlabeled, annotations","al, et, dsd, ia, ib, pairs, dual, samples, stage, swap","{'swap': 0.6434263813152499, 'disentangling': 0.5618206145941378, 'dual': 0.519961622472878}","swap, disentangling, pairs, parts, unlabeled, labeled, encodings, dual, annotations, attribute"
Algorithmic Regularization in Learning Deep Homogeneous Models: Layers are Automatically Balanced,"Simon S. Du, Wei Hu, Jason D. Lee","We study the implicit regularization imposed by gradient descent for learning multi-layer homogeneous functions including feed-forward fully connected and convolutional deep neural networks with linear, ReLU or Leaky ReLU activation. We rigorously prove that gradient flow (i.e. gradient descent with infinitesimal step size) effectively enforces the differences between squared norms across different layers to remain invariant without any explicit regularization. This result implies that if the weights are initially small, gradient flow automatically balances the magnitudes of all layers. Using a discretization argument, we analyze gradient descent with positive step size for the non-convex low-rank asymmetric matrix factorization problem without any regularization. Inspired by our findings for gradient flow, we prove that gradient descent with step sizes $\eta_t=O(t^{−(1/2+\delta)}) (0<\delta\le1/2)$ automatically balances two low-rank factors and converges to a bounded global optimum. Furthermore, for rank-1 asymmetric matrix factorization we give a finer analysis showing gradient descent with constant step size converges to the global minimum at a globally linear rate. We believe that the idea of examining the invariance imposed by first order algorithms in learning homogeneous models could serve as a fundamental building block for studying optimization for learning deep models.",https://proceedings.neurips.cc/paper_files/paper/2018/file/fe131d7f5a6b38b23cc967316c13dae2-Paper.pdf,2018,"gradient, descent, step, flow, learning, rank, regularization, size, asymmetric, automatically","gradient, descent, layer, step, theorem, al, et, layers, small, ph","{'automatically': 0.42752766521780544, 'balanced': 0.4035334581401342, 'homogeneous': 0.4035334581401342, 'algorithmic': 0.38650929729150135, 'layers': 0.3733043322495974, 'regularization': 0.3044725614388933, 'models': 0.22431461817921855, 'deep': 0.20032041110154725, 'learning': 0.14016837990542297}","gradient, descent, flow, step, asymmetric, balances, rank, homogeneous, regularization, imposed"
Third-order Smoothness Helps: Faster Stochastic Optimization Algorithms for Finding Local Minima,"Yaodong Yu, Pan Xu, Quanquan Gu","We propose stochastic optimization algorithms that can find local minima faster than existing algorithms for nonconvex optimization problems, by exploiting the third-order smoothness to escape non-degenerate saddle points more efficiently. More specifically, the proposed algorithm only needs $\tilde{O}(\epsilon^{-10/3})$ stochastic gradient evaluations to converge to an approximate local minimum $\mathbf{x}$, which satisfies $\|\nabla f(\mathbf{x})\|_2\leq\epsilon$ and $\lambda_{\min}(\nabla^2 f(\mathbf{x}))\geq -\sqrt{\epsilon}$ in unconstrained stochastic optimization, where $\tilde{O}(\cdot)$ hides logarithm polynomial terms and constants. This improves upon the $\tilde{O}(\epsilon^{-7/2})$ gradient complexity achieved by the state-of-the-art stochastic local minima finding algorithms by a factor of $\tilde{O}(\epsilon^{-1/6})$. Experiments on two nonconvex optimization problems demonstrate the effectiveness of our algorithm and corroborate our theory.",https://proceedings.neurips.cc/paper_files/paper/2018/file/fea9c11c4ad9a395a636ed944a28b51a-Paper.pdf,2018,"epsilon, optimization, stochastic, tilde, algorithms, local, mathbf, algorithm, gradient, minima","stochastic, order, algorithm, curvature, negative, gradient, optimization, third, function, step","{'helps': 0.37837543984884153, 'smoothness': 0.35713981138439393, 'third': 0.35713981138439393, 'finding': 0.34207289320992396, 'minima': 0.31276385349776814, 'faster': 0.29408353689198014, 'order': 0.28453471810655884, 'local': 0.2694677999320889, 'algorithms': 0.2460941805740362, 'stochastic': 0.2164867155301975}","tilde, epsilon, mathbf, nabla, stochastic, optimization, local, minima, nonconvex, algorithms"
Robot Learning in Homes: Improving Generalization and Reducing Dataset Bias,"Abhinav Gupta, Adithyavairavan Murali, Dhiraj Prakashchand Gandhi, Lerrel Pinto","Data-driven approaches to solving robotic tasks have gained a lot of traction in recent years. However, most existing policies are trained on large-scale datasets collected in curated lab settings. If we aim to deploy these models in unstructured visual environments like people's homes, they will be unable to cope with the mismatch in data distribution. In such light, we present the first systematic effort in collecting a large dataset for robotic grasping in homes. First, to scale and parallelize data collection, we built a low cost mobile manipulator assembled for under 3K USD. Second, data collected using low cost robots suffer from noisy labels due to imperfect execution and calibration errors. To handle this, we develop a framework which factors out the noise as a latent variable. Our model is trained on 28K grasps collected in several houses under an array of different environmental conditions. We evaluate our models by physically executing grasps on a collection of novel objects in multiple unseen homes. The models trained with our home dataset showed a marked improvement of 43.7% over a baseline model trained with data collected in lab. Our architecture which explicitly models the latent noise in the dataset also performed 10% better than one that did not factor out the noise. We hope this effort inspires the robotics community to look outside the lab and embrace learning based approaches to handle inaccurate cheap robots.",https://proceedings.neurips.cc/paper_files/paper/2018/file/febefe1cc5c87748ea02036dbe9e3d67-Paper.pdf,2018,"data, collected, models, trained, dataset, homes, lab, noise, approaches, collection","data, grasp, robot, model, lab, noise, home, lca, homes, grasping","{'dataset': 0.41661892378641097, 'homes': 0.41661892378641097, 'reducing': 0.3932369498392631, 'robot': 0.3932369498392631, 'bias': 0.3532652013625282, 'improving': 0.32380739799650915, 'generalization': 0.3086776265962904, 'learning': 0.1365918613835941}","homes, lab, collected, grasps, trained, robotic, robots, noise, effort, dataset"
LAG: Lazily Aggregated Gradient for Communication-Efficient Distributed Learning,"Tianyi Chen, Georgios Giannakis, Tao Sun, Wotao Yin","This paper presents a new class of gradient methods for distributed 
machine learning that adaptively skip the gradient calculations to 
learn with reduced communication and computation. Simple rules 
are designed to detect slowly-varying gradients and, therefore, 
trigger the reuse of outdated gradients. The resultant gradient-based 
algorithms are termed Lazily Aggregated Gradient --- justifying our 
acronym LAG used henceforth. Theoretically, the merits of 
this contribution are: i) the convergence rate is the same as batch 
gradient descent in strongly-convex, convex, and nonconvex cases; 
and, ii) if the distributed datasets are heterogeneous (quantified by 
certain measurable constants), the communication rounds needed 
to achieve a targeted accuracy are reduced thanks to the adaptive 
reuse of lagged gradients. Numerical experiments on both 
synthetic and real data corroborate a significant communication 
reduction compared to alternatives.",https://proceedings.neurips.cc/paper_files/paper/2018/file/feecee9f1643651799ede2740927317a-Paper.pdf,2018,"gradient, communication, gradients, convex, distributed, reduced, reuse, accuracy, achieve, acronym","lag, θk, communication, lm, gd, server, iteration, worker, workers, wk","{'aggregated': 0.4545336075348479, 'lag': 0.4545336075348479, 'lazily': 0.4545336075348479, 'communication': 0.35327570677821973, 'distributed': 0.31289342386802904, 'gradient': 0.26150683885882287, 'efficient': 0.25864657035957844, 'learning': 0.14902249506653348}","gradient, communication, reuse, gradients, reduced, distributed, acronym, henceforth, lagged, lazily"
Equality of Opportunity in Classification: A Causal Approach,"Junzhe Zhang, Elias Bareinboim","The Equalized Odds (for short, EO)  is one of the most popular measures of discrimination used in the supervised learning setting. It ascertains fairness through the balance of the misclassification rates (false positive and negative) across the protected groups -- e.g., in the context of law enforcement, an African-American defendant who would not commit a future crime will have an equal opportunity of being released, compared to a non-recidivating Caucasian defendant. Despite this noble goal, it has been acknowledged in the literature that statistical tests based on the EO are oblivious to the underlying causal mechanisms that generated the disparity in the first place (Hardt et al. 2016). This leads to a critical disconnect between statistical measures readable from the data and the meaning of discrimination in the legal system, where compelling evidence that the observed disparity is tied to a specific causal process deemed unfair by society is required to characterize discrimination. The goal of this paper is to develop a principled approach to connect the statistical disparities characterized by the EO  and the underlying, elusive, and frequently unobserved, causal mechanisms that generated such inequality. We start by introducing a new family of counterfactual measures that allows one to explain the misclassification disparities in terms of the underlying mechanisms in an arbitrary, non-parametric structural causal model. This will, in turn, allow legal and data analysts to interpret currently deployed classifiers through causal lens, linking the statistical disparities found in the data to the corresponding causal processes. Leveraging the new family of counterfactual measures, we develop a learning procedure to construct a classifier that is statistically efficient, interpretable, and compatible with the basic human intuition of fairness. We demonstrate our results through experiments in both real (COMPAS) and synthetic datasets.",https://proceedings.neurips.cc/paper_files/paper/2018/file/ff1418e8cc993fe8abcfe3ce2003e5c5-Paper.pdf,2018,"causal, measures, statistical, data, discrimination, disparities, eo, mechanisms, underlying, counterfactual","x0, x1, counterfactual, classiﬁer, causal, set, erd, ers, er, prediction","{'equality': 0.5355392266663475, 'opportunity': 0.5355392266663475, 'classification': 0.3813952016673282, 'causal': 0.37690124353793564, 'approach': 0.37266422711056746}","causal, disparities, eo, measures, discrimination, mechanisms, statistical, defendant, legal, counterfactual"
Modern Neural Networks Generalize on Small Data Sets,"Matthew Olson, Abraham Wyner, Richard Berk","In this paper,  we  use a linear program to empirically decompose fitted neural networks into ensembles of low-bias sub-networks. We show that these sub-networks are relatively uncorrelated which leads to an  internal regularization process, very much like a random forest, which can explain why a neural network is surprisingly resistant to overfitting.  We then demonstrate this in practice by applying large neural networks, with hundreds of parameters per training observation, to a  collection of 116 real-world data sets from the UCI Machine Learning Repository.   This collection of data sets contains a much smaller number of training examples than the types of image classification tasks generally studied in the deep learning literature, as well as non-trivial label noise. We show  that even in this setting deep neural nets are capable of achieving superior classification accuracy without overfitting.",https://proceedings.neurips.cc/paper_files/paper/2018/file/fface8385abbf94b4593a0ed53a0c70f-Paper.pdf,2018,"networks, neural, classification, collection, data, deep, learning, much, overfitting, sets","data, network, neural, networks, training, sets, sub, random, forest, accuracy","{'generalize': 0.4846909117831728, 'modern': 0.4846909117831728, 'small': 0.4381881196423896, 'sets': 0.4109857300612009, 'data': 0.2743326504046988, 'neural': 0.21707257447647385, 'networks': 0.21645423509834735}","overfitting, collection, networks, neural, sub, sets, much, 116, uncorrelated, forest"
